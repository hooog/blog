---
title: MLlib基本统计工具
date: 2018-03-13 11:59:21
update: 
comments: true
categories:
  - Machine learning
tags:
  - MLlib
---

<!--more-->
本文参考- Machine learning with Scala by CDA 吴昊天

## 一、概述

​	给定一个数据集，数据分析师一般会先观察一下数据集的基本情况，称之为汇总统计或者概要性统计。一般的概要性统计用于概括一系列观测值，包括位置或集中趋势（比如算术平均值、中位数、众数和四分位均值），展型（比如四分位间距、绝对偏差和绝对距离偏差、各阶矩等），统计离差，分布的形状，依赖性等。除此之外，spark.mllib库也提供了一些其他的基本的统计分析工具，包括相关性、分层抽样、假设检验，随机数生成等。接下来，我们将从以下几个方面进行介绍：

- 概括统计 summary statistics 
- 相关性 correlations 
- 分层抽样 Stratified sampling
- 假设检验 hypothesis testing 
- 随机数生成 random data generation 
- 核密度估计 Kernel density estimation

## 二、摘要统计（Summary statistics）

### 1.基本概念

 	对于RDD[Vector]类型的变量，Spark MLlib提供了一种叫colStats()的统计方法，调用该方法会返回一个类型为MultivariateStatisticalSummary的实例。通过这个实例看，我们可以获得每一列的最大值，最小值，均值、方差、总数等。

### 2.基本操作

- 首先导入基本类

  ```scala
  import org.apache.spark.mllib.linalg.{Vector, Vectors}
  import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
  ```

- 输入数据

  ```scala
  val observations = sc.parallelize(
    Seq(
      Vectors.dense(1.0, 10.0, 100.0),
      Vectors.dense(2.0, 20.0, 200.0),
      Vectors.dense(3.0, 30.0, 300.0)
    )
  )
  ```

- 使用colStats()方法

  接下来，我们调用colStats()方法（以列为基础的统计），得到一个MultivariateStatisticalSummary类型的变量

  ```scala
  val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)
  ```

  最后，依次调用统计方法，得到相应统计结果

  ```scala
  println(summary.count)
  println(summary.mean)
  println(summary.variance)
  println(summary.max)
  println(summary.min)
  println(summary.normL1)
  println(summary.normL2)
  println(summary.numNonzeros)
  ```

  其中，主要方法的含义与返回值类型如下

  | 方法名         | 方法含义                 | 返回值类型  |
  | ----------- | -------------------- | ------ |
  | count       | 列的大小                 | long   |
  | mean        | 每列的均值                | vector |
  | variance    | 每列的方差                | vector |
  | max         | 每列的最大值               | vector |
  | min         | 每列的最小值               | vector |
  | normL1      | 每列的L1范数（各元素绝对值之和）    | vector |
  | normL2      | 每列的L2范数（各元素平方和后求平方根） | vector |
  | numNonzeros | 每列非零向量的个数            | vector |

## 三、相关性（Correlations）

### 1.基本概念

​	Correlations，相关度量，目前Spark支持两种相关性系数：皮尔逊相关系数（pearson）和斯皮尔曼等级相关系数（spearman）。相关系数是用以反映变量之间相关关系密切程度的统计指标。简单的来说就是相关系数绝对值越大（值越接近1或者-1）,当取值为0表示不相关，取值为(0~-1]表示负相关，取值为(0, 1]表示正相关。

​	Pearson相关系数表达的是两个数值变量的线性相关性, 它一般适用于正态分布。其取值范围是[-1, 1], 当取值为0表示不相关，取值为[-1~0)表示负相关，取值为(0, 1]表示正相关。

![pearson](http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/12/Pearson.png)

​	Spearman相关系数也用来表达两个变量的相关性，但是它没有Pearson相关系数对变量的分布要求那么严格，另外Spearman相关系数可以更好地用于测度变量的排序关系。其计算公式为：

![spearman](http://dblab.xmu.edu.cn/blog/wp-content/uploads/2016/12/Spearman.gif)

​	根据输入类型的不同，输出的结果也产生相应的变化。如果输入的是两个RDD[Double]，则输出的是一个double类型的结果；如果输入的是一个RDD[Vector]，则对应的输出的是一个相关系数矩阵。

### 2.基本操作

- 导入必要基本类

  ```scala
  import org.apache.spark.mllib.linalg._
  import org.apache.spark.mllib.stat.Statistics
  import org.apache.spark.rdd.RDD
  ```

- 输入数据

  ```scala
  val seriesX: RDD[Double] = sc.parallelize(Array(1, 2, 3, 3, 5))
  val seriesY: RDD[Double] = sc.parallelize(Array(11, 22, 33, 33, 555))
  ```

- 调用方法

  然后，我们调用Statistics包中的corr()函数来获取相关性，这里用的是”pearson”，当然根据不同需要也可以选择”spearman”

  ```scala
  val correlation: Double = Statistics.corr(seriesX, seriesY, "pearson")
  println(s"Correlation is: $correlation")
  ```




- 求相关系数矩阵

  接下进行来求相关系数矩阵相关操作。方法是类似的，首先创建一个RDD[Vector]

  ```scala
   val data: RDD[Vector] = sc.parallelize(
    Seq(
      Vectors.dense(1.0, 10.0, 100.0),
      Vectors.dense(2.0, 20.0, 200.0),
      Vectors.dense(5.0, 33.0, 366.0))
  )
  ```

  然后，我们调用Statistics包中的corr()函数，这里也同样可以选择”pearson”或者”spearman”，得到相关系数矩阵

  ```scala
  val correlMatrix: Matrix = Statistics.corr(data, "pearson")
  println(correlMatrix.toString)
  ```

  相关矩阵也叫相关系数矩阵，是由矩阵各列间的相关系数构成的。也就是说，相关矩阵第i行第j列的元素是原矩阵第i列和第j列的相关系数。可以看到，输入两个RDD[Double]或一个RDD[Vector]，求相关性得到结果是一致的。

## 四、分层抽样（Stratified sampling）

### 1.概述

​	分层取样（Stratified sampling）顾名思义，就是将数据根据不同的特征分成不同的组，然后按特定条件从不同的组中获取样本，并重新组成新的数组。分层取样算法是直接集成到键值对类型 RDD[(K, V)] 的 sampleByKey 和 sampleByKeyExact 方法，无需通过额外的 spark.mllib 库来支持。

### 2.sampleByKey 方法

#### 2.1 基本概念

​	sampleByKey 方法需要作用于一个键值对数组，其中 key 用于分类，value可以是任意数。然后通过 fractions 参数来定义分类条件和采样几率。fractions 参数被定义成一个 Map[K, Double] 类型，Key是键值的分层条件，Double 是该满足条件的 Key 条件的采样比例，1.0 代表 100%。

#### 2.2 基本操作

- 导入基本类

  ```scala
  import org.apache.spark.SparkContext
  import org.apache.spark.SparkContext._
  import org.apache.spark.rdd.PairRDDFunctions
  ```

  ​

- 输入数据

  创建数据集，分成 “female” 和 “male” 两类

  ```scala
  val data = sc.makeRDD(Array(
       |     ("female","Lily"),
       |     ("female","Lucy"),
       |     ("female","Emily"),
       |     ("female","Kate"),
       |     ("female","Alice"),
       |     ("male","Tom"),
       |     ("male","Roy"),
       |     ("male","David"),
       |     ("male","Frank"),
       |     ("male","Jack")))
  ```

  ​

- 进行采样

  然后，我们通过 fractions 参数来定义分类条件和采样几率

  ```scala
  val fractions : Map[String, Double]= Map("female"->0.6,"male"->0.4)
  ```

  这里，设置采取60%的female和40%的male，因为数据中female和male各有5个样本，所以理想中的抽样结果应该是有3个female和2个male。接下来用sampleByKey进行抽样

  ```scala
  val approxSample = data.sampleByKey(withReplacement = false, fractions, 1)
  approxSample.collect().foreach {println}
  ```

  从上面可以看到，本应该抽取3个female和2个male，但结果抽取了5个female和1个male，结果并不够准确，不过在样本数量足够大且要求一定效率的情况下，用sampleByKey进行抽样还是可行的。

### 3.sampleByKeyExact 方法

​	sampleByKey 和 sampleByKeyExact 的区别在于 sampleByKey 每次都通过给定的概率以一种类似于掷硬币的方式来决定这个观察值是否被放入样本，因此一遍就可以过滤完所有数据，最后得到一个近似大小的样本，但往往不够准确。而 sampleByKeyExtra 会对全量数据做采样计算。对于每个类别，其都会产生 （fk⋅nk）个样本，其中fk是键为k的样本类别采样的比例；nk是键k所拥有的样本数。 sampleByKeyExtra 采样的结果会更准确，有99.99%的置信度，但耗费的计算资源也更多。下面是sampleByKeyExact的例子:

```scala
val exactSample = data.sampleByKeyExact(withReplacement = false, fractions, 1)
exactSample.collect().foreach {println}
```

 从实验结果可以看出，所得结果和预想一致，但当样本数量比较大时，可能会耗时较久。其中，sampleByKeyExact抽样方法中所涉及到的参数解释如下：

| 参数              | 含义          |
| --------------- | ----------- |
| withReplacement | 每次抽样是否有放回   |
| fractions       | 控制不同key的抽样率 |
| seed            | 随机数种子       |

## 六、假设检验 Hypothesis testing

​	Spark目前支持皮尔森卡方检测（Pearson’s chi-squared tests），包括“适配度检定”（Goodness of fit）以及“独立性检定”（independence）。在进行假设检验时，我们需要导入下列必要的包

```scala
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.mllib.stat.test.ChiSqTestResult
import org.apache.spark.rdd.RDD
```

​	接下来，我们从数据集中选择要分析的数据，比如说我们取出iris数据集中的前两条数据v1和v2。不同的输入类型决定了是做拟合度检验还是独立性检验。拟合度检验要求输入为Vector, 独立性检验要求输入是Matrix。

输入数据：

```scala
val vec: Vector = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)
```

###1.适合度检验 Goodness fo fit

​	Goodness fo fit（适合度检验）：验证一组观察值的次数分配是否异于理论上的分配。其 H0假设（虚无假设，null hypothesis）为一个样本中已发生事件的次数分配会服从某个特定的理论分配。实际执行多项式试验而得到的观察次数，与虚无假设的期望次数相比较，检验二者接近的程度，利用样本数据以检验总体分布是否为某一特定分布的统计方法。

通常情况下这个特定的理论分配指的是均匀分配，目前Spark默认的是均匀分配。

```scala
val goodnessOfFitTestResult = Statistics.chiSqTest(vec)
println(s"$goodnessOfFitTestResult\n")
```

​	可以看到P值，自由度，检验统计量，所使用的方法，以及零假设等信息。我们先简单介绍下每个输出的意义：

`method`: 方法。这里采用pearson方法;

`statistic`： 检验统计量。简单来说就是用来决定是否可以拒绝原假设的证据。检验统计量的值是利用样本数据计算得到的，它代表了样本中的信息。检验统计量的绝对值越大，拒绝原假设的理由越充分，反之，不拒绝原假设的理由越充分;计学根据显著性检验方法所得到的P 值。一般以P < 0.05 为显著， P<0.01 为非常显著，其含义是样本间的差异由抽样误差所致的概率小于0.05 或0.01

`degrees of freedom`：自由度。表示可自由变动的样本观测值的数目，

`pValue`

​	一般来说，假设检验主要看P值就够了。在本例中pValue =0.133，说明两组的差别无显著意义。通过V1的观测值[5.1, 3.5, 1.4, 0.2]，无法拒绝其服从于期望分配（这里默认是均匀分配）的假设。

### 2.独立性检验 Indenpendence

​	卡方独立性检验是用来检验两个属性间是否独立。其中一个属性做为行，另外一个做为列，通过貌似相关的关系考察其是否真实存在相关性。比如天气温变化和肺炎发病率。

首先，构造一个矩阵

```scala
val mat: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))
```

然后进行皮尔森相独立性检验

```scala
val independenceTestResult = Statistics.chiSqTest(mat)
println(s"$independenceTestResult\n")
```

接下来进行

```scala
val obs: RDD[LabeledPoint] =
  sc.parallelize(
    Seq(
      LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0)),
      LabeledPoint(1.0, Vectors.dense(1.0, 2.0, 0.0)),
      LabeledPoint(-1.0, Vectors.dense(-1.0, 0.0, -0.5)
      )
    )
  )
```

 进行独立性检验，返回一个包含每个特征对于标签的卡方检验的数组：

```scala
val featureTestResults= Statistics.chiSqTest(obs)
```

​	这里实际上是把特征数据中的每一列都与标签进行独立性检验。可以看出，P值都非常小，说明可以拒绝“某列与标签列无关”的假设。也就是说，可以认为每一列的数据都与最后的标签有相关性。

```scala
featureTestResults.zipWithIndex.foreach { case (k, v) =>
  println("Column " + (v + 1).toString + ":")
  println(k)
}
```

同时，spark也支持Kolmogorov-Smirnov 检验(符合理论分布或二者分布一致)

```scala
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.rdd.RDD

val data: RDD[Double] = sc.parallelize(Seq(0.1, 0.15, 0.2, 0.3, 0.25))

val testResult = Statistics.kolmogorovSmirnovTest(data, "norm", 0, 1)

println(testResult)
println()
```

对比分布

```scala
val myCDF = Map(0.1 -> 0.2, 0.15 -> 0.6, 0.2 -> 0.05, 0.3 -> 0.05, 0.25 -> 0.1)
val testResult2 = Statistics.kolmogorovSmirnovTest(data, myCDF)
println(testResult2)
```





## 七、随机数生成 Random data generation

​	RandomRDDs 是一个工具集，用来生成含有随机数的RDD，可以按各种给定的分布模式生成数据集，Random RDDs包下现支持正态分布、泊松分布和均匀分布三种分布方式。RandomRDDs提供随机double RDDS或vector RDDS。

​	下面，我们将生成一个随机double RDD，其值是标准正态分布N（0，1），然后将其映射到N（1，4）。先导入必要的包

```scala
import org.apache.spark.SparkContext
import org.apache.spark.mllib.random.RandomRDDs._
```

 生成1000000个服从正态分配N(0,1)的RDD[Double]，并且分布在 10 个分区中：

```scala
scala> val u = normalRDD(sc, 10000000L, 10)
```

 再把生成的随机数转化成N(1,4) 正态分布：

```scala
scala> val v = u.map(x => 1.0 + 2.0 * x)
```

## 八、核密度估计 Kernel density estimation

​	Spark ML 提供了一个工具类 KernelDensity 用于核密度估算，核密度估算的意思是根据已知的样本估计未知的密度，属於非参数检验方法之一。核密度估计的原理是。观察某一事物的已知分布，如果某一个数在观察中出现了，可认为这个数的概率密度很大，和这个数比较近的数的概率密度也会比较大，而那些离这个数远的数的概率密度会比较小。Spark1.6.2版本支持高斯核(Gaussian kernel)。

 首先导入必要的包

```scala
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD
```

输入数据

```scala
val data: RDD[Double] = sc.parallelize(Seq(1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9))
```

用样本数据构建核函数，这里用假设检验中得到的iris的第一个属性的数据作为样本数据进行估计：

```scala
val kd = new KernelDensity()
  .setSample(data)
  .setBandwidth(3.0)
```

其中setBandwidth表示高斯核的宽度，为一个平滑参数，可以看做是高斯核的标准差。

构造了核密度估计kd，就可以对给定数据数据进行核估计：

```scala
val densities = kd.estimate(Array(-1.0, 2.0, 5.0))
```

这里表示的是，在样本-1.0, 2.0, 5.0,等样本点上，其估算的概率密度函数值。