---
title: MLlib聚类算法
date: 2018-03-13 11:59:21
update: 
comments: true
categories:
  - Machine learning
tags:
  - MLlib
---

<!--more-->
本文参考- Machine learning with Scala by CDA 吴昊天

​	**聚类（Clustering）** 是机器学习中一类重要的方法。其主要思想使用样本的不同特征属性，根据某一给定的相似度度量方式（如欧式距离）找到相似的样本，并根据距离将样本划分成不同的组。聚类属于典型的**无监督学习（Unsupervised Learning）** 方法。与监督学习（如分类器）相比[1](http://dblab.xmu.edu.cn/blog/1288-2/#fn-1288-1)，无监督学习的训练集没有人为标注的结果。在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。

较权威的聚类问题的定义是：

**所谓聚类问题，就是给定一个元素集合D，其中每个元素具有n个可观察属性，使用某种算法将D划分成k个子集，要求每个子集内部的元素之间相异度尽可能低，而不同子集的元素相异度尽可能高。其中每个子集叫做一个簇。**

​	聚类分析以相似性为基础，其目标是使同一类对象的相似度尽可能地大，不同类对象之间的相似度尽可能地小。换句话说，在一个聚类中的模式之间比不在同一聚类中的模式之间具有更多的相似性。

​	目前聚类的方法很多，很难对聚类方法提出一个简洁的分类，因为这些类别可能重叠，从而使得一种方法具有几类的特征。但根据基本思想的不同，聚类分析计算方法主要有如下几种：划分算法、层次算法、密度算法、图论聚类法、网格算法和模型算法等。聚类方法已经被广泛运用在如图像处理、客户精准营销、生物信息学等多个领域，也可以作为使用分类方法前对数据进行预先探索的一种手段来使用。

​	Spark的MLlib库提供了许多可用的聚类方法的实现，如 **KMeans**、**高斯混合模型**、**Power Iteration Clustering（PIC）**、**隐狄利克雷分布（LDA）** 以及 **KMeans** 方法的变种 **二分KMeans（Bisecting KMeans）** 和 **流式KMeans（Streaming KMeans）**等。

## 一、KMeans算法

### 1.基本原理

​	**KMeans** 是一个迭代求解的聚类算法，其属于 **划分（Partitioning）** 型的聚类方法，即首先创建K个划分，然后迭代地将样本从一个划分转移到另一个划分来改善最终聚类的质量，**KMeans** 的过程大致如下：

```markdown
1.根据给定的k值，选取k个样本点作为初始划分中心；
2.计算所有样本点到每一个划分中心的距离，并将所有样本点划分到距离最近的划分中心；
3.计算每个划分中样本点的平均值，将其作为新的中心；
循环进行2~3步直至达到最大迭代次数，或划分中心的变化小于某一预定义阈值
```

​	显然，初始划分中心的选取在很大程度上决定了最终聚类的质量，MLlib内置的`KMeans`类提供了名为 **KMeans||** 的初始划分中心选择方法，它是著名的 **KMeans++** 方法的并行化版本，其思想是令初始聚类中心尽可能的互相远离，具体实现细节可以参见斯坦福大学的B Bahmani在PVLDB上的论文[Scalable K-Means++](http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf)。

### 2.数据集的读取

​	我们通过`SparkContext`自带的`textFile(..)`方法将文件读入，并进行转换，形成一个`RDD<Vector>`。

```scala
//引入相应的包
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors
//读入文件
val data = sc.textFile("file:///usr/local/spark/data/mllib/kmeans_data.txt")
val parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble))).cache()
```

​	这里我们对RDD使用了filter算子，并通过正则表达式将鸢尾花的类标签过滤掉，Scala语言的正则表达式语法与Java语言类似，正则表达式`\\d*(\\.?)\\d*`可以用于匹配实数类型的数字，`\\d*`使用了`*`限定符，表示匹配0次或多次的数字字符，`\\.?`使用了`?`限定符，表示匹配0次或1次的小数点，关于正则表达式的更多知识，可以自行查阅相关资料，这里不再叙述。

### 3.模型训练与分析

​	可以通过创建一个`KMeans`类并调用其`run(RDD[Vector])`方法来训练一个KMeans模型`KMeansModel`，在该方法调用前需要设置一系列参数，如下表所示：

-  K ：聚类数目，默认为2 
-  maxIterations： 最大迭代次数，默认为20 
-  initializationMode ：初始化模式，默认为”k-means||” 
-  runs ： 运行次数，默认为：1 
-  initializationSteps ： 初始化步数，用于KMeans||，默认为5 
-  epsilon ： 迭代停止的阈值，默认为1e-4 

其中，每一个参数均可通过名为`setXXX(...)`（如maxIterations即为`setMaxIterations()`）的方法进行设置。

​	由于`KMeans`类只有无参的构造函数，其对象创建、参数设置需要分别进行，且往往使用的只有存放模型的`KMeansModel`类对象，花功夫创建出的`KMeans`类自象本身却并未使用。故MLlib也提供了包装好的高层次方法`KMeans.train(...)`，传入训练样本和相应的参数，即返回一个训练好的`KMeansModel`对象，十分方便。

​	该方法有4个重载形式，分别可以指定不同的输入参数，具体可以查阅MLlib的API文档，这里我们使用`KMeans.train(data, k, maxIterations, runs)`形式，只需要输入k值、最大迭代次数和运行次数，其他参数使用默认值，如下所示：

```scala
val numClusters = 2
val numIterations = 20
val clusters = KMeans.train(parsedData, numClusters, numIterations)
```

这样，模型即创建成功了。可以通过`KMeansModel`类自带的`clusterCenters`属性获取到模型的所有聚类中心情况：

```scala
clusters.clusterCenters.foreach(
      center => {
        println("Clustering Center:"+center)
      })
```

也可以通过`predict()`方法来确定每个样本所属的聚类：

```scala
parsedData.collect().foreach(
      sample => {
        val predictedCluster = clusters.predict(sample)
        println(sample.toString + " belongs to cluster " + predictedCluster)
      })
```

同时，`KMeansModel`类还提供了计算 **集合内误差平方和（Within Set Sum of Squared Error, WSSSE)** 的方法来度量聚类的有效性：

```scala
val WSSSE = clusters.computeCost(parsedData)
println("Within Set Sum of Squared Errors = " + WSSSE)
```

​	对于那些无法预先知道K值的情况（本文中使用的Iris数据集很明确其K值为3），可以通过 **WSSSE** 的计算构建出 **K-WSSSE** 间的相关关系，从而确定K的值，一般来说，最优的K值即是 **K-WSSSE** 曲线的 **拐点（Elbow）** 位置（当然，对于某些情况来说，我们还需要考虑K值的语义可解释性，而不仅仅是教条地参考WSSSE曲线）。

## 二、高斯混合模型(GMM)聚类算法

### 1.基本原理

​	**高斯混合模型（Gaussian Mixture Model, GMM）** 是一种概率式的聚类方法，属于生成式模型，它假设所有的数据样本都是由某一个给定参数的 **多元高斯分布** 所生成的。具体地，给定类个数`K`，对于给定样本空间中的样本 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-137bed5220372f1cad4f3cdf4529593e_l3.svg)

，一个高斯混合模型的概率密度函数可以由K个多元高斯分布组合成的混合分布表示：

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1d75e089a9a823703aa88d08ad53936e_l3.svg)

其中，

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-a0f35b7e777b0ecf33b511cfb3174001_l3.svg)

是以 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg)

为均值向量， 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg)

为协方差矩阵的多元高斯分布的概率密度函数，可以看出，高斯混合模型由K个不同的多元高斯分布共同组成，每一个分布被称为高斯混合模型中的一个 **成分(Component)**， 而

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1a47f13ac9a37fcb6911a1b8e17cbb35_l3.svg)

为第`i`个多元高斯分布在混合模型中的 **权重** ，且有 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-9312fca34e9b3afde8787a29c58fe192_l3.svg)

。

假设已有一个存在的高斯混合模型，那么，样本空间中的样本的生成过程即是：以 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-af3d8faef7e634b7b15f83cb1606b714_l3.svg)

作为概率（实际上，权重可以直观理解成相应成分产生的样本占总样本的比例），选择出一个混合成分，根据该混合成分的概率密度函数，采样产生出相应的样本。

那么，利用GMM进行聚类的过程是利用GMM生成数据样本的“逆过程”：给定聚类簇数`K`，通过给定的数据集，以某一种 **参数估计** 的方法，推导出每一个混合成分的参数（即均值向量 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg)

、协方差矩阵 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg)

和权重 

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-356e473b3185b432024c4643855f1b9d_l3.svg)

），每一个多元高斯分布成分即对应于聚类后的一个簇。高斯混合模型在训练时使用了极大似然估计法，最大化以下对数似然函数：

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-c006bbb984e97b258cf6bcc1d62ee2d7_l3.svg)

![img](http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-f57e2abf78c1ba038d4969a8fc513e7a_l3.svg)

显然，该优化式无法直接通过解析方式求得解，故可采用 **期望-最大化(Expectation-Maximization, EM)** 方法求解，具体过程如下（为了简洁，这里省去了具体的数学表达式，详细可见[wikipedia](https://en.wikipedia.org/wiki/Mixture_model)）：

```
1.根据给定的K值，初始化K个多元高斯分布以及其权重；
2.根据贝叶斯定理，估计每个样本由每个成分生成的后验概率；(EM方法中的E步)
3.根据均值，协方差的定义以及2步求出的后验概率，更新均值向量、协方差矩阵和权重；（EM方法的M步）
重复2~3步，直到似然函数增加值已小于收敛阈值，或达到最大迭代次数
```

​	当参数估计过程完成后，对于每一个样本点，根据贝叶斯定理计算出其属于每一个簇的后验概率，并将样本划分到后验概率最大的簇上去。相对于KMeans等直接给出样本点的簇划分的聚类方法，GMM这种给出样本点属于每个簇的概率的聚类方法，被称为 **软聚类(Soft Clustering / Soft Assignment)** 。

### 2.模型的训练与分析

​	Spark的ML库提供的高斯混合模型都在`org.apache.spark.ml.clustering`包下，和其他的聚类方法类似，其具体实现分为两个类：用于抽象GMM的超参数并进行训练的`GaussianMixture`类（`Estimator`）和训练后的模型`GaussianMixtureModel`类（`Transformer`），在使用前，引入需要的包：

```scala
import org.apache.spark.ml.clustering.{GaussianMixture,GaussianMixtureModel}
import org.apache.spark.ml.linalg.Vectors
```

开启`RDD`的隐式转换：

```scala
import spark.implicits._
```

​	我们仍采用Iris数据集进行实验。为了便于生成相应的`DataFrame`，这里定义一个名为`model_instance`的`case class`作为`DataFrame`每一行（一个数据样本）的数据类型。

```scala
case class model_instance (features: Vector)
```

在定义数据类型完成后，即可将数据读入`RDD[model_instance]`的结构中，并通过`RDD`的隐式转换`.toDF()`方法完成`RDD`到`DataFrame`的转换：

```scala
val rawData = sc.textFile("file:///usr/local/spark/iris.txt")
val df = rawData.map(line =>
     | { model_instance( Vectors.dense(line.split(",").filter(p => p.matches("\\d*(\\.?)\\d*"))
     | .map(_.toDouble)) )}).toDF()
```

​	与MLlib的操作类似，我们使用了filter算子，过滤掉类标签，正则表达式`\\d*(\\.?)\\d*`可以用于匹配实数类型的数字，`\\d*`使用了`*`限定符，表示匹配0次或多次的数字字符，`\\.?`使用了`?`限定符，表示匹配0次或1次的小数点。

​	可以通过创建一个`GaussianMixture`类，设置相应的超参数，并调用`fit(..)`方法来训练一个GMM模型`GaussianMixtureModel`，在该方法调用前需要设置一系列超参数，如下表所示：

- K:聚类数目，默认为2 
- maxIter : 最大迭代次数，默认为100 
- seed : 随机数种子，默认为随机Long值 
- Tol : 对数似然函数收敛阈值，默认为0.01 

其中，每一个超参数均可通过名为`setXXX(...)`（如maxIterations即为`setMaxIterations()`）的方法进行设置。这里，我们建立一个简单的`GaussianMixture`对象，设定其聚类数目为3，其他参数取默认值。

```scala
val gm = new GaussianMixture().setK(3)
 |              .setPredictionCol("Prediction")
 |              .setProbabilityCol("Probability")
val gmm = gm.fit(df)
```

和`KMeans`等硬聚类方法不同的是，除了可以得到对样本的聚簇归属预测外，还可以得到样本属于各个聚簇的概率（这里我们存在”Probability”列中）。

​	调用`transform()`方法处理数据集之后，打印数据集，可以看到每一个样本的预测簇以及其概率分布向量

```scala
val result = gmm.transform(df)
result.show(150, false)
```

​	得到模型后，即可查看模型的相关参数，与KMeans方法不同，GMM不直接给出聚类中心，而是给出各个混合成分（多元高斯分布）的参数。在ML的实现中，GMM的每一个混合成分都使用一个`MultivariateGaussian`类（位于`org.apache.spark.ml.stat.distribution`包）来存储，我们可以使用`GaussianMixtureModel`类的`weights`成员获取到各个混合成分的权重，使用`gaussians`成员来获取到各个混合成分的参数（均值向量和协方差矩阵）：

```scala
for (i <- 0 until gmm.getK) {
     | println("Component %d : weight is %f \n mu vector is %s \n sigma matrix is %s" format
     | (i, gmm.weights(i), gmm.gaussians(i).mean, gmm.gaussians(i).cov))
     | }
```

