---
title: 机器学习scikit-learn 中的设置以及预估对象
date: 2018-03-18 17:05:59
password:
top:
categories:
  - Machine learning
tags:
  - scikit-learn 
---
<!--more-->



## 数据集

Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 样本 ，第二个维度代表 特征 （每一行代表一个样本，每一列代表一种特征）。

### 样例: iris 数据集（鸢尾花卉数据集）
这个数据集包含150个样本，每个样本包含4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，详细数据可以通过``iris.DESCR``查看。
如果原始数据不是``(n_samples, n_features)``的形状时，使用之前需要进行预处理以供scikit-learn使用。



```python
from sklearn import datasets
iris = datasets.load_iris()
data = iris.data
data.shape
```




    (150, 4)



### 数据预处理样例:digits数据集(手写数字数据集)
digits数据集包含1797个手写数字的图像，每个图像为8*8像素
为了在scikit中使用这一数据集，需要将每一张8×8的图像转换成长度为64的特征向量


```python
digits = datasets.load_digits()
digits.images.shape

import matplotlib.pyplot as plt 
#添加魔法才能绘图
%matplotlib inline     

plt.imshow(digits.images[-1],cmap=plt.cm.gray_r) 
#为了在scikit中使用这一数据集，需要将每一张8×8的图像转换成长度为64的特征向量
data = digits.images.reshape((digits.images.shape[0], -1))
```


![png](/images/sklearn/output_4_0.png)


## 预估对象

拟合数据: scikit-learn实现最重要的一个API是`estimator`。`estimators`是基于数据进行学习的任何对象，它可以是一个分类器，回归或者是一个聚类算法，或者是从原始数据中提取/过滤有用特征的变换器。
所有的拟合模型对象拥有一个名为``fit``的方法，参数是一个数据集（通常是一个2维列表）:

使用方式：
> `estimator.fit(data)`

拟合模型对象构造参数: 在创建一个拟合模型时，可以设置相关参数，在创建之后也可以修改对应的参数:
> 
`estimator = Estimator(param1=1, param2=2)
estimator.param1`

拟合参数: 当拟合模型完成对数据的拟合之后，可以从拟合模型中获取拟合的参数结果，所有拟合完成的参数均以下划线(_)作为结尾:
> `estimator.estimated_param_ `

# 监督学习：从高维观察预测输出变量
监督学习解决的问题
监督学习 在于学习两个数据集的联系：观察数据 X 和我们正在尝试预测的额外变量 y (通常称“目标”或“标签”)， 而且通常是长度为 n_samples 的一维数组。
scikit-learn 中所有监督的 估计量 <https://en.wikipedia.org/wiki/Estimator> 都有一个用来拟合模型的 fit(X, y) 方法，和根据给定的没有标签观察值 X 返回预测的带标签的 y 的 predict(X) 方法。
词汇：分类和回归
如果预测任务是为了将观察值分类到有限的标签集合中，换句话说，就是给观察对象命名，那任务就被称为 分类 任务。另外，如果任务是为了预测一个连续的目标变量，那就被称为 回归 任务。
当在 scikit-learn 中进行分类时，y 是一个整数或字符型的向量。
注：

**回归是统计学中最有力的工具之一。机器学习监督学习算法分为分类算法和回归算法两种，其实就是根据类别标签分布类型为离散型、连续性而定义的。
分类算法用于离散型分布预测，如KNN、决策树、朴素贝叶斯、adaboost、SVM、Logistic回归都是分类算法；回归算法用于连续型分布预测，针对的是数值型的样本，使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。**

## 最近邻和维度惩罚
### 鸢尾属植物分类：

鸢尾属植物数据集是根据花瓣长度、花瓣度度、萼片长度和萼片宽度4个特征对3种不同类型的鸢尾属植物进行分类:



```python
import numpy as np
from sklearn import datasets
iris = datasets.load_iris()
iris_X = iris.data
iris_y = iris.target
np.unique(iris_y)   #a = np.unique(A)对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表
```




    array([0, 1, 2])



图片：鸢尾花.png

### K近邻分类器

**最近邻**: 也许是最简单的分类器：给定一个新的观察值 X_test，用最接近的特征向量在训练集(比如，用于训练估计器的数据)找到观察值。(请看 Scikit-learn 在线学习文档的 最近邻章节 获取更多关于这种分类器的信息)

### 训练集和测试集
当用任意的学习算法进行实验时，最重要的就是不要在用于拟合估计器的数据上测试一个估计器的预期值，因为这不会评估在 新数据 上估计器的执行情况。这也是数据集经常被分为 训练 和 测试 数据的原因。
### KNN(k 最近邻)分类器例子:

K最近邻(k-Nearest Neighbor，KNN)分类算法的核心思想是如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法可用于多分类，KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，作为预测值。

KNeighborsClassifier在scikit-learn 在sklearn.neighbors包之中。KNeighborsClassifier使用很简单，三步：

1）创建KNeighborsClassifier对象，

2）调用fit函数，

3）调用predict函数进行预测。以下代码说明了用法。


```python
# 数据展示
print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 15

# import some data to play with
iris = datasets.load_iris()

# we only take the first two features. We could avoid this ugly
# slicing by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

h = .02  # step size in the mesh

# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

for weights in ['uniform', 'distance']:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n_neighbors, weights))

plt.show()
```

    Automatically created module for IPython interactive environment



![png](/images/sklearn/output_11_1.png)



![png](/images/sklearn/output_11_2.png)



```python
# 将鸢尾属植物数据集分解为训练集和测试集
# 随机排列，用于使分解的数据随机分布
np.random.seed(0)
indices = np.random.permutation(len(iris_X))#np.random.permutation(x)返回一个随机排列,这里以0为随机数种子返回1～150的随机排列
iris_X_train = iris_X[indices[:-10]]
iris_y_train = iris_y[indices[:-10]]
iris_X_test  = iris_X[indices[-10:]]
iris_y_test  = iris_y[indices[-10:]]
# 创建和拟合一个最近邻分类器
from sklearn.neighbors import KNeighborsClassifier #K最邻近分类器
knn = KNeighborsClassifier()
knn.fit(iris_X_train, iris_y_train) 

print("预测值：",knn.predict(iris_X_test))
print("目标值：",iris_y_test)
# 结果显示预测准确率90%
```

    预测值： [1 2 1 0 0 0 2 1 2 0]
    目标值： [1 1 1 0 0 0 2 1 2 0]


### 维度惩罚
**为了使一个估计器有效，你需要邻接点间的距离小于一些值：d，这取决于具体问题。在一维中，这需要平均 n sim 1/d 点。在上文 k-NN 例子中，如果数据只是由一个0到1的特征值和 n 训练观察值所描述，那么新数据将不会超过 1/n。因此，最近邻决策规则会很有效率，因为与类间特征变量范围相比， 1/n 很小。
如果特征数是 p，你现在就需要 n \sim 1/d^p 点。也就是说我们在一维 [0, 1] 空间里需要10个点，在 p 维里就需要 10^p 个点。当 p 增大时，为了得到一个好的估计器，相应的训练点数量就需要成倍增大。
比如，如果每个点只是单个数字(8个字节)，那么一个 k-NN 估计器在一个非常小的 p \sim 20 维度下就需要比现在估计的整个互联网的大小(±1000 艾字节或更多)还要多的训练数据。
这叫 维度惩罚，是机器学习领域的核心问题。**

## 线性模型：从回归到稀疏


### 糖尿病数据集
糖尿病数据集包括442名患者的10个生理特征(年龄，性别，体重，血压)，和一年后的疾病级别指标:

手头上的任务是为了从生理特征预测疾病级别。


```python
diabetes = datasets.load_diabetes()
diabetes_X_train = diabetes.data[:-20]
diabetes_X_test  = diabetes.data[-20:]
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test  = diabetes.target[-20:]
```

### 线性回归

LinearRegression，最简单的拟合线性模型形式，是通过调整数据集的一系列参数令残差平方和尽可能小。

Linear models: $ y = X\beta + \epsilon$

$X$: 数据

$y$: 目标变量

$\beta$: 回归系数

$\epsilon$: 观察噪声


```python
from sklearn import linear_model
regr = linear_model.LinearRegression()
regr.fit(diabetes_X_train, diabetes_y_train)

print(regr.coef_) #这个参数反映的是每个特征对总体特征的影响力，下次找机会咨询老师

# 均方误差
mean = np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)

print("\n\n测试线性相关性")
regr.score(diabetes_X_test, diabetes_y_test) 
# 方差分数：1 是完美的预测
# 0 意味着 X 和 y 之间没有线性关系。
```

    [  3.03499549e-01  -2.37639315e+02   5.10530605e+02   3.27736980e+02
      -8.14131709e+02   4.92814588e+02   1.02848452e+02   1.84606489e+02
       7.43519617e+02   7.60951722e+01]
    
    
    测试线性相关性





    0.58507530226905713



**注：这里为什么不使用pandas的pd.corr()方法？
pd.corr()方法只能处理一维的数据。
**

### 收缩（岭回归 ）

如果每个维度的数据点很少，观察噪声就会导致很大的方差：

[岭回归学习笔记](https://www.cnblogs.com/magle/p/5878967.html)


```python
X = np.c_[ .5, 1].T
y = [.5, 1]
test = np.c_[ 0, 2].T
regr = linear_model.LinearRegression()

import matplotlib.pyplot as plt 
plt.figure() 

np.random.seed(0)
for _ in range(6): 
    this_X = .1*np.random.normal(size=(2, 1)) + X
    regr.fit(this_X, y)
    plt.plot(test, regr.predict(test)) 
    plt.scatter(this_X, y, s=3)  
```


![png](/images/sklearn/output_21_0.png)


高纬统计学习中的一个解决方法是 收缩 回归系数到0：任何两个随机选择的观察值数据集都很可能是不相关的。这称为 **岭回归 ：**

*岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。*

这是 `bias/variance tradeoff` 中的一个例子：**岭参数 alpha 越大，偏差越大，方差越小。**


```python
regr = linear_model.Ridge(alpha=.1)

plt.figure() 

np.random.seed(0)
for _ in range(6): 
    this_X = .1*np.random.normal(size=(2, 1)) + X
    regr.fit(this_X, y)
    plt.plot(test, regr.predict(test)) 
    plt.scatter(this_X, y, s=3) 
```


![png](/images/sklearn/output_23_0.png)


下面我们可以选择 alpha 来最小化排除错误，这里使用糖尿病数据集而不是人为数据:


```python
alphas
```




    array([ 0.0001    ,  0.00039811,  0.00158489,  0.00630957,  0.02511886,
            0.1       ])




```python
alphas = np.logspace(-4, -1, 6) #生成6个在10的-4次方 ～ 10的-1次方之间的等比数列
from __future__ import print_function
for alpha in alphas:
    newregr = regr.set_params(alpha=alpha)
    newregr.fit(diabetes_X_train, diabetes_y_train,)
    print(newregr.score(diabetes_X_test, diabetes_y_test))

    # print([regr.set_params(alpha=alpha
#             ).fit(diabetes_X_train, diabetes_y_train,
#             ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas]) 
```

    0.585111068388
    0.585207301544
    0.58546775407
    0.58555120365
    0.583071708555
    0.570589994373


*Note 捕获拟合参数噪声使得模型不能归纳新的数据称为 过拟合。岭回归产生的偏差被称为 正则化。*

### 稀疏

图片： 稀疏.png

*Note 整个糖尿病数据集包括11个维度(10个特征维度和1个目标变量)。很难直观地表示出来，但是记住那是一个比较 空 的空间可能比较有用。*

为了提高问题的条件(比如，缓解`维度惩罚`)，只选择信息特征和设置无信息时就会变得有趣，比如特征2到0。岭回归会减小他们的值，但不会减到0.另一种抑制方法，称为 **Lasso (最小绝对收缩和选择算子)**，可以把一些系数设为0。这些方法称为 稀疏法，稀疏可以看作是奥卡姆剃刀的应用：模型越简单越好。


```python
regr = linear_model.Lasso()
scores = [regr.set_params(alpha=alpha
            ).fit(diabetes_X_train, diabetes_y_train
            ).score(diabetes_X_test, diabetes_y_test)
       for alpha in alphas]
best_alpha = alphas[scores.index(max(scores))]  #取出相关值最大的索引，用其作为最佳alpha取值
regr.alpha = best_alpha

regr.fit(diabetes_X_train, diabetes_y_train)



print(regr.coef_) #返回9个特征分别对应的相关系数
```

    [   0.         -212.43764548  517.19478111  313.77959962 -160.8303982    -0.
     -187.19554705   69.38229038  508.66011217   71.84239008]


### 同一个问题的不同算法
不同的算法可以用于解决同一个数学问题。比如在 scikit-learn 里 Lasso 对象使用 coordinate descent 方法解决 lasso 回归问题，对于大型数据集很有效。但是，scikit-learn 也提供了使用** LARS 算法 的:class:LassoLars 对象，对于处理带权向量非常稀疏的数据非常有效(比如，问题的观察值很少)。**

### 分类

对于分类，比如标定 鸢尾属植物 任务，线性回归就不是好方法了，因为它会给数据很多远离决策边界的权值。一个线性方法是为了拟合 sigmoid 函数 或 logistic 函数：
$$y = \textrm{sigmoid}(X\beta - \textrm{offset}) + \epsilon =
\frac{1}{1 + \textrm{exp}(- X\beta + \textrm{offset})} + \epsilon$$



```python
print(__doc__)


# Code source: Gael Varoquaux
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model

# this is our test set, it's just a straight line with some
# Gaussian noise
xmin, xmax = -5, 5
n_samples = 100
np.random.seed(0)
X = np.random.normal(size=n_samples)
y = (X > 0).astype(np.float)
X[X > 0] *= 4
X += .3 * np.random.normal(size=n_samples)

X = X[:, np.newaxis]
# run the classifier
clf = linear_model.LogisticRegression(C=1e5)
clf.fit(X, y)

# and plot the result
plt.figure(1, figsize=(4, 3))
plt.clf()
plt.scatter(X.ravel(), y, color='black', zorder=20)
X_test = np.linspace(-5, 10, 300)


def model(x):
    return 1 / (1 + np.exp(-x))
loss = model(X_test * clf.coef_ + clf.intercept_).ravel()
plt.plot(X_test, loss, color='red', linewidth=3)

ols = linear_model.LinearRegression()
ols.fit(X, y)
plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)
plt.axhline(.5, color='.5')

plt.ylabel('y')
plt.xlabel('X')
plt.xticks(range(-5, 10))
plt.yticks([0, 0.5, 1])
plt.ylim(-.25, 1.25)
plt.xlim(-4, 10)
plt.legend(('Logistic Regression Model', 'Linear Regression Model'),
           loc="lower right", fontsize='small')
plt.show()
```

    Automatically created module for IPython interactive environment



![png](/images/sklearn/output_34_1.png)


### 逻辑回归：（`LogisticRegression`）

```python
logistic = linear_model.LogisticRegression(C=1e5)
logistic.fit(iris_X_train, iris_y_train)
```


```python
print(__doc__)


# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
from sklearn import linear_model, datasets

# import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2]  # we only take the first two features.
Y = iris.target

h = .02  # step size in the mesh

logreg = linear_model.LogisticRegression(C=1e5)

# we create an instance of Neighbours Classifier and fit the data.
logreg.fit(X, Y)

# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure(1, figsize=(4, 3))
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

plt.show()
```

    Automatically created module for IPython interactive environment



![png](/images/sklearn/output_36_1.png)


### 多类分类
如果你有很多类需要预测，一种常用方法就是去拟合一对多分类器，然后使用根据投票为最后做决定。
### 使用 logistic 回归进行收缩和稀疏
LogisticRegression 对象中的 C 参数控制着正则化数量：C 值越大，正则化数量越小。penalty="l2" 提供 收缩(比如，无稀疏系数)，同时 `penalty=”l1”` 提供稀疏化。

### 练习
尝试用最近邻和线性模型分类数字数据集。留出最后 10%的数据，并测试观察值预期效果。


```python
from sklearn import datasets, neighbors, linear_model

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

n_samples = len(X_digits)
X_train = X_digits[:int(.9 * n_samples)]
y_train = y_digits[:int(.9 * n_samples)]
X_test = X_digits[int(.9 * n_samples):]
y_test = y_digits[int(.9 * n_samples):]

knn = KNeighborsClassifier()
logistic = linear_model.LogisticRegression()

print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))
print('LogisticRegression score: %f'
      % logistic.fit(X_train, y_train).score(X_test, y_test))
# 测试证明针对此数据，KNN最邻近分类效果更佳
```

    KNN score: 0.961111
    LogisticRegression score: 0.938889


## 支持向量积(SVMs)
### 线性 SVMs

支持向量机 属于判别模型家族：它们尝试通过找到样例的一个组合来构建一个两类之间最大化的平面。通过 C 参数进行正则化设置：C 的值小意味着边缘是通过分割线周围的所有观测样例进行计算得到的(更正则化)；C 的值大意味着边缘是通过邻近分割线的观测样例计算得到的(更少正则化)。

#### [例子](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py)

SVMs 可以用于回归或分类
- 回归:–:class: SVR (支持向量回归)
- 分类 –:class: SVC (支持向量分类)

**Warning 规格化数据**

**对很多估计器来说，包括 SVMs，为每个特征值使用单位标准偏差的数据集，是获得好的预测重要前提。**

### 使用核

在特征空间类并不总是线性可分的。解决办法就是构建一个不是线性的但能是多项式的函数做代替。这要使用 核技巧(kernel trick)，它可以被看作通过设置 kernels 在观察样例上创建决策力量：

#### [线性核](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_kernels.html)
```python
svc = svm.SVC(kernel='linear')
```
#### [多项式核](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_kernels.html)
```python
svc = svm.SVC(kernel='poly',
              degree=3)
# degree: polynomial degree
```

#### [RBF（径向基函数）](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_kernels.html)
```python
svc = svm.SVC(kernel='rbf')
# gamma: inverse of size of
# radial kernel
```


### 交互例子
查看 SVM GUI 通过下载 svm_gui.py；通过左右按键添加两类数据点，拟合模型并改变参数和数据。

### 练习

根据特征1和特征2，尝试用 SVMs 把1和2类从鸢尾属植物数据集中分出来。为每一个类留下10%，并测试这些观察值预期效果。
警告: 类是有序的，不要留下最后10%，不然你只能测试一个类了。
提示: 为了直观显示，你可以在网格上使用 decision_function 方法。


```python
"""
================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
"""
print(__doc__)


import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets, svm

iris = datasets.load_iris()
X = iris.data
y = iris.target

X = X[y != 0, :2]  #选出y不为0的1类和2类，取前两列特征
y = y[y != 0] #同上

n_sample = len(X)

np.random.seed(0)
order = np.random.permutation(n_sample)  #生成n_sample个随机数，用permutation方法打乱顺序（洗牌）
X = X[order]  #以order为索引打乱数据
y = y[order].astype(np.float)

X_train = X[:int(.9 * n_sample)]
y_train = y[:int(.9 * n_sample)]
X_test = X[int(.9 * n_sample):]
y_test = y[int(.9 * n_sample):]

# fit the model
for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
    clf = svm.SVC(kernel=kernel, gamma=10)
    clf.fit(X_train, y_train)

    plt.figure(fig_num)
    plt.clf()
    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
                edgecolor='k', s=20)

    # Circle out the test data
    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',
                zorder=10, edgecolor='k')

    plt.axis('tight')
    x_min = X[:, 0].min()
    x_max = X[:, 0].max()
    y_min = X[:, 1].min()
    y_max = X[:, 1].max()

    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(XX.shape)
    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])

    plt.title(kernel)
plt.show()

```

    
    ================================
    SVM Exercise
    ================================
    
    A tutorial exercise for using different SVM kernels.
    
    This exercise is used in the :ref:`using_kernels_tut` part of the
    :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
    



![png](/images/sklearn/output_45_1.png)



![png](/images/sklearn/output_45_2.png)



![png](/images/sklearn/output_45_3.png)


# 模型选择 选择估计量及其参数
## 分数和交叉验证分数（取最佳拟合的方式）

如我们所见，每一个估计量都有一个可以在新数据上判定拟合质量(或预期值)的 score 方法。**越大越好**


```python
from sklearn import datasets, svm
digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target
svc = svm.SVC(C=1, kernel='linear') #创建支持向量机，线性核模型
svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])
#每一个估计量都有一个可以在新数据上判定拟合质量(或预期值)的 score 方法。越大越好
```




    0.97999999999999998



### KFold 交叉验证
为了更好地预测精度(我们可以用它作为模型的拟合优度代理)，我们可以连续分解用于我们训练和测试用的 折叠数据。

以下被称为 **KFold 交叉验证.**


```python
import numpy as np
X_folds = np.array_split(X_digits, 3)  #以3为步长进行切割
y_folds = np.array_split(y_digits, 3)
scores = list()
for k in range(3):
    # 为了稍后的 ‘弹出’ 操作，我们使用 ‘列表’ 来复制数据
    X_train = list(X_folds)
    X_test  = X_train.pop(k)    #删除索引为k的列
    X_train = np.concatenate(X_train) #numpy的连接函数，用法类似pandas的concat
    y_train = list(y_folds)
    y_test  = y_train.pop(k)
    y_train = np.concatenate(y_train)
    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))
print(scores)

# 测试发现删除第二列时的拟合效果最佳
```

    [0.93489148580968284, 0.95659432387312182, 0.93989983305509184]


## 交叉验证生成器 （第一次没搞懂）
scikit-learn 有可以生成训练/测试索引列表的类，可用于流行的交叉验证策略。
类提供了 split 方法，方法允许输入能被分解的数据集，并为每次选择的交叉验证策略迭代生成训练/测试集索引。
下面是使用 split 方法的例子。


```python
from sklearn.model_selection import KFold, cross_val_score
X = ["a", "a", "b", "c", "c", "c"]
k_fold = KFold(n_splits=3)
for train_indices, test_indices in k_fold.split(X):
     print('Train: %s | test: %s' % (train_indices, test_indices))
```

    Train: [2 3 4 5] | test: [0 1]
    Train: [0 1 4 5] | test: [2 3]
    Train: [0 1 2 3] | test: [4 5]



```python
# 这样就实现执行交叉验证了
[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
         for train, test in k_fold.split(X_digits)]
```




    [0.93489148580968284, 0.95659432387312182, 0.93989983305509184]



交叉验证分数可以使用 cross_val_score 直接计算出来。给定一个估计量，交叉验证对象，和输入数据集， cross_val_score 函数就会反复分解出训练和测试集的数据，然后使用训练集和为每次迭代交叉验证运算出的基于测试集的分数来训练估计量。

默认情况下，估计器的 score 方法被用于运算个体分数。
可以参考 metrics 模块 学习更多可用的评分方法。


```python
# 上部分代码的cross_val_score函数实现
cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)
```




    array([ 0.93489149,  0.95659432,  0.93989983])



n_jobs=-1 意味着运算会被调度到所有 CPU 上进行。

或者，可以提供 **scoring** 参数来指定替换的评分方法。


```python
cross_val_score(svc, X_digits, y_digits, cv=k_fold,
                scoring='precision_macro')
```




    array([ 0.93969761,  0.95911415,  0.94041254])



#### 交叉验证生成器
| [Fold (n_splits, shuffle, random_state)](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)    |    [StratifiedKFold (n_splits, shuffle, random_state)](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)|[GroupKFold (n_splits)](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold)   |
| :-------- | --------:| :------: |
|将其分解为 K 个折叠，在 K-1 上训练，然后排除测试。  |和 K-Fold 一样，但会保留每个折叠里的类分布。 |  确保相同组不会在测试和训练集里。|
| [ShuffleSplit (n_splits, test_size, train_size, random_state)](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit) |     [StratifiedShuffleSplit](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit)|[GroupShuffleSplit](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit)  |
| 生成基于随机排列的训练/测试索引。  |和 shuffle 分解一样，但会保留每个迭代里的类分布。 |确保相同组不会在测试和训练集里。 |
| [LeaveOneGroupOut ()](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut)  |[LeavePGroupsOut (n_groups)](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut) | [LeaveOneOut()](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut)  |
| 使用数组分组来给观察分组。	  | 忽略 P 组。 |忽略一个观察。 |
| [LeavePOut (p)](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut)  | [PredefinedSplit](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit) |   
| 忽略 P 观察。	  | 生成基于预定义分解的训练/测试索引。 |  

### 练习
在数字数据集中，用一个线性内核绘制一个 SVC 估计器的交叉验证分数来作为 C 参数函数(使用从1到10的点对数网格).


```python
print(__doc__)


import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets, svm

digits = datasets.load_digits()
X = digits.data
y = digits.target

svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)

scores = list()
scores_std = list()
for C in C_s:
    svc.C = C
    this_scores = cross_val_score(svc, X, y, n_jobs=1)
    scores.append(np.mean(this_scores))
    scores_std.append(np.std(this_scores))

# Do the plotting
import matplotlib.pyplot as plt
plt.figure(1, figsize=(4, 3))
plt.clf()
plt.semilogx(C_s, scores)
plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')
plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')
locs, labels = plt.yticks()
plt.yticks(locs, list(map(lambda x: "%g" % x, locs)))
plt.ylabel('CV score')
plt.xlabel('Parameter C')
plt.ylim(0, 1.1)
plt.show()
```

    
    ================================
    SVM Exercise
    ================================
    
    A tutorial exercise for using different SVM kernels.
    
    This exercise is used in the :ref:`using_kernels_tut` part of the
    :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
    



![png](/images/sklearn/output_59_1.png)


## 网格搜索和交叉验证估计量
### 网格搜索
scikit-learn 提供了一个对象，在给定数据情况下，在一个参数网格，估计器拟合期间计算分数，并选择参数来最大化交叉验证分数。这个对象在构建过程中获取估计器并提供一个估计器 API。


```python
from sklearn.model_selection import GridSearchCV, cross_val_score
Cs = np.logspace(-6, -1, 10)
clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),
                   n_jobs=-1)
clf.fit(X_digits[:1000], y_digits[:1000])        

clf.best_score_                                  

clf.best_estimator_.C 
```




    0.0077426368268112772




```python
# Prediction performance on test set is not as good as on train set
clf.score(X_digits[1000:], y_digits[1000:])    #最佳拟合效果
```




    0.94353826850690092



默认情况下， [GridSearchCV](http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) 使用一个三倍折叠交叉验证。但是，如果它检测到分类器被传递，而不是回归，它就会使用分层的三倍。

#### 嵌套交叉验证
```python
cross_val_score(clf, X_digits, y_digits)
```
```
两个交叉验证循环并行执行：一个由 GridSearchCV 估计器设置 gamma，另一个 cross_val_score 则是测量估计器的预期执行情况。结果分数是对新数据上的预期分数的无偏估计。
```
**Warning 你不可以并行运算嵌套对象(n_jobs 与1不同)。**



```python
cross_val_score(clf, X_digits, y_digits)
```




    array([ 0.93853821,  0.96327212,  0.94463087])



### 交叉验证估计量

设置参数的交叉验证可以更有效地完成一个基础算法。这就是为什么对某些估计量来说，scikit-learn 提供了 交叉验证 估计量自动设置它们的参数。

这些估计量和它们的副本称呼类似，在名字后加 ‘CV’。


```python
from sklearn import linear_model, datasets
lasso = linear_model.LassoCV()
diabetes = datasets.load_diabetes()
X_diabetes = diabetes.data
y_diabetes = diabetes.target
lasso.fit(X_diabetes, y_diabetes)


# 估计器自动选择它的 lambda:
lasso.alpha_ 
```




    0.012291895087486173



### 练习
在糖尿病数据集中，找到最优正则化参数 α。
另外： 你有多相信 α 的选择？


```python
from __future__ import print_function
print(__doc__)

import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]

lasso = Lasso(random_state=0)
alphas = np.logspace(-4, -0.5, 30)

tuned_parameters = [{'alpha': alphas}]
n_folds = 3

clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)
clf.fit(X, y)
scores = clf.cv_results_['mean_test_score']
scores_std = clf.cv_results_['std_test_score']
plt.figure().set_size_inches(8, 6)
plt.semilogx(alphas, scores)

# plot error lines showing +/- std. errors of the scores
std_error = scores_std / np.sqrt(n_folds)

plt.semilogx(alphas, scores + std_error, 'b--')
plt.semilogx(alphas, scores - std_error, 'b--')

# alpha=0.2 controls the translucency of the fill color
plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)

plt.ylabel('CV score +/- std error')
plt.xlabel('alpha')
plt.axhline(np.max(scores), linestyle='--', color='.5')
plt.xlim([alphas[0], alphas[-1]])

# #############################################################################
# Bonus: how much can you trust the selection of alpha?

# To answer this question we use the LassoCV object that sets its alpha
# parameter automatically from the data by internal cross-validation (i.e. it
# performs cross-validation on the training data it receives).
# We use external cross-validation to see how much the automatically obtained
# alphas differ across different cross-validation folds.
lasso_cv = LassoCV(alphas=alphas, random_state=0)
k_fold = KFold(3)

print("Answer to the bonus question:",
      "how much can you trust the selection of alpha?")
print()
print("Alpha parameters maximising the generalization score on different")
print("subsets of the data:")
for k, (train, test) in enumerate(k_fold.split(X, y)):
    lasso_cv.fit(X[train], y[train])
    print("[fold {0}] alpha: {1:.5f}, score: {2:.5f}".
          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))
print()
print("Answer: Not very much since we obtained different alphas for different")
print("subsets of the data and moreover, the scores for these alphas differ")
print("quite substantially.")

plt.show()
```

    
    ================================
    SVM Exercise
    ================================
    
    A tutorial exercise for using different SVM kernels.
    
    This exercise is used in the :ref:`using_kernels_tut` part of the
    :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
    
    Answer to the bonus question: how much can you trust the selection of alpha?
    
    Alpha parameters maximising the generalization score on different
    subsets of the data:
    [fold 0] alpha: 0.10405, score: 0.53573
    [fold 1] alpha: 0.05968, score: 0.16278
    [fold 2] alpha: 0.10405, score: 0.44437
    
    Answer: Not very much since we obtained different alphas for different
    subsets of the data and moreover, the scores for these alphas differ
    quite substantially.



![png](/images/sklearn/output_68_1.png)


# 无监督学习: 寻求数据表示
## 聚类: 对样本数据进行分组

### 可以利用聚类解决的问题
对于 iris 数据集来说，我们知道所有样本有 3 种不同的类型，但是并不知道每一个样本是那种类型：此时我们可以尝试一个 **clustering task（聚类任务）** 聚类算法: 将样本进行分组，相似的样本被聚在一起，而不同组别之间的样本是有明显区别的，这样的分组方式就是 
**clusters（聚类）**
### K-means 聚类算法

关于聚类有很多不同的聚类标准和相关算法，其中最简便的算法是 [K-means](http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html#k-means) 。


```python
from sklearn import cluster, datasets
iris = datasets.load_iris()
X_iris = iris.data
y_iris = iris.target

k_means = cluster.KMeans(n_clusters=3)
k_means.fit(X_iris) 

print(k_means.labels_[::10])

print(y_iris[::10])
```

    [1 1 1 1 1 2 2 2 2 2 0 0 0 0 0]
    [0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]


#### **示例**
The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.

**Warning k_means 算法无法保证聚类结果完全绝对真实的反应实际情况。首先，选择正确合适的聚类数量不是一件容易的事情，第二，该算法对初始值的设置敏感，容易陷入局部最优。尽管 scikit-learn 采取了不同的方式来缓解以上问题，目前仍没有完美的解决方案。**

**Don’t over-interpret clustering results（不要过分解读聚类结果）**


```python
print(__doc__)


# Code source: Gaël Varoquaux
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
# Though the following import is not directly being used, it is required
# for 3D projection to work
from mpl_toolkits.mplot3d import Axes3D

from sklearn.cluster import KMeans
from sklearn import datasets

np.random.seed(5)

iris = datasets.load_iris()
X = iris.data
y = iris.target

estimators = [('k_means_iris_8', KMeans(n_clusters=8)),
              ('k_means_iris_3', KMeans(n_clusters=3)),
              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,
                                               init='random'))]

fignum = 1
titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']
for name, est in estimators:
    fig = plt.figure(fignum, figsize=(4, 3))
    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
    est.fit(X)
    labels = est.labels_

    ax.scatter(X[:, 3], X[:, 0], X[:, 2],
               c=labels.astype(np.float), edgecolor='k')

    ax.w_xaxis.set_ticklabels([])
    ax.w_yaxis.set_ticklabels([])
    ax.w_zaxis.set_ticklabels([])
    ax.set_xlabel('Petal width')
    ax.set_ylabel('Sepal length')
    ax.set_zlabel('Petal length')
    ax.set_title(titles[fignum - 1])
    ax.dist = 12
    fignum = fignum + 1

# Plot the ground truth
fig = plt.figure(fignum, figsize=(4, 3))
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

for name, label in [('Setosa', 0),
                    ('Versicolour', 1),
                    ('Virginica', 2)]:
    ax.text3D(X[y == label, 3].mean(),
              X[y == label, 0].mean(),
              X[y == label, 2].mean() + 2, name,
              horizontalalignment='center',
              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
ax.set_title('Ground Truth')
ax.dist = 12

fig.show()
```

    
    ================================
    SVM Exercise
    ================================
    
    A tutorial exercise for using different SVM kernels.
    
    This exercise is used in the :ref:`using_kernels_tut` part of the
    :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
    


    /Users/hadoop/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
      "matplotlib is currently using a non-GUI backend, "



![png](/images/sklearn/output_72_2.png)



![png](/images/sklearn/output_72_3.png)



![png](/images/sklearn/output_72_4.png)



![png](/images/sklearn/output_72_5.png)


### Application example: vector quantization（应用案例:向量量化(vector quantization)）(多看几遍)
一般来说聚类，特别是 K_means 聚类可以作为一种用少量样本来压缩信息的方式。这种方式就是 [vector quantization](https://en.wikipedia.org/wiki/Vector_quantization) 。例如，K_means 算法可以用于对一张图片进行色调分离:


```python
import scipy as sp
try:
    face = sp.face(gray=True)
except AttributeError:
    from scipy import misc
    face = misc.face(gray=True)
X = face.reshape((-1, 1)) # We need an (n_sample, n_feature) array
k_means = cluster.KMeans(n_clusters=5, n_init=1)
k_means.fit(X) 

values = k_means.cluster_centers_.squeeze()
labels = k_means.labels_
face_compressed = np.choose(labels, values)
face_compressed.shape = face.shape
```

### 分层聚类算法: 谨慎使用

#### [层次聚类](http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html#hierarchical-clustering) （分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种：

**Agglomerative（聚合）** 
- 自底向上的方法: 初始阶段，每一个样本将自己作为单独的一个簇，聚类的簇以最小化距离的标准进行迭代聚合。当感兴趣的簇只有少量的样本时，该方法是很合适的。如果需要聚类的 簇数量很大，该方法比K_means算法的计算效率也更高。 * 

**Divisive（分裂）** 
- 自顶向下的方法: 初始阶段，所有的样本是一个簇，当一个簇下移时，它被迭代的进 行分裂。当估计聚类簇数量较大的数据时，该算法不仅效率低(由于样本始于一个簇，需要被递归的进行 分裂)，而且从统计学的角度来讲也是不合适的。

#### 连接约束聚类

对于逐次聚合聚类，通过连接图可以指定哪些样本可以被聚合在一个簇。在 scikit 中，图由邻接矩阵来表示，通常该矩阵是一个稀疏矩阵。这种表示方法是非常有用的，例如在聚类图像时检索连接区域(有时也被称为连接要素):


```python
import matplotlib.pyplot as plt

from sklearn.feature_extraction.image import grid_to_graph
from sklearn.cluster import AgglomerativeClustering


# #############################################################################
# Generate data
try:  # SciPy >= 0.16 have face in misc
    from scipy.misc import face
    face = face(gray=True)
except ImportError:
    face = sp.face(gray=True)

# Resize it to 10% of the original size to speed up the processing
face = sp.misc.imresize(face, 0.10) / 255.

X = np.reshape(face, (-1, 1))

# #############################################################################
# Define the structure A of the data. Pixels connected to their neighbors.
connectivity = grid_to_graph(*face.shape)

# #############################################################################
```

#### 特征聚集

我们已经知道，稀疏性可以缓解特征维度带来的问题，i.e 即与特征数量相比，样本数量太少。 另一个解决该问题的方式是合并相似的维度：**feature agglomeration（特征聚集）**。该方法可以通过对特征聚类来实现。换 句话说，就是对样本数据转置后进行聚类。

`transform and inverse_transform methods
Some estimators expose a transform method, for instance to reduce the dimensionality of the dataset.`


```python
digits = datasets.load_digits()
images = digits.images
X = np.reshape(images, (len(images), -1))
connectivity = grid_to_graph(*images[0].shape)

agglo = cluster.FeatureAgglomeration(connectivity=connectivity,
                                     n_clusters=32)
agglo.fit(X) 

X_reduced = agglo.transform(X)

X_approx = agglo.inverse_transform(X_reduced)
images_approx = np.reshape(X_approx, images.shape)
```

## 分解: 将一个信号转换成多个成份并且加载¶
### Components and loadings（成分和载荷）
如果 X 是多维数据，那么我们试图解决的问题是在不同的观察基础上对数据进行重写。我们希望学习得到载荷 L 和成分 C 使得 X = L C 。提取成分 C 有多种不同的方法
### 主成份分析: PCA

[主成分分析（PCA）](http://sklearn.apachecn.org/cn/0.19.0/modules/decomposition.html#pca) 将能够解释数据信息最大方差的的连续成分提取出来

[豆瓣](https://www.douban.com/group/topic/104320268/)


```python
print(__doc__)

# Authors: Gael Varoquaux
#          Jaques Grobler
#          Kevin Hughes
# License: BSD 3 clause

from sklearn.decomposition import PCA

from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats


# #############################################################################
# Create the data

e = np.exp(1)
np.random.seed(4)


def pdf(x):
    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)
                  + stats.norm(scale=4 / e).pdf(x))

y = np.random.normal(scale=0.5, size=(30000))
x = np.random.normal(scale=0.5, size=(30000))
z = np.random.normal(scale=0.1, size=len(x))

density = pdf(x) * pdf(y)
pdf_z = pdf(5 * z)

density *= pdf_z

a = x + y
b = 2 * y
c = a - b + z

norm = np.sqrt(a.var() + b.var())
a /= norm
b /= norm


# #############################################################################
# Plot the figures
def plot_figs(fig_num, elev, azim):
    fig = plt.figure(fig_num, figsize=(4, 3))
    plt.clf()
    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)

    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)
    Y = np.c_[a, b, c]

    # Using SciPy's SVD, this would be:
    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)

    pca = PCA(n_components=3)
    pca.fit(Y)
    pca_score = pca.explained_variance_ratio_
    V = pca.components_

    x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score / pca_score.min()

    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T
    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]
    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]
    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]
    x_pca_plane.shape = (2, 2)
    y_pca_plane.shape = (2, 2)
    z_pca_plane.shape = (2, 2)
    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)
    ax.w_xaxis.set_ticklabels([])
    ax.w_yaxis.set_ticklabels([])
    ax.w_zaxis.set_ticklabels([])


elev = -40
azim = -80
plot_figs(1, elev, azim)

elev = 30
azim = 20
plot_figs(2, elev, azim)

plt.show()
```

    
    ================================
    SVM Exercise
    ================================
    
    A tutorial exercise for using different SVM kernels.
    
    This exercise is used in the :ref:`using_kernels_tut` part of the
    :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
    



![png](/images/sklearn/output_80_1.png)



![png](/images/sklearn/output_80_2.png)


上图中样本点的分布在一个方向上是非常平坦的：即三个单变量特征中的任何一个都可以有另外两个特征来表示。主成分分析法(PCA)可以找到使得数据分布不 flat 的矢量方向(可以反映数据主要信息的特征)。
当用主成分分析(PCA)来 transform（转换） 数据时，可以通过在子空间上投影来降低数据的维数。




```python
# Create a signal with only 2 useful dimensions
x1 = np.random.normal(size=100)
x2 = np.random.normal(size=100)
x3 = x1 + x2
X = np.c_[x1, x2, x3]  #把三个np数组连接到一起

from sklearn import decomposition
pca = decomposition.PCA()
pca.fit(X)


print(pca.explained_variance_)  #explained_variance_ 属性可以中查看各个主成分的解释方差：


# As we can see, only the 2 first components are useful
pca.n_components = 2
X_reduced = pca.fit_transform(X)
X_reduced.shape
```

    [  2.96375875e+00   7.23284879e-01   1.13533782e-31]





    (100, 2)



### 独立成分分析: ICA

独立成分分析（ICA） 可以提取数据信息中的独立成分，这些成分载荷的分布包含了最多的 的独立信息。该方法能够恢复 non-Gaussian（非高斯） 独立信号:

[Blind source separation using FastICA](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/decomposition/plot_ica_blind_source_separation.html)


```python
print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from scipy import signal

from sklearn.decomposition import FastICA, PCA

# #############################################################################
# Generate sample data
np.random.seed(0)
n_samples = 2000
time = np.linspace(0, 8, n_samples)

s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal
s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal
s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal

S = np.c_[s1, s2, s3]
S += 0.2 * np.random.normal(size=S.shape)  # Add noise

S /= S.std(axis=0)  # Standardize data
# Mix data
A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix
X = np.dot(S, A.T)  # Generate observations

# Compute ICA
ica = FastICA(n_components=3)
S_ = ica.fit_transform(X)  # Reconstruct signals
A_ = ica.mixing_  # Get estimated mixing matrix

# We can `prove` that the ICA model applies by reverting the unmixing.
assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)

# For comparison, compute PCA
pca = PCA(n_components=3)
H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components

# #############################################################################
# Plot results

plt.figure()

models = [X, S, S_, H]
names = ['Observations (mixed signal)',
         'True Sources',
         'ICA recovered signals',
         'PCA recovered signals']
colors = ['red', 'steelblue', 'orange']

for ii, (model, name) in enumerate(zip(models, names), 1):
    plt.subplot(4, 1, ii)
    plt.title(name)
    for sig, color in zip(model.T, colors):
        plt.plot(sig, color=color)

plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)
plt.show()

```

    
    ================================
    SVM Exercise
    ================================
    
    A tutorial exercise for using different SVM kernels.
    
    This exercise is used in the :ref:`using_kernels_tut` part of the
    :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
    



![png](/images/sklearn/output_84_1.png)



```python
# Generate sample data
import numpy as np
from scipy import signal
time = np.linspace(0, 10, 2000)
s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal
s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal
s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal
S = np.c_[s1, s2, s3]
S += 0.2 * np.random.normal(size=S.shape)  # Add noise
S /= S.std(axis=0)  # Standardize data
# Mix data
A = np.array([[1, 1, 1], [0.5, 2, 1], [1.5, 1, 2]])  # Mixing matrix
X = np.dot(S, A.T)  # Generate observations

# Compute ICA
ica = decomposition.FastICA()
S_ = ica.fit_transform(X)  # Get the estimated sources
A_ = ica.mixing_.T
np.allclose(X,  np.dot(S_, A_) + ica.mean_)
```




    True



# 把它们放在一起 (参考)

## 模型管道化

我们已经知道一些模型可以做数据转换，一些模型可以用来预测变量。我们可以建立一个组合模型同时完成以上工作:


```python
import numpy as np
import matplotlib.pyplot as plt

from sklearn import linear_model, decomposition, datasets
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

logistic = linear_model.LogisticRegression()

pca = decomposition.PCA()
pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])

digits = datasets.load_digits()
X_digits = digits.data
y_digits = digits.target

# Plot the PCA spectrum
pca.fit(X_digits)

plt.figure(1, figsize=(4, 3))
plt.clf()
plt.axes([.2, .2, .7, .7])
plt.plot(pca.explained_variance_, linewidth=2)
plt.axis('tight')
plt.xlabel('n_components')
plt.ylabel('explained_variance_')

# Prediction
n_components = [20, 40, 64]
Cs = np.logspace(-4, 4, 3)

# Parameters of pipelines can be set using ‘__’ separated parameter names:
estimator = GridSearchCV(pipe,
                         dict(pca__n_components=n_components,
                              logistic__C=Cs))
estimator.fit(X_digits, y_digits)

plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
            linestyle=':', label='n_components chosen')
plt.legend(prop=dict(size=12))
plt.show()
```


![png](/images/sklearn/output_88_0.png)


## 用特征面进行人脸识别

该实例用到的数据集来自 LFW_(Labeled Faces in the Wild)。数据已经进行了初步预处理

http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)


```python
"""
===================================================
Faces recognition example using eigenfaces and SVMs
===================================================

The dataset used in this example is a preprocessed excerpt of the
"Labeled Faces in the Wild", aka LFW_:

  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)

.. _LFW: http://vis-www.cs.umass.edu/lfw/

Expected results for the top 5 most represented people in the dataset:

================== ============ ======= ========== =======
                   precision    recall  f1-score   support
================== ============ ======= ========== =======
     Ariel Sharon       0.67      0.92      0.77        13
     Colin Powell       0.75      0.78      0.76        60
  Donald Rumsfeld       0.78      0.67      0.72        27
    George W Bush       0.86      0.86      0.86       146
Gerhard Schroeder       0.76      0.76      0.76        25
      Hugo Chavez       0.67      0.67      0.67        15
       Tony Blair       0.81      0.69      0.75        36

      avg / total       0.80      0.80      0.80       322
================== ============ ======= ========== =======

"""
from __future__ import print_function

from time import time
import logging
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import PCA
from sklearn.svm import SVC


print(__doc__)

# Display progress logs on stdout
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')


# #############################################################################
# Download the data, if not already on disk and load it as numpy arrays

lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

# introspect the images arrays to find the shapes (for plotting)
n_samples, h, w = lfw_people.images.shape

# for machine learning we use the 2 data directly (as relative pixel
# positions info is ignored by this model)
X = lfw_people.data
n_features = X.shape[1]

# the label to predict is the id of the person
y = lfw_people.target
target_names = lfw_people.target_names
n_classes = target_names.shape[0]

print("Total dataset size:")
print("n_samples: %d" % n_samples)
print("n_features: %d" % n_features)
print("n_classes: %d" % n_classes)


# #############################################################################
# Split into a training set and a test set using a stratified k fold

# split into a training and testing set
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42)


# #############################################################################
# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled
# dataset): unsupervised feature extraction / dimensionality reduction
n_components = 150

print("Extracting the top %d eigenfaces from %d faces"
      % (n_components, X_train.shape[0]))
t0 = time()
pca = PCA(n_components=n_components, svd_solver='randomized',
          whiten=True).fit(X_train)
print("done in %0.3fs" % (time() - t0))

eigenfaces = pca.components_.reshape((n_components, h, w))

print("Projecting the input data on the eigenfaces orthonormal basis")
t0 = time()
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("done in %0.3fs" % (time() - t0))


# #############################################################################
# Train a SVM classification model

print("Fitting the classifier to the training set")
t0 = time()
param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)
clf = clf.fit(X_train_pca, y_train)
print("done in %0.3fs" % (time() - t0))
print("Best estimator found by grid search:")
print(clf.best_estimator_)


# #############################################################################
# Quantitative evaluation of the model quality on the test set

print("Predicting people's names on the test set")
t0 = time()
y_pred = clf.predict(X_test_pca)
print("done in %0.3fs" % (time() - t0))

print(classification_report(y_test, y_pred, target_names=target_names))
print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))


# #############################################################################
# Qualitative evaluation of the predictions using matplotlib

def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
    """Helper function to plot a gallery of portraits"""
    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
    for i in range(n_row * n_col):
        plt.subplot(n_row, n_col, i + 1)
        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
        plt.title(titles[i], size=12)
        plt.xticks(())
        plt.yticks(())


# plot the result of the prediction on a portion of the test set

def title(y_pred, y_test, target_names, i):
    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]
    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]
    return 'predicted: %s\ntrue:      %s' % (pred_name, true_name)

prediction_titles = [title(y_pred, y_test, target_names, i)
                     for i in range(y_pred.shape[0])]

plot_gallery(X_test, prediction_titles, h, w)

# plot the gallery of the most significative eigenfaces

eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
plot_gallery(eigenfaces, eigenface_titles, h, w)

plt.show()
```

    Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012
    2018-03-18 15:54:33,510 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012


    
    ===================================================
    Faces recognition example using eigenfaces and SVMs
    ===================================================
    
    The dataset used in this example is a preprocessed excerpt of the
    "Labeled Faces in the Wild", aka LFW_:
    
      http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)
    
    .. _LFW: http://vis-www.cs.umass.edu/lfw/
    
    Expected results for the top 5 most represented people in the dataset:
    
    ================== ============ ======= ========== =======
                       precision    recall  f1-score   support
    ================== ============ ======= ========== =======
         Ariel Sharon       0.67      0.92      0.77        13
         Colin Powell       0.75      0.78      0.76        60
      Donald Rumsfeld       0.78      0.67      0.72        27
        George W Bush       0.86      0.86      0.86       146
    Gerhard Schroeder       0.76      0.76      0.76        25
          Hugo Chavez       0.67      0.67      0.67        15
           Tony Blair       0.81      0.69      0.75        36
    
          avg / total       0.80      0.80      0.80       322
    ================== ============ ======= ========== =======
    
    


    Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009
    2018-03-18 15:54:38,109 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009
    Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006
    2018-03-18 15:54:41,419 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006
    Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015
    2018-03-18 15:54:49,563 Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015


## 开放性问题: 股票市场结构

我们可以预测 Google 在特定时间段内的股价变动吗？

[Learning a graph structure](http://sklearn.apachecn.org/cn/0.19.0/auto_examples/applications/plot_stock_market.html#stock-market)
