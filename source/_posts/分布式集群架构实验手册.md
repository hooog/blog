---
title: Hadoop分布式集群架构实验手册
date: 2018-03-15 11:59:21
update: 
comments: true
categories:
  - Hadoop
tags:
  - Environment
top: 10
---
<!--more-->

本文参考- Machine learning with Scala by CDA 吴昊天

## 一、基础软件准备

　　在进行分布式架构的学习前，首先需进行PC机的基础环境准备，由于Apache Hadoop是基于JAVA语言开发，所要求的系统环境适用于Windows、Linux和Mac OS操作系统。根据其适用程度，我们首推Linux系统或Mac OS系统，而二者相比选择Linux系统适用面更为广泛，因此本次教学我们将利用虚拟机运行Linux系统，并在其上进行分布式架构相关操作教学与实践。同时由于为了更加符合企业应用实际，我们推荐在后期学习过程中，使用模拟终端利用命令行操控虚拟机，并利用开源软件实现本机与虚拟机文件互传。

注：

- 实验手册涉及软件安装均为开源或社区版免费软件；
- 软件下载及安装主要以Windows为例进行讲解，对于Mac OS也可类似进行安装与操作，如未特意说明，二者在下载、安装与使用过程中无明显区别；
- 该实验手册版本为2017年12月版，适用于CDA大数据分析师脱产班（第七期）课程分布式集群架构部分。

### 1.虚拟机软件选择与安装

#### 1.1 主流虚拟机软件介绍与选择

　　就目前而言，VMware Workstation Pro是使用最为广泛、功能最为强大的虚拟机软件，主要用于IT开发和系统管理等商业环境，相比之下，其社区免费版VMware Workstation Player，虽然在近些年的更新中不断增加新的功能，但在功能上仍然有一定的局限。而开源虚拟软件Oracle VM VirtualBox，则在所有免费虚拟机软件中表现较为突出，成为大多数教学、实验等非商业环境中的首选，因此，本次教学将利用Oracle VM VirtualBox进行虚拟机安装。

#### 1.2 Virtual Box下载与安装 

- Virtual Box下载

  从**[Virtual Box官方下载地址](https://www.virtualbox.org/wiki/Downloads)**下载最新版本Virtual Box安装包（以5.2.4为例），官方下载地址同时提供Windows和Mac OS系统对应的版本。

- Virtual Box安装

打开已下载安装程序

![1](G:\屏幕截图\1.jpg)

点击下一步

![2](G:\屏幕截图\2.jpg)

选择安装位置，点击下一步

![3](G:\屏幕截图\3.jpg)

根据自身安装需求进行选择，然后点击下一步

![4](G:\屏幕截图\4.jpg)

点击是，进行后续操作

![5](G:\屏幕截图\5.jpg)

点击安装，开始安装

![6](G:\屏幕截图\6.jpg)

安装完成

### 2.Linux操作系统选择与安装

#### 2.1 Linux系统选择

　　Linux是类Unix操作系统，我们现在所谓的Linux操作系统，从严格意义上来说是指拥有Linux内核的操作系统，其中，Ubuntu和CentOS操作系统分别是桌面端和服务器端占比最高的开源操作系统，而这两个操作系统也将是本次讲学过程中主要涉及的两个操作系统。

- Ubuntu

  拥有人性化的桌面操作界面，除商业用途外，也非常适合PC用户处理日常事务。该系统每隔6个月，也就是每年4月和10月发布一个新版本，一般以年月标注为版本号，如13.04、13.10等，其中，尾注为LTS（Long Term Support）的版本为长期支持版本，一般隔年的4月版本为长期支持版，如12.04 LTS、14.04LTS等，同时标注为Kylin的版本为中文支持版，由中文社区维护。

- CentOS

  Community Enterprise Operating System，中文译为社区企业操作系统，是RHEL（Red Hat Enterprise Linux）源码再编译的操作系统，由于其高稳定性，常用于企业级服务器。新版本CentOS大约每两年发布一次，并且每个新版本会定期（一般为6个月）更新一次。CentOS于2014年宣布加入Red Hat，但仍将保持免费。

- Ubuntu与CentOS对比

  对命令行操作系统不太熟悉的分布式集群架构初学者而言，Ubuntu友善的桌面操作系统将是更好的选择，而对于企业应用来说，更加稳定的CentOS系统无疑是更好的选择，同时抛弃了桌面操作系统的版本在系统资源占用方面也更有优势。本次课程在同时兼顾学习曲线和企业实际应用的情况下，主要将以Ubuntu操作系统为例进行教学，同时提供CentOS操作系统版本的分布式集群搭建的相关说明，以供学员课后自主学习。

#### 2.2 Ubuntu操作系统选择与安装

##### 2.2.1系统选择

- 系统版本选择

  本次教学采用Ubuntu 14.04 LTS Kylin版本，值得注意的是，如果要安装Spark 2.1以上，则最好选择Ubuntu 16.04或更新版本系统。同时，对于内存大于4G的电脑，建议使用64位操作系统。

- 关于Kylin中文版

  对于Ubuntu系统而言，由于有独立中文版本并且有中文社区进行维护，因此系统语言选择并不会对系统造成任何影响，但若采用别的系统，则需要在系统语言选择上多加考虑，如CentOS 7系统中选择中文可能会对HDP安装造成影响。

##### 2.2.2系统安装

　　接下来，进行Ubuntu 14.04 LTS Kylin在Oracle VM VirtualBox中的安装。

- Ubuntu系统下载

  在**[Ubuntu中文社区（优麒麟）官网](http://www.ubuntukylin.com/)**或**[Ubuntu官网下载地址](https://www.ubuntu.com/download)**选择对应系统版本进行下载，本教程采用Ubuntu 14.04 LTS Kylin 64位系统版本。

- Ubuntu系统安装

  - 开启虚拟化

    由于64操作系统的虚拟机必须要求主机CPU开启虚拟化，因此我们在正式安装虚拟机前需检查电脑是否已经开启虚拟化，如果虚拟化未开启，则在VirtualBox中就将无法选择64位操作系统进行虚拟机安装。对于大多数PC机而言，开启虚拟化需要进入BIOS进行设置，不同厂商电脑进入BIOS界面方法不同，此部分请同学们自行根据自身电脑品牌，进入BIOS界面进行设置，此处以HP台式机为例进行说明。

  - 利用VirtualBox搭建虚拟机环境

    打开VirtualBox点击新建虚拟机

    ![1](G:\屏幕截图\安装Ubuntu部分\1.jpg)

    输入虚拟机名称，此处可将系统名称作为虚拟机电脑名称，同时在类型中选择Linux系统，并且选择Ubuntu64位版本，点击下一步

    ![2](G:\屏幕截图\安装Ubuntu部分\2.jpg)

    对于操作系统内存选择，4G内存电脑建议分配1G内存给虚拟机，8G内存电脑则可分配2G给虚拟机，16G及以上则建议分配3G及以上给虚拟机，此处以3G为例进行安装，调整完毕后选择下一步

    ![3](G:\屏幕截图\安装Ubuntu部分\3.jpg)

    接下来，选择现在创建虚拟硬盘（c），点击创建

    ![4](G:\屏幕截图\安装Ubuntu部分\4.jpg)

    接下来，选择VDI（VirtualBox磁盘映像），点击下一步

    ![6](G:\屏幕截图\安装Ubuntu部分\6.jpg)

    虚拟硬盘选择动态分配，点击下一步

    ![7](G:\屏幕截图\安装Ubuntu部分\7.jpg)

    接下来选择虚拟机备份地址，建议放置于磁盘空间较大的分区，同时分配虚拟机可使用的最大硬盘空间，如果仅作为学习使用，则20G足以，如果需要安装HDP，则至少需要40G以上，此处以20G为例进行安装。选择完毕后选择创建，

    ![8](G:\屏幕截图\安装Ubuntu部分\8.jpg)

  - 安装Ubuntu系统

    点击设置

    ![9](G:\屏幕截图\安装Ubuntu部分\9.jpg)

    在设置中选择存储，并点击没有盘片，在属性栏中点击盘片图标

    ![10](G:\屏幕截图\安装Ubuntu部分\10.jpg)

    在弹出的窗口中选择虚拟光盘文件，即下载好的操作系统镜像文件位置，然后点击打开

    ![11](G:\屏幕截图\安装Ubuntu部分\11.jpg)

    完成后自动返回存储界面，将显示已经有光盘镜像文件，点击OK

    ![12](G:\屏幕截图\安装Ubuntu部分\12.jpg)

    在返回的Virtual界面中选择刚才配置好的虚拟机，点击启动

    ![13](G:\屏幕截图\安装Ubuntu部分\13.jpg)

    进入Ubuntu系统安装界面，Ubuntu系统会自动捕捉鼠标，操作起来相对人性化

    ![14](G:\屏幕截图\安装Ubuntu部分\14.jpg)

    选择中文（简体），单机安装Ubuntu Kylin

    ![15](G:\屏幕截图\安装Ubuntu部分\15.jpg)

    更新或安装第三方软件，可根据自身喜好选择，无论选择与否对安装Hadoop集群不会造成太大影响，此处以不选择为例进行安装，点击继续

    ![16](G:\屏幕截图\安装Ubuntu部分\16.jpg)

    安装类型选择其他选项，自行创建、调整分区，点击继续

    ![17](G:\屏幕截图\安装Ubuntu部分\17.jpg)

    点击新建分区表

    ![18](G:\屏幕截图\安装Ubuntu部分\18.jpg)

    在弹出的对话框中选择继续

    ![19](G:\屏幕截图\安装Ubuntu部分\19.jpg)

    创建分区，添加交换空间和根目录，首先选择空闲，然后点击左下方加号

    ![20](G:\屏幕截图\安装Ubuntu部分\20.jpg)

    在弹出的对话框中，选择1024M（1G）作为交换空间，新建分区类型为主分区，新建分区位置为空间起始位置，用于选择交换空间，点击确定

    ![21](G:\屏幕截图\安装Ubuntu部分\21.jpg)

    继续创建根目录，在返回的界面中，点击空闲，再点击左下方加号

    ![22](G:\屏幕截图\安装Ubuntu部分\22.jpg)

    在弹出对话框中，只更改挂载点为/，其他选项不变，点击确定

    ![23](G:\屏幕截图\安装Ubuntu部分\23.jpg)

    然后开始安装，在返回的界面中点击现在安装

    ![24](G:\屏幕截图\安装Ubuntu部分\24.jpg)

     选择时区，保持默认东八区既可，点击继续![25](G:\屏幕截图\安装Ubuntu部分\25.jpg)

    键盘布局保持默认既可，即左右两边都选汉语，点击继续

    ![26](G:\屏幕截图\安装Ubuntu部分\26.jpg)

    设置用户名、计算机名和登录密码，根据可自身喜好进行填写，由于在后续搭建集群时将反复输入账户密码，建议设置短密码。设置完毕后点击继续

    ![27](G:\屏幕截图\安装Ubuntu部分\27.jpg)

    接下来，系统将开始自动安装，请保持网络通畅，并耐心等待

    ![28](G:\屏幕截图\安装Ubuntu部分\28.jpg)

    安装完成后，点击现在重启，以完成安装

    ![29](G:\屏幕截图\安装Ubuntu部分\29.jpg)

    重启时，可能会卡在如下界面

    ![30](G:\屏幕截图\安装Ubuntu部分\30.jpg)

    此时可以自行关闭虚拟机再打开，点击关闭按钮，在弹出对话框中选择强制退出

    ![31](G:\屏幕截图\安装Ubuntu部分\31.jpg)

    再次点击启动按钮，输入账户密码，进入操作系统

    ![32](G:\屏幕截图\安装Ubuntu部分\32.jpg)

    对于弹出的升级操作系统提示，选择不升级

    ![33](G:\屏幕截图\安装Ubuntu部分\33.jpg)

    接下来，安装Ubuntu系统增强功能，以方便后续操作。点击设备，在弹出对话框中选择安装增强功能

    ![34](G:\屏幕截图\安装Ubuntu部分\34.jpg)

    在弹出的提示框中，选择运行（我们能看到，在未安装增强功能前，窗口化的虚拟机操作系统缩放不是很方便）

    ![35](G:\屏幕截图\安装Ubuntu部分\35.jpg)

    输入密码，进行授权

    ![36](G:\屏幕截图\安装Ubuntu部分\36.jpg)

    安装完成后，单机回车退出命令行界面

    ![37](G:\屏幕截图\安装Ubuntu部分\37.jpg)

    接下来，重启虚拟机操作系统，既可体验安装完增强版后的Ubuntu系统了，可以看到，系统支持自动缩放了。

    点击桌面左侧火狐浏览器按钮，检测是否能够正常上网![38](G:\屏幕截图\安装Ubuntu部分\38.jpg)

    至此，Ubuntu安装过程正式完成。



### 3.其他功能软件准备与安装

#### 3.1 模拟终端软件下载与安装

　　主要将用于远程连接主机，在操作集群或者连接服务器时将大幅简化许多操作。

- 基于Windows操作系统

  推荐使用XShell

  从**[Xshell官方下载地址](http://www.netsarang.com/download/down_form.html?code=522)**下载Xshell 5（需选择家庭或学生用户）

  利用Xshell连接虚拟机，以及后续安装的分布式集群。

#### 3.2 图形化SFTP（安全文件传输协议）客户端下载

　　该软件主要用于主机与虚拟机之间文件互传，若采用VMware安装虚拟机，则无需额外软件提供文件互传功能，但若使用VirtualBox或在远程连接服务器时，利用SFTP客户端进行文件传输就显得非常重要。

- 基于Windows操作系统

  推荐使用WinSCP进行安装，在**[官网下载地址](https://winscp.net/eng/download.php)**选择最新版进行下载安装，与一般软件安装过程相同，此处不做赘述。





## 二、Hadoop单机模式与伪分布式的安装与配置

### 1.Hadoop版本选择

　　Hadoop版本主要分为原生Apache Hadoop版和Hadoop商业发行版两种，而其中商业发行版Hadoop多为原生Apache Hadoop的集群模式下的优化版，除少数社区版外，商业发行版大多需要付费使用，且对于初学者而言使用难度较大，因此本教程主要采用Apache Hadoop进行安装与使用方面教学，同时在教程的最后，我们将简单介绍HortonWorks公司的Hadoop发行版HDP（Hortonworks Data Platform）安装使用方法。

　　而对于Apache Hadoop的版本选择，在兼顾实用性及学员机器性能的情况下，考虑安装Hadoop 2.2.0版本。

### 2.Hadoop下载与安装

#### 2.1 Apache Hadoop下载

　　在**[Apache Hadoop官网](http://hadoop.apache.org/)**可下载Hadoop生态部分核心组件，同时也可查阅Hadoop各版本说明及操作指南，以及各版本Hadoop源码，在下载时注意选择镜像站点，推荐在清华站点或者阿里云站点进行下载，在后续的其他核心组件下载安装时也是如此。选择Hadoop 2.2.0进行下载。

#### 2.2 Apache Hadoop安装

##### 2.2.1 Ubuntu中安装Hadoop

- 打开安装好的Ubuntu虚拟机，打开命令行界面（control+alt+t）

  ![1](G:\屏幕截图\Ubuntu安装Hadoop部分\1.jpg)

- 创建Hadoop用户

  首先需创建Hadoop用户，在命令行中输入

  ```shell
  sudo useradd -m hadoop -s /bin/bash
  ```

  ```markdown
  sudo命令
  教程中会大量使用到sudo命令。sudo是ubuntu中一种权限管理机制，管理员可以授权给一些普通用户去执行一些需要root权限执行的操作。当使用sudo命令时，就需要输入您当前用户的密码。
  ```

  ```markdown
  粘贴
  Ubuntu操作系统中粘贴快捷键为control+shift+v，复制为control+shift+c，注意要在设置中开启粘贴功能。
  ```

  为Hadoop用户创建密码

  ```shell
  sudo passwd hadoop
  ```

  按提示输入两次密码

  为Hadoop用户添加管理员权限

  ```shell
  sudo adduser hadoop sudo
  ```

  然后选择注销，重新选择Hadoop用户进行登录，也可直接切换用户

  ![2](G:\屏幕截图\Ubuntu安装Hadoop部分\2.jpg)

- 更新apt

  在命令行中输入

  ```shell
  sudo apt-get update
  ```

  若在更新过程中出现校验和不符的情况，则需要更新软件源，更新软件源在设置-软件和更新中进行设置，在“下载自”中选择阿里云镜像站点后再次尝试

  ![3](G:\屏幕截图\Ubuntu安装Hadoop部分\3.jpg)

- 安装vim

  vim作为vi文本编辑器的增强版，在后续很多情景下都将发挥较大作用，因此建议提前安装，如果没安装vim，也可采用gedit或者vi文本编辑器进行文本编辑

  ```shell
  sudo apt-get install vim
  ```

  若需要确认，输入y即可

  ```markdown
  vim简单操作指南
  vim的常用模式有分为命令模式，插入模式，可视模式，正常模式。本教程中，只需要用到正常模式和插入模式。二者间的切换即可以帮助你完成本指南的学习。
  正常模式
  正常模式主要用来浏览文本内容。一开始打开vim都是正常模式。在任何模式下按下Esc键就可以返回正常模式
  插入编辑模式
  插入编辑模式则用来向文本中添加内容的。在正常模式下，输入i键即可进入插入编辑模式
  退出vim
  如果有利用vim修改任何的文本，一定要记得保存。Esc键退回到正常模式中，然后输入:wq即可保存文本并退出vim
  ```

- 安装SSH

  SSH是一种网络协议，用于计算机之间加密登录，在配置集群过程中必不可少。Ubuntu 默认已安装了 SSH client，此外还需要安装 SSH server

  ```shell
  sudo apt-get install openssh-server
  ```

  尝试使用ssh登录本机

  ```shell
  ssh localhost
  ```

  首次登陆时会有如下提示，先输入密码，然后输入账户密码既可

  ![4](G:\屏幕截图\Ubuntu安装Hadoop部分\4.jpg)

  登录成功字样

  ![5](G:\屏幕截图\Ubuntu安装Hadoop部分\5.jpg)

  在命令行中输入exit既可退出

- 配置SSH无密码登录

  目前登录仍然需要密码，在集群配置过程中将非常麻烦，因此需要配置免密码登录

  在命令行中输入

  ```shell
  cd ~/.ssh/                             # 若没有该目录，请先执行一次ssh localhost
  ssh-keygen -t rsa                      # 会有提示，都按回车就可以
  cat ./id_rsa.pub >> ./authorized_keys  # 加入授权
  ```

  再次使用ssh local命令，此时无需密码既可登录。在进行后续操作前，记得exit退出

- 安装JAVA

  对于运行Hadoop集群而言， Oracle 的 JDK或是 OpenJDK均可，部分新版Hadoop生态组件需要JAVA8支持，此处由于安装Hadoop 2.2.0版本，安装JAVA7既可。

  在命令行中输入

  ```shell
  sudo apt-get install openjdk-7-jre openjdk-7-jdk
  ```

  安装完成后，输入下述命令找到JDK安装路径

  ```shell
  dpkg -L openjdk-7-jdk | grep '/bin/javac'
  ```

  根据提示找到路径，如若出现如下提示

  ![6](G:\屏幕截图\Ubuntu安装Hadoop部分\6.jpg)

  则说明java安装路径为/usr/lib/jvm/java-7-openjdk-i386

  配置JAVA环境，本文中在profile中进行环境变量的配置

  ```shell
  vim ~/.profile
  ```

  在弹出框中输入i，进入编辑模式，并在起始行输入JAVA路径

  ```shell
  export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
  ```

  输入完成后，esc退出编辑模式，并输入:wq保存退出

  在命令行中输入

  ```shell
  source ~/.profile
  ```

  使刚才配置的环境变量生效

- 验证JAVA环境配置成功

  在命令行中输入

  ```shell
  echo $JAVA_HOME                # 检验变量值，打印设置的环境变量路径
  java -version
  $JAVA_HOME/bin/java -version   # 与直接执行 java -version 一样
  ```

  若出现下述字样，则说明JAVA环境变量配置成功

  ![7](G:\屏幕截图\Ubuntu安装Hadoop部分\7.jpg)


- 安装Hadoop 2.2.0

  一般来说，虚拟机/服务中安装文件主要有两种方式，一种是主机或节点间互传，第二种则是在直接在虚拟机/服务器中连接下载点进行下载，由于本次Hadoop安装版本为2.2.0，属于中国区镜像站点已经不再维护版本，而从其他站点下载速度较慢，因此考虑从主机用SFTP软件直接传输安装文件至虚拟机。

  - 更改虚拟机网络设置

    VirtualBox默认虚拟机网络配置为NAT网络，此时虚拟机与主机共享网络，无论是Xshell和Winscp均无法正确获得虚拟机IP地址，因此首先要更改虚拟机网络配置。为了兼顾企业级实用性，此处考虑采用NAT模式和host-only（仅主机）双网卡模式进行配置

    关闭虚拟机，在控制台中选择设置，进入设置界面

    ![8](G:\屏幕截图\Ubuntu安装Hadoop部分\8.jpg)

    在网络中，网卡1保留NAT网络设置

    ![9](G:\屏幕截图\Ubuntu安装Hadoop部分\9.jpg)

    选中网卡2，勾选启用网络连接，同时选择连接方式为仅主机适配器

    ![10](G:\屏幕截图\Ubuntu安装Hadoop部分\10.jpg)

    点击OK，完成配置，并启动虚拟机

  - 查询虚拟机IP

    打开命令行，输入

    ```shell
    ifconfig       
    ```

    ![11](G:\屏幕截图\Ubuntu安装Hadoop部分\11.jpg)

    在eth1中，本机inet网址为192.168.56.101，就以此ip采用WinSCP进行连接

  - 利用WinSCP进行连接

    打开WinSCP，点击新建连接，输入IP、登录账户（hadoop），及登录密码

    ![img](file:///G:/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE/Ubuntu%E5%AE%89%E8%A3%85Hadoop%E9%83%A8%E5%88%86/12.jpg?lastModify=1514714541)

    点击登录，在弹出的提示框中点击是

    ![13](G:\屏幕截图\Ubuntu安装Hadoop部分\13.jpg)

    进入界面，通过拖拉拽既可实现文件传输

    ![15](G:\屏幕截图\Ubuntu安装Hadoop部分\15.jpg)


  - 利用Xshell连接虚拟机

      在后续的分布式集群安装部署过程中，将同时打开多个虚拟机，此时来回切换窗口进行命令行输行不是很方便，同时对于操作服务器时，也不太可能直接利用服务器提供的服务台进行命令行输入，基本也是采用Xshell或其他模拟终端软件进行服务器操作，因此我们需要提前熟悉利用模拟终端软件进行虚拟机控制。

      打开Xshell，点击新建

![16](G:\屏幕截图\Ubuntu安装Hadoop部分\16.jpg)

​         在弹出的对话框中，输入IP、名称

![17](G:\屏幕截图\Ubuntu安装Hadoop部分\17.jpg)

​         选择用户身份验证，输入账户密码

![18](G:\屏幕截图\Ubuntu安装Hadoop部分\18.jpg)

其余可根据自身选好进行配置，如窗口、外观等，配置完成后点击确定，进行连接，在弹出的对话框中选择保存并接受

![19](G:\屏幕截图\Ubuntu安装Hadoop部分\19.jpg)

出现下述对话框则说明连接成功

![20](G:\屏幕截图\Ubuntu安装Hadoop部分\20.jpg)

接下大部分命令行操作均会在Xshell中进行


  - 利用WinSCP将安装文件Hadoop 2.2.0传输至下载文件夹中

    ![21](G:\屏幕截图\Ubuntu安装Hadoop部分\21.jpg)

  - 在Xshell中输入命令，安装Hadoop

    ```shell
    cd ~/下载                                                 # 进入“下载”文件夹
    sudo tar -zxf hadoop-2.2.0.tar.gz -C /usr/local          # 解压到/usr/local中
    cd /usr/local/
    sudo mv ./hadoop-2.2.0/ ./hadoop                         # 将文件夹名改为hadoop
    sudo chown -R hadoop ./hadoop                            # 修改文件权限
    ```

    ![22](G:\屏幕截图\Ubuntu安装Hadoop部分\22.jpg)

  - 配置Hadoop环境变量

    在命令行中输入

    ```shell
    vim ~/.profile
    ```

    在弹出的框体中输入i进入编辑模式，然后输入下述内容

    ```shell
    export HADOOP_HOME=/usr/local/hadoop
    export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
    ```

    ![23](G:\屏幕截图\Ubuntu安装Hadoop部分\23.jpg)

    按esc退出编辑模式，输入:wq保存并退出

    在命令行中输入

    ```shell
    source ~/.profile
    ```

    使之生效。

    **注：在后续环境变量设置中，将不再反复赘述编辑器打开关闭过程，只对需要更改的环境变量内容进行说明。**

  - 检验Hadoop是否安装成功

    在命令行中输入

    ```shell
    hadoop version
    ```

    ![25](G:\屏幕截图\Ubuntu安装Hadoop部分\25.jpg)

    出现版本号则说明安装配置成功

  - 进一步配置Hadoop环境变量

    通常而言，Hadoop安装到此为止就已经结束，但为了避免后续可能出现的bug以及便于使用，此处增加一些Hadoop环境变量，改变后profile中环境变量相关内容汇总如下:

    ```shell
    #JAVA
    export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
    export PATH=$PATH:$JAVA_HOME/bin
    #Hadoop
    export HADOOP_HOME=/usr/local/hadoop
    export PATH=$PATH:$HADOOP_HOME/bin
    export PATH=$PATH:$HADOOP_HOME/sbin
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export HADOOP_OPTS="-DJava.library.path=$HADOOP_HOME/lib"
    export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
    ```

    ![26](G:\屏幕截图\Ubuntu安装Hadoop部分\26.jpg)

### 3.Hadoop单机模式配置与运行

　　Hadoop 默认模式为非分布式模式（本地模式），无需进行其他配置即可运行。非分布式即单 Java 进程，方便进行调试。此处运行部分Hadoop自带MapReduce例子来验证Hadoop能够正常运行，同时体验Hadoop中MapReduce运行命令格式。

在命令行中输入

```shell
cd /usr/local/hadoop            # 进入Hadoop安装文件夹
mkdir ./input                   # 创建输入文件夹
cp ./etc/hadoop/*.xml ./input   # 将配置文件作为输入文件
./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output 'dfs[a-z.]+'           # 运行MapReduce中grep例子，筛选符合正则表达式dfs[a-z]+的单词并统计其出现的次数，并将结果放入output文件夹中
cat ./output/*                  # 查看运行结果
```

出现下述结果则说明运行成功

![27](G:\屏幕截图\Ubuntu安装Hadoop部分\27.jpg)

注：若要重复运行程序，需更改输出文件夹位置，或清空原输出文件夹

删除原文件夹使用命令

```sh
rm -r ./output
```

### 4.Hadoop伪分布式配置与运行

　　Hadoop 可以在单节点上以伪分布式的方式运行，此时Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。Hadoop通过改变其配置文件来更改运行模式，进行伪分布式运行时需要更改core-site.xml、hdfs-site.xml、mapred-site.xml和yarn-site.xml四个配置文件。Hadoop的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。

#### 4.1 修改hadoop-env.sh

```sh
cd /usr/local/hadoop/etc/hadoop                  # 进入hadoop配置文件夹
vim hadoop-env.sh                        
```

在弹出的框体中输入

```sh
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
```

#### 4.2 修改**core-site.xml**文件

在命令行中输入

```shell
vim core-site.xml
```

在弹出的框体中输入配置内容

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

#### 4.3 修改**hdfs-site.xml**文件

在命令行中输入

```shell
vim hdfs-site.xml
```

在弹出的框体中输入配置内容

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
</configuration>
```

#### 4.4 修改mapred-site.xml文件 

在命令行中输入

```shell
cp mapred-site.xml.template mapred-site.xml  # 复制模板
vim mapred-site.xml
```

在弹出的框体中输入配置内容

```xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

#### 4.5 修改yarn-site.xml文件

在命令行中输入

```shell
vim yarn-site.xml
```

在弹出的框体中输入配置内容

```xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
</configuration>
```

#### 4.6启动集群

在运行伪分布式前，需先对NameNode进行格式化，在命令行中输入

```shell
hdfs namenode -format
```

若出现successfully formatted字样，则说明格式化成功

![29](G:\屏幕截图\Ubuntu安装Hadoop部分\29.jpg)

启动hadoop集群

```shell
start-all.sh                   # 一次性启动所有服务

```

或使用以下命令单独启动各项服务

```shell
start-dfs.sh
start-yarn.sh
```

利用jps命令查看是否启动成功，如果出现下述进程，则证明启动成功

![30](G:\屏幕截图\Ubuntu安装Hadoop部分\30.jpg)

启动后，利用Web端口检测集群运行情况

- ResourceManager Web界面

  - 虚拟机中浏览器中输入：http://localhost:8088

  ![31](G:\屏幕截图\Ubuntu安装Hadoop部分\31.jpg)

- NameNode HDFS Web界面

  - 虚拟机中浏览器中输入：http://localhost:50070

  ![32](G:\屏幕截图\Ubuntu安装Hadoop部分\32.jpg)

#### 4.7 测试运行

接下来，测试grep例子在伪分布式集群模式上的运行

- 在HDFS中放置输入文件

  由于伪分布式运行时，数据从HDFS上进行读取，在上传数据至HDFS之前，我们需要在HDFS中创建用户目录。关于HDFS相关操作指令，详见附录一：HDFS基本操作指令

  ```shell
  hdfs dfs -mkdir -p /user/hadoop
  ```

  接着，我们选取hadoop配置文件夹中的xml文件作为输入文件复制到HDFS系统中

  ```shell
  hdfs dfs -mkdir input
  cd /usr/local/hadoop
  hdfs dfs -put ./etc/hadoop/*.xml input
  ```

  查看HDFS中文件

  ```shell
  hdfs dfs -ls input
  ```

- 运行grep例子

  伪分布式运行 MapReduce 作业的方式跟单机模式相同，只是读取文件和输出文件保存均在HDFS中

   ```shell
  hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep input output 'dfs[a-z.]+'
   ```

  运行过程部分如下所示：

  ![33](G:\屏幕截图\Ubuntu安装Hadoop部分\33.jpg)

  查看保存在HDFS中的运行结果

  ```shell
  hdfs dfs -cat output/*
  ```

  运行结果如下所示

  ![34](G:\屏幕截图\Ubuntu安装Hadoop部分\34.jpg)

  当然，我们也可以将结果取回本地

  ```shell
  cd /usr/local/hadoop
  rm -r ./output                          # 先删除本地的 output 文件夹（如果存在）
  hdfs dfs -get output ./output           # 将 HDFS 上的 output 文件夹拷贝到本机
  cat ./output/*
  ```

  注意，Hadoop 运行程序时，输出目录不能存在，否则会提示错误 "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://localhost:9000/user/hadoop/output already exists" ，因此若要再次执行，需要执行如下命令删除 output 文件夹:

  ```shell
  hdfs dfs -rm -r output    # 删除 output 文件夹
  ```

  运行完成后，在8088端口中能看到完成的任务概况

  ![35](G:\屏幕截图\Ubuntu安装Hadoop部分\35.jpg)

## 三、Hadoop生态部分核心组件安装与配置

　　在安装完成伪分布式后，我们还要进一步在现有的虚拟机中安装其他Hadoop生态中的核心组件，以进一步完善集群功能。Hadoop生态中各常用组件及其主要功能一览如下：

| Category                     | Objective  | Description                              |
| ---------------------------- | ---------- | ---------------------------------------- |
| Data Access                  | Pig        | 一种数据流语言和运行环境，适合于使用Hadoop和MapReduce平台来查询大型半结构化数据集。 |
|                              | Hive       | 一个基于Hadoop的数据仓库工具，可以用于对Hadoop文件中的数据集进行数据整理、特殊查询和分析存储。 |
|                              | HCatalog   | 基于Hive的对Hadoop中表和底层数据管理统一管理和服务平台。        |
|                              | Tez        | Tez是Apache开源的支持DAG作业的计算框架，直接源于MapReduce框架，Hortonworks把Tez应用到数据仓库Hive的优化中，使得性能提升了约100倍。 |
|                              | Storm      | 是一个免费、开源的分布式实时计算系统，其对于实时计算的意义类似于Hadoop对于批处理的意义。 |
|                              | HBase      | 是一个提供高可靠性、高性能、可伸缩、实时读写、分布式的列式数据库，一般采用HDFS作为其底层数据存储。 |
|                              | Spark      | 是一个高性能的、集诸多计算功能于一身的分布式计算框架。              |
|                              | Solr       | Solr是一个独立的企业级搜索应用服务器，它对外提供类似于Web-service的API接口。 |
| Data Management              | HDFS       | Hadoop分布式文件系统，是Hadoop两大核心项目之一。           |
|                              | YARN       | 新一代资源调度框架。                               |
| Data Governance and Workflow | Falcon     | 是一个面向Hadoop的、新的数据处理和管理平台,设计用于数据移动、数据管道协调、生命周期管理和数据发现。 |
|                              | Atlas      | 是一个可伸缩和可扩展的核心功能治理服务，企业可以利用它高效的管理 Hadoop 以及整个企业数据生态的集成。 |
|                              | Sqoop      | 用于改进数据的互操作性，主要用来在Hadoop和关系型数据库之间交换数据。    |
|                              | Flume      | 是cloudera提供的一个高可用的、高可靠的、分布式的海量日志采集、聚合和传输的系统。 |
|                              | Kafka      | 是LinkIn公司开发的一种高吞吐量的分布式发布订阅消息系统，在大数据生态中可作为数据交换枢纽，不同类型的分布式系统可统一接入Kafka。 |
| Operations                   | Cloudbreak | 用于在云端部署Hadoop集群。                         |
|                              | Ambari     | 是一种基于Web的工具，支持Apache Hadoop集群的安装、部署、配置和管理。 |
|                              | ZooKeeper  | 是高效可靠的协同工作系统，提供分布式锁之类的服务，用于构建分布式应用，减轻分布式应用程序锁承担的协调任务。 |
|                              | Ooize      | 是一种框架,它让我们可以把多个Map/Reduce作业组合到一个逻辑工作单元中。 |
| Security                     | Ranger     | 提供一个集中式安全管理框架, 并解决授权和审计，它可以对Hadoop生态的组件如HDFS、Yarn、Hive、Hbase等进行细粒度的数据访问控制。 |
|                              | Knox       | 是为了简化和标准化发布和实现安全的 Hadoop 集群,通过集中式的 REST APIs 访问服务。 |
| NoteBook                     | Zeppelin   | 基于Hadoop集群的科学计算平台，提供类Jupyter功能，但远比Jupyter更深层次嵌入Hadoop生态，现已成为Apache顶级项目 |

 本课程中，根据组件功能，我们选取以下组件进行安装：

| Category                     | Objective | releases |
| ---------------------------- | --------- | -------- |
| Operations                   | ZooKeeper |          |
| Data Governance and Workflow | Sqoop     |          |
| Data Access                  | Hive      |          |
|                              | HBase     |          |
|                              | Spark     |          |
| NoteBook                     | Zeppelin  |          |
|                              |           |          |
|                              |           |          |

同时，本部所有组件暂时以单机模式或伪分布式进行安装，在后续部署分布式集群时，再对各组件进行配置修改，以符合集群模式运行要求。

### 1.ZooKeeper安装与配置

####1.1 ZooKeeper下载

　　在**[ZooKeeper官网](http://zookeeper.apache.org/)**可下载最新版本ZooKeeper，并且能够查阅版本改动说明，本次课程采用3.4.6版本进行安装。可以采用WinSCP传输zookeeper-3.4.6.tar至虚拟机“下载”文件夹中，再进行后续安装。

#### 1.2 ZooKeeper安装与配置

- ZooKeeper解压安装

  ```shell
  cd ~/下载                                              # 进入下载文件夹
  sudo tar -zxf zookeeper-3.4.6.tar.gz -C /usr/local    # 安装至/usr/local文件夹内
  cd /usr/local                                         # 进入/usr/local文件夹
  sudo mv ./zookeeper-3.4.6/ ./zookeeper                # 更名为zookeeper
  sudo chown -R hadoop ./zookeeper                      # 修改zookeeper权限
  ```

- 修改ZooKeeper配置

  ```shell
  cd /usr/local/zookeeper/conf                          # 进入ZooKeeper配置文件目录
  sudo cp zoo_sample.cfg zoo.cfg                        # 复制配置样例文件为正式配置文件
  vim zoo.cfg                                           # 进入配置文件
  ```

  关键参数解读：

  - tickTime 
    时长单位为毫秒，为zk使用的基本时间度量单位。例如，1 * tickTime是客户端与zk服务端的心跳时间，2 * tickTime是客户端会话的超时时间。 
    tickTime的默认值为2000毫秒，更低的tickTime值可以更快地发现超时问题，但也会导致更高的网络流量（心跳消息）和更高的CPU使用率（会话的跟踪处理）。
  - clientPort 
    zk服务进程监听的TCP端口，默认情况下，服务端会监听2181端口。
  - dataDir 
    用于配置存储快照文件的目录。如果没有配置dataLogDir，那么事务日志也会存储在此目录。默认配置为临时文件。

  在运行单机模式进行测试时，使用默认配置既可

- 配置ZooKeeper环境变量

  ```shell
  vim ~/.profile
  ```

  添加相关路径

  ```sh
  #ZooKeeper
  export ZOOKEEPER_HOME=/usr/local/zookeeper 
  export PATH=$PATH:$ZOOKEEPER_HOME/bin
  ```

  并使之生效

  ```sh
  source ~/.profile
  ```


- 启动ZooKeeper

  ```shell
  cd /usr/local/zookeeper                          # 进入zookeeper安装文件夹
  ./bin/zkServer.sh start                          # 启动zookeeper
  ```

  输入启动命令后，出现STARTED字样则说明启动成功

  ![1](G:\屏幕截图\其他组件安装部分\1.jpg)

  zookeeper正常启动后，能在jps中查看到QuorumPeerMain进程

  ![2](G:\屏幕截图\其他组件安装部分\2.jpg)

### 2.Spark安装与配置

#### 2.1 Scala下载与安装

　　在安装Spark前，我们需先安装Scala，在可**[Scala官网](http://www.scala-lang.org/)**下载最新版本Scala，并且能够查阅版本改动说明，本次课程采用2.10.4版本进行安装。可以采用WinSCP传输scala-2.10.4至虚拟机“下载”文件夹中，再进行后续安装。

- Spark解压安装

  ```shell
  cd ~/下载                                             # 进入下载文件夹
  sudo tar -zxf scala-2.10.4.tgz -C /usr/local          # 安装至/usr/local文件夹内
  cd /usr/local                                         # 进入/usr/local文件夹
  sudo mv ./scala-2.10.4/ ./scala                       # 更名为scala
  sudo chown -R hadoop ./scala                          # 修改scala权限
  ```

- 添加环境变量

  ```shell
  vim ~/.profile
  ```

  添加scala安装路径

  ```shell
  #scala
  export SCALA_HOME=/usr/local/scala
  export PATH=$PATH:$SCALA_HOME/bin
  ```

  并保存修改

  ```shell
  source ~/.profile
  ```

  尝试启动scala

  ```shell
  scala
  ```

  能进入scala的shell界面，则说明安装成功，输入

  ```scala
  exit
  ```

  退出交互界面。

#### 2.2 Spark下载

　　在**[Spark官网](http://spark.apache.org/)**可下载最新版本Spark，并且能够查阅版本改动说明，本次课程采用1.4.0版本进行安装。可以采用WinSCP传输spark-1.4.0-bin-hadoop2.4.tgz至虚拟机“下载”文件夹中，再进行后续安装。值得注意的是， 接在Spark版本号后面的是对应推荐的Hadoop运行版本，在企业实际应用中，如果Spark版本和对应hadoop版本不匹配，推荐安装`Pre-build with user-provided Hadoop `的Spark版本。

#### 2.3 Spark安装与配置

- Spark解压安装

  ```shell
  cd ~/下载                                              # 进入下载文件夹
  sudo tar -zxf spark-1.4.0-bin-hadoop2.4.tgz -C /usr/local   # 安装至/usr/local文件夹内
  cd /usr/local                                         # 进入/usr/local文件夹
  sudo mv ./spark-1.4.0-bin-hadoop2.4/ ./spark          # 更名为spark
  sudo chown -R hadoop ./spark                          # 修改sqoop权限
  ```

- 添加环境变量

  ```shell
  vim ~/.profile
  ```

  添加spark安装路径

  ```shell
  #spark
  export SPARK_HOME=/usr/local/spark
  export PATH=$PATH:$SPARK_HOME/bin
  ```

  并保存修改

  ```shell
  source ~/.profile
  ```

- 修改Spark配置

  ```shell
  cd /usr/local/spark/conf                              # 进入spark配置文件夹
  sudo cp spark-env.sh.template spark-env.sh            # 复制spark-env临时文件为配置文件
  vim spark-env.sh                                      # 编辑spark配置文件
  ```

  添加下述配置信息

  ```sh
  export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
  ```

  有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。在伪分布式模式下仅测试是否安装成功时，其他配置暂时可不做修改。

- 测试运行

  运行Spark自带示例，检测是否安装成功

  ```shell
  run-example SparkPi 2>&1 | grep "Pi is"                # 尝试运行示例Pi，并抓取结果
  ```

  ![14](G:\屏幕截图\其他组件安装部分\14.jpg)

  同时，可尝试运行Spark-shell

  ```shell
  spark-shell                                # 进入后可输入exit退出
  ```

  若想过滤每次启动的INFO信息，可进行以下设置

  ```shell
  cd /usr/local/spark/conf                   # 进入配置文件夹
  cp log4j.properties.template log4j.properties
  vim log4j.properties
  ```

  将第二行中的INFO改成WARN，保存并退出

  ![15](G:\屏幕截图\其他组件安装部分\15.jpg)

  再次进入spark-shell，则不会显示INFO信息了。



### 3.HBase安装与配置

#### 3.1HBase下载

　　在**[HBase官网](http://hbase.apache.org/)**可下载最新版本HBase，并且能够查阅版本改动说明，本次课程采用0.98.13版本进行安装。可以采用WinSCP传输hbase-0.98.13-hadoop2-bin.tar至虚拟机“下载”文件夹中，再进行后续安装。

#### 3.2 安装与配置

- HBase解压安装

  ```shell
  cd ~/下载                                              # 进入下载文件夹
  sudo tar -zxf hbase-0.98.13-hadoop2-bin.tar.gz -C /usr/local    # 安装至/usr/local文件夹内
  cd /usr/local                                         # 进入/usr/local文件夹
  sudo mv ./hbase-0.98.13-hadoop2/ ./hbase              # 更名为hbase
  sudo chown -R hadoop ./hbase                          # 修改hbase权限
  ```

- 修改HBase相关配置

  - 添加HBase安装路径至系统环境变量

    ```sh
    vim ~/.profile
    ```

    添加下述路径

    ```shell
    #HBase
    export HBASE_HOME=/usr/local/hbase
    export PATH=$PATH:$HBASE_HOME/bin
    ```

    并使之生效

    ```shell
    source ~/.profile
    ```

    进而检测HBase是否安装成功

    ```shell
    hbase version
    ```

    若出现如下字样，则说明安装成功

    ![3](G:\屏幕截图\其他组件安装部分\3.jpg)

  - 采用默认配置，测试HBase是否正常运行

    启动Hadoop集群

    ```shell
    start-all.sh                                    # 如集群已启动，则忽略此步
    start-hbase.sh                                  # 启动HBase
    hbase shell                                     # 进入hbase的shell交互界面
    ```

    出现下述情况，则说明启动成功

    ![4](G:\屏幕截图\其他组件安装部分\4.jpg)

    在shell界面中，输入help可查看HBase相关命令，输入exit则可退出。关于HBase的数据基本操作，详见附录三。

  -  停止运行hbase

    ```shell
    stop-hbase.sh
    ```

    ​

### 4.Hive安装与配置

#### 4.1Hive下载

　　在**[Hive官网](https://hive.apache.org/)**可下载最新版本Hive，并且能够查阅版本改动说明，本次课程采用2.1.1版本进行安装。可以采用WinSCP传输apache-hive-2.1.1-bin.tar至虚拟机“下载”文件夹中，再进行后续安装。

#### 4.2 安装与配置

- Hive解压安装

  ```shell
  cd ~/下载                                              # 进入下载文件夹
  sudo tar -zxf apache-hive-2.1.1-bin.tar.gz -C /usr/local    # 安装至/usr/local文件夹内
  cd /usr/local                                         # 进入/usr/local文件夹
  sudo mv ./apache-hive-2.1.1-bin/ ./hive               # 更名为hive
  sudo chown -R hadoop ./hive                           # 修改hive权限
  ```

- 安装MySQL

    由于Hive需要数据库存储元数据（用来存储数据库中数据主属性信息），且只有以MySQL为元数据库时才能开启分布式，因此我们需要在Ubuntu中安装MySQL。

  - 利用apt-get安装

    ```shell
    sudo apt-get update                          #更新软件源
    sudo apt-get install mysql-server            #安装mysql
    ```

    安装过程会提示设置MySQL中root用户密码，根据自身喜好设置既可

  - 启动MySQL

    ```shell
    service mysql start                       # 关闭MySQL把start改成stop既可
    ```

    使用下述命令验证MySQL是否成功启动

    ```shell
    sudo netstat -tap | grep mysql
    ```

    端口处于listen状态则说明启动成功

    ![11](G:\屏幕截图\其他组件安装部分\11.jpg)

  - 进入MySQL Shell界面

    ```shell
    mysql -u root -p
    ```

    然后输入刚才设置的root密码既可

  -   更改配置

    在默认状态下，是无法利用Sqoop向MySQL中导入中文的，在mysql交互界面输入

    ```mysql
    show variables like "char%";
    ```

    ![12](G:\屏幕截图\其他组件安装部分\12.jpg)

    可以看到，character_set_server默认是latin1

    因此需要修改部分配置，首先在MySQL交互界面输入

    ```mysql
    exit;
    ```

    退出，然后进入配置文件编辑环境

    ```shell
    cd /etc/mysql/                                     # 进入配置文件夹
    sudo vim my.cnf                                    # 修改配置文件
    ```

    Basic Settings中加入一行

    ```sh
    character_set_server=utf8
    ```

    保存退出，并重启MySQL服务

    ```shell
    service mysql restart
    ```

    登录MySQL

    ```shell
    mysql -u root -p
    ```

    查看当前设置编码

    ```mysql
    show variables like "char%";
    ```

    ![13](G:\屏幕截图\其他组件安装部分\13.jpg)

    已更改成功，退出MySQL，关闭MySQL服务

    ```mysql
    exit;
    ```

    ```shell
    service mysql stop
    ```

    ​

- 修改Hive相关配置

  - 添加Hive安装路径至系统环境变量

    ```sh
    vim ~/.profile
    ```

    添加下述路径

    ```shell
    #Hive
    export HIVE_HOME=/usr/local/hive
    export PATH=$PATH:$HIVE_HOME/bin
    ```

    并使之生效

    ```shell
    source ~/.profile
    ```

  - 修改配置文件

    ```shell
    cd /usr/local/hive/conf                              # 进入配置文件夹
    cp hive-default.xml.template hive-default.xml        # 复制配置临时文件为配置文件
    vim hive-site.xml                                 # 创建并修改其他配置文件
    ```

    写入下述配置信息

    ```xml
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
        <description>JDBC connect string for a JDBC metastore</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
        <description>username to use against metastore database</description>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>hive</value>
        <description>password to use against metastore database</description>
      </property>
    </configuration>
    ```

    ​

  - 配置MySQL为元数据库

    拷贝JDBC包至HIVE_HOME/lib中，JDBC包的下载参考前述部分

    ```shell
    cd ~/下载
    cp mysql-connector-java-5.1.26-bin.jar /usr/local/hive/lib
    ```

    启动MySQL服务，并登录

    ```shell
     service mysql start                    # 启动mysql服务
     mysql -u root -p                       # 登陆shell界面
    ```

    新建hive数据库，来保存元数据

    ```mysql
    create database hive;    # 此hive数据库与hive-site.xml中localhost:3306/hive的hive对应，用来保存hive元数据
    ```

    配置MySQL允许hive接入

    ```mysql
    grant all on *.* to hive@localhost identified by 'hive';   # 将所有数据库的所有表的所有权限赋给hive用户，后面的hive是配置hive-site.xml中配置的连接密码
    flush privileges;          # 刷新mysql系统权限关系表
    exit;                      # 退出MySQL
    ```

    启动hive

    ```shell
    start-all.sh                                 # 先启动集群
    schematool -dbType mysql -initSchema         # 首次启动hive2，需要初始化元数据库
    hive                                         # 启动hive
    ```

    进入hive界面，则说明安装成功，同时，输入

    ```mysql
    exit;
    ```

    既可退出shell界面。

### 5.Sqoop安装与配置

#### 5.1 Sqoop下载

　　在**[Sqoop官网](http://sqoop.apache.org/)**可下载最新版本sqoop，并且能够查阅版本改动说明，本次课程采用1.4.6版本进行安装。可以采用WinSCP传输sqoopr-1.4.6.tar至虚拟机“下载”文件夹中，再进行后续安装。也可尝试使用wget命令直接在官网进行下载，使用命令如下：

```shell
wget https://mirrors.tuna.tsinghua.edu.cn/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
```

#### 5.2 Sqoop安装与配置

- Sqoop解压安装

  ```shell
  cd ~/下载                                              # 进入下载文件夹
  sudo tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /usr/local    # 安装至/usr/local文件夹内
  cd /usr/local                                         # 进入/usr/local文件夹
  sudo mv ./sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ ./sqoop                   # 更名为sqoop
  sudo chown -R hadoop ./sqoop                          # 修改sqoop权限
  ```

- 修改Sqoop配置

  ```shell
  cd /usr/local/sqoop/conf                              # 进入sqoop配置文件夹
  sudo cp sqoop-env-template.sh sqoop-env.sh            # 复制sqoop-env临时文件为配置文件
  vim sqoop-env.sh                                      # 编辑sqoop配置文件
  ```

  对文件中下述路径进行修改

  ```sh
  #Set path to where bin/hadoop is available
  export HADOOP_COMMON_HOME=/usr/local/hadoop

  #Set path to where hadoop-*-core.jar is available
  export HADOOP_MAPRED_HOME=/usr/local/hadoop

  #set the path to where bin/hbase is available
  export HBASE_HOME=/usr/local/hbase

  #Set the path to where bin/hive is available
  export HIVE_HOME=/usr/local/hive

  #Set the path for where zookeper config dir is
  export ZOOCFGDIR=/usr/local/zookeeper
  ```

  配置环境变量

  ```shell
  vim ~/.profile
  ```

  添加下述路径

  ```shell
  export SQOOP_HOME=/usr/local/sqoop
  export PATH=$PATH:$SBT_HOME/bin:$SQOOP_HOME/bin
  export CLASSPATH=$CLASSPATH:$SQOOP_HOME/lib
  ```

  并使之生效

  ```shell
  source ~/.bashrc
  ```

  由于Sqoop采用JDBC进行数据传输，因此我们还需要MySQL驱动包拷贝到Sqoop安装文件中的lib文件夹内。首先利用WinSCP将准备好的`mysql-connector-java-5.1.38`文件拷贝至下载文件夹，也可在**[MySQL官网对应位置](https://www.mysql.com/products/connector/)**进行下载，若是官网下载，则还需要先解压，后拷贝jar包

  ```shell
  cd ~/下载
  sudo cp mysql-connector-java-5.1.26-bin.jar /usr/local/sqoop/lib/  # 拷贝jar包至Sqoop中lib文件夹内
  ```

  接下来尝试与MySQL连接

  ```shell
  service mysql start
  sqoop list-databases --connect jdbc:mysql://127.0.0.1:3306/ --username root -P
  ```

  出现databases的list则说明配置安装成功

  ![19](G:\屏幕截图\其他组件安装部分\19.jpg)

### 6.Zeppelin安装与配置

#### 6.1 Zeppelin下载

　　在**[Zeppelin官网](http://zeppelin.apache.org/)**可下载最新版本Zeppelin，并且能够查阅版本改动说明，本次课程采用0.6.0版本进行安装。可以采用WinSCP传输zeppelin-0.6.0-bin-all至虚拟机“下载”文件夹中，再进行后续安装。

#### 6.2 Zeppelin安装与配置

- Zeppelin解压安装

  ```shell
  cd ~/下载                                                # 进入下载文件夹
  sudo tar -zxf zeppelin-0.6.0-bin-all.tgz -C /usr/local   # 安装至/usr/local文件夹内
  cd /usr/local                                            # 进入/usr/local文件夹
  sudo mv ./zeppelin-0.6.0-bin-all/ ./zeppelin             # 更名为zeppelin
  sudo chown -R hadoop ./zeppelin                          # 修改zeppelin权限
  ```

- 修改Zeppelin-env配置

  ```shell
  cd /usr/local/zeppelin/conf                       # 进入zeppelin配置文件夹
  cp zeppelin-env.sh.template zeppelin-env.sh       # 复制zeppelin-env临时文件为配置文件
  vim zeppelin-env.sh                               # 编辑zeppelin配置文件
  ```

  对文件中下述内容进行修改

  ```sh
  #java
  export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
  export PATH=$PATH:$JAVA_HOME/bin
  #hadoop
  export HADOOP_HOME=/usr/local/hadoop
  export HADOOP_CONF_DIF=/usr/local/hadoop/etc/hadoop
  #spark
  export SPARK_HOME=/usr/local/spark
  #hive
  export HIVE_HOME=/usr/local/hive
  #python
  export PYSPARK_PYTHON=/usr/bin/python
  export PYTHON_HOME=/usr/bin/python
  #Scala
  export SCALA_HOME=/usr/local/scala
  export PATH=$PATH:$SCALA_HOME/bin
  ```

  保存并退出

- 修改Zeppelin-site配置

  ```shell
  cd /usr/local/zeppelin/conf                       # 进入zeppelin配置文件夹
  cp zeppelin-site.xml.template zeppelin-site.xml   # 复制zeppelin-site临时文件为配置文件
  vim zeppelin-site.xml                             # 编辑zeppelin配置文件
  ```

  此处暂时只修改端口，避免冲突

  ```xml
  <property>
    <name>zeppelin.server.port</name>
    <value>18080</value>
    <description>Server port.</description>
  </property>
  ```

- 测试启动Zeppelin

  ```shell
  start-all.sh                                       # 启动hadoop相关进程
  /usr/local/spark/sbin/start-all.sh                 # 启动spark相关进程
  /usr/local/zeppelin/bin/zeppelin-daemon.sh start   # 启动Zeppelin
  ```

  ![16](G:\屏幕截图\其他组件安装部分\16.jpg)

  然后在虚拟机中打开火狐浏览器，输入http://local:18080查看Zeppelin界面

  ![19](G:\屏幕截图\其他组件安装部分\19.jpg)

    创建Test Note 1测试伪分布式模式下Zeppelin能否正常工作

  ![20](G:\屏幕截图\其他组件安装部分\20.jpg)

  点击创建，进入Note界面，在默认Spark的命令行中测试能否正常使用

  ```scala
  sc.master
  ```

  若能正常显示结果，则说明能安装配置成功

  ![21](G:\屏幕截图\其他组件安装部分\21.jpg)

## 四、Hadoop分布式集群安装与配置

　　接下来，正式进入集群安装模式，为了减少工作量，我们将在之前配置好的伪分布式虚拟机的基础上，利用复制克隆虚拟机的方式进行分布式集群的安装部署。同样，我们将在附录中给出基于CentOS系统分布式集群安装部署方法，此处仍然以Ubuntu系统为例进行安装。

### 1.复制虚拟机

打开VirtualBox，在虚拟机关闭状态下，右键选中之前安装好的伪分布式虚拟机Ubuntu，选择复制

![1](G:\屏幕截图\分布式集群安装部分\1.jpg)

输入复制后的虚拟机名称，接下来要复制三台虚拟机用作分布式集群搭建，此处以slave1位例，后面二者操作类似。最好勾选重置MAC地址，然后点击下一步。

![2](G:\屏幕截图\分布式集群安装部分\2.jpg)

选择完全复制，点击复制，等待复制完成

![3](G:\屏幕截图\分布式集群安装部分\3.jpg)

复制完成后，先进入slave1进行相关设置，选择slave1虚拟机，点击启动

![4](G:\屏幕截图\分布式集群安装部分\4.jpg)

### 2.设置slave1虚拟机

启动虚拟机后，仍然以hadoop用户进行登录。

#### 2.1 配置虚拟机网络

在虚拟机内进入命令行

```shell
sudo vim /etc/network/interfaces
```

进入编辑模式，输入下述内容

```sh
# NAT interface
auto eth0
iface eth0 inet dhcp                         # 网卡1是NAT模式，采用动态分配方式自动获取ip

# host only interface
auto eth1
iface eth1 inet static                       # 网卡2是仅主机模式，用于建立内部网络，设置静态IP
address         192.168.56.102               # 注意，此处从102开始，101分配给master，100分配给伪分布式
netmask         255.255.255.0
network         192.168.56.0
broadcast       192.168.56.255
```

保存并退出

#### 2.2 设置hostname

```shell
sudo vim /etc/hostname
```

在弹出的对话框中删除原有内容，输入slave1，保存并退出

![5](G:\屏幕截图\分布式集群安装部分\5.jpg)

#### 2.3 设置hosts映射

```sh
sudo vim /etc/hosts
```

保持文件内原有内容**不变**，另起一行输入下述内容

```sh
192.168.56.101     master
192.168.56.102     slave1
192.168.56.103     slave2
```

![6](G:\屏幕截图\分布式集群安装部分\6.jpg)

保存并退出。

#### 2.4 更改Hadoop相关配置

```shell
cd /usr/local/hadoop/etc/hadoop                      # 进入配置文件夹
```

- 更改core-site.xml相关配置

  ```shell
  vim core-site.xml
  ```

  将localhost更改为master

  ```xml
  <configuration>
      <property>
          <name>hadoop.tmp.dir</name>
          <value>file:/usr/local/hadoop/tmp</value>
          <description>Abase for other temporary directories.</description>
      </property>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://master:9000</value>
      </property>
  </configuration>
  ```

- 更改hdfs-site.xml相关配置

  ```shell
  vim hdfs-site.xml
  ```

  进行以下更改

  ```xml
  <configuration>
          <property>
                  <name>dfs.namenode.secondary.http-address</name>
                  <value>master:50090</value>
          </property>
          <property>
                  <name>dfs.replication</name>
                  <value>2</value>
          </property>
          <property>
                  <name>dfs.namenode.name.dir</name>
                  <value>file:/usr/local/hadoop/tmp/dfs/name</value>
          </property>
          <property>
                  <name>dfs.datanode.data.dir</name>
                  <value>file:/usr/local/hadoop/tmp/dfs/data</value>
          </property>
  </configuration>
  ```

- 更改mapred-site.xml相关配置

  ```shell
  vim mapred-site.xml
  ```

  进行以下更改

  ```xml
  <configuration>
          <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
          </property>
          <property>
                  <name>mapreduce.jobhistory.address</name>
                  <value>master:10020</value>
          </property>
          <property>
                  <name>mapreduce.jobhistory.webapp.address</name>
                  <value>master:19888</value>
          </property>
  </configuration>
  ```

- 更改yarn-site.xml相关配置

  ```shell
  vim yarn-site.xml
  ```

  进行以下更改

  ```xml
  <configuration>
          <property>
                  <name>yarn.resourcemanager.hostname</name>
                  <value>master</value>
          </property>
          <property>
                  <name>yarn.nodemanager.aux-services</name>
                  <value>mapreduce_shuffle</value>
          </property>
  </configuration>
  ```

- 更改slaves相关配置

  ```shell
  vim slaves
  ```

  在此文件中填写作为slave节点的host名称

  删除原有内容，输入

  ```sh
  slave1
  slave2
  ```

  保存并退出

  ![7](G:\屏幕截图\分布式集群安装部分\7.jpg)

#### 2.5 更改Spark相关配置

- 修改spark-env.sh相关配置

  ```shell
  cd /usr/local/spark/conf                       # 进入Spark配置文件夹
  vim spark-env.sh
  ```

  添加下列内容

  ```sh
  #java
  export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
  #scala
  export SCALA_HOME=/usr/local/scala
  #hadoop
  export HADOOP_HOME=/usr/local/hadoop
  export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
  export YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop
  #hive
  export HIVE_HOME=/usr/local/hive
  export HIVE_CONF_DIR=/usr/local/hive/conf
  #spark
  export SPARK_HOME=/usr/local/spark
  export SPARK_LOCAL_DIRS=/usr/local/spark
  export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
  export SPARK_WORKER_CORES=1
  export SPARK_WORKER_INSTANCES=1 
  export SPARK_WORKER_MEMORY=1g
  export SPARK_MASTER_IP=master
  export SPARK_LIBRARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native
  ```

- 修改slaves文件

  ```shell
  cp slaves.template slaves
  vim slaves
  ```

  和Hadoop中slaves设置一样，输入作为slaves的节点名称

  ```sh
  slave1
  slave2
  ```

  保存并退出

#### 2.6 更改ZooKeeper相关配置

- 修改zoo.cfg文件

  ```shell
  cd /usr/local/zookeeper/conf
  vim zoo.cfg
  ```

  修改dataDir指向

  ```sh
  dataDir=/usr/local/zookeeper/data
  ```

  在配置文件最低端增加各节点服务标识

  ```sh
  server.0=master:2888:3888  
  server.1=slave1:2888:3888  
  server.2=slave2:2888:3888
  ```

  其中，server.x为服务机器标识，2888和3888为端口号。保存并退出

- 创建指向文件夹及标识文件

  ```
  cd /usr/local/zookeeper
  mkdir data
  cd data
  vim myid
  ```

  在打开文本中输入1，用来标识本机服务

#### 2.7 更改HBase相关配置

- 修改hbase-env.sh文件

  ```shell
  cd /usr/local/hbase/conf
  vim hbase-env.sh
  ```

  添加下述内容

  ```sh
  #java
  export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-i386
  #spark
  export SPARK_HOME=/usr/local/spark
  #zookeeper
  export HBASE_MANAGERS_ZK=ture
  ```

- 修改hbase-site.xml文件

  ```shell
  vim hbase-site.xml
  ```

  添加下述内容

  ```xml
  <property>  
     <name>hbase.rootdir</name>  
     <value>hdfs://master:9000/hbase</value>  
  </property>  
  <property>  
      <name>hbase.cluster.distributed</name>  
      <value>true</value> 
  </property>  
  <property>  
      <name>hbase.zookeeper.quorum</name>  
      <value>master</value>  
  </property>  
  <property>  
       <name>dfs.replication</name>  
       <value>2</value>  
  </property>
  ```

####2.8 其他组件更改

- sqoop

  对于sqoop而言，只需要在NameNode上安装即可，此处暂不做修改

- hive

  只需要修改site中MySQL连接IP即可

  ```shell
  cd /usr/local/hive/conf
  vim hive-site.xml
  ```

  将localhost改为master

  ```xml
  <configuration>
    <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true</value>
      <description>JDBC connect string for a JDBC metastore</description>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
      <description>Driver class name for a JDBC metastore</description>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>hive</value>
      <description>username to use against metastore database</description>
    </property>
    <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value>hive</value>
      <description>password to use against metastore database</description>
    </property>
  </configuration>
  ```

- zeppelin

  需要在zeppelin-env.sh文件中添加master路径

  ```shell
  cd /usr/local/zeppelin/conf
  vim zeppelin-env.sh
  ```

  加入下述内容

  ```sh
  export MASTER=spark://192.168.56.101:7077
  ```

### 3.复制slave1

关闭虚拟机，安装前述方法复制slave1，分别命名为master和slave2，然后分别打开复制好的两个虚拟机，更改其hostname和interface，更改完毕后重启这两个虚拟机，并打开之前配好的slave1，准备建立节点间免密登录。同时，记得更改myid本机标识，master为0，slave2为2.

###4.配置各节点免密码登录

利用Xshell连接三台虚拟机，然后再Xshell中继续进行操作。

![8](G:\屏幕截图\分布式集群安装部分\8.jpg)

- 检查节点间通信

  在主节点上输入

  ```shell
  ping slave1
  ping slave2
  ```

  在其他节点上进行类似操作，使用control+c断开连接检测

- 检查主节点公钥

  在主节点上输入

  ```shell
  cd ~/.ssh
  ll
  ```

  若有id_rsa（私钥）及id_rsa.pub（公钥），则继续进行下一步操作（之前配置过ssh，因此公私钥已经生成）

- 将主节点公钥传输至两个子节点中

  ```shell
  scp ~/.ssh/id_rsa.pub hadoop@slave1:~/下载
  scp ~/.ssh/id_rsa.pub hadoop@slave2:~/下载
  ```

-  在子节点中将主节点公钥加入授权

  分别在两个子节点中进行操作

  ```shell
  cat ~/下载/id_rsa.pub >> ~/.ssh/authorized_keys
  rm -rf ~/下载/id_rsa.pub                            # 用完即删
  ```

- 测试主节点能否ssh连接子节点

  在主节点中输入

  ```shell
  ssh slave1
  exit
  ssh slave2 
  exit
  ```

### 5.启动集群

至此，全部配置完毕，准备启动集群，并检测集群是否能正常运行。

在主节点上执行

```shell
hdfs namenode -format
start-all.sh
```

查看各节点jps，是否正常运行

接着测试Spark集群，在主节点上输入

```shell
/usr/local/spark/sbin/start-all.sh
```

接着进入hive和HBase交互界面，测试能否正常运行



##附录一：Linux基本操作指令

###1、查看当前目录下的内容：ls

```shell
ls
ls -l
ls -l -a                                      # 查看隐藏文件/夹
```

###2、进入到指令的文件夹内：cd

   说明：$符号前是~，说明当前是位于用户主目录下(/home/hduser)

  ```shell
cd /		                                   # 进入到文件系统根目录
cd usr                                         # 进入到当前目录下的usr目录(相对目录)
cd /home/hduser                                # 进入到用户主目录(以/开头，是绝对目录)
cd ..	                                       # 进入到当前目录的上一级目录
cd ~		                                   # 进入到用户主目录
  ```

### 3.创建文件夹：mkdir

```shell
mkdir a	                                       # 创建文件夹a
```

###4.修改文件夹：mv

 ```shell
mv a b                                          # 更名a为b
 ```

###5.删除文件夹：rm

```shell
rm -r b                                         # 删除文件夹b                      
```

###6.创建文件：touch

```shell
touch a.txt                                      # 创建txt格式文件a
```

###7.编辑文件：gedit

 ```shell
gedit a.txt                                       # 编辑文件a
 ```

###8.查看文件内容：cat

 ```shell
cat a.txt 
 ```

###9.拷贝文件或文件夹：cp

```shell
touch a.data 
cp a.data b.data
```

###11.清屏命令：clear

## 附录二：HDFS基本操作指令

### 1.HDFS的shell命令方式

​        即使用HDFS操作语句的关键字

```sh
hadoop fs         # 适用于任何不同的文件系统，包括本地文件系统和HDFS文件系统
hadoop dfs        # 只能适用HDFS文件系统
hdfs dfs          # 同hadoop dfs
```

​        在实际操作过程使用何种指令可因人而异，出于习惯考虑下列操作实例均以`hadoop fs`关键字开头。

- 注：可使用以下命令查看总共支持哪些命令

  ```sh
  hadoop fs 
  ```

- 注：使用help命令可查看具体命令使用方法

  ```sh
  hadoop fs -help put
  ```

- 另外，可使用web对HDFS文件系统实施监控。

### 2.创建HDFS目录

- 基本指令

  ```sh
  hadoop fs -mkdir
  ```

- 实例

  ```sh
  hadoop fs -mkdir /user                # 创建user目录
  hadoop fs -mkdir /user/root           # 创建user子目录
  hadoop fs -mkdir /user/root/test      # 创建root子目录
  hadoop fs -mkdir -p /dir1/dir2/dir3   # 同时创建多级子目录
  ```

### 3.查看HDFS目录

- 基本指令

  ```sh
  hadoop fs -ls
  ```

- 实例

  ```sh
  hadoop fs -ls                         # 列出当前用户下的所有目录
  hadoop fs -ls /                       # 列出当前用户下的文件
  hadoop fs -ls /user                   # 列出user下所有目目录
  hadoop fs -ls /user/root              # 列出root下所有目录
  hadoop fs -ls -R /                    # 同时列出全部子目录
  ```

### 4.复制本地文件到HDFS

- 基本指令

  ```sh
  hadoop fs -copyFromLocal
  ```

- 实例

  - 复制本地文件README.txt到HDFS文件系统的test目录下

    ```sh
    hadoop fs -copyFromLocal /usr/local/hadoop/README.txt /user/root/test
    ```


- 当文件已经存在时，可强制复制，作用相当于复制并替换

  ```sh
  hadoop fs -copyFromLocal -f /usr/local/hadoop/README.txt /user/root/test
  ```


- 同时复制多个文件

  ```sh
  hadoop fs -copyFromLocal /usr/local/hadoop/NOTICE.txt /usr/local/hadoop/NOTICE.txt /user/root/test
  ```


- 复制目录到HDFS目录

  ```sh
  hadoop fs -copyFromLocal /usr/local/hadoop/etc /user/root/test
  ```

### 5.使用put命令复制本地文件到HDFS

- 基本指令

  ```sh
  hadoop fs -put
  ```

- 实例

  - 将原本显示在屏幕上的内容存储到HDFS文件

    ```sh
    echo  abc | hadoop fs -put - /user/root/test/echoin.txt
    ```

  - 将本地目录列表存储到HDFS文件

    ```sh
    ls /usr/local/hadoop | hadoop fs -put -/user/root/test/hadooplist.txt
    ```

### 6.列出HDFS目录下文件内容

- 基本指令

  ```sh
  hadoop fs -cat
  ```

- 实例

  - 显示HDFS文件目录下README.txt文件内容

    ```sh
    hadoop fs -cat /user/root/test/README.txt
    ```

  - 一页页显示

    ```sh
    hadoop fs -cat /user/root/test/README.txt|more
    ```

### 7.使用copy命令复制HDFS至本地

- 基本指令

  ```sh
  hadoop fs -copyToLocal
  ```

- 实例

  - 将hadooplist.txt文件复制到本地当前文件夹内

    ```sh
    hadoop fs -copyToLocal /user/root/test/hadooplist.txt
    ```

  - 将etc文件目录复制到本地当前文件夹内

    ```sh
    hadoop fs -copyToLocal /user/root/test/etc
    ```

### 8.使用get命令复制HDFS文件至本地

- 基本指令

  ```sh
  hadoop fs -get
  ```

- 实例

  将HDFS文件复制到当前文件夹内并命名

  ```sh
  hadoop fs -get /user/root/test/README.txt localREADME.txt
  ```

### 9.复制HDFS文件

- 基本指令

  ```sh
  hadoop fs -cp
  ```

- 实例

  ```sh
  hadoop fs -mkdir /user/root/test/temp       # 建立temp文件夹
  hadoop fs -cp /user/root/test/README.txt /user/root/test/temp # 将test中文件复制到temp中
  ```

### 10.删除HDFS文件

- 基本指令

  ```sh
  hadoop fs -rm
  ```

- 实例

  - 删除文件

    ```sh
    hadoop fs -rm /user/root/test/README.txt   # 删除README文件
    ```

  - 删除目录

    ```sh
    hadoop fs -rm -R /user/root/test/etc     # 删除etc目录
    ```


##附录三：打包编译MapReduce程序

通常来说，运行MapReduce程序有两种方法，其一是在命令行中手动编译并打包运行，另一种则是使用Eclipse编译运行MapReduce程序，考虑到能够看到运行过程，此处介绍较为原始的命令行打包运行MapReduce方法。其中java算法代码采用MapReduce自带实例WordCount。

### 1.下载源码

​        由于要使用MapReduce源码中的算法代码，因此需要下载Hadoop源码。

```sh
cd ~
mkdir WordCount                       # 作为运行WordCount的工作空间
cd WordCount
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.2.0/hadoop-2.2.0-src.tar.gz
```

​        下载完成后，将java文件WordCount放至工作空间

```sh
tar -zxf hadoop-2.2.0-src.tar.gz
cd hadoop-2.7.3-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples
ll                    # 查看所有自带算法java文件
cp WordCount.java ~/WordCount
```

- 附：**Hadoop 2.7.3中WordCount.java源码**

  ```java
  /**
   * Licensed to the Apache Software Foundation (ASF) under one
   * or more contributor license agreements.  See the NOTICE file
   * distributed with this work for additional information
   * regarding copyright ownership.  The ASF licenses this file
   * to you under the Apache License, Version 2.0 (the
   * "License"); you may not use this file except in compliance
   * with the License.  You may obtain a copy of the License at
   *
   *     http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   */
  package org.apache.hadoop.examples;

  import java.io.IOException;
  import java.util.StringTokenizer;

  import org.apache.hadoop.conf.Configuration;
  import org.apache.hadoop.fs.Path;
  import org.apache.hadoop.io.IntWritable;
  import org.apache.hadoop.io.Text;
  import org.apache.hadoop.mapreduce.Job;
  import org.apache.hadoop.mapreduce.Mapper;
  import org.apache.hadoop.mapreduce.Reducer;
  import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
  import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
  import org.apache.hadoop.util.GenericOptionsParser;

  public class WordCount {

    public static class TokenizerMapper 
         extends Mapper<Object, Text, Text, IntWritable>{
      
      private final static IntWritable one = new IntWritable(1);
      private Text word = new Text();
        
      public void map(Object key, Text value, Context context
                      ) throws IOException, InterruptedException {
        StringTokenizer itr = new StringTokenizer(value.toString());
        while (itr.hasMoreTokens()) {
          word.set(itr.nextToken());
          context.write(word, one);
        }
      }
    }
    
    public static class IntSumReducer 
         extends Reducer<Text,IntWritable,Text,IntWritable> {
      private IntWritable result = new IntWritable();

      public void reduce(Text key, Iterable<IntWritable> values, 
                         Context context
                         ) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
          sum += val.get();
        }
        result.set(sum);
        context.write(key, result);
      }
    }

    public static void main(String[] args) throws Exception {
      Configuration conf = new Configuration();
      String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
      if (otherArgs.length < 2) {
        System.err.println("Usage: wordcount <in> [<in>...] <out>");
        System.exit(2);
      }
      Job job = Job.getInstance(conf, "word count");
      job.setJarByClass(WordCount.class);
      job.setMapperClass(TokenizerMapper.class);
      job.setCombinerClass(IntSumReducer.class);
      job.setReducerClass(IntSumReducer.class);
      job.setOutputKeyClass(Text.class);
      job.setOutputValueClass(IntWritable.class);
      for (int i = 0; i < otherArgs.length - 1; ++i) {
        FileInputFormat.addInputPath(job, new Path(otherArgs[i]));
      }
      FileOutputFormat.setOutputPath(job,
        new Path(otherArgs[otherArgs.length - 1]));
      System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
  }
  ```

  ​

### 2.将依赖jar包放入路径

- 查看Hadoop所需jar包

  ```sh
  hadoop classpath
  ```

- 将Hadoop全部classpath放入路径

  ​        Hadoop文件中自带所需jar包，需要将这些jar包路径变量添加如CLASSPATH中才能运行自己编译的MapReduce程序。

  ​        修改环境变量：

  ```sh
  vim ~/.bashrc
  ```

  ​        在打开对话框中添加CLASSPATH变量值：

  ```sh
  export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
  ```

  ​        使之生效：

  ```sh
  source ~/.bashrc
  ```

### 3.运行WordCount.java

- 编译WordCount.java

  ```sh
  cd ~/WordCount              
  javac WordCount.java
  ```

  编译完成后可见生成了几个.class文件。

- 将class文件打包成jar

  ```sh
  jar -cvf WordCount.jar ./WordCount*.class
  ```

- 创建测试文件

  ```sh
  mkdir input           # 测试文件目录
  echo "echo of the rainbow" > ./input/file0
  echo "the waiting game" > ./input/file1
  ls ./input/
  ```

- 将本地文件上传至HDFS

  ```sh
  hadoop fs -rm -R /user/hadoop/test/input    # 存在input文件夹则删除，不存在则跳过
  hadoop fs -put ./input /user/root/test
  ```

- 运行WordCount

  ```sh
  hadoop jar WordCount.jar org/apache/hadoop/examples/WordCount /user/root/test/input /user/root/test/output
  ```

- 查看运行结果

  - 在8088端口查看任务运行状态

  - 在50070端口查看生成文件

    注意到计算结果被存放至part-r-00000文件中，用cat命令可查看文件内容。

    ```sh
    hadoop fs -cat /user/root/test/output/part-r-00000
    ```

    ​

##附录四：HBase数据基本操作

以下操作均在hbase shell界面中进行

### 1.创建表

使用create命令创建表

```shell
create 'student','Sname','Ssex','Sage','Sdept','course'
```

输出结果如下

![5](G:\屏幕截图\其他组件安装部分\5.jpg)

此时，即创建了一个“student”表，属性有：Sname,Ssex,Sage,Sdept,course。因为HBase的表中会有一个系统默认的属性作为主键，故主键无需自行创建。创建完“student”表后，可通过describe命令查看“student”表的基本信息。

### 2.添加数据

HBase中用put命令添加数据，一次只能为一个表的一行数据的一个列添加一个数据。

执行put命令来添加主键为95001，学号为95001，名字为LiYing的一行数据

```shell
put 'student','95001','Sname:','LiYing'
```

结果如下

![6](G:\屏幕截图\其他组件安装部分\6.jpg)

执行put命令来为95001行下的course列族的math列添加了一个数据

```shell
put 'student','95001','course:math','80'
```

结果如下

![7](G:\屏幕截图\其他组件安装部分\7.jpg)

### 3.查看、删除数据

HBase中有两个用于查看数据的命令：1、get命令，用于查看表的某一行数据；2、scan命令用于查看某个表的全部数据。在HBase中用delete以及deleteall命令进行删除数据操作，它们的区别是：1、delete用于删除一个数据，是put的反向操作；2、deleteall操作用于删除一行数据。

执行delete命令来删除student表中95001行下的Ssex列的所有版本的数据，用get命令来查看

```shell
delete 'student','95001','Ssex'
get 'student','95001'
```

结果如下

![9](G:\屏幕截图\其他组件安装部分\9.jpg)

执行deleteall命令来删除student表中的95001行的全部数据，再用scan查看表

```shell
deleteall 'student','95001'
scan 'student'
```

结果如下

![10](G:\屏幕截图\其他组件安装部分\10.jpg)





##附录四：利用CentOS搭建分布式集群

下载安装虚拟机部分不再赘述，此处仅对安装好虚拟机之后的安装部分进行讲解

## 一、准备CentOS 7.2

### 1.修改网络配置

​        安装好CentOS系统后默认网络关闭，因此需要先开启网络服务，才能进行后续安装，开启网络服务需要修改网络配置文件：

```sh
cd /etc/sysconfig/network-scripts/
ls
vi ifcfg-ens160
```

修改：ONBOOT=yes



### 2.安装部分需求组件

​        此处安装一些常用软件

```sh
yum -y update
yum -y install vim
yum -y install wget
yum -y install epel-release
yum -y install python-pip
yum -y install gcc
yum -y install gcc-c++
yum -y install python-devel
```

​

### 3.验证本机无密码登陆

​        由于后续各节点需要利用ssh协议相互通信，因此此处先配置本机能够免密码登陆。

```sh
rpm -qa | grep ssh     # 验证本机是否安装SSH，一般情况下CentOS系统都已默认安装
ssh localhost          # 登陆到本机，需要确认yes和本机密码
exit                   # 退出
cd ~/.ssh/
ssh-keygen -t rsa      # 生成密钥
cat id_rsa.pub >> authorized_keys   # 将生成公钥加入授权
ssh localhost              # 至此便可免密码登陆本机
exit
```

​

### 4.安装java 8

- 选择java 8版本并下载： **[下载地址](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)** 
- 解压并安装：

```sh
mkdir ~/download          # 作为WinSCP传输文件的文件夹
cd ~/download/            
```

- 利用`WinSCP`传输下载的java文件至download文件夹

  ```
  tar -zxf jdk-8u131-linux-x64.tar.gz -C /usr/local
  cd /usr/local
  mv ./jdk1.8.0_131/ ./java
  ```

- 添加java环境变量

```sh
vim ~/.bashrc
```

​        添加下述路径： 

```sh
export JAVA_HOME=/usr/local/java
export JRE_HOME=/usr/java/local/jre
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib
export JAVA_HOME JRE_HOME PATH CLASSPATH
```

```sh
source ~/.bashrc
```



### 5.关闭防火墙

​        在配置过程中暂时关闭并设置开机不启动防火墙，在安装完毕后可重启防火墙，但如果需要安装ambari且在ambari运行过程中防火墙处于开启状态，则ambari将会报错，因此建议关闭防火墙并设置默认防火墙关闭。

```sh
systemctl stop firewalld            # 关闭防火墙
systemctl status firewalld          # 检验防火墙状态
systemctl disable firewalld         # 将防火墙默认状态设置为关闭
```



### 6.设置SELinux

将SELinux设置为关闭：

```sh
vim /etc/selinux/config
```

修改：SELINUX=disabled



### 7.配置ntp服务

​        ntp即为网络时间协议，由于涉及到各节点间任务调度，因此需要统一各节点时间。

```sh
yum -y install ntp            # 安装ntp
systemctl start ntpd           # 开启ntp服务
systemctl enable ntpd          # 设置默认状态为开启
systemctl status ntpd          # 查看ntp状态
```

### 8.配置`httpd`服务

​        `httpd`是Apache超文本传输协议(HTTP)服务器的主程序，由于后续可能需要配置HDP，则需要设置本地软件源，因此提前配置好`httpd`相关服务。

```sh
yum -y install httpd            # 安装httpd
systemctl start httpd           # 开启ntp服务
systemctl enable httpd          # 设置默认状态为开启
systemctl status httpd          # 查看httpd状态
```

在任意浏览器输入http://(主机地址)，若出现testing界面，则说明安装成功。



## 二、安装Hadoop 2.7.3

### 1.下载并安装

选择版本并下载：**[下载地址](https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz)**

利用`WinSCP`传输下载文件至download文件夹，然后进行解压与安装：

```sh
cd ~/download
tar -zxf hadoop-2.7.3.tar.gz -C /usr/local
cd /usr/local/
mv ./hadoop-2.7.3/ ./hadoop
```

### 2.配置环境变量

```sh
vim ~/.bashrc
```

​        在打开对话框中添加下列路径：

```sh
#hadoop
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```

```sh
source ~/.bashrc
```

### 3.修改配置文件

​        Hadoop运行方式由其相关配置文件决定，因此在运行前需要修改配置文件。

```sh
cd /usr/local/hadoop/etc/hadoop
```

- 修改slaves、masters文件

  ```sh
  vim slaves
  ```

  在弹出窗口中输入作为slaves节点的节点名称：

  ```sh
  data
  ```

  ```sh
  vim masters
  ```

  在弹出窗口中输入作为master节点的节点名称：

  ```sh
  master
  ```

- 修改core-site.xml

  ```sh
  vim core-site.xml
  ```

  ```xml
  <configuration>
      <property>
              <name>fs.defaultFS</name>
              <value>hdfs://master:9000</value>
      </property>
      <property>
              <name>hadoop.tmp.dir</name>
              <value>file:/usr/local/hadoop/tmp</value>
              <description>Abase for other temporary directories.</description>
      </property>
  </configuration>
  ```

- 修改hdfs-site.xml

  ```sh
  vim hdfs-site.xml
  ```

  ```xml
  <configuration>
      <property>
              <name>dfs.namenode.secondary.http-address</name>
              <value>master:50090</value>
      </property>
      <property>
              <name>dfs.replication</name>
              <value>2</value>
      </property>
      <property>
              <name>dfs.namenode.name.dir</name>
              <value>file:/usr/local/hadoop/tmp/dfs/name</value>
      </property>
      <property>
              <name>dfs.datanode.data.dir</name>
              <value>file:/usr/local/hadoop/tmp/dfs/data</value>
      </property>
  </configuration>
  ```

- 修改mapred-site.xml

  ```sh
  cp mapred-site.xml.template mapred-site.xml  # 复制模板
  vim mapred-site.xml
  ```

  ```xml
  <configuration>
      <property>
              <name>mapreduce.framework.name</name>
              <value>yarn</value>
      </property>
      <property>
              <name>mapreduce.jobhistory.address</name>
              <value>master:10020</value>
      </property>
      <property>
              <name>mapreduce.jobhistory.webapp.address</name>
              <value>master:19888</value>
      </property>
  </configuration>
  ```

- 修改yarn-site.xml

  ```sh
  vim yarn-site.xml
  ```

  ```xml
  <configuration>
      <property>
              <name>yarn.resourcemanager.hostname</name>
              <value>master</value>
      </property>
      <property>
              <name>yarn.nodemanager.aux-services</name>
              <value>mapreduce_shuffle</value>
      </property>
  </configuration>
  ```



## 三、安装Scala和Spark 2.1.1

### 1.下载并安装

由于spark 2.0 采用scala 2.11.8版本编译，因此考虑下载scala 2.11.8

**[Scala 2.11.8 下载地址](https://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz)**

**[Spark 2.0 下载地址](https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz)**

下载完毕后利用`WinSCP`将下载文件传输至download文件夹



### 2.设置环境变量

```sh
vim ~/.bashrc
```

​        在弹出对话框中输入下述路径：

```sh
#scala
export SCALA_HOME=/usr/local/scala
export PATH=$PATH:$SCALA_HOME/bin
#spark
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin
```

```sh
source ~/.bashrc
```



### 3.修改Spark相关配置

- 修改slaves文件

  ```sh
  cd /usr/local/spark/conf
  cp slave.template slaves
  vim slaves
  ```

  ​        在弹出对话框中输入作为slaves节点的节点名称（同时删除原有节点名称localhost）

  ```sh
  data
  ```

- 修改spark-env.sh文件

  ```sh
  cp spark-env.sh.template spark-env.sh
  vim spark-env.sh
  ```

  ​        在弹出对话框中输入下述spark配置

  ```sh
  #java
  export JAVA_HOME=/usr/local/java
  #scala
  export SCALA_HOME=/usr/local/scala
  #hadoop
  export HADOOP_HOME=/usr/local/hadoop
  export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
  export YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop
  #hive
  export HIVE_HOME=/usr/local/hive
  export HIVE_CONF_DIR=/usr/local/hive/conf
  #spark
  export SPARK_HOME=/usr/local/spark
  export SPARK_LOCAL_DIRS=/usr/local/spark
  export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
  export SPARK_WORKER_CORES=1
  export SPARK_WORKER_INSTANCES=2 
  export SPARK_MASTER_IP=192.168.*.*       # 填写master IP
  export SPARK_MASTER_WEBUI_PORT=18080
  export SPARK_LIBRARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native
  ```

  ​        在配置文件中不仅包括各关键组件路径，还由包括主机IP在内的各运行参数。

  ​

## 四、部署其他节点

### 1.将虚拟机复制至其他节点

​        根据配置要求，复制虚拟机，本次部署在42上安装一个虚拟机作为master节点，在41及43服务器上分别安装3个虚拟机作为slaves节点。



### 2.修改网络配置（举例，实际情况视本机IP而定）

​        之前的网络设置只设置了开启网络，此时需要修改动态IP为静态IP，以master节点为例，进行下述操作：

```sh
cd /etc/sysconfig/network-scripts
ls
vim ifcfg-ens160
```

修改：

BOOTPROTO=static

IPADDR=192.168.2.220

NETMASK=255.255.240.0

GATEWAY=192.168.2.1

DNS1=192.168.2.1

DNS2=61.139.2.69



### 3.修改各节点hostname

​        根据运行功能，修改各节点hostname，master节点名称为master，其余节点统一设置名称为datax，以master节点为例。

```sh
vim /etc/hostname
```

​        在弹出对话框中输入hostname（注意要删除原有（localhost）：

```sh
master
```

### 4.修改各节点hosts映射关系

```sh
vim /etc/hosts
```

​        在弹出对话框中输入hosts映射关系(注：原有部分不能删除)：

```sh
192.168.2.220  master
192.168.2.221  data
```

### 5.配置各节点免密码登陆

​        主要配置主节点（master）免密码登陆其他slaves节点，将主节点公钥授权至其他节点中。

```sh
cd ~/.ssh
scp ~/.ssh/id_rsa.pub root@data1:~/download
```

​        在data1上将公钥加入授权：

```sh
cat ~/download/id_rsa.pub >> ~/.ssh/authorized_keys
rm -rf ~/download/id_rsa.pub    # 用完即删
```

​        测试是否能登陆data1

```sh
ssh data1
```

​        以此类推设置其他节点免密码登陆。



### 6.测试Hadoop、Spark是否能够正常启动

```sh
hdfs namenode -format                   # 对namenode进行格式化
start-all.sh                            # 启动hadoop相关进程
/usr/local/spark/sbin/start-all.sh      # 启动spark相关进程
```

  ​        在各监视端口查看各节点是否正常运行：

  ​        Hadoop监控端口：<http://192.168.2.220:8088>

​          HDFS监控端口：<http://192.168.2.220:50070>

​          Spark监控端口：<http://192.168.2.220:18080>



## 五、安装Hive 1.2.2

### 1.下载并安装

下载Hive 1.2.2 稳定版本：**[下载地址](https://mirrors.tuna.tsinghua.edu.cn/apache/hive/hive-1.2.2/apache-hive-1.2.2-bin.tar.gz)** 

 利用`WinSCP`将下载文件复制到download文件夹。

```sh
tar -zxf apache-hive-1.2.2-bin.tar.gz -C /usr/local
cd /usr/local
mv ./apache-hive-1.2.2-bin/ ./hive
```



### 2.添加环境变量

```sh
vim ~/.bashrc
```

​        在弹出对话框中添加下述路径：

```sh
#hive
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
```

```sh
source ~/.bashrc
```



### 3.修改相关配置

​        由于使用Zeppelin作为Hive调度方法，仅涉及到使用Hive计算框架(MR)的使用，因此只需要使用默认配置即可。

```sh
cd /usr/local/hive/conf
cp hive-default.xml.template hive-default.xml
```



## 六、安装Zeppelin 0.7.0

### 1.下载并安装

下载Zeppelin 0.7.0版本：**[下载地址](http://archive.apache.org/dist/zeppelin/zeppelin-0.7.0/zeppelin-0.7.0-bin-all.tgz)**

利用`WinSCP`传输下载文件至download文件夹。

```sh
tar -zxf ~/download/zeppelin-0.7.0-bin-all.tgz -C /usr/local
cd /usr/local/
mv ./zeppelin-0.7.0-bin-all/ ./zeppelin
```



### 2.使用spark相关jar包替换zeppelin相关jar包

```sh
cd /usr/local/zeppelin/lib
rm -rf jackson-annotation<Tab>
rm -rf jackson-core<Tab>
rm -rf jackson-databind<Tab>
rm -rf hadoop-annotation-2.6.0.jar
rm -rf hadoop-auth-2.6.0.jar
rm -rf hadoop-common-2.6.0.jar
cd /usr/local/spark/jars
cp jackson-annotation-2.6.5.jar /usr/local/zeppelin/lib
cp jackson-core-2.6.5.jar /usr/local/zeppelin/lib
cp jackson-databind-2.6.5.jar /usr/local/zeppelin/lib
cp hadoop-annotation-2.7.3.jar /usr/local/zeppelin/lib
cp hadoop-auth-2.7.3.jar /usr/local/zeppelin/lib
cp hadoop-common-2.7.3.jar /usr/local/zeppelin/lib
```



### 3.修改Zeppelin配置

- 修改site相关配置：

```sh
cd /usr/local/zeppelin/conf
cp zeppelin-site.xml.template zeppelin-site.xml
vim zeppelin-site.xml
```

(可选)设置无法匿名登陆以增加安全性：

```xml
<property>
  <name>zeppelin.anonymous.allowed</name>
  <value>false</value>
  <description>Anonymous user allowed by default</description>
</property>
```

设置登陆端口以避免冲突：

```xml
<property>
  <name>zeppelin.server.port</name>
  <value>28080</value>
  <description>Server port.</description>
</property>
```

- 修改env相关配置

```sh
cp zeppelin-env.sh.template zeppelin-env.sh
vim zeppelin-env.sh
```

在弹出对话框中加入下述路径：

```sh
#java
export JAVA_HOME=/usr/local/java
#hadoop
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIF=/usr/local/hadoop/etc/hadoop
#spark
export SPARK_HOME=/usr/local/spark
export MASTER=spark://192.168.*.*:7077               # 填写master IP
#hive
export HIVE_HOME=/usr/local/hive
#python
export PYSPARK_PYTHON=/usr/bin/python
export PYTHON_HOME=/usr/bin/python
```

- 测试能否正常启动Zeppelin(注：要先启动hadoop和spark相关服务)

  ```sh
  start-all.sh                                              # 启动hadoop
  /usr/local/spark/sbin/start-all.sh                        # 启动Spark
  /usr/local/zeppelin/bin/zeppelin-daemon.sh start          # 启动Zeppelin
  ```

  - 监控端口：

  Hadoop：http://masterIP:8088

  HDFS：http://masterIP:50070

  Spark：http://masterIP:18080

  Zeppelin：http://masterIP:28080