---
title: SVM支持向量机原理及核函数
date: 2018-04-7 20:05:59
password:
top:
categories:
  - Machine learning
tags:
  - scikit-learn 
---
<!--more-->

## 支持向量机原理

### 大距离分类算法

1、名词解释：

- **分割超平面**：如下图所示，构造一个分割线把圆形的点和方形的点分开，这个线称为**分割超平面**。

- **支持向量**：离分割超平面最近的点

- **间距**：支持向量到分割超平面距离的两倍

**SVM算法的原理就是找到一个分割超平面，它能把数据正确的分类，并且间距最大！**

![](https://ws2.sinaimg.cn/large/006tNc79ly1fq43bc7pj3j30gx0c6gm7.jpg)

2、计算间距

在而为空间里，可以使用方程$w_1x_1+w_2x_2+b=0$来表示分割超平面。针对高纬度空间，可以写成一般化的向量形式，即$w^Tx+b=0$。这里画出与分割线超平面平行的两条直线，分别穿过两个类别的支持向量。这两条直线的方程分别为$w^Tx+b=-1$和$w^Tx+b=1$。如下图所示：

![](https://ws4.sinaimg.cn/large/006tNc79ly1fq43bz1cb3j30hg0c63za.jpg)

根据点到直线的距离公式，可以算出支持向量A到分割超平面的距离为：$$d=\frac{\left| w^{T}A+b \right|}{\left| \left| w \right| \right|}$$

由于点$A$在直线$w^Tx+b=-1$和$w^Tx+b=1$在，代入可的支持向量$A$到分割超平面的距离为$d=\frac{1}{\left| \left| w \right| \right|}$。为了使间距最大，只需找到合适的参数$w$和$b$，使$\frac{1}{\left| \left| w \right| \right|}$最大即可。$||w||$使向量$w$的**L2范数**，计算公式为：
$$\left| \left| w \right| \right|=\sqrt{\sum_{i=1}^{n}{w_{i}^{2}}}$$
求$\frac{1}{\left| \left| w \right| \right|}$的最大值即使求$||w||^2$的最小值：
$$\left| \left| w \right| \right|^{2}=\sum_{i=1}^{n}{w_{i}^{2}}$$

其中$n$为限量$w$的纬度。除了间距最大，分割超平面更能用来解决分类问题！回到上图，针对方形点$x$，必须满足$w^Tx+b≥1$的约束条件。针对圆形的点必须满足$w^Tx+b<=-1$的约束条件。
类别是离散的值，分别使用-1表示圆点类别，1表示方点类别，即$y\in \left\(-1,1 \right\)$。针对数据集中的所有样本$x^{(i)},y^{(i)}$，只要满足以下约束条件，则由以下参数$w$和参数$b$定义的分割超平面进行分类：$$y^{(i)}(w^Tx^{(i)}+b)≥1$$

**一句话概括：求解SVM算法，就是在满足约束条件$y^{(i)}(w^Tx^{(i)}+b)≥1$的前提下，求解$||w||^2$的最小值。**

### 松弛系数

针对现行不可分的数据集，上面的方法就不能用了。解决这个问题的办法就是引入一个参数$\epsilon$，称为**松弛系数**。然后把优化的目标函数变为：
$$\mbox{argmin}\left| \left| w^{2} \right| \right|+R\sum_{i=1}^{m}{\epsilon_{i}}$$

![](https://ws3.sinaimg.cn/large/006tNc79ly1fq43cif8rwj30wy0e9mz8.jpg)

其中$m$为数据集的个数，$R$为算法参数，其约束条件变为：$$y^{\left( i \right)}\left( w^T{x^{\left( i \right)}}+b \right)\geq 1-\epsilon_{i}$$

理解松弛系数：

可以把$ε_{i}$理解为样本$x^{(i)}$违反一大间距规则的程度。针对大多数满足约束条件的样本$ε=0$。而对部分违反最大间距规则的样本$ε>0$。参数$R$则表示对违反约束的样本的"惩罚"。$R$越大对违反约束的点“惩罚力度”越大反之越小 。这样模型就会倾向于**允许部分点违反最大间距规则**。

把$y^{\left( i \right)}\left( w^{T}x^{\left( i \right)}+b \right)$作为横坐标，违反约束条件的代价$J_i$作为纵坐标画图：

![](https://ws3.sinaimg.cn/large/006tNc79ly1fq43cufjjfj30ih0d1dgg.jpg)

上图可以看出，针对哪些没有违反约束条件的样本，其成本为0。违反了约束条件的样本其成本与$ε$成正比，斜线的斜率为$R$。

因此，引入松弛系数类似于逻辑回归的成本函数引入正则项，目的是为了纠正过拟合问题，让SVM对噪声数据由更强的忍耐性。如上上图所示，当出现违反大间距规则的噪声样本出现时，仍能让分割超平面是原来的样子，这就是松弛系数的作用。

## 核函数

核函数是**特征转换**函数。

### 最简单的核函数

回顾上面内容，我们的任务是找出合适的参数$w,b$，使得分割超平面间距最大，且能正确对数据进行分类。**间距最大是我们的优化目标**。真确地对数据分类是约束条件。即**在满足约束条件$y^{(i)}(w^Tx^{(i)}+b)≥1$的前提下，求解$||w||^2$的最小值。**

**拉格朗日乘子法**是解决约束条件下求函数极值的理想方法。其方法是引入非负系数$α$来作为约束条件的权重：$$L=\frac{1}{2}\left| \left| w \right| \right|^{2}-\sum_{i=1}^{m}{\alpha _{i}\left( y^{\left( i \right)}\left( w^{T}x^{\left( i \right)}+b \right)-1 \right)}$$

由于极值的偏导数为0，因此这需要让$L$对$w$求导使之为0得到$w$和$α$对关系：$$w=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}x^{\left( i \right)}}$$
接着继续求$L$对$b$对偏导数得出:$$\sum_{i=1}^{m}{y^{\left( i \right)}\alpha_{i}=0}$$
把这两个式子代入$L$通过数学运算得出：$$L=\sum_{i=1}^{m}{a_{i}-}\frac{1}{2}\sum_{i=1}^{m}{\sum_{j=1}^{m}{a_{i}}a_{j}y^{\left( i \right)}y^{\left( j \right)}x^{\left( i \right)T}x^{\left( j \right)}}$$
这个公式中$m$是数据集个数，$a$是拉格朗日乘子法引入的一个系数，针对数据集中的每个样本$x^{(i)}$,都有对应的$a_i$。$x^{(i)}$是数据集中地$i$个样本的输入，它是一个向量，$y^{(i)}$是对应的输出标签，值为$y\in \left\( -1,1 \right\)$。

这个公式的最小值求解这里就不说明了。最后求出的$a$有个明显的特点。即大部分$a_i=0$。因为只有那些支持向量所对应的样本直接决定了间隙的大小。实际上以上推导出这个公式就是为了引入支持向量机的另外一个核心概念：**核函数**:$$K({x^{({i})},x^{({j})}})=x^{({i})T}x^{({j})}$$

$L$里的$x^{(i)T}x^{(j)}$部分，其中$x^{(i)}$是一个特征向量，所以$x^{(i)T}x^{(j)}$是一个数值，就是两个输入特征向量的内积。预测函数为：$$w^{T}x+b=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}x^{\left( i \right)T}x+b}$$

当$w^{T}x+b>0$，预测函数为类别1，当$w^{T}x+b<0$，预测类别为-1。注意到预测函数里也包含式子$x^{({i})T}x$。我们把$K({x^{({i})},x^{({j})}})=x^{({i})T}x^{({j})}$称为核函数。 $x^{(i)T}x^{(j)}$是两个向量内积，它的物理含义是衡量两个向量的**相似性**。典型地，当两个向量相互垂直是，即完全线性无关，此时$x^{(i)T}x^{(j)}=0$。引入核函数后预测函数为：
$$w^{T}x+b=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}K\left( x^{\left( i \right)},x \right)+b}$$

### 相似性函数
假设数据集已有一个数图特征，如下图，如何进行分类。

![](https://ws2.sinaimg.cn/large/006tNc79ly1fq43d6r1acj30w50feab6.jpg)

解决这个问题的方式是：**用一定规则把这些无法进行线性分割的样本映射到更高纬度的空间里，然后找出超平面**。

SVM的核函数就是为了实现这种相似性映射。最简单的核函数是$K({x^{({i})},x^{({j})}})=x^{({i})T}x^{({j})}$，它衡量的是两个输入特征向量的相似性。可以通过定义和函数$K({x^{({i})},x^{({j})}})$来重新定义相似性，从而得到想要的映射。例如在基因测试领域，我们需要根据DNA分子的特征来定义相似性函数，即和函数。在文本处理领域，也可以自己定义和函数来衡量两个词之间的相似性。

怎么把低维度的空间映射到高纬度的空间呢？

举个例子：联想下利用多项式解决线性回归欠拟合问题的方法。如果输入特征是一维的$[x_1]$变量，我们把它变成二维的一个方法是把输入特征转化为$[x_1,2x_1^2]$，定义这种特征映射的函数就称之为**相似性函数$Φ({x})$**。这样在原来低维度计算相似性的运算$x^{({i})T}x^{({j})}$，就可以转换为高纬度空间里进行相似性运算$Φ({x^{({i})}})^{T}Φ({x^{({i})}})$。

**核函数$K({x^{({i})},x^{({j})}})$和相似性函数$Φ({x})$的关系：**

相似性函数是特征的**映射函数**，起到转换的作用。而核函数是**特征向量的内积**。经过相似性函数转换后，核函数变成$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\Phi \left( x^{\left( i \right)} \right)^{T}\Phi \left( x^{\left( i \right)} \right)$。

### 常用核函数

核函数一般和应用场景相关，在不同领域所应用的核函数可能也不相同。但是实际上也有一些通用核函数“万金油”，一般有两种：**多项式核函数**和**高斯核函数**。

1、多项式核函数：

![](https://ws4.sinaimg.cn/large/006tNc79ly1fq4aza9jwoj306q01p3yh.jpg)

2、高斯核函数：$$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\exp \left( -\frac{\left( x^{\left( i \right)}-x^{\left( j \right)} \right)^{2}}{2\sigma ^{2}} \right)$$

如果输入的特征是一维的标量，那么高斯核函数对应的形状就是一个反钟形的曲线，其参数$σ$控制反钟形的宽度。如下图所示：

![](https://ws1.sinaimg.cn/large/006tNc79ly1fq43dh16lcj30xp0bmq48.jpg)

由于$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\Phi \left( x^{\left( i \right)} \right)^{T}\Phi \left( x^{\left( i \right)} \right)$，经过合适的数学变换，可得高斯核函数对应的特征转换函数为：
$$\Phi \left( x \right)=\sum_{i=0}^{\infty }{\exp \left( -x^{2} \right)\sqrt{\frac{2^{i}}{i!}}}x^{i}$$
前面无限多项的累加器，其物理意义就是把特征向量转换到无限多维向量空间里，即**高斯函数可以吧输入特征扩展到无限多维空间里**。公式的推导公式会用到**泰勒公式**。


$$高斯预测函数=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}K\left( x^{\left( i \right)},x \right)+b}$$
其中$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)$是高斯核函数，$a_i$只在支持向量对应的样本出不为0.由此可知，**预测函数时中心点在支持向量机处的高斯函数的线性组合**，其线性组合的系数为$a_iy^{(i)}$。因此，高斯核函数也称为**RBF核函数**，即反钟形函数的线性组合。

### 核函数的对比
$$简单线性核函数K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=x^{\left( i \right)T}x^{\left( j \right)}$$

$$多项式核函数：$$![](https://ws4.sinaimg.cn/large/006tNc79ly1fq4aza9jwoj306q01p3yh.jpg)
$$高斯核函数K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\exp \left( -\frac{\left( x^{\left( i \right)}-x^{\left( j \right)} \right)^{2}}{2\sigma ^{2}} \right)$$
1、**线性核函数：**这是最简单的核函数，它直接计算两个输入特征向量的内积。
- 优点：简单高效，结果易解释，总能生成一个最简洁的线性分割超平面
- 缺点：只适用线性可分的数据集

2、**多项式核函数：**通过多项式来作为特征映射函数
- 优点：可以拟合出复杂的分割超平面。
- 缺点：参数太多。有$γ,c,n$三个参数要选择，选择起来比较困难；另外多项式的阶数不宜太高否则会给模型求解带来困难。

3、**高斯核函数：**
- 优点：可以把特征映射到无限多维，并且没有多项式计算那么困难，参数也比较好选择。
- 缺点：不容易解释，计算速度比较慢，容易过拟合。

### 核函数的选择

1、最一般的选择原则是针对数据量很大的时候，可以选择复杂一点的模型。虽然复杂模型容易过拟合，但由于数据量很大，可以有效弥补过拟合问题。如果数据集较小选择简单点的模型，否则很容易过拟合，此时特别要注意模型是否欠拟合，如果欠拟合可以增加多项式纠正欠拟合。

2、根据样本量$m$和特征量$n$进行选择：
- 特征相比样本较大（如m=10～1000，n=10000）：选逻辑回归或者线性函数SVM
- 特征较少，样本量中（如m=10～10000，n=1～1000）：选择高斯SVM
- 特征量少，样本多（如m=50000+，n=1~1000)：选多项式或高斯SVM
