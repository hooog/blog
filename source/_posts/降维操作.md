---
title: MLlib降维操作
date: 2018-03-13 11:59:21
update: 
comments: true
categories:
  - Machine learning
tags:
  - MLlib
---

<!--more-->
本文参考- Machine learning with Scala by CDA 吴昊天

​	**降维（Dimensionality Reduction）** 是机器学习中的一种重要的特征处理手段，它可以减少计算过程中考虑到的随机变量（即特征）的个数，其被广泛应用于各种机器学习问题中，用于消除噪声、对抗数据稀疏问题。它在尽可能维持原始数据的内在结构的前提下，得到一组描述原数据的，低维度的隐式特征（或称主要特征）。

​	MLlib机器学习库提供了两个常用的降维方法：**奇异值分解（Singular Value Decomposition，SVD）** 和 **主成分分析（Principal Component Analysis，PCA）**，下面我们将通过实例介绍其具体的使用方法。

##一、奇异值分解（SVD）

###1、概念介绍

​	**奇异值分解（SVD** 来源于代数学中的矩阵分解问题，对于一个方阵来说，我们可以利用矩阵特征值和特征向量的特殊性质（矩阵点乘特征向量等于特征值数乘特征向量），通过求特征值与特征向量来达到矩阵分解的效果：

 $A=Q∑Q^-1$

这里,$Q$是由特征向量组成的矩阵，而$∑$是特征值降序排列构成的一个对角矩阵（对角线上每个值是一个特征值，按降序排列，其他值为0），特征值的数值表示对应的特征的重要性。在很多情况下，最大的一小部分特征值的和即可以约等于所有特征值的和，而通过矩阵分解的降维就是通过在$Q$和$E$中删去那些比较小的特征值及其对应的特征向量，使用一小部分的特征值和特征向量来描述整个矩阵，从而达到降维的效果。

​	但是，实际问题中大多数矩阵是以奇异矩阵形式，而不是方阵的形式出现的，奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为

$m*n$

奇异矩阵
$A=UEV^T$
分解成三个部分 :
其中$U$
和$V$
是两个正交矩阵，其中的每一行（每一列）分别被称为 **左奇异向量** 和 **右奇异向量**，他们和∑ 

中对角线上的奇异值相对应，通常情况下我们只需要取一个较小的值$k$
，保留前$k$
个奇异向量和奇异值即可，其中$U$
的维度是
$m*k$
	
$V$的维度是$n*k$

∑
是一个
$k*k$
的方阵，从而达到降维效果。

### 2、SVD变换

​	Mllib内置的奇异值分解功能位于`org.apache.spark.mllib.linalg`包下的`RowMatrix`和`IndexedRowMatrix`类中，所以，我们必须先通过已有数据创建出相应矩阵类型的对象，然后调用该类的成员方法来进行SVD分解，这里以`RowMatrix`为例：

首先，引入需要的类：

```scala
import org.apache.spark.mllib.linalg.distributed.RowMatrix
```

准备好一个矩阵，这里我们采用一个简单的文件`a.mat`来存储一个尺寸为(4,9)的矩阵，其内容如下：

```sh
1 2 3 4 5 6 7 8 9
5 6 7 8 9 0 8 6 7
9 0 8 7 1 4 3 2 1
6 4 2 1 3 4 2 1 5
```

随后，将该文本文件读入成`RDD[Vector]`，并转换成`RowMatrix`，即可调用`RowMatrix`自带的`computeSVD`方法计算分解结果，这一结果保存在类型为`SingularValueDecomposition`的`svd`对象中：

```scala
val data = sc.textFile("a.mat").map(_.split(" ").map(_.toDouble)).map(line => Vectors.dense(line))
val rm = new RowMatrix(data)    //通过RDD[Vectors]创建行矩阵 
val svd = rm.computeSVD(3)      //保留前3个奇异值
```

通过访问`svd`对象的`V`、`s`、`U`成员分别拿到进行SVD分解后的右奇异矩阵、奇异值向量和左奇异矩阵：

```scala
svd.s
svd.V
svd.U
```

这里可以看到，由于限定了取前三个奇异值，所以奇异值向量`s`包含有三个从大到小排列的奇异值，而右奇异矩阵`V`中的每一列都代表了对应的右奇异向量。`U`成员得到的是一个`null`值，这是因为在实际运用中，只需要`V`和`S`两个成员，即可通过矩阵计算达到降维的效果。

​	如果需要获得`U`成员，可以在进行SVD分解时，指定`computeU`参数，令其等于`True`，即可在分解后的`svd`对象中拿到`U`成员，则需要进行如下操作

```scala
val svd = rm.computeSVD(3, computeU = true)
```

## 二、主成分分析（PCA）

### 1、概念介绍

​	**主成分分析（PCA）** 是一种对数据进行旋转变换的统计学方法，其本质是在线性空间中进行一个基变换，使得变换后的数据投影在一组新的“坐标轴”上的方差最大化，随后，裁剪掉变换后方差很小的“坐标轴”，剩下的新“坐标轴”即被称为 **主成分（Principal Component）** ，它们可以在一个较低维度的子空间中尽可能地表示原有数据的性质。主成分分析被广泛应用在各种统计学、机器学习问题中，是最常见的降维方法之一。PCA有许多具体的实现方法，可以通过计算协方差矩阵，甚至是通过上文提到的SVD分解来进行PCA变换。

### 2、PCA变换

​	MLlib提供了两种进行PCA变换的方法，第一种与上文提到的SVD分解类似，位于`org.apache.spark.mllib.linalg`包下的`RowMatrix`中，这里，我们同样读入上文中提到的`a.mat`文件，对其进行PCA变换：

```scala
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed.RowMatrix
val rm = new RowMatrix(data)                 //通过RDD[Vectors]创建行矩阵
val pc = rm.computePrincipalComponents(3)    //保留前3个主成分
```

可以看到，主成分矩阵是一个尺寸为(9,3)的矩阵，其中每一列代表一个主成分（新坐标轴），每一行代表原有的一个特征，而`a.mat`矩阵可以看成是一个有4个样本，9个特征的数据集，那么，主成分矩阵相当于把原有的9维特征空间投影到一个3维的空间中，从而达到降维的效果。

​	同时，我们可以通过矩阵乘法来完成对原矩阵的PCA变换，可以看到原有的(4,9)矩阵被变换成新的(4,3)矩阵。

```scala
val projected = rm.multiply(pc)
projected.rows.collect()
```

需要注意的是，MLlib提供的PCA变换方法最多只能处理65535维的数据。

### 3、“模型式”的PCA变换实现

​	除了矩阵类内置的PCA变换外，MLlib还提供了一种“模型式”的PCA变换实现，它位于`org.apache.spark.mllib.feature`包下的`PCA`类，它可以接受`RDD[Vectors]`作为参数，进行PCA变换。该方法特别适用于原始数据是`LabeledPoint`类型的情况，只需取出`LabeledPoint`的`feature`成员（它是`RDD[Vector]`类型），对其做PCA操作后再放回，即可在不影响原有标签情况下进行PCA变换。

首先引入需要使用到的类：

```scala
import org.apache.spark.mllib.feature.PCA
import org.apache.spark.mllib.regression.LabeledPoint
```

依然使用前文的`a.mat`矩阵，为了创造出`LabeledPoint`，我们为第一个样本标注标签为`0.0`，其他为`1.0`。

```scala
val data = sc.textFile("file:///home/hadoop/a.mat").map(_.split(" ").map(_.toDouble)).map(line => {
     |     LabeledPoint( if(line(0) > 1.0) 1.toDouble else 0.toDouble, Vectors.dense(line) )
     | })
data.collect()
```

随后，创建一个`PCA`类的对象，在构造器中给定主成分个数为3，并调用其`fit`方法来生成一个`PCAModel`类的对象`pca`，该对象保存了对应的主成分矩阵：

```scala
val pca = new PCA(3).fit(data.map(_.features))
```

对于`LabeledPoint`型的数据来说，可使用`map`算子对每一条数据进行处理，将`features`成员替换成PCA变换后的特征即可：

```scala
val projected = data.map(p => p.copy(features = pca.transform(p.features)))
projected.collect()
```


