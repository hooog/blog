<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="scikit-learn," />





  <link rel="alternate" href="/atom.xml" title="Hoooge's Blog" type="application/atom+xml" />






<meta name="description" content="数据集Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 样本 ，第二个维度代表 特征 （每一行代表一个样本，每一列代表一种特征）。 样例: iris 数据集（鸢尾花卉数据集）这个数据集包含150个样本，每个样本包含4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，详细数据可以通过iris.DESCR查看。如果原始数">
<meta name="keywords" content="scikit-learn">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习scikit-learn 中的设置以及预估对象">
<meta property="og:url" content="http://www.ihoge.cn/2018/关于科学数据处理的统计学习教程.html">
<meta property="og:site_name" content="Hoooge&#39;s Blog">
<meta property="og:description" content="数据集Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 样本 ，第二个维度代表 特征 （每一行代表一个样本，每一列代表一种特征）。 样例: iris 数据集（鸢尾花卉数据集）这个数据集包含150个样本，每个样本包含4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，详细数据可以通过iris.DESCR查看。如果原始数">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_4_0.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_11_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_11_2.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_21_0.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_23_0.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_34_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_36_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_45_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_45_2.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_45_3.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_59_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_68_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_72_2.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_72_3.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_72_4.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_72_5.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_80_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_80_2.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_84_1.png">
<meta property="og:image" content="http://www.ihoge.cn/images/sklearn/output_88_0.png">
<meta property="og:updated_time" content="2018-04-06T17:31:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习scikit-learn 中的设置以及预估对象">
<meta name="twitter:description" content="数据集Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 样本 ，第二个维度代表 特征 （每一行代表一个样本，每一列代表一种特征）。 样例: iris 数据集（鸢尾花卉数据集）这个数据集包含150个样本，每个样本包含4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，详细数据可以通过iris.DESCR查看。如果原始数">
<meta name="twitter:image" content="http://www.ihoge.cn/images/sklearn/output_4_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.ihoge.cn/2018/关于科学数据处理的统计学习教程.html"/>





  <title>机器学习scikit-learn 中的设置以及预估对象 | Hoooge's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/hooog" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hoooge's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Life is short</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.ihoge.cn/2018/关于科学数据处理的统计学习教程.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="刘知行">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoooge's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习scikit-learn 中的设置以及预估对象</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-18T17:05:59+08:00">
                2018-03-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i> 浏览
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>K
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <a id="more"></a>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 样本 ，第二个维度代表 特征 （每一行代表一个样本，每一列代表一种特征）。</p>
<h3 id="样例-iris-数据集（鸢尾花卉数据集）"><a href="#样例-iris-数据集（鸢尾花卉数据集）" class="headerlink" title="样例: iris 数据集（鸢尾花卉数据集）"></a>样例: iris 数据集（鸢尾花卉数据集）</h3><p>这个数据集包含150个样本，每个样本包含4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，详细数据可以通过<code>iris.DESCR</code>查看。<br>如果原始数据不是<code>(n_samples, n_features)</code>的形状时，使用之前需要进行预处理以供scikit-learn使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">data = iris.data</span><br><span class="line">data.shape</span><br></pre></td></tr></table></figure>
<pre><code>(150, 4)
</code></pre><h3 id="数据预处理样例-digits数据集-手写数字数据集"><a href="#数据预处理样例-digits数据集-手写数字数据集" class="headerlink" title="数据预处理样例:digits数据集(手写数字数据集)"></a>数据预处理样例:digits数据集(手写数字数据集)</h3><p>digits数据集包含1797个手写数字的图像，每个图像为8*8像素<br>为了在scikit中使用这一数据集，需要将每一张8×8的图像转换成长度为64的特征向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">digits = datasets.load_digits()</span><br><span class="line">digits.images.shape</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="comment">#添加魔法才能绘图</span></span><br><span class="line">%matplotlib inline     </span><br><span class="line"></span><br><span class="line">plt.imshow(digits.images[<span class="number">-1</span>],cmap=plt.cm.gray_r) </span><br><span class="line"><span class="comment">#为了在scikit中使用这一数据集，需要将每一张8×8的图像转换成长度为64的特征向量</span></span><br><span class="line">data = digits.images.reshape((digits.images.shape[<span class="number">0</span>], <span class="number">-1</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/images/sklearn/output_4_0.png" alt="png"></p>
<h2 id="预估对象"><a href="#预估对象" class="headerlink" title="预估对象"></a>预估对象</h2><p>拟合数据: scikit-learn实现最重要的一个API是<code>estimator</code>。<code>estimators</code>是基于数据进行学习的任何对象，它可以是一个分类器，回归或者是一个聚类算法，或者是从原始数据中提取/过滤有用特征的变换器。<br>所有的拟合模型对象拥有一个名为<code>fit</code>的方法，参数是一个数据集（通常是一个2维列表）:</p>
<p>使用方式：</p>
<blockquote>
<p><code>estimator.fit(data)</code></p>
</blockquote>
<p>拟合模型对象构造参数: 在创建一个拟合模型时，可以设置相关参数，在创建之后也可以修改对应的参数:</p>
<blockquote>
</blockquote>
<p><code>estimator = Estimator(param1=1, param2=2)
estimator.param1</code></p>
<p>拟合参数: 当拟合模型完成对数据的拟合之后，可以从拟合模型中获取拟合的参数结果，所有拟合完成的参数均以下划线(_)作为结尾:</p>
<blockquote>
<p><code>estimator.estimated_param_</code></p>
</blockquote>
<h1 id="监督学习：从高维观察预测输出变量"><a href="#监督学习：从高维观察预测输出变量" class="headerlink" title="监督学习：从高维观察预测输出变量"></a>监督学习：从高维观察预测输出变量</h1><p>监督学习解决的问题<br>监督学习 在于学习两个数据集的联系：观察数据 X 和我们正在尝试预测的额外变量 y (通常称“目标”或“标签”)， 而且通常是长度为 n_samples 的一维数组。<br>scikit-learn 中所有监督的 估计量 <a href="https://en.wikipedia.org/wiki/Estimator" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Estimator</a> 都有一个用来拟合模型的 fit(X, y) 方法，和根据给定的没有标签观察值 X 返回预测的带标签的 y 的 predict(X) 方法。<br>词汇：分类和回归<br>如果预测任务是为了将观察值分类到有限的标签集合中，换句话说，就是给观察对象命名，那任务就被称为 分类 任务。另外，如果任务是为了预测一个连续的目标变量，那就被称为 回归 任务。<br>当在 scikit-learn 中进行分类时，y 是一个整数或字符型的向量。<br>注：</p>
<p><strong>回归是统计学中最有力的工具之一。机器学习监督学习算法分为分类算法和回归算法两种，其实就是根据类别标签分布类型为离散型、连续性而定义的。<br>分类算法用于离散型分布预测，如KNN、决策树、朴素贝叶斯、adaboost、SVM、Logistic回归都是分类算法；回归算法用于连续型分布预测，针对的是数值型的样本，使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。</strong></p>
<h2 id="最近邻和维度惩罚"><a href="#最近邻和维度惩罚" class="headerlink" title="最近邻和维度惩罚"></a>最近邻和维度惩罚</h2><h3 id="鸢尾属植物分类："><a href="#鸢尾属植物分类：" class="headerlink" title="鸢尾属植物分类："></a>鸢尾属植物分类：</h3><p>鸢尾属植物数据集是根据花瓣长度、花瓣度度、萼片长度和萼片宽度4个特征对3种不同类型的鸢尾属植物进行分类:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">iris_X = iris.data</span><br><span class="line">iris_y = iris.target</span><br><span class="line">np.unique(iris_y)   <span class="comment">#a = np.unique(A)对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表</span></span><br></pre></td></tr></table></figure>
<pre><code>array([0, 1, 2])
</code></pre><p>图片：鸢尾花.png</p>
<h3 id="K近邻分类器"><a href="#K近邻分类器" class="headerlink" title="K近邻分类器"></a>K近邻分类器</h3><p><strong>最近邻</strong>: 也许是最简单的分类器：给定一个新的观察值 X_test，用最接近的特征向量在训练集(比如，用于训练估计器的数据)找到观察值。(请看 Scikit-learn 在线学习文档的 最近邻章节 获取更多关于这种分类器的信息)</p>
<h3 id="训练集和测试集"><a href="#训练集和测试集" class="headerlink" title="训练集和测试集"></a>训练集和测试集</h3><p>当用任意的学习算法进行实验时，最重要的就是不要在用于拟合估计器的数据上测试一个估计器的预期值，因为这不会评估在 新数据 上估计器的执行情况。这也是数据集经常被分为 训练 和 测试 数据的原因。</p>
<h3 id="KNN-k-最近邻-分类器例子"><a href="#KNN-k-最近邻-分类器例子" class="headerlink" title="KNN(k 最近邻)分类器例子:"></a>KNN(k 最近邻)分类器例子:</h3><p>K最近邻(k-Nearest Neighbor，KNN)分类算法的核心思想是如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法可用于多分类，KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，作为预测值。</p>
<p>KNeighborsClassifier在scikit-learn 在sklearn.neighbors包之中。KNeighborsClassifier使用很简单，三步：</p>
<p>1）创建KNeighborsClassifier对象，</p>
<p>2）调用fit函数，</p>
<p>3）调用predict函数进行预测。以下代码说明了用法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据展示</span></span><br><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets</span><br><span class="line"></span><br><span class="line">n_neighbors = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import some data to play with</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we only take the first two features. We could avoid this ugly</span></span><br><span class="line"><span class="comment"># slicing by using a two-dim dataset</span></span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create color maps</span></span><br><span class="line">cmap_light = ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">'uniform'</span>, <span class="string">'distance'</span>]:</span><br><span class="line">    <span class="comment"># we create an instance of Neighbours Classifier and fit the data.</span></span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">"3-Class classification (k = %i, weights = '%s')"</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Automatically created module for IPython interactive environment
</code></pre><p><img src="/images/sklearn/output_11_1.png" alt="png"></p>
<p><img src="/images/sklearn/output_11_2.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将鸢尾属植物数据集分解为训练集和测试集</span></span><br><span class="line"><span class="comment"># 随机排列，用于使分解的数据随机分布</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">indices = np.random.permutation(len(iris_X))<span class="comment">#np.random.permutation(x)返回一个随机排列,这里以0为随机数种子返回1～150的随机排列</span></span><br><span class="line">iris_X_train = iris_X[indices[:<span class="number">-10</span>]]</span><br><span class="line">iris_y_train = iris_y[indices[:<span class="number">-10</span>]]</span><br><span class="line">iris_X_test  = iris_X[indices[<span class="number">-10</span>:]]</span><br><span class="line">iris_y_test  = iris_y[indices[<span class="number">-10</span>:]]</span><br><span class="line"><span class="comment"># 创建和拟合一个最近邻分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier <span class="comment">#K最邻近分类器</span></span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line">knn.fit(iris_X_train, iris_y_train) </span><br><span class="line"></span><br><span class="line">print(<span class="string">"预测值："</span>,knn.predict(iris_X_test))</span><br><span class="line">print(<span class="string">"目标值："</span>,iris_y_test)</span><br><span class="line"><span class="comment"># 结果显示预测准确率90%</span></span><br></pre></td></tr></table></figure>
<pre><code>预测值： [1 2 1 0 0 0 2 1 2 0]
目标值： [1 1 1 0 0 0 2 1 2 0]
</code></pre><h3 id="维度惩罚"><a href="#维度惩罚" class="headerlink" title="维度惩罚"></a>维度惩罚</h3><p><strong>为了使一个估计器有效，你需要邻接点间的距离小于一些值：d，这取决于具体问题。在一维中，这需要平均 n sim 1/d 点。在上文 k-NN 例子中，如果数据只是由一个0到1的特征值和 n 训练观察值所描述，那么新数据将不会超过 1/n。因此，最近邻决策规则会很有效率，因为与类间特征变量范围相比， 1/n 很小。<br>如果特征数是 p，你现在就需要 n \sim 1/d^p 点。也就是说我们在一维 [0, 1] 空间里需要10个点，在 p 维里就需要 10^p 个点。当 p 增大时，为了得到一个好的估计器，相应的训练点数量就需要成倍增大。<br>比如，如果每个点只是单个数字(8个字节)，那么一个 k-NN 估计器在一个非常小的 p \sim 20 维度下就需要比现在估计的整个互联网的大小(±1000 艾字节或更多)还要多的训练数据。<br>这叫 维度惩罚，是机器学习领域的核心问题。</strong></p>
<h2 id="线性模型：从回归到稀疏"><a href="#线性模型：从回归到稀疏" class="headerlink" title="线性模型：从回归到稀疏"></a>线性模型：从回归到稀疏</h2><h3 id="糖尿病数据集"><a href="#糖尿病数据集" class="headerlink" title="糖尿病数据集"></a>糖尿病数据集</h3><p>糖尿病数据集包括442名患者的10个生理特征(年龄，性别，体重，血压)，和一年后的疾病级别指标:</p>
<p>手头上的任务是为了从生理特征预测疾病级别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">diabetes = datasets.load_diabetes()</span><br><span class="line">diabetes_X_train = diabetes.data[:<span class="number">-20</span>]</span><br><span class="line">diabetes_X_test  = diabetes.data[<span class="number">-20</span>:]</span><br><span class="line">diabetes_y_train = diabetes.target[:<span class="number">-20</span>]</span><br><span class="line">diabetes_y_test  = diabetes.target[<span class="number">-20</span>:]</span><br></pre></td></tr></table></figure>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>LinearRegression，最简单的拟合线性模型形式，是通过调整数据集的一系列参数令残差平方和尽可能小。</p>
<p>Linear models: $ y = X\beta + \epsilon$</p>
<p>$X$: 数据</p>
<p>$y$: 目标变量</p>
<p>$\beta$: 回归系数</p>
<p>$\epsilon$: 观察噪声</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line">print(regr.coef_) <span class="comment">#这个参数反映的是每个特征对总体特征的影响力，下次找机会咨询老师</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 均方误差</span></span><br><span class="line">mean = np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"\n\n测试线性相关性"</span>)</span><br><span class="line">regr.score(diabetes_X_test, diabetes_y_test) </span><br><span class="line"><span class="comment"># 方差分数：1 是完美的预测</span></span><br><span class="line"><span class="comment"># 0 意味着 X 和 y 之间没有线性关系。</span></span><br></pre></td></tr></table></figure>
<pre><code>[  3.03499549e-01  -2.37639315e+02   5.10530605e+02   3.27736980e+02
  -8.14131709e+02   4.92814588e+02   1.02848452e+02   1.84606489e+02
   7.43519617e+02   7.60951722e+01]


测试线性相关性





0.58507530226905713
</code></pre><p><strong>注：这里为什么不使用pandas的pd.corr()方法？<br>pd.corr()方法只能处理一维的数据。
</strong></p>
<h3 id="收缩（岭回归-）"><a href="#收缩（岭回归-）" class="headerlink" title="收缩（岭回归 ）"></a>收缩（岭回归 ）</h3><p>如果每个维度的数据点很少，观察噪声就会导致很大的方差：</p>
<p><a href="https://www.cnblogs.com/magle/p/5878967.html" target="_blank" rel="noopener">岭回归学习笔记</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = np.c_[ <span class="number">.5</span>, <span class="number">1</span>].T</span><br><span class="line">y = [<span class="number">.5</span>, <span class="number">1</span>]</span><br><span class="line">test = np.c_[ <span class="number">0</span>, <span class="number">2</span>].T</span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line">plt.figure() </span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">6</span>): </span><br><span class="line">    this_X = <span class="number">.1</span>*np.random.normal(size=(<span class="number">2</span>, <span class="number">1</span>)) + X</span><br><span class="line">    regr.fit(this_X, y)</span><br><span class="line">    plt.plot(test, regr.predict(test)) </span><br><span class="line">    plt.scatter(this_X, y, s=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/sklearn/output_21_0.png" alt="png"></p>
<p>高纬统计学习中的一个解决方法是 收缩 回归系数到0：任何两个随机选择的观察值数据集都很可能是不相关的。这称为 <strong>岭回归 ：</strong></p>
<p><em>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</em></p>
<p>这是 <code>bias/variance tradeoff</code> 中的一个例子：<strong>岭参数 alpha 越大，偏差越大，方差越小。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">regr = linear_model.Ridge(alpha=<span class="number">.1</span>)</span><br><span class="line"></span><br><span class="line">plt.figure() </span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">6</span>): </span><br><span class="line">    this_X = <span class="number">.1</span>*np.random.normal(size=(<span class="number">2</span>, <span class="number">1</span>)) + X</span><br><span class="line">    regr.fit(this_X, y)</span><br><span class="line">    plt.plot(test, regr.predict(test)) </span><br><span class="line">    plt.scatter(this_X, y, s=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/sklearn/output_23_0.png" alt="png"></p>
<p>下面我们可以选择 alpha 来最小化排除错误，这里使用糖尿病数据集而不是人为数据:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alphas</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.0001    ,  0.00039811,  0.00158489,  0.00630957,  0.02511886,
        0.1       ])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">alphas = np.logspace(<span class="number">-4</span>, <span class="number">-1</span>, <span class="number">6</span>) <span class="comment">#生成6个在10的-4次方 ～ 10的-1次方之间的等比数列</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> alphas:</span><br><span class="line">    newregr = regr.set_params(alpha=alpha)</span><br><span class="line">    newregr.fit(diabetes_X_train, diabetes_y_train,)</span><br><span class="line">    print(newregr.score(diabetes_X_test, diabetes_y_test))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print([regr.set_params(alpha=alpha</span></span><br><span class="line"><span class="comment">#             ).fit(diabetes_X_train, diabetes_y_train,</span></span><br><span class="line"><span class="comment">#             ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas])</span></span><br></pre></td></tr></table></figure>
<pre><code>0.585111068388
0.585207301544
0.58546775407
0.58555120365
0.583071708555
0.570589994373
</code></pre><p><em>Note 捕获拟合参数噪声使得模型不能归纳新的数据称为 过拟合。岭回归产生的偏差被称为 正则化。</em></p>
<h3 id="稀疏"><a href="#稀疏" class="headerlink" title="稀疏"></a>稀疏</h3><p>图片： 稀疏.png</p>
<p><em>Note 整个糖尿病数据集包括11个维度(10个特征维度和1个目标变量)。很难直观地表示出来，但是记住那是一个比较 空 的空间可能比较有用。</em></p>
<p>为了提高问题的条件(比如，缓解<code>维度惩罚</code>)，只选择信息特征和设置无信息时就会变得有趣，比如特征2到0。岭回归会减小他们的值，但不会减到0.另一种抑制方法，称为 <strong>Lasso (最小绝对收缩和选择算子)</strong>，可以把一些系数设为0。这些方法称为 稀疏法，稀疏可以看作是奥卡姆剃刀的应用：模型越简单越好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">regr = linear_model.Lasso()</span><br><span class="line">scores = [regr.set_params(alpha=alpha</span><br><span class="line">            ).fit(diabetes_X_train, diabetes_y_train</span><br><span class="line">            ).score(diabetes_X_test, diabetes_y_test)</span><br><span class="line">       <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</span><br><span class="line">best_alpha = alphas[scores.index(max(scores))]  <span class="comment">#取出相关值最大的索引，用其作为最佳alpha取值</span></span><br><span class="line">regr.alpha = best_alpha</span><br><span class="line"></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(regr.coef_) <span class="comment">#返回9个特征分别对应的相关系数</span></span><br></pre></td></tr></table></figure>
<pre><code>[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982    -0.
 -187.19554705   69.38229038  508.66011217   71.84239008]
</code></pre><h3 id="同一个问题的不同算法"><a href="#同一个问题的不同算法" class="headerlink" title="同一个问题的不同算法"></a>同一个问题的不同算法</h3><p>不同的算法可以用于解决同一个数学问题。比如在 scikit-learn 里 Lasso 对象使用 coordinate descent 方法解决 lasso 回归问题，对于大型数据集很有效。但是，scikit-learn 也提供了使用<strong> LARS 算法 的:class:LassoLars 对象，对于处理带权向量非常稀疏的数据非常有效(比如，问题的观察值很少)。</strong></p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>对于分类，比如标定 鸢尾属植物 任务，线性回归就不是好方法了，因为它会给数据很多远离决策边界的权值。一个线性方法是为了拟合 sigmoid 函数 或 logistic 函数：<br>$$y = \textrm{sigmoid}(X\beta - \textrm{offset}) + \epsilon =<br>\frac{1}{1 + \textrm{exp}(- X\beta + \textrm{offset})} + \epsilon$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Code source: Gael Varoquaux</span></span><br><span class="line"><span class="comment"># License: BSD 3 clause</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># this is our test set, it's just a straight line with some</span></span><br><span class="line"><span class="comment"># Gaussian noise</span></span><br><span class="line">xmin, xmax = <span class="number">-5</span>, <span class="number">5</span></span><br><span class="line">n_samples = <span class="number">100</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.normal(size=n_samples)</span><br><span class="line">y = (X &gt; <span class="number">0</span>).astype(np.float)</span><br><span class="line">X[X &gt; <span class="number">0</span>] *= <span class="number">4</span></span><br><span class="line">X += <span class="number">.3</span> * np.random.normal(size=n_samples)</span><br><span class="line"></span><br><span class="line">X = X[:, np.newaxis]</span><br><span class="line"><span class="comment"># run the classifier</span></span><br><span class="line">clf = linear_model.LogisticRegression(C=<span class="number">1e5</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and plot the result</span></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.clf()</span><br><span class="line">plt.scatter(X.ravel(), y, color=<span class="string">'black'</span>, zorder=<span class="number">20</span>)</span><br><span class="line">X_test = np.linspace(<span class="number">-5</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">loss = model(X_test * clf.coef_ + clf.intercept_).ravel()</span><br><span class="line">plt.plot(X_test, loss, color=<span class="string">'red'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">ols = linear_model.LinearRegression()</span><br><span class="line">ols.fit(X, y)</span><br><span class="line">plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.axhline(<span class="number">.5</span>, color=<span class="string">'.5'</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'y'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.xticks(range(<span class="number">-5</span>, <span class="number">10</span>))</span><br><span class="line">plt.yticks([<span class="number">0</span>, <span class="number">0.5</span>, <span class="number">1</span>])</span><br><span class="line">plt.ylim(<span class="number">-.25</span>, <span class="number">1.25</span>)</span><br><span class="line">plt.xlim(<span class="number">-4</span>, <span class="number">10</span>)</span><br><span class="line">plt.legend((<span class="string">'Logistic Regression Model'</span>, <span class="string">'Linear Regression Model'</span>),</span><br><span class="line">           loc=<span class="string">"lower right"</span>, fontsize=<span class="string">'small'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Automatically created module for IPython interactive environment
</code></pre><p><img src="/images/sklearn/output_34_1.png" alt="png"></p>
<h3 id="逻辑回归：（LogisticRegression）"><a href="#逻辑回归：（LogisticRegression）" class="headerlink" title="逻辑回归：（LogisticRegression）"></a>逻辑回归：（<code>LogisticRegression</code>）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logistic = linear_model.LogisticRegression(C=<span class="number">1e5</span>)</span><br><span class="line">logistic.fit(iris_X_train, iris_y_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Code source: Gaël Varoquaux</span></span><br><span class="line"><span class="comment"># Modified for documentation by Jaques Grobler</span></span><br><span class="line"><span class="comment"># License: BSD 3 clause</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># import some data to play with</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># we only take the first two features.</span></span><br><span class="line">Y = iris.target</span><br><span class="line"></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line">logreg = linear_model.LogisticRegression(C=<span class="number">1e5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># we create an instance of Neighbours Classifier and fit the data.</span></span><br><span class="line">logreg.fit(X, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line"><span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put the result into a color plot</span></span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot also the training points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, edgecolors=<span class="string">'k'</span>, cmap=plt.cm.Paired)</span><br><span class="line">plt.xlabel(<span class="string">'Sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal width'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br><span class="line">plt.xticks(())</span><br><span class="line">plt.yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Automatically created module for IPython interactive environment
</code></pre><p><img src="/images/sklearn/output_36_1.png" alt="png"></p>
<h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p>如果你有很多类需要预测，一种常用方法就是去拟合一对多分类器，然后使用根据投票为最后做决定。</p>
<h3 id="使用-logistic-回归进行收缩和稀疏"><a href="#使用-logistic-回归进行收缩和稀疏" class="headerlink" title="使用 logistic 回归进行收缩和稀疏"></a>使用 logistic 回归进行收缩和稀疏</h3><p>LogisticRegression 对象中的 C 参数控制着正则化数量：C 值越大，正则化数量越小。penalty=”l2” 提供 收缩(比如，无稀疏系数)，同时 <code>penalty=”l1”</code> 提供稀疏化。</p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><p>尝试用最近邻和线性模型分类数字数据集。留出最后 10%的数据，并测试观察值预期效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, neighbors, linear_model</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X_digits = digits.data</span><br><span class="line">y_digits = digits.target</span><br><span class="line"></span><br><span class="line">n_samples = len(X_digits)</span><br><span class="line">X_train = X_digits[:int(<span class="number">.9</span> * n_samples)]</span><br><span class="line">y_train = y_digits[:int(<span class="number">.9</span> * n_samples)]</span><br><span class="line">X_test = X_digits[int(<span class="number">.9</span> * n_samples):]</span><br><span class="line">y_test = y_digits[int(<span class="number">.9</span> * n_samples):]</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier()</span><br><span class="line">logistic = linear_model.LogisticRegression()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'KNN score: %f'</span> % knn.fit(X_train, y_train).score(X_test, y_test))</span><br><span class="line">print(<span class="string">'LogisticRegression score: %f'</span></span><br><span class="line">      % logistic.fit(X_train, y_train).score(X_test, y_test))</span><br><span class="line"><span class="comment"># 测试证明针对此数据，KNN最邻近分类效果更佳</span></span><br></pre></td></tr></table></figure>
<pre><code>KNN score: 0.961111
LogisticRegression score: 0.938889
</code></pre><h2 id="支持向量积-SVMs"><a href="#支持向量积-SVMs" class="headerlink" title="支持向量积(SVMs)"></a>支持向量积(SVMs)</h2><h3 id="线性-SVMs"><a href="#线性-SVMs" class="headerlink" title="线性 SVMs"></a>线性 SVMs</h3><p>支持向量机 属于判别模型家族：它们尝试通过找到样例的一个组合来构建一个两类之间最大化的平面。通过 C 参数进行正则化设置：C 的值小意味着边缘是通过分割线周围的所有观测样例进行计算得到的(更正则化)；C 的值大意味着边缘是通过邻近分割线的观测样例计算得到的(更少正则化)。</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a><a href="http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py" target="_blank" rel="noopener">例子</a></h4><p>SVMs 可以用于回归或分类</p>
<ul>
<li>回归:–:class: SVR (支持向量回归)</li>
<li>分类 –:class: SVC (支持向量分类)</li>
</ul>
<p><strong>Warning 规格化数据</strong></p>
<p><strong>对很多估计器来说，包括 SVMs，为每个特征值使用单位标准偏差的数据集，是获得好的预测重要前提。</strong></p>
<h3 id="使用核"><a href="#使用核" class="headerlink" title="使用核"></a>使用核</h3><p>在特征空间类并不总是线性可分的。解决办法就是构建一个不是线性的但能是多项式的函数做代替。这要使用 核技巧(kernel trick)，它可以被看作通过设置 kernels 在观察样例上创建决策力量：</p>
<h4 id="线性核"><a href="#线性核" class="headerlink" title="线性核"></a><a href="http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_kernels.html" target="_blank" rel="noopener">线性核</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svc = svm.SVC(kernel=<span class="string">'linear'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="多项式核"><a href="#多项式核" class="headerlink" title="多项式核"></a><a href="http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_kernels.html" target="_blank" rel="noopener">多项式核</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">svc = svm.SVC(kernel=<span class="string">'poly'</span>,</span><br><span class="line">              degree=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># degree: polynomial degree</span></span><br></pre></td></tr></table></figure>
<h4 id="RBF（径向基函数）"><a href="#RBF（径向基函数）" class="headerlink" title="RBF（径向基函数）"></a><a href="http://sklearn.apachecn.org/cn/0.19.0/auto_examples/svm/plot_svm_kernels.html" target="_blank" rel="noopener">RBF（径向基函数）</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">svc = svm.SVC(kernel=<span class="string">'rbf'</span>)</span><br><span class="line"><span class="comment"># gamma: inverse of size of</span></span><br><span class="line"><span class="comment"># radial kernel</span></span><br></pre></td></tr></table></figure>
<h3 id="交互例子"><a href="#交互例子" class="headerlink" title="交互例子"></a>交互例子</h3><p>查看 SVM GUI 通过下载 svm_gui.py；通过左右按键添加两类数据点，拟合模型并改变参数和数据。</p>
<h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h3><p>根据特征1和特征2，尝试用 SVMs 把1和2类从鸢尾属植物数据集中分出来。为每一个类留下10%，并测试这些观察值预期效果。<br>警告: 类是有序的，不要留下最后10%，不然你只能测试一个类了。<br>提示: 为了直观显示，你可以在网格上使用 decision_function 方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">================================</span></span><br><span class="line"><span class="string">SVM Exercise</span></span><br><span class="line"><span class="string">================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A tutorial exercise for using different SVM kernels.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">This exercise is used in the :ref:`using_kernels_tut` part of the</span></span><br><span class="line"><span class="string">:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, svm</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X = X[y != <span class="number">0</span>, :<span class="number">2</span>]  <span class="comment">#选出y不为0的1类和2类，取前两列特征</span></span><br><span class="line">y = y[y != <span class="number">0</span>] <span class="comment">#同上</span></span><br><span class="line"></span><br><span class="line">n_sample = len(X)</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">order = np.random.permutation(n_sample)  <span class="comment">#生成n_sample个随机数，用permutation方法打乱顺序（洗牌）</span></span><br><span class="line">X = X[order]  <span class="comment">#以order为索引打乱数据</span></span><br><span class="line">y = y[order].astype(np.float)</span><br><span class="line"></span><br><span class="line">X_train = X[:int(<span class="number">.9</span> * n_sample)]</span><br><span class="line">y_train = y[:int(<span class="number">.9</span> * n_sample)]</span><br><span class="line">X_test = X[int(<span class="number">.9</span> * n_sample):]</span><br><span class="line">y_test = y[int(<span class="number">.9</span> * n_sample):]</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit the model</span></span><br><span class="line"><span class="keyword">for</span> fig_num, kernel <span class="keyword">in</span> enumerate((<span class="string">'linear'</span>, <span class="string">'rbf'</span>, <span class="string">'poly'</span>)):</span><br><span class="line">    clf = svm.SVC(kernel=kernel, gamma=<span class="number">10</span>)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    plt.figure(fig_num)</span><br><span class="line">    plt.clf()</span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, zorder=<span class="number">10</span>, cmap=plt.cm.Paired,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Circle out the test data</span></span><br><span class="line">    plt.scatter(X_test[:, <span class="number">0</span>], X_test[:, <span class="number">1</span>], s=<span class="number">80</span>, facecolors=<span class="string">'none'</span>,</span><br><span class="line">                zorder=<span class="number">10</span>, edgecolor=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line">    plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">    x_min = X[:, <span class="number">0</span>].min()</span><br><span class="line">    x_max = X[:, <span class="number">0</span>].max()</span><br><span class="line">    y_min = X[:, <span class="number">1</span>].min()</span><br><span class="line">    y_max = X[:, <span class="number">1</span>].max()</span><br><span class="line"></span><br><span class="line">    XX, YY = np.mgrid[x_min:x_max:<span class="number">200j</span>, y_min:y_max:<span class="number">200j</span>]</span><br><span class="line">    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(XX.shape)</span><br><span class="line">    plt.pcolormesh(XX, YY, Z &gt; <span class="number">0</span>, cmap=plt.cm.Paired)</span><br><span class="line">    plt.contour(XX, YY, Z, colors=[<span class="string">'k'</span>, <span class="string">'k'</span>, <span class="string">'k'</span>],</span><br><span class="line">                linestyles=[<span class="string">'--'</span>, <span class="string">'-'</span>, <span class="string">'--'</span>], levels=[<span class="number">-.5</span>, <span class="number">0</span>, <span class="number">.5</span>])</span><br><span class="line"></span><br><span class="line">    plt.title(kernel)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
</code></pre><p><img src="/images/sklearn/output_45_1.png" alt="png"></p>
<p><img src="/images/sklearn/output_45_2.png" alt="png"></p>
<p><img src="/images/sklearn/output_45_3.png" alt="png"></p>
<h1 id="模型选择-选择估计量及其参数"><a href="#模型选择-选择估计量及其参数" class="headerlink" title="模型选择 选择估计量及其参数"></a>模型选择 选择估计量及其参数</h1><h2 id="分数和交叉验证分数（取最佳拟合的方式）"><a href="#分数和交叉验证分数（取最佳拟合的方式）" class="headerlink" title="分数和交叉验证分数（取最佳拟合的方式）"></a>分数和交叉验证分数（取最佳拟合的方式）</h2><p>如我们所见，每一个估计量都有一个可以在新数据上判定拟合质量(或预期值)的 score 方法。<strong>越大越好</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, svm</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X_digits = digits.data</span><br><span class="line">y_digits = digits.target</span><br><span class="line">svc = svm.SVC(C=<span class="number">1</span>, kernel=<span class="string">'linear'</span>) <span class="comment">#创建支持向量机，线性核模型</span></span><br><span class="line">svc.fit(X_digits[:<span class="number">-100</span>], y_digits[:<span class="number">-100</span>]).score(X_digits[<span class="number">-100</span>:], y_digits[<span class="number">-100</span>:])</span><br><span class="line"><span class="comment">#每一个估计量都有一个可以在新数据上判定拟合质量(或预期值)的 score 方法。越大越好</span></span><br></pre></td></tr></table></figure>
<pre><code>0.97999999999999998
</code></pre><h3 id="KFold-交叉验证"><a href="#KFold-交叉验证" class="headerlink" title="KFold 交叉验证"></a>KFold 交叉验证</h3><p>为了更好地预测精度(我们可以用它作为模型的拟合优度代理)，我们可以连续分解用于我们训练和测试用的 折叠数据。</p>
<p>以下被称为 <strong>KFold 交叉验证.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_folds = np.array_split(X_digits, <span class="number">3</span>)  <span class="comment">#以3为步长进行切割</span></span><br><span class="line">y_folds = np.array_split(y_digits, <span class="number">3</span>)</span><br><span class="line">scores = list()</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="comment"># 为了稍后的 ‘弹出’ 操作，我们使用 ‘列表’ 来复制数据</span></span><br><span class="line">    X_train = list(X_folds)</span><br><span class="line">    X_test  = X_train.pop(k)    <span class="comment">#删除索引为k的列</span></span><br><span class="line">    X_train = np.concatenate(X_train) <span class="comment">#numpy的连接函数，用法类似pandas的concat</span></span><br><span class="line">    y_train = list(y_folds)</span><br><span class="line">    y_test  = y_train.pop(k)</span><br><span class="line">    y_train = np.concatenate(y_train)</span><br><span class="line">    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))</span><br><span class="line">print(scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试发现删除第二列时的拟合效果最佳</span></span><br></pre></td></tr></table></figure>
<pre><code>[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
</code></pre><h2 id="交叉验证生成器-（第一次没搞懂）"><a href="#交叉验证生成器-（第一次没搞懂）" class="headerlink" title="交叉验证生成器 （第一次没搞懂）"></a>交叉验证生成器 （第一次没搞懂）</h2><p>scikit-learn 有可以生成训练/测试索引列表的类，可用于流行的交叉验证策略。<br>类提供了 split 方法，方法允许输入能被分解的数据集，并为每次选择的交叉验证策略迭代生成训练/测试集索引。<br>下面是使用 split 方法的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score</span><br><span class="line">X = [<span class="string">"a"</span>, <span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"c"</span>, <span class="string">"c"</span>]</span><br><span class="line">k_fold = KFold(n_splits=<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> train_indices, test_indices <span class="keyword">in</span> k_fold.split(X):</span><br><span class="line">     print(<span class="string">'Train: %s | test: %s'</span> % (train_indices, test_indices))</span><br></pre></td></tr></table></figure>
<pre><code>Train: [2 3 4 5] | test: [0 1]
Train: [0 1 4 5] | test: [2 3]
Train: [0 1 2 3] | test: [4 5]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这样就实现执行交叉验证了</span></span><br><span class="line">[svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])</span><br><span class="line">         <span class="keyword">for</span> train, test <span class="keyword">in</span> k_fold.split(X_digits)]</span><br></pre></td></tr></table></figure>
<pre><code>[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
</code></pre><p>交叉验证分数可以使用 cross_val_score 直接计算出来。给定一个估计量，交叉验证对象，和输入数据集， cross_val_score 函数就会反复分解出训练和测试集的数据，然后使用训练集和为每次迭代交叉验证运算出的基于测试集的分数来训练估计量。</p>
<p>默认情况下，估计器的 score 方法被用于运算个体分数。<br>可以参考 metrics 模块 学习更多可用的评分方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上部分代码的cross_val_score函数实现</span></span><br><span class="line">cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.93489149,  0.95659432,  0.93989983])
</code></pre><p>n_jobs=-1 意味着运算会被调度到所有 CPU 上进行。</p>
<p>或者，可以提供 <strong>scoring</strong> 参数来指定替换的评分方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(svc, X_digits, y_digits, cv=k_fold,</span><br><span class="line">                scoring=<span class="string">'precision_macro'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.93969761,  0.95911415,  0.94041254])
</code></pre><h4 id="交叉验证生成器"><a href="#交叉验证生成器" class="headerlink" title="交叉验证生成器"></a>交叉验证生成器</h4><table>
<thead>
<tr>
<th style="text-align:left"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" target="_blank" rel="noopener">Fold (n_splits, shuffle, random_state)</a></th>
<th style="text-align:right"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" target="_blank" rel="noopener">StratifiedKFold (n_splits, shuffle, random_state)</a></th>
<th style="text-align:center"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold" target="_blank" rel="noopener">GroupKFold (n_splits)</a></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">将其分解为 K 个折叠，在 K-1 上训练，然后排除测试。</td>
<td style="text-align:right">和 K-Fold 一样，但会保留每个折叠里的类分布。</td>
<td style="text-align:center">确保相同组不会在测试和训练集里。</td>
</tr>
<tr>
<td style="text-align:left"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" target="_blank" rel="noopener">ShuffleSplit (n_splits, test_size, train_size, random_state)</a></td>
<td style="text-align:right"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit" target="_blank" rel="noopener">StratifiedShuffleSplit</a></td>
<td style="text-align:center"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit" target="_blank" rel="noopener">GroupShuffleSplit</a></td>
</tr>
<tr>
<td style="text-align:left">生成基于随机排列的训练/测试索引。</td>
<td style="text-align:right">和 shuffle 分解一样，但会保留每个迭代里的类分布。</td>
<td style="text-align:center">确保相同组不会在测试和训练集里。</td>
</tr>
<tr>
<td style="text-align:left"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" target="_blank" rel="noopener">LeaveOneGroupOut ()</a></td>
<td style="text-align:right"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" target="_blank" rel="noopener">LeavePGroupsOut (n_groups)</a></td>
<td style="text-align:center"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut" target="_blank" rel="noopener">LeaveOneOut()</a></td>
</tr>
<tr>
<td style="text-align:left">使用数组分组来给观察分组。</td>
<td style="text-align:right">忽略 P 组。</td>
<td style="text-align:center">忽略一个观察。</td>
</tr>
<tr>
<td style="text-align:left"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut" target="_blank" rel="noopener">LeavePOut (p)</a></td>
<td style="text-align:right"><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit" target="_blank" rel="noopener">PredefinedSplit</a></td>
</tr>
<tr>
<td style="text-align:left">忽略 P 观察。</td>
<td style="text-align:right">生成基于预定义分解的训练/测试索引。</td>
</tr>
</tbody>
</table>
<h3 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h3><p>在数字数据集中，用一个线性内核绘制一个 SVC 估计器的交叉验证分数来作为 C 参数函数(使用从1到10的点对数网格).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, svm</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line"></span><br><span class="line">svc = svm.SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">C_s = np.logspace(<span class="number">-10</span>, <span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">scores = list()</span><br><span class="line">scores_std = list()</span><br><span class="line"><span class="keyword">for</span> C <span class="keyword">in</span> C_s:</span><br><span class="line">    svc.C = C</span><br><span class="line">    this_scores = cross_val_score(svc, X, y, n_jobs=<span class="number">1</span>)</span><br><span class="line">    scores.append(np.mean(this_scores))</span><br><span class="line">    scores_std.append(np.std(this_scores))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Do the plotting</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.clf()</span><br><span class="line">plt.semilogx(C_s, scores)</span><br><span class="line">plt.semilogx(C_s, np.array(scores) + np.array(scores_std), <span class="string">'b--'</span>)</span><br><span class="line">plt.semilogx(C_s, np.array(scores) - np.array(scores_std), <span class="string">'b--'</span>)</span><br><span class="line">locs, labels = plt.yticks()</span><br><span class="line">plt.yticks(locs, list(map(<span class="keyword">lambda</span> x: <span class="string">"%g"</span> % x, locs)))</span><br><span class="line">plt.ylabel(<span class="string">'CV score'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Parameter C'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.1</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
</code></pre><p><img src="/images/sklearn/output_59_1.png" alt="png"></p>
<h2 id="网格搜索和交叉验证估计量"><a href="#网格搜索和交叉验证估计量" class="headerlink" title="网格搜索和交叉验证估计量"></a>网格搜索和交叉验证估计量</h2><h3 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>scikit-learn 提供了一个对象，在给定数据情况下，在一个参数网格，估计器拟合期间计算分数，并选择参数来最大化交叉验证分数。这个对象在构建过程中获取估计器并提供一个估计器 API。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV, cross_val_score</span><br><span class="line">Cs = np.logspace(<span class="number">-6</span>, <span class="number">-1</span>, <span class="number">10</span>)</span><br><span class="line">clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),</span><br><span class="line">                   n_jobs=<span class="number">-1</span>)</span><br><span class="line">clf.fit(X_digits[:<span class="number">1000</span>], y_digits[:<span class="number">1000</span>])        </span><br><span class="line"></span><br><span class="line">clf.best_score_                                  </span><br><span class="line"></span><br><span class="line">clf.best_estimator_.C</span><br></pre></td></tr></table></figure>
<pre><code>0.0077426368268112772
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Prediction performance on test set is not as good as on train set</span></span><br><span class="line">clf.score(X_digits[<span class="number">1000</span>:], y_digits[<span class="number">1000</span>:])    <span class="comment">#最佳拟合效果</span></span><br></pre></td></tr></table></figure>
<pre><code>0.94353826850690092
</code></pre><p>默认情况下， <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" target="_blank" rel="noopener">GridSearchCV</a> 使用一个三倍折叠交叉验证。但是，如果它检测到分类器被传递，而不是回归，它就会使用分层的三倍。</p>
<h4 id="嵌套交叉验证"><a href="#嵌套交叉验证" class="headerlink" title="嵌套交叉验证"></a>嵌套交叉验证</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(clf, X_digits, y_digits)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">两个交叉验证循环并行执行：一个由 GridSearchCV 估计器设置 gamma，另一个 cross_val_score 则是测量估计器的预期执行情况。结果分数是对新数据上的预期分数的无偏估计。</span><br></pre></td></tr></table></figure>
<p><strong>Warning 你不可以并行运算嵌套对象(n_jobs 与1不同)。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_val_score(clf, X_digits, y_digits)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.93853821,  0.96327212,  0.94463087])
</code></pre><h3 id="交叉验证估计量"><a href="#交叉验证估计量" class="headerlink" title="交叉验证估计量"></a>交叉验证估计量</h3><p>设置参数的交叉验证可以更有效地完成一个基础算法。这就是为什么对某些估计量来说，scikit-learn 提供了 交叉验证 估计量自动设置它们的参数。</p>
<p>这些估计量和它们的副本称呼类似，在名字后加 ‘CV’。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, datasets</span><br><span class="line">lasso = linear_model.LassoCV()</span><br><span class="line">diabetes = datasets.load_diabetes()</span><br><span class="line">X_diabetes = diabetes.data</span><br><span class="line">y_diabetes = diabetes.target</span><br><span class="line">lasso.fit(X_diabetes, y_diabetes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 估计器自动选择它的 lambda:</span></span><br><span class="line">lasso.alpha_</span><br></pre></td></tr></table></figure>
<pre><code>0.012291895087486173
</code></pre><h3 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h3><p>在糖尿病数据集中，找到最优正则化参数 α。<br>另外： 你有多相信 α 的选择？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">diabetes = datasets.load_diabetes()</span><br><span class="line">X = diabetes.data[:<span class="number">150</span>]</span><br><span class="line">y = diabetes.target[:<span class="number">150</span>]</span><br><span class="line"></span><br><span class="line">lasso = Lasso(random_state=<span class="number">0</span>)</span><br><span class="line">alphas = np.logspace(<span class="number">-4</span>, <span class="number">-0.5</span>, <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">tuned_parameters = [&#123;<span class="string">'alpha'</span>: alphas&#125;]</span><br><span class="line">n_folds = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=<span class="keyword">False</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">scores = clf.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">scores_std = clf.cv_results_[<span class="string">'std_test_score'</span>]</span><br><span class="line">plt.figure().set_size_inches(<span class="number">8</span>, <span class="number">6</span>)</span><br><span class="line">plt.semilogx(alphas, scores)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot error lines showing +/- std. errors of the scores</span></span><br><span class="line">std_error = scores_std / np.sqrt(n_folds)</span><br><span class="line"></span><br><span class="line">plt.semilogx(alphas, scores + std_error, <span class="string">'b--'</span>)</span><br><span class="line">plt.semilogx(alphas, scores - std_error, <span class="string">'b--'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># alpha=0.2 controls the translucency of the fill color</span></span><br><span class="line">plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'CV score +/- std error'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'alpha'</span>)</span><br><span class="line">plt.axhline(np.max(scores), linestyle=<span class="string">'--'</span>, color=<span class="string">'.5'</span>)</span><br><span class="line">plt.xlim([alphas[<span class="number">0</span>], alphas[<span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Bonus: how much can you trust the selection of alpha?</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To answer this question we use the LassoCV object that sets its alpha</span></span><br><span class="line"><span class="comment"># parameter automatically from the data by internal cross-validation (i.e. it</span></span><br><span class="line"><span class="comment"># performs cross-validation on the training data it receives).</span></span><br><span class="line"><span class="comment"># We use external cross-validation to see how much the automatically obtained</span></span><br><span class="line"><span class="comment"># alphas differ across different cross-validation folds.</span></span><br><span class="line">lasso_cv = LassoCV(alphas=alphas, random_state=<span class="number">0</span>)</span><br><span class="line">k_fold = KFold(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Answer to the bonus question:"</span>,</span><br><span class="line">      <span class="string">"how much can you trust the selection of alpha?"</span>)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Alpha parameters maximising the generalization score on different"</span>)</span><br><span class="line">print(<span class="string">"subsets of the data:"</span>)</span><br><span class="line"><span class="keyword">for</span> k, (train, test) <span class="keyword">in</span> enumerate(k_fold.split(X, y)):</span><br><span class="line">    lasso_cv.fit(X[train], y[train])</span><br><span class="line">    print(<span class="string">"[fold &#123;0&#125;] alpha: &#123;1:.5f&#125;, score: &#123;2:.5f&#125;"</span>.</span><br><span class="line">          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Answer: Not very much since we obtained different alphas for different"</span>)</span><br><span class="line">print(<span class="string">"subsets of the data and moreover, the scores for these alphas differ"</span>)</span><br><span class="line">print(<span class="string">"quite substantially."</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.

Answer to the bonus question: how much can you trust the selection of alpha?

Alpha parameters maximising the generalization score on different
subsets of the data:
[fold 0] alpha: 0.10405, score: 0.53573
[fold 1] alpha: 0.05968, score: 0.16278
[fold 2] alpha: 0.10405, score: 0.44437

Answer: Not very much since we obtained different alphas for different
subsets of the data and moreover, the scores for these alphas differ
quite substantially.
</code></pre><p><img src="/images/sklearn/output_68_1.png" alt="png"></p>
<h1 id="无监督学习-寻求数据表示"><a href="#无监督学习-寻求数据表示" class="headerlink" title="无监督学习: 寻求数据表示"></a>无监督学习: 寻求数据表示</h1><h2 id="聚类-对样本数据进行分组"><a href="#聚类-对样本数据进行分组" class="headerlink" title="聚类: 对样本数据进行分组"></a>聚类: 对样本数据进行分组</h2><h3 id="可以利用聚类解决的问题"><a href="#可以利用聚类解决的问题" class="headerlink" title="可以利用聚类解决的问题"></a>可以利用聚类解决的问题</h3><p>对于 iris 数据集来说，我们知道所有样本有 3 种不同的类型，但是并不知道每一个样本是那种类型：此时我们可以尝试一个 <strong>clustering task（聚类任务）</strong> 聚类算法: 将样本进行分组，相似的样本被聚在一起，而不同组别之间的样本是有明显区别的，这样的分组方式就是<br><strong>clusters（聚类）</strong></p>
<h3 id="K-means-聚类算法"><a href="#K-means-聚类算法" class="headerlink" title="K-means 聚类算法"></a>K-means 聚类算法</h3><p>关于聚类有很多不同的聚类标准和相关算法，其中最简便的算法是 <a href="http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html#k-means" target="_blank" rel="noopener">K-means</a> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cluster, datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X_iris = iris.data</span><br><span class="line">y_iris = iris.target</span><br><span class="line"></span><br><span class="line">k_means = cluster.KMeans(n_clusters=<span class="number">3</span>)</span><br><span class="line">k_means.fit(X_iris) </span><br><span class="line"></span><br><span class="line">print(k_means.labels_[::<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">print(y_iris[::<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[1 1 1 1 1 2 2 2 2 2 0 0 0 0 0]
[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]
</code></pre><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a><strong>示例</strong></h4><p>The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground truth.</p>
<p><strong>Warning k_means 算法无法保证聚类结果完全绝对真实的反应实际情况。首先，选择正确合适的聚类数量不是一件容易的事情，第二，该算法对初始值的设置敏感，容易陷入局部最优。尽管 scikit-learn 采取了不同的方式来缓解以上问题，目前仍没有完美的解决方案。</strong></p>
<p><strong>Don’t over-interpret clustering results（不要过分解读聚类结果）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Code source: Gaël Varoquaux</span></span><br><span class="line"><span class="comment"># Modified for documentation by Jaques Grobler</span></span><br><span class="line"><span class="comment"># License: BSD 3 clause</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Though the following import is not directly being used, it is required</span></span><br><span class="line"><span class="comment"># for 3D projection to work</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">estimators = [(<span class="string">'k_means_iris_8'</span>, KMeans(n_clusters=<span class="number">8</span>)),</span><br><span class="line">              (<span class="string">'k_means_iris_3'</span>, KMeans(n_clusters=<span class="number">3</span>)),</span><br><span class="line">              (<span class="string">'k_means_iris_bad_init'</span>, KMeans(n_clusters=<span class="number">3</span>, n_init=<span class="number">1</span>,</span><br><span class="line">                                               init=<span class="string">'random'</span>))]</span><br><span class="line"></span><br><span class="line">fignum = <span class="number">1</span></span><br><span class="line">titles = [<span class="string">'8 clusters'</span>, <span class="string">'3 clusters'</span>, <span class="string">'3 clusters, bad initialization'</span>]</span><br><span class="line"><span class="keyword">for</span> name, est <span class="keyword">in</span> estimators:</span><br><span class="line">    fig = plt.figure(fignum, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">    ax = Axes3D(fig, rect=[<span class="number">0</span>, <span class="number">0</span>, <span class="number">.95</span>, <span class="number">1</span>], elev=<span class="number">48</span>, azim=<span class="number">134</span>)</span><br><span class="line">    est.fit(X)</span><br><span class="line">    labels = est.labels_</span><br><span class="line"></span><br><span class="line">    ax.scatter(X[:, <span class="number">3</span>], X[:, <span class="number">0</span>], X[:, <span class="number">2</span>],</span><br><span class="line">               c=labels.astype(np.float), edgecolor=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line">    ax.w_xaxis.set_ticklabels([])</span><br><span class="line">    ax.w_yaxis.set_ticklabels([])</span><br><span class="line">    ax.w_zaxis.set_ticklabels([])</span><br><span class="line">    ax.set_xlabel(<span class="string">'Petal width'</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Sepal length'</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">'Petal length'</span>)</span><br><span class="line">    ax.set_title(titles[fignum - <span class="number">1</span>])</span><br><span class="line">    ax.dist = <span class="number">12</span></span><br><span class="line">    fignum = fignum + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the ground truth</span></span><br><span class="line">fig = plt.figure(fignum, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">ax = Axes3D(fig, rect=[<span class="number">0</span>, <span class="number">0</span>, <span class="number">.95</span>, <span class="number">1</span>], elev=<span class="number">48</span>, azim=<span class="number">134</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, label <span class="keyword">in</span> [(<span class="string">'Setosa'</span>, <span class="number">0</span>),</span><br><span class="line">                    (<span class="string">'Versicolour'</span>, <span class="number">1</span>),</span><br><span class="line">                    (<span class="string">'Virginica'</span>, <span class="number">2</span>)]:</span><br><span class="line">    ax.text3D(X[y == label, <span class="number">3</span>].mean(),</span><br><span class="line">              X[y == label, <span class="number">0</span>].mean(),</span><br><span class="line">              X[y == label, <span class="number">2</span>].mean() + <span class="number">2</span>, name,</span><br><span class="line">              horizontalalignment=<span class="string">'center'</span>,</span><br><span class="line">              bbox=dict(alpha=<span class="number">.2</span>, edgecolor=<span class="string">'w'</span>, facecolor=<span class="string">'w'</span>))</span><br><span class="line"><span class="comment"># Reorder the labels to have colors matching the cluster results</span></span><br><span class="line">y = np.choose(y, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]).astype(np.float)</span><br><span class="line">ax.scatter(X[:, <span class="number">3</span>], X[:, <span class="number">0</span>], X[:, <span class="number">2</span>], c=y, edgecolor=<span class="string">'k'</span>)</span><br><span class="line"></span><br><span class="line">ax.w_xaxis.set_ticklabels([])</span><br><span class="line">ax.w_yaxis.set_ticklabels([])</span><br><span class="line">ax.w_zaxis.set_ticklabels([])</span><br><span class="line">ax.set_xlabel(<span class="string">'Petal width'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'Sepal length'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'Petal length'</span>)</span><br><span class="line">ax.set_title(<span class="string">'Ground Truth'</span>)</span><br><span class="line">ax.dist = <span class="number">12</span></span><br><span class="line"></span><br><span class="line">fig.show()</span><br></pre></td></tr></table></figure>
<pre><code>================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.



/Users/hadoop/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
  &quot;matplotlib is currently using a non-GUI backend, &quot;
</code></pre><p><img src="/images/sklearn/output_72_2.png" alt="png"></p>
<p><img src="/images/sklearn/output_72_3.png" alt="png"></p>
<p><img src="/images/sklearn/output_72_4.png" alt="png"></p>
<p><img src="/images/sklearn/output_72_5.png" alt="png"></p>
<h3 id="Application-example-vector-quantization（应用案例-向量量化-vector-quantization-）-多看几遍"><a href="#Application-example-vector-quantization（应用案例-向量量化-vector-quantization-）-多看几遍" class="headerlink" title="Application example: vector quantization（应用案例:向量量化(vector quantization)）(多看几遍)"></a>Application example: vector quantization（应用案例:向量量化(vector quantization)）(多看几遍)</h3><p>一般来说聚类，特别是 K_means 聚类可以作为一种用少量样本来压缩信息的方式。这种方式就是 <a href="https://en.wikipedia.org/wiki/Vector_quantization" target="_blank" rel="noopener">vector quantization</a> 。例如，K_means 算法可以用于对一张图片进行色调分离:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    face = sp.face(gray=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">except</span> AttributeError:</span><br><span class="line">    <span class="keyword">from</span> scipy <span class="keyword">import</span> misc</span><br><span class="line">    face = misc.face(gray=<span class="keyword">True</span>)</span><br><span class="line">X = face.reshape((<span class="number">-1</span>, <span class="number">1</span>)) <span class="comment"># We need an (n_sample, n_feature) array</span></span><br><span class="line">k_means = cluster.KMeans(n_clusters=<span class="number">5</span>, n_init=<span class="number">1</span>)</span><br><span class="line">k_means.fit(X) </span><br><span class="line"></span><br><span class="line">values = k_means.cluster_centers_.squeeze()</span><br><span class="line">labels = k_means.labels_</span><br><span class="line">face_compressed = np.choose(labels, values)</span><br><span class="line">face_compressed.shape = face.shape</span><br></pre></td></tr></table></figure>
<h3 id="分层聚类算法-谨慎使用"><a href="#分层聚类算法-谨慎使用" class="headerlink" title="分层聚类算法: 谨慎使用"></a>分层聚类算法: 谨慎使用</h3><h4 id="层次聚类-（分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种："><a href="#层次聚类-（分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种：" class="headerlink" title="层次聚类 （分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种："></a><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html#hierarchical-clustering" target="_blank" rel="noopener">层次聚类</a> （分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种：</h4><p><strong>Agglomerative（聚合）</strong> </p>
<ul>
<li>自底向上的方法: 初始阶段，每一个样本将自己作为单独的一个簇，聚类的簇以最小化距离的标准进行迭代聚合。当感兴趣的簇只有少量的样本时，该方法是很合适的。如果需要聚类的 簇数量很大，该方法比K_means算法的计算效率也更高。 * </li>
</ul>
<p><strong>Divisive（分裂）</strong> </p>
<ul>
<li>自顶向下的方法: 初始阶段，所有的样本是一个簇，当一个簇下移时，它被迭代的进 行分裂。当估计聚类簇数量较大的数据时，该算法不仅效率低(由于样本始于一个簇，需要被递归的进行 分裂)，而且从统计学的角度来讲也是不合适的。</li>
</ul>
<h4 id="连接约束聚类"><a href="#连接约束聚类" class="headerlink" title="连接约束聚类"></a>连接约束聚类</h4><p>对于逐次聚合聚类，通过连接图可以指定哪些样本可以被聚合在一个簇。在 scikit 中，图由邻接矩阵来表示，通常该矩阵是一个稀疏矩阵。这种表示方法是非常有用的，例如在聚类图像时检索连接区域(有时也被称为连接要素):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.image <span class="keyword">import</span> grid_to_graph</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate data</span></span><br><span class="line"><span class="keyword">try</span>:  <span class="comment"># SciPy &gt;= 0.16 have face in misc</span></span><br><span class="line">    <span class="keyword">from</span> scipy.misc <span class="keyword">import</span> face</span><br><span class="line">    face = face(gray=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    face = sp.face(gray=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize it to 10% of the original size to speed up the processing</span></span><br><span class="line">face = sp.misc.imresize(face, <span class="number">0.10</span>) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">X = np.reshape(face, (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Define the structure A of the data. Pixels connected to their neighbors.</span></span><br><span class="line">connectivity = grid_to_graph(*face.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br></pre></td></tr></table></figure>
<h4 id="特征聚集"><a href="#特征聚集" class="headerlink" title="特征聚集"></a>特征聚集</h4><p>我们已经知道，稀疏性可以缓解特征维度带来的问题，i.e 即与特征数量相比，样本数量太少。 另一个解决该问题的方式是合并相似的维度：<strong>feature agglomeration（特征聚集）</strong>。该方法可以通过对特征聚类来实现。换 句话说，就是对样本数据转置后进行聚类。</p>
<p><code>transform and inverse_transform methods
Some estimators expose a transform method, for instance to reduce the dimensionality of the dataset.</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">digits = datasets.load_digits()</span><br><span class="line">images = digits.images</span><br><span class="line">X = np.reshape(images, (len(images), <span class="number">-1</span>))</span><br><span class="line">connectivity = grid_to_graph(*images[<span class="number">0</span>].shape)</span><br><span class="line"></span><br><span class="line">agglo = cluster.FeatureAgglomeration(connectivity=connectivity,</span><br><span class="line">                                     n_clusters=<span class="number">32</span>)</span><br><span class="line">agglo.fit(X) </span><br><span class="line"></span><br><span class="line">X_reduced = agglo.transform(X)</span><br><span class="line"></span><br><span class="line">X_approx = agglo.inverse_transform(X_reduced)</span><br><span class="line">images_approx = np.reshape(X_approx, images.shape)</span><br></pre></td></tr></table></figure>
<h2 id="分解-将一个信号转换成多个成份并且加载¶"><a href="#分解-将一个信号转换成多个成份并且加载¶" class="headerlink" title="分解: 将一个信号转换成多个成份并且加载¶"></a>分解: 将一个信号转换成多个成份并且加载¶</h2><h3 id="Components-and-loadings（成分和载荷）"><a href="#Components-and-loadings（成分和载荷）" class="headerlink" title="Components and loadings（成分和载荷）"></a>Components and loadings（成分和载荷）</h3><p>如果 X 是多维数据，那么我们试图解决的问题是在不同的观察基础上对数据进行重写。我们希望学习得到载荷 L 和成分 C 使得 X = L C 。提取成分 C 有多种不同的方法</p>
<h3 id="主成份分析-PCA"><a href="#主成份分析-PCA" class="headerlink" title="主成份分析: PCA"></a>主成份分析: PCA</h3><p><a href="http://sklearn.apachecn.org/cn/0.19.0/modules/decomposition.html#pca" target="_blank" rel="noopener">主成分分析（PCA）</a> 将能够解释数据信息最大方差的的连续成分提取出来</p>
<p><a href="https://www.douban.com/group/topic/104320268/" target="_blank" rel="noopener">豆瓣</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Authors: Gael Varoquaux</span></span><br><span class="line"><span class="comment">#          Jaques Grobler</span></span><br><span class="line"><span class="comment">#          Kevin Hughes</span></span><br><span class="line"><span class="comment"># License: BSD 3 clause</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Create the data</span></span><br><span class="line"></span><br><span class="line">e = np.exp(<span class="number">1</span>)</span><br><span class="line">np.random.seed(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pdf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (stats.norm(scale=<span class="number">0.25</span> / e).pdf(x)</span><br><span class="line">                  + stats.norm(scale=<span class="number">4</span> / e).pdf(x))</span><br><span class="line"></span><br><span class="line">y = np.random.normal(scale=<span class="number">0.5</span>, size=(<span class="number">30000</span>))</span><br><span class="line">x = np.random.normal(scale=<span class="number">0.5</span>, size=(<span class="number">30000</span>))</span><br><span class="line">z = np.random.normal(scale=<span class="number">0.1</span>, size=len(x))</span><br><span class="line"></span><br><span class="line">density = pdf(x) * pdf(y)</span><br><span class="line">pdf_z = pdf(<span class="number">5</span> * z)</span><br><span class="line"></span><br><span class="line">density *= pdf_z</span><br><span class="line"></span><br><span class="line">a = x + y</span><br><span class="line">b = <span class="number">2</span> * y</span><br><span class="line">c = a - b + z</span><br><span class="line"></span><br><span class="line">norm = np.sqrt(a.var() + b.var())</span><br><span class="line">a /= norm</span><br><span class="line">b /= norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot the figures</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_figs</span><span class="params">(fig_num, elev, azim)</span>:</span></span><br><span class="line">    fig = plt.figure(fig_num, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">    plt.clf()</span><br><span class="line">    ax = Axes3D(fig, rect=[<span class="number">0</span>, <span class="number">0</span>, <span class="number">.95</span>, <span class="number">1</span>], elev=elev, azim=azim)</span><br><span class="line"></span><br><span class="line">    ax.scatter(a[::<span class="number">10</span>], b[::<span class="number">10</span>], c[::<span class="number">10</span>], c=density[::<span class="number">10</span>], marker=<span class="string">'+'</span>, alpha=<span class="number">.4</span>)</span><br><span class="line">    Y = np.c_[a, b, c]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Using SciPy's SVD, this would be:</span></span><br><span class="line">    <span class="comment"># _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)</span></span><br><span class="line"></span><br><span class="line">    pca = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">    pca.fit(Y)</span><br><span class="line">    pca_score = pca.explained_variance_ratio_</span><br><span class="line">    V = pca.components_</span><br><span class="line"></span><br><span class="line">    x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score / pca_score.min()</span><br><span class="line"></span><br><span class="line">    x_pca_axis, y_pca_axis, z_pca_axis = <span class="number">3</span> * V.T</span><br><span class="line">    x_pca_plane = np.r_[x_pca_axis[:<span class="number">2</span>], - x_pca_axis[<span class="number">1</span>::<span class="number">-1</span>]]</span><br><span class="line">    y_pca_plane = np.r_[y_pca_axis[:<span class="number">2</span>], - y_pca_axis[<span class="number">1</span>::<span class="number">-1</span>]]</span><br><span class="line">    z_pca_plane = np.r_[z_pca_axis[:<span class="number">2</span>], - z_pca_axis[<span class="number">1</span>::<span class="number">-1</span>]]</span><br><span class="line">    x_pca_plane.shape = (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    y_pca_plane.shape = (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    z_pca_plane.shape = (<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)</span><br><span class="line">    ax.w_xaxis.set_ticklabels([])</span><br><span class="line">    ax.w_yaxis.set_ticklabels([])</span><br><span class="line">    ax.w_zaxis.set_ticklabels([])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">elev = <span class="number">-40</span></span><br><span class="line">azim = <span class="number">-80</span></span><br><span class="line">plot_figs(<span class="number">1</span>, elev, azim)</span><br><span class="line"></span><br><span class="line">elev = <span class="number">30</span></span><br><span class="line">azim = <span class="number">20</span></span><br><span class="line">plot_figs(<span class="number">2</span>, elev, azim)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
</code></pre><p><img src="/images/sklearn/output_80_1.png" alt="png"></p>
<p><img src="/images/sklearn/output_80_2.png" alt="png"></p>
<p>上图中样本点的分布在一个方向上是非常平坦的：即三个单变量特征中的任何一个都可以有另外两个特征来表示。主成分分析法(PCA)可以找到使得数据分布不 flat 的矢量方向(可以反映数据主要信息的特征)。<br>当用主成分分析(PCA)来 transform（转换） 数据时，可以通过在子空间上投影来降低数据的维数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a signal with only 2 useful dimensions</span></span><br><span class="line">x1 = np.random.normal(size=<span class="number">100</span>)</span><br><span class="line">x2 = np.random.normal(size=<span class="number">100</span>)</span><br><span class="line">x3 = x1 + x2</span><br><span class="line">X = np.c_[x1, x2, x3]  <span class="comment">#把三个np数组连接到一起</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line">pca = decomposition.PCA()</span><br><span class="line">pca.fit(X)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(pca.explained_variance_)  <span class="comment">#explained_variance_ 属性可以中查看各个主成分的解释方差：</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># As we can see, only the 2 first components are useful</span></span><br><span class="line">pca.n_components = <span class="number">2</span></span><br><span class="line">X_reduced = pca.fit_transform(X)</span><br><span class="line">X_reduced.shape</span><br></pre></td></tr></table></figure>
<pre><code>[  2.96375875e+00   7.23284879e-01   1.13533782e-31]





(100, 2)
</code></pre><h3 id="独立成分分析-ICA"><a href="#独立成分分析-ICA" class="headerlink" title="独立成分分析: ICA"></a>独立成分分析: ICA</h3><p>独立成分分析（ICA） 可以提取数据信息中的独立成分，这些成分载荷的分布包含了最多的 的独立信息。该方法能够恢复 non-Gaussian（非高斯） 独立信号:</p>
<p><a href="http://sklearn.apachecn.org/cn/0.19.0/auto_examples/decomposition/plot_ica_blind_source_separation.html" target="_blank" rel="noopener">Blind source separation using FastICA</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> FastICA, PCA</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">n_samples = <span class="number">2000</span></span><br><span class="line">time = np.linspace(<span class="number">0</span>, <span class="number">8</span>, n_samples)</span><br><span class="line"></span><br><span class="line">s1 = np.sin(<span class="number">2</span> * time)  <span class="comment"># Signal 1 : sinusoidal signal</span></span><br><span class="line">s2 = np.sign(np.sin(<span class="number">3</span> * time))  <span class="comment"># Signal 2 : square signal</span></span><br><span class="line">s3 = signal.sawtooth(<span class="number">2</span> * np.pi * time)  <span class="comment"># Signal 3: saw tooth signal</span></span><br><span class="line"></span><br><span class="line">S = np.c_[s1, s2, s3]</span><br><span class="line">S += <span class="number">0.2</span> * np.random.normal(size=S.shape)  <span class="comment"># Add noise</span></span><br><span class="line"></span><br><span class="line">S /= S.std(axis=<span class="number">0</span>)  <span class="comment"># Standardize data</span></span><br><span class="line"><span class="comment"># Mix data</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">1.0</span>], [<span class="number">1.5</span>, <span class="number">1.0</span>, <span class="number">2.0</span>]])  <span class="comment"># Mixing matrix</span></span><br><span class="line">X = np.dot(S, A.T)  <span class="comment"># Generate observations</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute ICA</span></span><br><span class="line">ica = FastICA(n_components=<span class="number">3</span>)</span><br><span class="line">S_ = ica.fit_transform(X)  <span class="comment"># Reconstruct signals</span></span><br><span class="line">A_ = ica.mixing_  <span class="comment"># Get estimated mixing matrix</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We can `prove` that the ICA model applies by reverting the unmixing.</span></span><br><span class="line"><span class="keyword">assert</span> np.allclose(X, np.dot(S_, A_.T) + ica.mean_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For comparison, compute PCA</span></span><br><span class="line">pca = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">H = pca.fit_transform(X)  <span class="comment"># Reconstruct signals based on orthogonal components</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot results</span></span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">models = [X, S, S_, H]</span><br><span class="line">names = [<span class="string">'Observations (mixed signal)'</span>,</span><br><span class="line">         <span class="string">'True Sources'</span>,</span><br><span class="line">         <span class="string">'ICA recovered signals'</span>,</span><br><span class="line">         <span class="string">'PCA recovered signals'</span>]</span><br><span class="line">colors = [<span class="string">'red'</span>, <span class="string">'steelblue'</span>, <span class="string">'orange'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii, (model, name) <span class="keyword">in</span> enumerate(zip(models, names), <span class="number">1</span>):</span><br><span class="line">    plt.subplot(<span class="number">4</span>, <span class="number">1</span>, ii)</span><br><span class="line">    plt.title(name)</span><br><span class="line">    <span class="keyword">for</span> sig, color <span class="keyword">in</span> zip(model.T, colors):</span><br><span class="line">        plt.plot(sig, color=color)</span><br><span class="line"></span><br><span class="line">plt.subplots_adjust(<span class="number">0.09</span>, <span class="number">0.04</span>, <span class="number">0.94</span>, <span class="number">0.94</span>, <span class="number">0.26</span>, <span class="number">0.46</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>================================
SVM Exercise
================================

A tutorial exercise for using different SVM kernels.

This exercise is used in the :ref:`using_kernels_tut` part of the
:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
</code></pre><p><img src="/images/sklearn/output_84_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> signal</span><br><span class="line">time = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2000</span>)</span><br><span class="line">s1 = np.sin(<span class="number">2</span> * time)  <span class="comment"># Signal 1 : sinusoidal signal</span></span><br><span class="line">s2 = np.sign(np.sin(<span class="number">3</span> * time))  <span class="comment"># Signal 2 : square signal</span></span><br><span class="line">s3 = signal.sawtooth(<span class="number">2</span> * np.pi * time)  <span class="comment"># Signal 3: saw tooth signal</span></span><br><span class="line">S = np.c_[s1, s2, s3]</span><br><span class="line">S += <span class="number">0.2</span> * np.random.normal(size=S.shape)  <span class="comment"># Add noise</span></span><br><span class="line">S /= S.std(axis=<span class="number">0</span>)  <span class="comment"># Standardize data</span></span><br><span class="line"><span class="comment"># Mix data</span></span><br><span class="line">A = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1.5</span>, <span class="number">1</span>, <span class="number">2</span>]])  <span class="comment"># Mixing matrix</span></span><br><span class="line">X = np.dot(S, A.T)  <span class="comment"># Generate observations</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute ICA</span></span><br><span class="line">ica = decomposition.FastICA()</span><br><span class="line">S_ = ica.fit_transform(X)  <span class="comment"># Get the estimated sources</span></span><br><span class="line">A_ = ica.mixing_.T</span><br><span class="line">np.allclose(X,  np.dot(S_, A_) + ica.mean_)</span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><h1 id="把它们放在一起-参考"><a href="#把它们放在一起-参考" class="headerlink" title="把它们放在一起 (参考)"></a>把它们放在一起 (参考)</h1><h2 id="模型管道化"><a href="#模型管道化" class="headerlink" title="模型管道化"></a>模型管道化</h2><p>我们已经知道一些模型可以做数据转换，一些模型可以用来预测变量。我们可以建立一个组合模型同时完成以上工作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model, decomposition, datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">logistic = linear_model.LogisticRegression()</span><br><span class="line"></span><br><span class="line">pca = decomposition.PCA()</span><br><span class="line">pipe = Pipeline(steps=[(<span class="string">'pca'</span>, pca), (<span class="string">'logistic'</span>, logistic)])</span><br><span class="line"></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line">X_digits = digits.data</span><br><span class="line">y_digits = digits.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the PCA spectrum</span></span><br><span class="line">pca.fit(X_digits)</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.clf()</span><br><span class="line">plt.axes([<span class="number">.2</span>, <span class="number">.2</span>, <span class="number">.7</span>, <span class="number">.7</span>])</span><br><span class="line">plt.plot(pca.explained_variance_, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'n_components'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'explained_variance_'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prediction</span></span><br><span class="line">n_components = [<span class="number">20</span>, <span class="number">40</span>, <span class="number">64</span>]</span><br><span class="line">Cs = np.logspace(<span class="number">-4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameters of pipelines can be set using ‘__’ separated parameter names:</span></span><br><span class="line">estimator = GridSearchCV(pipe,</span><br><span class="line">                         dict(pca__n_components=n_components,</span><br><span class="line">                              logistic__C=Cs))</span><br><span class="line">estimator.fit(X_digits, y_digits)</span><br><span class="line"></span><br><span class="line">plt.axvline(estimator.best_estimator_.named_steps[<span class="string">'pca'</span>].n_components,</span><br><span class="line">            linestyle=<span class="string">':'</span>, label=<span class="string">'n_components chosen'</span>)</span><br><span class="line">plt.legend(prop=dict(size=<span class="number">12</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/sklearn/output_88_0.png" alt="png"></p>
<h2 id="用特征面进行人脸识别"><a href="#用特征面进行人脸识别" class="headerlink" title="用特征面进行人脸识别"></a>用特征面进行人脸识别</h2><p>该实例用到的数据集来自 LFW_(Labeled Faces in the Wild)。数据已经进行了初步预处理</p>
<p><a href="http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz" target="_blank" rel="noopener">http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz</a> (233MB)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">===================================================</span></span><br><span class="line"><span class="string">Faces recognition example using eigenfaces and SVMs</span></span><br><span class="line"><span class="string">===================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">The dataset used in this example is a preprocessed excerpt of the</span></span><br><span class="line"><span class="string">"Labeled Faces in the Wild", aka LFW_:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">.. _LFW: http://vis-www.cs.umass.edu/lfw/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Expected results for the top 5 most represented people in the dataset:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">================== ============ ======= ========== =======</span></span><br><span class="line"><span class="string">                   precision    recall  f1-score   support</span></span><br><span class="line"><span class="string">================== ============ ======= ========== =======</span></span><br><span class="line"><span class="string">     Ariel Sharon       0.67      0.92      0.77        13</span></span><br><span class="line"><span class="string">     Colin Powell       0.75      0.78      0.76        60</span></span><br><span class="line"><span class="string">  Donald Rumsfeld       0.78      0.67      0.72        27</span></span><br><span class="line"><span class="string">    George W Bush       0.86      0.86      0.86       146</span></span><br><span class="line"><span class="string">Gerhard Schroeder       0.76      0.76      0.76        25</span></span><br><span class="line"><span class="string">      Hugo Chavez       0.67      0.67      0.67        15</span></span><br><span class="line"><span class="string">       Tony Blair       0.81      0.69      0.75        36</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      avg / total       0.80      0.80      0.80       322</span></span><br><span class="line"><span class="string">================== ============ ======= ========== =======</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_lfw_people</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(__doc__)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display progress logs on stdout</span></span><br><span class="line">logging.basicConfig(level=logging.INFO, format=<span class="string">'%(asctime)s %(message)s'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Download the data, if not already on disk and load it as numpy arrays</span></span><br><span class="line"></span><br><span class="line">lfw_people = fetch_lfw_people(min_faces_per_person=<span class="number">70</span>, resize=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># introspect the images arrays to find the shapes (for plotting)</span></span><br><span class="line">n_samples, h, w = lfw_people.images.shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># for machine learning we use the 2 data directly (as relative pixel</span></span><br><span class="line"><span class="comment"># positions info is ignored by this model)</span></span><br><span class="line">X = lfw_people.data</span><br><span class="line">n_features = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># the label to predict is the id of the person</span></span><br><span class="line">y = lfw_people.target</span><br><span class="line">target_names = lfw_people.target_names</span><br><span class="line">n_classes = target_names.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Total dataset size:"</span>)</span><br><span class="line">print(<span class="string">"n_samples: %d"</span> % n_samples)</span><br><span class="line">print(<span class="string">"n_features: %d"</span> % n_features)</span><br><span class="line">print(<span class="string">"n_classes: %d"</span> % n_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Split into a training set and a test set using a stratified k fold</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># split into a training and testing set</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled</span></span><br><span class="line"><span class="comment"># dataset): unsupervised feature extraction / dimensionality reduction</span></span><br><span class="line">n_components = <span class="number">150</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Extracting the top %d eigenfaces from %d faces"</span></span><br><span class="line">      % (n_components, X_train.shape[<span class="number">0</span>]))</span><br><span class="line">t0 = time()</span><br><span class="line">pca = PCA(n_components=n_components, svd_solver=<span class="string">'randomized'</span>,</span><br><span class="line">          whiten=<span class="keyword">True</span>).fit(X_train)</span><br><span class="line">print(<span class="string">"done in %0.3fs"</span> % (time() - t0))</span><br><span class="line"></span><br><span class="line">eigenfaces = pca.components_.reshape((n_components, h, w))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Projecting the input data on the eigenfaces orthonormal basis"</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br><span class="line">print(<span class="string">"done in %0.3fs"</span> % (time() - t0))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Train a SVM classification model</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Fitting the classifier to the training set"</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">param_grid = &#123;<span class="string">'C'</span>: [<span class="number">1e3</span>, <span class="number">5e3</span>, <span class="number">1e4</span>, <span class="number">5e4</span>, <span class="number">1e5</span>],</span><br><span class="line">              <span class="string">'gamma'</span>: [<span class="number">0.0001</span>, <span class="number">0.0005</span>, <span class="number">0.001</span>, <span class="number">0.005</span>, <span class="number">0.01</span>, <span class="number">0.1</span>], &#125;</span><br><span class="line">clf = GridSearchCV(SVC(kernel=<span class="string">'rbf'</span>, class_weight=<span class="string">'balanced'</span>), param_grid)</span><br><span class="line">clf = clf.fit(X_train_pca, y_train)</span><br><span class="line">print(<span class="string">"done in %0.3fs"</span> % (time() - t0))</span><br><span class="line">print(<span class="string">"Best estimator found by grid search:"</span>)</span><br><span class="line">print(clf.best_estimator_)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Quantitative evaluation of the model quality on the test set</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Predicting people's names on the test set"</span>)</span><br><span class="line">t0 = time()</span><br><span class="line">y_pred = clf.predict(X_test_pca)</span><br><span class="line">print(<span class="string">"done in %0.3fs"</span> % (time() - t0))</span><br><span class="line"></span><br><span class="line">print(classification_report(y_test, y_pred, target_names=target_names))</span><br><span class="line">print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Qualitative evaluation of the predictions using matplotlib</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gallery</span><span class="params">(images, titles, h, w, n_row=<span class="number">3</span>, n_col=<span class="number">4</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Helper function to plot a gallery of portraits"""</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">1.8</span> * n_col, <span class="number">2.4</span> * n_row))</span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0</span>, left=<span class="number">.01</span>, right=<span class="number">.99</span>, top=<span class="number">.90</span>, hspace=<span class="number">.35</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_row * n_col):</span><br><span class="line">        plt.subplot(n_row, n_col, i + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)</span><br><span class="line">        plt.title(titles[i], size=<span class="number">12</span>)</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the result of the prediction on a portion of the test set</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">title</span><span class="params">(y_pred, y_test, target_names, i)</span>:</span></span><br><span class="line">    pred_name = target_names[y_pred[i]].rsplit(<span class="string">' '</span>, <span class="number">1</span>)[<span class="number">-1</span>]</span><br><span class="line">    true_name = target_names[y_test[i]].rsplit(<span class="string">' '</span>, <span class="number">1</span>)[<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">'predicted: %s\ntrue:      %s'</span> % (pred_name, true_name)</span><br><span class="line"></span><br><span class="line">prediction_titles = [title(y_pred, y_test, target_names, i)</span><br><span class="line">                     <span class="keyword">for</span> i <span class="keyword">in</span> range(y_pred.shape[<span class="number">0</span>])]</span><br><span class="line"></span><br><span class="line">plot_gallery(X_test, prediction_titles, h, w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the gallery of the most significative eigenfaces</span></span><br><span class="line"></span><br><span class="line">eigenface_titles = [<span class="string">"eigenface %d"</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> range(eigenfaces.shape[<span class="number">0</span>])]</span><br><span class="line">plot_gallery(eigenfaces, eigenface_titles, h, w)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012
2018-03-18 15:54:33,510 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976012



===================================================
Faces recognition example using eigenfaces and SVMs
===================================================

The dataset used in this example is a preprocessed excerpt of the
&quot;Labeled Faces in the Wild&quot;, aka LFW_:

  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)

.. _LFW: http://vis-www.cs.umass.edu/lfw/

Expected results for the top 5 most represented people in the dataset:

================== ============ ======= ========== =======
                   precision    recall  f1-score   support
================== ============ ======= ========== =======
     Ariel Sharon       0.67      0.92      0.77        13
     Colin Powell       0.75      0.78      0.76        60
  Donald Rumsfeld       0.78      0.67      0.72        27
    George W Bush       0.86      0.86      0.86       146
Gerhard Schroeder       0.76      0.76      0.76        25
      Hugo Chavez       0.67      0.67      0.67        15
       Tony Blair       0.81      0.69      0.75        36

      avg / total       0.80      0.80      0.80       322
================== ============ ======= ========== =======




Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009
2018-03-18 15:54:38,109 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976009
Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006
2018-03-18 15:54:41,419 Downloading LFW metadata: https://ndownloader.figshare.com/files/5976006
Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015
2018-03-18 15:54:49,563 Downloading LFW data (~200MB): https://ndownloader.figshare.com/files/5976015
</code></pre><h2 id="开放性问题-股票市场结构"><a href="#开放性问题-股票市场结构" class="headerlink" title="开放性问题: 股票市场结构"></a>开放性问题: 股票市场结构</h2><p>我们可以预测 Google 在特定时间段内的股价变动吗？</p>
<p><a href="http://sklearn.apachecn.org/cn/0.19.0/auto_examples/applications/plot_stock_market.html#stock-market" target="_blank" rel="noopener">Learning a graph structure</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/scikit-learn/" rel="tag"><i class="fa fa-tag"></i> scikit-learn</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/pymysql.html" rel="next" title="利用Pyecharts连接Mysql并对数据进行可视化">
                <i class="fa fa-chevron-left"></i> 利用Pyecharts连接Mysql并对数据进行可视化
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/Pyhive.html" rel="prev" title="利用Pyhive实现Python连接Hive数据仓库">
                利用Pyhive实现Python连接Hive数据仓库 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zNTIxOC8xMTc1NA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">刘知行</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">44</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hooog" target="_blank" title="Github">
                      
                        <i class="fa fa-fw fa-globe"></i>Github</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hooog" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-globe"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hooog" target="_blank" title="简书">
                      
                        <i class="fa fa-fw fa-globe"></i>简书</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集"><span class="nav-number">1.</span> <span class="nav-text">数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#样例-iris-数据集（鸢尾花卉数据集）"><span class="nav-number">1.1.</span> <span class="nav-text">样例: iris 数据集（鸢尾花卉数据集）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据预处理样例-digits数据集-手写数字数据集"><span class="nav-number">1.2.</span> <span class="nav-text">数据预处理样例:digits数据集(手写数字数据集)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预估对象"><span class="nav-number">2.</span> <span class="nav-text">预估对象</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#监督学习：从高维观察预测输出变量"><span class="nav-number"></span> <span class="nav-text">监督学习：从高维观察预测输出变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#最近邻和维度惩罚"><span class="nav-number">1.</span> <span class="nav-text">最近邻和维度惩罚</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#鸢尾属植物分类："><span class="nav-number">1.1.</span> <span class="nav-text">鸢尾属植物分类：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K近邻分类器"><span class="nav-number">1.2.</span> <span class="nav-text">K近邻分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练集和测试集"><span class="nav-number">1.3.</span> <span class="nav-text">训练集和测试集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN-k-最近邻-分类器例子"><span class="nav-number">1.4.</span> <span class="nav-text">KNN(k 最近邻)分类器例子:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#维度惩罚"><span class="nav-number">1.5.</span> <span class="nav-text">维度惩罚</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性模型：从回归到稀疏"><span class="nav-number">2.</span> <span class="nav-text">线性模型：从回归到稀疏</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#糖尿病数据集"><span class="nav-number">2.1.</span> <span class="nav-text">糖尿病数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归"><span class="nav-number">2.2.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#收缩（岭回归-）"><span class="nav-number">2.3.</span> <span class="nav-text">收缩（岭回归 ）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#稀疏"><span class="nav-number">2.4.</span> <span class="nav-text">稀疏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#同一个问题的不同算法"><span class="nav-number">2.5.</span> <span class="nav-text">同一个问题的不同算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类"><span class="nav-number">2.6.</span> <span class="nav-text">分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归：（LogisticRegression）"><span class="nav-number">2.7.</span> <span class="nav-text">逻辑回归：（LogisticRegression）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多类分类"><span class="nav-number">2.8.</span> <span class="nav-text">多类分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-logistic-回归进行收缩和稀疏"><span class="nav-number">2.9.</span> <span class="nav-text">使用 logistic 回归进行收缩和稀疏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习"><span class="nav-number">2.10.</span> <span class="nav-text">练习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量积-SVMs"><span class="nav-number">3.</span> <span class="nav-text">支持向量积(SVMs)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性-SVMs"><span class="nav-number">3.1.</span> <span class="nav-text">线性 SVMs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#例子"><span class="nav-number">3.1.1.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用核"><span class="nav-number">3.2.</span> <span class="nav-text">使用核</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#线性核"><span class="nav-number">3.2.1.</span> <span class="nav-text">线性核</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多项式核"><span class="nav-number">3.2.2.</span> <span class="nav-text">多项式核</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RBF（径向基函数）"><span class="nav-number">3.2.3.</span> <span class="nav-text">RBF（径向基函数）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交互例子"><span class="nav-number">3.3.</span> <span class="nav-text">交互例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习-1"><span class="nav-number">3.4.</span> <span class="nav-text">练习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#模型选择-选择估计量及其参数"><span class="nav-number"></span> <span class="nav-text">模型选择 选择估计量及其参数</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#分数和交叉验证分数（取最佳拟合的方式）"><span class="nav-number">1.</span> <span class="nav-text">分数和交叉验证分数（取最佳拟合的方式）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KFold-交叉验证"><span class="nav-number">1.1.</span> <span class="nav-text">KFold 交叉验证</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉验证生成器-（第一次没搞懂）"><span class="nav-number">2.</span> <span class="nav-text">交叉验证生成器 （第一次没搞懂）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉验证生成器"><span class="nav-number">2.0.1.</span> <span class="nav-text">交叉验证生成器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习-2"><span class="nav-number">2.1.</span> <span class="nav-text">练习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#网格搜索和交叉验证估计量"><span class="nav-number">3.</span> <span class="nav-text">网格搜索和交叉验证估计量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#网格搜索"><span class="nav-number">3.1.</span> <span class="nav-text">网格搜索</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#嵌套交叉验证"><span class="nav-number">3.1.1.</span> <span class="nav-text">嵌套交叉验证</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#交叉验证估计量"><span class="nav-number">3.2.</span> <span class="nav-text">交叉验证估计量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#练习-3"><span class="nav-number">3.3.</span> <span class="nav-text">练习</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#无监督学习-寻求数据表示"><span class="nav-number"></span> <span class="nav-text">无监督学习: 寻求数据表示</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类-对样本数据进行分组"><span class="nav-number">1.</span> <span class="nav-text">聚类: 对样本数据进行分组</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#可以利用聚类解决的问题"><span class="nav-number">1.1.</span> <span class="nav-text">可以利用聚类解决的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-means-聚类算法"><span class="nav-number">1.2.</span> <span class="nav-text">K-means 聚类算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#示例"><span class="nav-number">1.2.1.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Application-example-vector-quantization（应用案例-向量量化-vector-quantization-）-多看几遍"><span class="nav-number">1.3.</span> <span class="nav-text">Application example: vector quantization（应用案例:向量量化(vector quantization)）(多看几遍)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分层聚类算法-谨慎使用"><span class="nav-number">1.4.</span> <span class="nav-text">分层聚类算法: 谨慎使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#层次聚类-（分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种："><span class="nav-number">1.4.1.</span> <span class="nav-text">层次聚类 （分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#连接约束聚类"><span class="nav-number">1.4.2.</span> <span class="nav-text">连接约束聚类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#特征聚集"><span class="nav-number">1.4.3.</span> <span class="nav-text">特征聚集</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分解-将一个信号转换成多个成份并且加载¶"><span class="nav-number">2.</span> <span class="nav-text">分解: 将一个信号转换成多个成份并且加载¶</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Components-and-loadings（成分和载荷）"><span class="nav-number">2.1.</span> <span class="nav-text">Components and loadings（成分和载荷）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#主成份分析-PCA"><span class="nav-number">2.2.</span> <span class="nav-text">主成份分析: PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#独立成分分析-ICA"><span class="nav-number">2.3.</span> <span class="nav-text">独立成分分析: ICA</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#把它们放在一起-参考"><span class="nav-number"></span> <span class="nav-text">把它们放在一起 (参考)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#模型管道化"><span class="nav-number">1.</span> <span class="nav-text">模型管道化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#用特征面进行人脸识别"><span class="nav-number">2.</span> <span class="nav-text">用特征面进行人脸识别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#开放性问题-股票市场结构"><span class="nav-number">3.</span> <span class="nav-text">开放性问题: 股票市场结构</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘知行</span>

  
</div>


  <div class="powered-by">
  <span>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></span>
  </div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
