<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hoooge&#39;s Blog</title>
  
  <subtitle>Life is short</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.ihoge.cn/"/>
  <updated>2018-05-14T03:59:02.233Z</updated>
  <id>http://www.ihoge.cn/</id>
  
  <author>
    <name>刘知行</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>梯度下降、随机梯度下降、批量梯度下降和牛顿法、拟牛顿法、共轭梯度法</title>
    <link href="http://www.ihoge.cn/2018/GradientDescent.html"/>
    <id>http://www.ihoge.cn/2018/GradientDescent.html</id>
    <published>2018-05-14T03:20:21.000Z</published>
    <updated>2018-05-14T03:59:02.233Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>李航老师在《统计学习方法》中将机器学习的三要素总结为：模型、策略和算法。其大致含义如下：</p><p><strong>模型</strong>：其实就是机器学习训练的过程中所要学习的条件概率分布或者决策函数。</p><p><strong>策略</strong>：就是使用一种什么样的评价，度量模型训练过程中的学习好坏的方法，同时根据这个方法去实施的调整模型的参数，以期望训练的模型将来对未知的数据具有最好的预测准确度。</p><p><strong>算法</strong>：算法是指模型的具体计算方法。它基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法去求解这个最优模型。</p><p>很多时候机器学习工程师又戏称调参工程师, 由此可见参数调优时作为机器学习工程师必须掌握的一项核心技能。</p><p>这篇文章的目的旨在对常用的参数调优算法进行一次梳理便于随时翻阅。</p><ul><li>梯度下降法 （梯度下降、随机梯度下降、批量梯度下降）</li><li>牛顿法 （牛顿法、拟牛顿法）</li><li>共轭梯度法（Conjugate Gradient）</li></ul><h1 id="1-梯度下降法（Gradient-Descent）"><a href="#1-梯度下降法（Gradient-Descent）" class="headerlink" title="1. 梯度下降法（Gradient Descent）"></a>1. 梯度下降法（Gradient Descent）</h1><h2 id="1-1-一般解释"><a href="#1-1-一般解释" class="headerlink" title="1.1 一般解释"></a>1.1 一般解释</h2><p>$f(x)$在$x_0$的梯度：就是$f(x)$变化最快的方向。梯度下降法是一个最优化算法，通常也称为<em>最速下降法</em>。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262278257042.jpg" alt=""><br>假设$f(x)$是一座山，站在半山腰，往x方向走1米，高度上升0.4米，也就是说x方向上的偏导是 0.4；往y方向走1米，高度上升0.3米，也就是说y方向上的偏导是 0.3；这样梯度方向就是 (0.4 , 0.3)，也就是往这个方向走1米，所上升的高度最高。梯度不仅仅是$f(x)$在某一点变化最快的方向，而且是上升最快的方向；如果想下山，下降最快的方向就是逆着梯度的方向，这就是梯度下降法，又叫最速下降法。</p><h2 id="1-2-梯度下降算法用途"><a href="#1-2-梯度下降算法用途" class="headerlink" title="1.2 梯度下降算法用途"></a>1.2 梯度下降算法用途</h2><p>最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现在已经不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。 </p><p>在梯度下降算法中，都是围绕以下这个式子展开：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262224519674.jpg" alt=""><br>其中在上面的式子中$h_θ(x)$代表，输入为x的时候的其当时θ参数下的输出值，与y相减则是一个相对误差，之后再平方乘以1/2，并且其中:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262225198545.jpg" alt=""><br>这里我列举了一个简单的例子，当然实际的x可以有n多个维度。我们知道曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。这个θ的更新过程可以描述为:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262225480705.jpg" alt=""><br>这里就是根据每一个 x 的分量以及当时的偏差值进行 θ 的更新，其中 α 为步长，这个参数如果设置的太大，那么很容易就在最优值附加徘徊；相反，如果设置的太小，则会导致收敛速度过慢。</p><p>关于步长和学习速率的关系,这里提一下其实这两个是一个概念，叫法不一样，最优化问题中叫步长，但一般在神经网络中也叫学习速率。</p><h2 id="1-3-梯度下降、随机梯度下降、批量梯度下降"><a href="#1-3-梯度下降、随机梯度下降、批量梯度下降" class="headerlink" title="1.3 梯度下降、随机梯度下降、批量梯度下降"></a>1.3 梯度下降、随机梯度下降、批量梯度下降</h2><ul><li><p><strong>梯度下降</strong>：梯度下降就是上面的推导，要留意，在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整θ.其计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦~</p></li><li><p><strong>随机梯度下降</strong>：可以看到多了随机两个字，随机也就是说用样本中的一个例子来近似所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中。随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。</p></li><li><p><strong>批量梯度下降</strong>：其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是随机指定一个例子替代样本不太准，而且批量的话还是非常可以反映样本的一个分布情况的。批量梯度下降最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p></li><li><p><strong>概括</strong>：</p><p>  随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><p>  随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p></li><li><p><strong>对批量梯度下降法和随机梯度下降法的总结：</strong></p><p>  批量梯度下降—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p><p>  随机梯度下降—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p></li></ul><!--### 梯度下降代码:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="comment">#这里更新权重的时候所有的样本点都用上了</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>output:iters_count 219iters_count 220iters_count 221iters_count 222iters_count 223iters_count 224iters_count 225theta: [3.0027765778748003, 3.997918297015663]final loss: 9.68238055213e-05iters: 225[Finished in 0.2s]</code></pre><h3 id="随机梯度下降代码"><a href="#随机梯度下降代码" class="headerlink" title="随机梯度下降代码:"></a>随机梯度下降代码:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每次选取一个值,随机一个点更新 θ</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="comment">#每一次选取随机的一个点进行权重的更新</span></span><br><span class="line">    i = random.randint(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>iters_count 1226iters_count 1227iters_count 1228iters_count 1229iters_count 1230iters_count 1231iters_count 1232theta: [3.002441488688225, 3.9975844154600226]final loss: 9.989420302e-05iters: 1232[Finished in 0.3s]</code></pre><h3 id="批量随机梯度下降代码："><a href="#批量随机梯度下降代码：" class="headerlink" title="批量随机梯度下降代码："></a>批量随机梯度下降代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里用2个样本点</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    i = random.randint(<span class="number">0</span>,<span class="number">3</span>) <span class="comment">#注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点</span></span><br><span class="line">    j = (i+<span class="number">1</span>)%<span class="number">4</span></span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[j][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[j][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[j]) * input_x[j][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[j]) * input_x[j][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>.....iters_count 543iters_count 544iters_count 545iters_count 546iters_count 547iters_count 548iters_count 549theta: [3.0023012574840764, 3.997553282857357]final loss: 9.81717138358e-05iters: 549</code></pre><p>–&gt;</p><h1 id="2-牛顿法（Newton’s-method"><a href="#2-牛顿法（Newton’s-method" class="headerlink" title="2. 牛顿法（Newton’s method)"></a>2. 牛顿法（Newton’s method)</h1><h2 id="2-1-牛顿法原理"><a href="#2-1-牛顿法原理" class="headerlink" title="2.1 牛顿法原理"></a>2.1 牛顿法原理</h2><p><strong>具体步骤：</strong></p><p>首先，选择一个接近函数 $f (x)$零点的 $x_0$，计算相应的 $f (x_0)$ 和切线斜率$f’(x_0)$（这里$f’(x_0)$ 表示函数 $f(x_0)$的导数）。然后我们计算穿过点$(x_0,  f(x_0))$ 并且斜率为$f’(x_0)$的直线和 x 轴的交点的x坐标，也就是求如下方程的解：</p><p>$x\; \cdot \; f’\left( x_{0} \right)+f\left( x_{0} \right)-x_{0\; }\cdot \; f’\left( x_{0} \right)=0$</p><p>或：</p><p>$f\left( x_{0} \right)+\left( x-x_{0} \right)f’’\left( x_{0} \right)=0$</p><p>我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f  (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。迭代公式可化简为如下所示：</p><p>$x_{n+1}=x_{n}\; -\; \frac{f\; \left( x_{n} \right)}{f\; ‘\; \left( x_{n} \right)}\; $</p><p>已经证明，如果f  ‘ 是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值x0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果f’(x)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。下图为一个牛顿法执行过程的例子。</p><p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法的搜索路径（二维情况）如下图所示：</p><p>牛顿法搜索动态示例图：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/1022856-20170916202719078-1588446775.gif" alt="1022856-20170916202719078-1588446775">　</p><p><strong>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法更快。比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。</strong></p><p>也可以这么理解：梯度下降主要是从一阶目标函数的一阶导推导而来的，形象点说，就是每次朝着当前梯度最大的方向收敛；二牛顿法是二阶收敛，每次考虑收敛方向的时候，还会考虑下一次的收敛的方向是否是最大（也就是梯度的梯度）。</p><p>从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。如下图是一个最小化一个目标方程的例子，红色曲线是利用牛顿法迭代求解，绿色曲线是利用梯度下降法求解。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15262579878778.jpg" alt=""></p><p>总结一下，就是牛顿法对目标函数的一阶导再求导，即可算出收敛的方向。</p><p><strong>梯度法和牛顿法对比</strong>： </p><pre><code>梯度法：又称最速下降法，是早期的解析法，收敛速度较慢。 牛顿法：收敛速度快，但不稳定，计算也较困难。</code></pre><p><strong>牛顿法的优缺点总结：</strong></p><pre><code>优点：二阶收敛，收敛速度快；缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</code></pre><p>在上面讨论的是2维情况，高维情况的牛顿迭代公式是：<br>$$x_{n+1}=x_n-[Hf(x_n)^{-1}]\nabla f(x_n)，n≥0$$<br>其中H时hessian矩阵，定义为：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262581746410.jpg" alt=""><br>高维情况也可以用牛顿迭代求解，但是Hessian矩阵引入的复杂性，使得牛顿迭代求解的难度增加，解决这个问题的办法是<strong>拟牛顿法</strong>（Quasi-Newton methond）：</p><h2 id="2-2-拟牛顿法（Quasi-Newton-Methods）"><a href="#2-2-拟牛顿法（Quasi-Newton-Methods）" class="headerlink" title="2.2 拟牛顿法（Quasi-Newton Methods）"></a>2.2 拟牛顿法（Quasi-Newton Methods）</h2><p>拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。</p><p><strong>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong>拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p><p><strong>具体步骤：</strong></p><p>拟牛顿法的基本思想如下。首先构造目标函数在当前迭代$x_k$的二次模型：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262661663849.jpg" alt=""><br>这里$B_k$是一个对称正定（正交）矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：<br>$x_{k+1}=x_k+\alpha _kp_k$<br>其中我们要求步长$\alpha_k$ 满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hesse矩阵$B_k$  代替真实的Hesse矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵$B_k$ 的更新。现在假设得到一个新的迭代$x_{k+1}$，并得到一个新的二次模型：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262663848192.jpg" alt=""><br>我们尽可能地利用上一步的信息来选取$B_k$。具体地，我们要求 :</p><p>$\nabla f(x_{k+1})-\nabla f(x_k)=\alpha _kB_{k+1}p_k$</p><p>从而得到：</p><p>$B_{k+1}(x_{k+1}-x_k)=\nabla f(x_{k+1})-\nabla f(x_k)$</p><p>这个公式被称为<strong>割线方程</strong>。常用的拟牛顿法有<strong>DFP算法</strong>和<strong>BFGS算法</strong>。</p><h1 id="3-共轭梯度法（Conjugate-Gradient）"><a href="#3-共轭梯度法（Conjugate-Gradient）" class="headerlink" title="3. 共轭梯度法（Conjugate Gradient）"></a>3. 共轭梯度法（Conjugate Gradient）</h1><p>共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p><p>具体的实现步骤请参加<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB">wiki百科共轭梯度法</a>。</p><p>下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：(绿色为梯度下降法，红色代表共轭梯度法)<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262668307656.jpg" alt=""></p><h1 id="4-其他优化方法"><a href="#4-其他优化方法" class="headerlink" title="4. 其他优化方法"></a>4. 其他优化方法</h1><h2 id="4-1-启发式优化方法"><a href="#4-1-启发式优化方法" class="headerlink" title="4.1 启发式优化方法"></a>4.1 启发式优化方法</h2><p>启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。</p><p>还有一种特殊的优化算法被称之多目标优化算法，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有NSGAII算法、MOEA/D算法以及人工免疫算法等。</p><h2 id="4-2-解决约束优化问题——拉格朗日乘数法"><a href="#4-2-解决约束优化问题——拉格朗日乘数法" class="headerlink" title="4.2 解决约束优化问题——拉格朗日乘数法"></a>4.2 解决约束优化问题——拉格朗日乘数法</h2><p>有关拉格朗日乘数法的介绍请见另一篇博客：<a href="http://www.cnblogs.com/maybe2030/p/4946256.html">《拉格朗日乘数法》</a></p>-->]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;李航老师在《统计学习方法》中将机器学习的三要素总结为：模型、策略和算法。其大致含义如下：&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.ihoge.cn/categories/Machine-Learning/"/>
    
    
      <category term="algorithm" scheme="http://www.ihoge.cn/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>LR 逻辑回归</title>
    <link href="http://www.ihoge.cn/2018/LR.html"/>
    <id>http://www.ihoge.cn/2018/LR.html</id>
    <published>2018-05-13T13:20:21.000Z</published>
    <updated>2018-05-13T13:27:16.244Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h2><p>逻辑回归也被称为对数几率回归，算法名虽然叫做逻辑回归，但是该算法是分类算法，个人认为这是因为逻辑回归用了和回归类似的方法来解决了分类问题。</p><p>逻辑回归模型是一种分类模型，用条件概率分布的形式表示 $P(Y|X)$，这里随机变量 X 取值为 n 维实数向量，例如$x=(x^{(1)},x^{(2)},…,x^{(n)})$，Y 取值为 0 或 1。即：<br>$$P\left( Y=1 \right|x)=\frac{\exp \left( w\cdot x+b \right)}{1+\exp \left( w\cdot x+b \right)}$$</p><p>$$P\left( Y=0 \right|0)=\frac{1}{1+\exp \left( w\cdot x+b \right)}$$</p><p>或：</p><p>$$\phi \left( x \right)=\frac{1}{1+e^{-w ^{T}x-b}}$$</p><p>假设有一个二分类问题，输出为$y\in \left{ 0,1 \right}$，二线性回归模型$z=w^Tx+b$是个实数值，我们希望有一个理想的阶跃函数来帮我什么实现z值到0/1值的转化，于是找到了Sigmoid函数来代替：<br>$$g\left( z \right)=\frac{1}{1+e^{-z}}$$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262130001555.jpg" alt=""></p><p>有了 Sigmoid 函数之后，由于其值取值范围在[0,1]。就可以将其视为类 1 的后验概率估计 $p(y=1|X)$。说白了，就是如果有了一个测试点 x，那么就可以用Sigmoid函数算出来的结果当作该点 x 属于类别 1 的概率大小。</p><p>于是，非常自然地，我们把 Sigmoid 函数计算得到的值大于等于0.5的归为类别1，小于0.5的归为类别0：</p><h2 id="逻辑函数的损失函数"><a href="#逻辑函数的损失函数" class="headerlink" title="逻辑函数的损失函数"></a>逻辑函数的损失函数</h2><p>接下来要做的就是根据给定的训练集，把参数 w 给求出来了。要找参数 w，首先就得把代价函数（Cost Function）给定义出来，也就是目标函数。</p><p>我们第一个想到的自然是模仿线性回归的做法，利用误差平方和来当代价函数:<br>$$J\left( \theta  \right)\; =\; \frac{1}{2m}\sum_{i=0}^{m}{\left( \phi \left( x^{i} \right)-y^{i} \right)^{2}}$$<br>这时将预测函数$g\left( z^{\left( i \right)} \right)=\frac{1}{1+e^{-x^{\left( i \right)}}}$代入损失函数的话，会发现这是一个非凸函数，这意味着代价函数有着许多的局部最小值，这不利于我们求解：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15262137313916.jpg" alt=""><br>那么我们不妨来换一个思路解决这个问题。前面，我们提到了 ϕ(z) 可以视为类1的后验估计，所以我们有：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262138718987.jpg" alt=""><br>其中 $p(y=1|x;w)$ 表示给定 w，那么 x 点 y=1 的概率大小。于是上面两式可以写成一般形式：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262139240592.jpg" alt=""></p><p><strong>注：以上的过程说明，最大似然估计与误差平方和等价！这就是为什么逻辑回归的损失函数可以用最大似然函数进行估计的原因。</strong></p><p>接下来我们就要用极大似然估计来根据给定的训练集估计出参数 w：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262139602585.jpg" alt=""><br>为了简化运算，我们对上面这个等式的两边都取一个对数：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262139862582.jpg" alt=""><br>我们现在要求的是使得 l(w) 最大的 w。没错，我们的代价函数出现了，我们在 l(w) 前面加个负号不就变成就最小了吗？不就变成我们代价函数了吗？<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262140306754.jpg" alt=""><br>为了更好地理解这个代价函数，我们不妨拿一个例子的来看看：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262140672037.jpg" alt=""><br>也就是说：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262140889161.jpg" alt=""><br>下面是函数图：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262141062691.jpg" alt=""><br>从图中不难看出，如果样本的值是1的话，估计值 ϕ(z) 越接近1付出的代价就越小，反之越大；同理，如果样本的值是0的话，估计值 ϕ(z) 越接近0付出的代价就越小，反之越大。</p><h2 id="逻辑回归的模型求解"><a href="#逻辑回归的模型求解" class="headerlink" title="逻辑回归的模型求解"></a>逻辑回归的模型求解</h2><p>在开始梯度下降之前，要这里插一句，Sigmoid function 有一个很好的性质就是 ：<br>$$\phi ‘\left( z \right)=\phi \left( z \right)\left( 1-\phi \left( z \right) \right)$$<br>这个后续会用到。</p><p>还有，我们要明确一点，梯度的负方向就是代价函数下降最快的方向：这里来解释下。借助泰勒公式展开，有：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262144340354.jpg" alt=""><br>其中，f′(x) 和 δ 为向量，那么这两者的内积就等于：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262144509150.jpg" alt=""><br>当 θ=π 时，也就是 δ 在 f′(x) 的负方向上时，取得最小值，也就是下降的最快的方向了。<br>于是有：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262144876995.jpg" alt=""><br>即：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262145365892.jpg" alt=""><br>其中，$w_j$ 表示第 j 个特征的权重；η 为学习率，用来控制步长。 重点来了:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262145869777.jpg" alt=""><br>所以，在使用梯度下降法更新权重时，只要根据下式即可：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262146594102.jpg" alt=""><br>此式与线性回归时更新权重用的式子极为相似，也许这也是逻辑回归要在后面加上回归两个字的原因吧。当然，在样本量极大的时候，每次更新权重会非常耗费时间，这时可以采用随机梯度下降法，这时每次迭代时需要将样本重新打乱，然后用下式不断更新权重：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262146980033.jpg" alt=""><br>也就是去掉了求和，而是针对每个样本点都进行更新。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;逻辑回归模型&quot;&gt;&lt;a href=&quot;#逻辑回归模型&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归模型&quot;&gt;&lt;/a&gt;逻辑回归模型&lt;/h2&gt;&lt;p&gt;逻辑回归也被称为对数几率回归，算法名虽然叫做逻辑回归，但是该算法是分类算
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.ihoge.cn/categories/Machine-Learning/"/>
    
    
      <category term="algorithm" scheme="http://www.ihoge.cn/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>KNN——不需要学习的算法</title>
    <link href="http://www.ihoge.cn/2018/KNN1.html"/>
    <id>http://www.ihoge.cn/2018/KNN1.html</id>
    <published>2018-05-11T15:38:21.000Z</published>
    <updated>2018-05-13T10:53:46.467Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="KNN——不需要学习的算法"><a href="#KNN——不需要学习的算法" class="headerlink" title="KNN——不需要学习的算法"></a>KNN——不需要学习的算法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>KNN（K-Nearest Neighbor）算法是机器学习中算法中最基础和简单的算法之一。它既能用于分类，也能用于回归。本文将在不同的小节详细地介绍 KNN 算法在分类和回归两种任务下的运用原理。</p><p>KNN 算法的思想非常简单：对于任意的 n 维输入向量，其对应于特征空间一个点，输出为该特征向量所对应的类别标签或者预测值。KNN 算法在机器学习算法中有一个十分特别的地方，那就是它没有一个显示的学习过程。它实际上的工作原理是利用训练数据对特征向量空间进行划分，并将其划分的结果作为其最终的算法模型。</p><h2 id="1-KNN分类算法"><a href="#1-KNN分类算法" class="headerlink" title="1. KNN分类算法"></a>1. KNN分类算法</h2><p>KNN 分类算法的分类预测过程十分的简单和容易理解：对于一个需要预测的输入向量 x，我们只需要在训练数据集中寻找 k 个与向量 x 最近的向量的集合，然后把 x 的类标预测为这 k 个样本中类标数最多的那一类。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15260483350913.jpg" alt=""><br>如图所示，$w_1,w_2,w_3$分别代表的是训练机中的三个类别。图中$x_u$最相近的5（k=5）个点如图中箭头所指，很明显与$x_u$最相近的5个点中最多的泪标为$w_1$，因此KNN算法将$x_u$的类别预测为$w_1$。</p><p>基于上述思想给出KNN算法公式原理：</p><h3 id="1-1-KNN算法原理"><a href="#1-1-KNN算法原理" class="headerlink" title="1.1 KNN算法原理"></a>1.1 KNN算法原理</h3><p><strong>输入</strong>：训练数据集</p><p>$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_N,y_N)}$</p><p>其中：<br>$x_i\in X⊆R^n$ 为 n 维的实例特征向量。<br>$y_i\in Y={c_1,c_2,…,c_K}$为实例的类别，$i=1,2,3,…,N$，预测实例$x$。</p><p><strong>输出</strong>：预测实例$x$所属类别$y$。</p><p><strong>算法执行步骤</strong>：</p><ol><li>根据给定的距离度量方法（一般情况下使用欧氏距离）在训练集$T$中寻找出与$x$最相近的$k$个样本点，并将这$k$个样本点所表示的集合记为$N_k(x)$；</li><li>根据多数投票原则确定实例$x$所属的类别$y$。</li></ol><p>从上述的 KNN 原理中，我们发现有两个因素必须确定才能是 KNN 分类算法真正能勾运行：（1）算法超参数K；（2）模型向量空间的距离度量。</p><h3 id="1-2-K值的确定"><a href="#1-2-K值的确定" class="headerlink" title="1.2 K值的确定"></a>1.2 K值的确定</h3><p>KNN 算法中只有唯一的一个超参数 K，很明显 K 值的选择对最终算法的预测结果会产生至关重要的影响。接下来就简单的讨论一下 K 值的大小对算法结果的影响以及一般情况下如何选择 K 值。</p><p>如果 K 值选择的比较小，这时候我们就相当于使用较小的领域中的训练样本对实例进行预测。这时候，算法的近似误差（Approximate Error）会减小，因为只有与输入实例相近的训练样本才能才会对预测结果起作用。但是它也会有明显的缺点：算法的估计误差会偏大，预测的结果会对近邻点十分敏感，也就是说如果近邻点是噪声点的话，那么预测就会出错。也就是说，K 值太小会使得 KNN 算法容易过拟合。</p><p>同理，如果 K 值选的比较大的话，这时候距离较远的训练样本都能够对实例的预测结果产生影响。这时候，而模型相对比较鲁棒，不会因个别噪声点对最终的预测产生影响。但是缺点也是十分明显的：算法的近似误差会偏大，距离较远的点（与预测实例不相似）也会同样对预测结果产生作用，使得预测产生较大偏差。此时相当于模型发生欠拟合。</p><p>因此，在实际的工程实践过程中，我们一般<strong>采用交叉验证的方式选取 K 值</strong>。从上面的分析也可以知道，一般 K 值取得比较小。<em>我们会选取 K 值在较小的范围，同时在测试集上准确率最高的那一个确定为最终的算法超参数 K。</em></p><h3 id="1-3-距离的度量"><a href="#1-3-距离的度量" class="headerlink" title="1.3 距离的度量"></a>1.3 距离的度量</h3><p>样本空间中两个点之间的距离度量表示的是两个样本点之间的相似程度：距离越短，表示相似程度越高；相反，距离越大，表示两个样本的相似程度低。</p><p>常用的距离度量方式有：</p><ol><li>闵可夫斯基距离；</li><li>欧氏距离；</li><li>曼哈顿距离；</li><li>切比雪夫距离；</li><li>余弦距离。</li></ol><h4 id="1-3-1-闵可夫斯基距离"><a href="#1-3-1-闵可夫斯基距离" class="headerlink" title="1.3.1 闵可夫斯基距离"></a>1.3.1 闵可夫斯基距离</h4><p>闵可夫斯基距离不是一种距离，而是一类距离的定义。对于 n 维空间中的两个点 $x(x_1,x_2,x_3,…,x_n)$和$y(y_1,y_2,y_3,…,y_n)$，那么$x$和$y$亮点之间的闵可夫斯基距离为：$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{p}}}$$<br>其中p是一个可变参数：</p><ul><li>当p=1时，被称为曼哈顿距离；</li><li>当p=2时，被称为欧式距离；</li><li>当p=$\infty $时，被称为切比雪夫距离。</li></ul><h4 id="1-3-2-欧式距离"><a href="#1-3-2-欧式距离" class="headerlink" title="1.3.2 欧式距离"></a>1.3.2 欧式距离</h4><p>由以上说明可知，欧式距离的计算公式为：<br>$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{2}}}$$</p><p>欧式距离（L2 范数）是最易于理解的一种距离计算方法，源自欧式空间中两点间的距离公式，也是最常用的距离度量方式。</p><h4 id="1-3-3-曼哈顿距离"><a href="#1-3-3-曼哈顿距离" class="headerlink" title="1.3.3 曼哈顿距离"></a>1.3.3 曼哈顿距离</h4><p>由闵可夫斯基距离定义可知，曼哈顿距离的计算公式为：<br>$$d_{xy}=\sum_{i=1}^{n}{\left| x_{i}-y_{i} \right|}$$</p><h2 id="KNN-算法核心：KDTree"><a href="#KNN-算法核心：KDTree" class="headerlink" title="KNN 算法核心：KDTree"></a>KNN 算法核心：KDTree</h2><p>通过以上的分析，我们知道 KNN 分类算法的思想非常简单，它采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？</p><p>也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。</p><p>因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。</p><h3 id="KD-树简介"><a href="#KD-树简介" class="headerlink" title="KD 树简介"></a>KD 树简介</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15260517984690.jpg" alt=""><br>KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><h3 id="KD-树的构造"><a href="#KD-树的构造" class="headerlink" title="KD 树的构造"></a>KD 树的构造</h3><p>KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。</p><ul><li><p><strong>构造跟节点</strong></p><p>  首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。</p></li><li><p><strong>递归构造子节点</strong></p><p>  递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。</p><p>  通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。</p></li></ul><h2 id="KNN-回归算法"><a href="#KNN-回归算法" class="headerlink" title="KNN 回归算法"></a>KNN 回归算法</h2><p>上面我们讲的 KNN 算法主要是用于分类的情况，实际上，KNN 算法也能够用于回归预测。下面介绍一下 KNN 算法如何用于回归的情况。</p><p>众所周知，KNN 算法用于分类的方法如下：首先，对于一个新来的预测实例，我们在训练集上寻找它的最相近的 K 个近邻；然后，采用投票法将它分到这 K 个邻居中的最多的那个类。</p><p>但是，怎么将 KNN 算法用于回归呢？其实大致的步骤是一样的，也是对新来的预测实例寻找 K 近邻，然后对这 K 个样本的目标值去均值即可作为新样本的预测值：</p><p>$$f\left( x \right)=\frac{1}{K}\sum_{i=1}^{K}{x_{i}}$$</p><h2 id="KNN-预测代码演示"><a href="#KNN-预测代码演示" class="headerlink" title="KNN 预测代码演示"></a>KNN 预测代码演示</h2><p>接下来，我们使用 scikit-learn 库中的 KNN 对 iris 数据集分类效果进行预测实战。众所周知，iris 数据集有四个维度的特征，但是为了方便展示效果，我们只使用其中的两个维度。完整的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets</span><br><span class="line"></span><br><span class="line">n_neighbors = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import some data to play with</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we only take the first two features. We could avoid this ugly</span></span><br><span class="line"><span class="comment"># slicing by using a two-dim dataset</span></span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create color maps</span></span><br><span class="line">cmap_light = ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">'uniform'</span>, <span class="string">'distance'</span>]:</span><br><span class="line">    <span class="comment"># we create an instance of Neighbours Classifier and fit the data.</span></span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    print(<span class="string">"train_score"</span>, clf.score(X,y))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">"3-Class classification (k = %i, weights = '%s')"</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15260528682435.jpg" alt=""></p><ul><li><p><strong>代码解释</strong></p><p>  代码中邻居数使用的是 n_neighbors = 15，只使用 iris 的前两维特征作为分类特征。权重度量采用了两种方式：<strong>均值（uniform）和距离（distance）</strong>。均值代表的是所有的 K 个近邻在分类时重要性选取的是一样的，该参数是默认参数；距离也就是说，分类时 K 个邻居中每个邻居所占的权重与它与预测实例的距离成反比。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;KNN——不需要学习的算法&quot;&gt;&lt;a href=&quot;#KNN——不需要学习的算法&quot; class=&quot;headerlink&quot; title=&quot;KNN——不需要学习的算法&quot;&gt;&lt;/a&gt;KNN——不需要学习的算法&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;
      
    
    </summary>
    
      <category term="Machine Learning" scheme="http://www.ihoge.cn/categories/Machine-Learning/"/>
    
    
      <category term="algorithm" scheme="http://www.ihoge.cn/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL 基础框架和核心组件</title>
    <link href="http://www.ihoge.cn/2018/SparkSQL1.html"/>
    <id>http://www.ihoge.cn/2018/SparkSQL1.html</id>
    <published>2018-05-09T17:00:21.000Z</published>
    <updated>2018-05-13T07:57:33.027Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark-SQL-基础框架和核心组件"><a href="#Spark-SQL-基础框架和核心组件" class="headerlink" title="Spark SQL 基础框架和核心组件"></a>Spark SQL 基础框架和核心组件</h1><p>今天被问到<code>Spark Sql</code>和<code>Dataframe</code>的关系，当时没有描述好。事后脑海中关于它们的一些细节才一一涌现出来。还是理解的不够深刻，所以这里重新做下相关概念知识的整理也算是一种复习。</p><h2 id="1-Spark-SQL发展史"><a href="#1-Spark-SQL发展史" class="headerlink" title="1. Spark SQL发展史"></a>1. Spark SQL发展史</h2><p>在<code>Spark</code>的早期版本，为什么解决<code>Hive</code>查询在性能上的问题，在<code>Spark</code>生态圈引入了一个名为<code>Shark</code>的项目。（使用<code>Spark</code>的计算引擎而不是<code>MapReduce</code>来执行<code>Hive</code>查询）。<code>Shark</code>是在<code>Hive</code>代码库上构建的，使用<code>Hive</code>查询编译器来解析<code>Hive</code>查询并生成一个抽象语法树，它会转化为一个具有某些基本优化的逻辑计划。<code>Shark</code>应用了额外的优化手段并创建了一个<code>RDD</code>操作额度物理计划，然后在<code>Spark</code>上执行。基于这种对<code>Hive</code>的过分依赖，也让<code>Shark</code>本身具有一些问题：</p><ol><li>只适用于查询<code>Hive</code>表。无法在RDD上进行关系查询。</li><li>在<code>Spark</code>程序中将<code>Hive-QL</code>作为字符串运行容易出错。</li><li>它的<code>Hive</code>优化器是为<code>MapReduce</code>附案例创建的，很难讲<code>Spark</code>扩展到新的数据源和新的处理模型。<br>基于以上原因，<code>Spark</code>团队终止了<code>Shark</code>，原班人马合并全力开发<code>Spark SQL</code>。下面是<code>Spark SQL</code>的发展史。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258813010356.jpg" alt=""><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258818229539.jpg" alt=""><h2 id="2-Spark-SQL的构架"><a href="#2-Spark-SQL的构架" class="headerlink" title="2. Spark SQL的构架"></a>2. Spark SQL的构架</h2><code>Spark SQL</code>是在<code>Spark</code>核心执行引擎上的一个库，它借助JDBC/ODBC公开了SQL接口，用于数据仓库应用程序，或通过命令行进行交互式查询。因此<strong>任何商务智能（BI）工具都可以连接到Spark SQL以内存速度执行分析</strong>。<br>它还提供了<code>Java、Scala、Python、R</code>支持的<code>Dataset API</code>和<code>Dataframe API</code>。（注意：Python和R不支持Dataset）<br><code>Spark SQL</code>用户可以使用<code>Data Sourse API</code>从各种数据源读取和写入数据，从而创建<code>Dataframe</code> 或者 <code>Dataset</code>。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258850302556.jpg" alt=""><br><code>Spark SQL</code>还扩展了用于所有其他<code>Spark</code>库（如 <code>SparkR、SparkStreaming、Structred Streaming、ML、MLlib、GraphX）的DataSet API、Dataframe API和Data Sources API</code>。如下图所示：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258851101623.jpg" alt=""></li></ol><p><code>Spark SQL</code>引入了<code>Catalyst</code>的可扩展优化器，一直吃大多数常见的数据源和算法。<code>Catalyst</code>支持添加新的数据源、优化规则和某些领域（如机器学习）使用的数据类型。</p><h2 id="3-Spark-SQL的四个组件"><a href="#3-Spark-SQL的四个组件" class="headerlink" title="3. Spark SQL的四个组件"></a>3. Spark SQL的四个组件</h2><p>Spark SQL 中的四个组件分别是：<strong>SQL、 Data Sources API、 DataFrame API、 DataSet API</strong><br>下面一一展开：</p><ul><li><p><strong>Spark SQL</strong>可以使用SQL语言向Hive表写入读取数据。</p><p>  SQL通过ODBC/JDBC或命令行选项在Java、Scala、Python、R语言中使用。在编程语言中使用SQL时，结果会转换成DataFrame。<strong>SQL</strong>的优点是可以轻松的处理Hive表，并且可以利用Thrift服务器将BI工具连接到分布式SQL引擎，并利用JDBC/ODBC接口提交SQL或HQL查询。</p></li><li><p><strong>Data Sources API</strong>为使用Spark SQL 读取和写入数据提供了统一的接口。</p><p>  Data Sources API的优点有：</p><pre><code>1. 容易加载/保存Dataframe2. 通过到数据源的*谓词下推*方式进行高效的数据访问，采用这种方式能从数据源读取更少的数据。3. 为任何新的数据源构建库4. 无需包含在Spark代码中5. 很容易与Spark组件包共享新的数据源。--谓词下推的思路是让筛选数据的条件表达式尽可能靠近数据源执行，从而减少数据额移动和传输。谓词就是筛选数据的条件，下推就是让这些筛选动作尽量在靠近数据源的位置执行。</code></pre></li><li><p><strong>DataFrame API</strong>旨在让大数据分析工作更为简单易行。</p><p>  这个API收到了R和Python（Pandas库）中的DataFrame的启发，但他被实际用于大规模数据集的分布式处理一支持大数据分析。<em>DataFrame可以看作是对现有RDD API的扩展，也是RDD纸上的一个抽象</em><br>  DataFrame的优点有：</p><pre><code>1. 易于使用域特定语言开发应用程序2. 在传统RDD基础上的高性能，并且在Scala、Java、Python、R中都具备近似性能3. 在数据源中自动发现模式类型（Schema）和分区4. 支持多种数据源5. 通过Catalyst优化器进行优化和代码生成6. 可以与RDD、DatSet、Pandas和外部数据源（RDBMS关系型数据库、HBase、Cassandra等）互操作</code></pre></li><li><p><strong>DataSet API</strong></p><p>  Spark1.6 版本引入的 DataSet API 结合了 RDD 和 DataFrame的最大优点。 DatSet 会使用编码器将JVM对象转换为用 Spark 的 Tungsten 二进制格式存储的 DatSet 表形式。<br>  它的优点有：</p><pre><code>1. 和 RDD 一样，类型安全性2. 和 DataFrame 一样， 比 RDD 更快3. 与 DataFrame 和 RDD 之间具有互操性4. 缓存的 DatSet 专用的空间比 RDD 更少5. 使用编码器进行序列话比 Java 或 Kyro 序列化更快 </code></pre><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15258847849628.jpg" alt=""></p></li></ul><h2 id="4-Dataframe-和-DataSet-的演变与优势"><a href="#4-Dataframe-和-DataSet-的演变与优势" class="headerlink" title="4. Dataframe 和 DataSet 的演变与优势"></a>4. Dataframe 和 DataSet 的演变与优势</h2><p>Spark SQL 的 DataFrame 相比 R 语言或 Python 的 DataFrame，有更丰富的隐藏后台优化。 条吗可以从文件、Pandas DataFrame、 Hive 中的表、像 MySQL 这样的外部数据库或者 RDD 中创建。 Scala、Java、Python、R 都支持 DataFrame API。 虽然 DataFrame 提供了关系操作和更高的性能，但缺乏类型安全性，这会导致运行时错误。</p><p>在 1.6 版本， DataSet 和 DataFrame 还是单独的类。在 2.0 版本中， DataSet 和 DataFrame API 被统一起来，为开发人员提供单一的 API。 DataFrame 时一个特定的 DataSet[T]，其中 T = Row 的类型， 因此 DataFrame 和 DataSet 共享相同的方法。</p><p>DataSet API 支持 Scala 和 Java，还不支持 Python 和 R。 但是， DataSet API 的许多优点已经天然在 Python 和 R 语言中存在了。 DataFrame 则支持所有这四种语言。</p><h3 id="4-1-为什么使用-DataFrame-和-DataSet"><a href="#4-1-为什么使用-DataFrame-和-DataSet" class="headerlink" title="4.1 为什么使用 DataFrame 和 DataSet"></a>4.1 为什么使用 DataFrame 和 DataSet</h3><p>Spark 要对闭包进行计算、将其序列话，并将他们发送到执行进程。这意味着代码是以原始形式发送的，没有经过优化。 在 RDD 中无法表示结构化数据，对 RDD 进行查询也不可行。使用 RDD 很容易但有时候处理远足会把代码弄乱。与 Hadoop 的世界相比，纯手写的 MapReduce 作业可能比 Hive 和 Pig 作业慢，因为自爱 Hive 和 Pig 下会进行隐含的优化。 DataFrame 也可以用类似的方式看待。</p><p>因此，为什么要使用 DataFrame 和 DataSet，简单的答案是：<strong>速度和易用</strong>！</p><p>DataFrame 提供了优化、速度、自动模式发现、使用多中数据源和多语言的支持。</p><h3 id="4-2-优化"><a href="#4-2-优化" class="headerlink" title="4.2 优化"></a>4.2 优化</h3><p>Catalyst 为 DataFrame 提供了两种优化方式：</p><ol><li>谓词下推到数据源，只读取需要的数据</li><li>创建用于执行的物理计划，并生成比手写代码更优化的 JVM 字节码。</li></ol><p>和 RDD 不同， DataFrame 并不定义 DAG 图。 它会创建抽象语法书， Catalyst 引擎会使用基于规则和机遇开销的优化方法对其进行解析、检查和改进。<br><img src="media/15261969377501.jpg" alt=""></p><h3 id="4-3-速度"><a href="#4-3-速度" class="headerlink" title="4.3 速度"></a>4.3 速度</h3><p><img src="media/15261970734054.jpg" alt=""><br>上图展示了在单个机器上对1000万个证书对的 groupby 聚合操作。 Scala 和 Python 的 DataFrame 操作具有类似的执行时间，因为条吗都被便衣为相同的 JVM 字节码用来执行。</p><p>DataSet 使用优化的编译器把对象进行序列话和反序列化，以便进行处理并通过网络传输。 这些编码器比 Java 或 Kryo 序列话具有更高的性能。</p><p><img src="media/15261972316614.jpg" alt=""></p><h3 id="4-4-自动模式发现"><a href="#4-4-自动模式发现" class="headerlink" title="4.4 自动模式发现"></a>4.4 自动模式发现</h3><p>要从 RDD 创建 DataFrame，必须提供一个模式。而从 JSON、Parquet 和 ORC 文件创建 DataFrame 时，会自动发现一个模式，包括分区的发现。 Data Sources API 框架让这种情况成为可能。</p><h3 id="4-5-多数据源，多编程语言"><a href="#4-5-多数据源，多编程语言" class="headerlink" title="4.5 多数据源，多编程语言"></a>4.5 多数据源，多编程语言</h3><p>DataFrame API 支持按照最常用的格式（JSON、Parquet、ORC 和 Hive 表）进行读写</p><p>DataFrame API 支持本地文件系统、HDFS、S3和采用 JDBC 协议的外部 RDBMS 数据库读取数据</p><p>DataFrame API 支持第三方扩展：Avro、CSV、XML、HBase、ElasticSearch、Cassandra等。（<a href="http://spark-packages.org" target="_blank" rel="noopener">http://spark-packages.org</a>) 提供了第三方组件包的完整列表</p><p>Spark SQL 可以在 Java、Scala、Python、R中实现。利用 Spark SQL 的分布式 SQL 引擎，我们也可以编写纯 SQL 语句。</p><h3 id="4-6-RDD-和其他-API-的互操作性"><a href="#4-6-RDD-和其他-API-的互操作性" class="headerlink" title="4.6 RDD 和其他 API 的互操作性"></a>4.6 RDD 和其他 API 的互操作性</h3><p>Dataframe 可以使用 <strong>.rdd .toDF .toPandas .toDS</strong> 方法转换为 RDD 和 Pandas Dataframe。此外 DataFrame 可以与 Spark Streaming 和机器学习组件库配合使用。</p><h3 id="4-7-仅选择和读取必要的数据"><a href="#4-7-仅选择和读取必要的数据" class="headerlink" title="4.7 仅选择和读取必要的数据"></a>4.7 仅选择和读取必要的数据</h3><p>DataFrame API 、 DataSet API 和 DataSources API 的一个优点是通过将谓词下推到数据源系统，从而提供更丰富的优化手段。 <strong>列修剪、谓词下推、分区修剪</strong>会有这些框架自动完成。 这样，只有需要用到的数据才会读取和处理。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark-SQL-基础框架和核心组件&quot;&gt;&lt;a href=&quot;#Spark-SQL-基础框架和核心组件&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL 基础框架和核心组件&quot;&gt;&lt;/a&gt;Spark SQL 基
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark sql" scheme="http://www.ihoge.cn/tags/spark-sql/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML - 聚类算法</title>
    <link href="http://www.ihoge.cn/2018/ML2.html"/>
    <id>http://www.ihoge.cn/2018/ML2.html</id>
    <published>2018-05-05T18:00:21.000Z</published>
    <updated>2018-05-05T17:23:04.407Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="Spark-ML-聚类算法"><a href="#Spark-ML-聚类算法" class="headerlink" title="Spark ML - 聚类算法"></a>Spark ML - 聚类算法</h2><h3 id="1-KMeans快速聚类"><a href="#1-KMeans快速聚类" class="headerlink" title="1.KMeans快速聚类"></a>1.KMeans快速聚类</h3><p>首先到UR需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">KMeans</span>,<span class="type">KMeansModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>​    在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(</span><br><span class="line">    line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib版的教程类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    在得到数据后，我们即可通过ML包的固有流程：创建<code>Estimator</code>并调用其<code>fit()</code>方法来生成相应的<code>Transformer</code>对象，很显然，在这里<code>KMeans</code>类是<code>Estimator</code>，而用于保存训练后模型的<code>KMeansModel</code>类则属于<code>Transformer</code>：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kmeansmodel = <span class="keyword">new</span> <span class="type">KMeans</span>().</span><br><span class="line">      setK(<span class="number">3</span>).</span><br><span class="line">      setFeaturesCol(<span class="string">"features"</span>).</span><br><span class="line">      setPredictionCol(<span class="string">"prediction"</span>).</span><br><span class="line">      fit(df)</span><br></pre></td></tr></table></figure><p>​    与MLlib版本类似，ML包下的KMeans方法也有<code>Seed</code>（随机数种子）、<code>Tol</code>（收敛阈值）、<code>K</code>（簇个数）、<code>MaxIter</code>（最大迭代次数）、<code>initMode</code>（初始化方式）、<code>initStep</code>（KMeans||方法的步数）等参数可供设置，和其他的ML框架算法一样，用户可以通过相应的<code>setXXX()</code>方法来进行设置，或以<code>ParamMap</code>的形式传入参数，这里为了简介期间，使用<code>setXXX()</code>方法设置了参数K，其余参数均采用默认值。</p><p>​    与MLlib中的实现不同，<code>KMeansModel</code>作为一个<code>Transformer</code>，不再提供<code>predict()</code>样式的方法，而是提供了一致性的<code>transform()</code>方法，用于将存储在<code>DataFrame</code>中的给定数据集进行整体处理，生成带有预测簇标签的数据集：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> results = kmeansmodel.transform(df)</span><br></pre></td></tr></table></figure><p>​    为了方便观察，我们可以使用<code>collect()</code>方法，该方法将<code>DataFrame</code>中所有的数据组织成一个<code>Array</code>对象进行返回：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results.collect().foreach(</span><br><span class="line">      row =&gt; &#123;</span><br><span class="line">        println( row(<span class="number">0</span>) + <span class="string">" is predicted as cluster "</span> + row(<span class="number">1</span>))</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>也可以通过<code>KMeansModel</code>类自带的<code>clusterCenters</code>属性获取到模型的所有聚类中心情况：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.clusterCenters.foreach(</span><br><span class="line">      center =&gt; &#123;</span><br><span class="line">        println(<span class="string">"Clustering Center:"</span>+center)</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>​    与MLlib下的实现相同，<code>KMeansModel</code>类也提供了计算 <strong>集合内误差平方和（Within Set Sum of Squared Error, WSSSE)</strong> 的方法来度量聚类的有效性，在真实K值未知的情况下，该值的变化可以作为选取合适K值的一个重要参考：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.computeCost(df)</span><br></pre></td></tr></table></figure><h3 id="2-高斯混合模型-GMM-聚类算法"><a href="#2-高斯混合模型-GMM-聚类算法" class="headerlink" title="2.高斯混合模型(GMM)聚类算法"></a>2.高斯混合模型(GMM)聚类算法</h3><h4 id="2-1-基本原理"><a href="#2-1-基本原理" class="headerlink" title="2.1 基本原理"></a>2.1 基本原理</h4><p>​    <strong>高斯混合模型（Gaussian Mixture Model, GMM）</strong> 是一种概率式的聚类方法，属于生成式模型，它假设所有的数据样本都是由某一个给定参数的 <strong>多元高斯分布</strong> 所生成的。具体地，给定类个数<code>K</code>，对于给定样本空间中的样本 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-137bed5220372f1cad4f3cdf4529593e_l3.svg" alt="img"></p><p>，一个高斯混合模型的概率密度函数可以由K个多元高斯分布组合成的混合分布表示：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1d75e089a9a823703aa88d08ad53936e_l3.svg" alt="img"></p><p>其中，</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-a0f35b7e777b0ecf33b511cfb3174001_l3.svg" alt="img"></p><p>是以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>为均值向量， </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>为协方差矩阵的多元高斯分布的概率密度函数，可以看出，高斯混合模型由K个不同的多元高斯分布共同组成，每一个分布被称为高斯混合模型中的一个 <strong>成分(Component)</strong>， 而</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1a47f13ac9a37fcb6911a1b8e17cbb35_l3.svg" alt="img"></p><p>为第<code>i</code>个多元高斯分布在混合模型中的 <strong>权重</strong> ，且有 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-9312fca34e9b3afde8787a29c58fe192_l3.svg" alt="img"></p><p>。</p><p>假设已有一个存在的高斯混合模型，那么，样本空间中的样本的生成过程即是：以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-af3d8faef7e634b7b15f83cb1606b714_l3.svg" alt="img"></p><p>作为概率（实际上，权重可以直观理解成相应成分产生的样本占总样本的比例），选择出一个混合成分，根据该混合成分的概率密度函数，采样产生出相应的样本。</p><p>那么，利用GMM进行聚类的过程是利用GMM生成数据样本的“逆过程”：给定聚类簇数<code>K</code>，通过给定的数据集，以某一种 <strong>参数估计</strong> 的方法，推导出每一个混合成分的参数（即均值向量 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>、协方差矩阵 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>和权重 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-356e473b3185b432024c4643855f1b9d_l3.svg" alt="img"></p><p>），每一个多元高斯分布成分即对应于聚类后的一个簇。高斯混合模型在训练时使用了极大似然估计法，最大化以下对数似然函数：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-c006bbb984e97b258cf6bcc1d62ee2d7_l3.svg" alt="img"></p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-f57e2abf78c1ba038d4969a8fc513e7a_l3.svg" alt="img"></p><p>显然，该优化式无法直接通过解析方式求得解，故可采用 <strong>期望-最大化(Expectation-Maximization, EM)</strong> 方法求解，具体过程如下（为了简洁，这里省去了具体的数学表达式，详细可见<a href="https://en.wikipedia.org/wiki/Mixture_model" target="_blank" rel="noopener">wikipedia</a>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.根据给定的K值，初始化K个多元高斯分布以及其权重；</span><br><span class="line">2.根据贝叶斯定理，估计每个样本由每个成分生成的后验概率；(EM方法中的E步)</span><br><span class="line">3.根据均值，协方差的定义以及2步求出的后验概率，更新均值向量、协方差矩阵和权重；（EM方法的M步）</span><br><span class="line">重复2~3步，直到似然函数增加值已小于收敛阈值，或达到最大迭代次数</span><br></pre></td></tr></table></figure><p>​    当参数估计过程完成后，对于每一个样本点，根据贝叶斯定理计算出其属于每一个簇的后验概率，并将样本划分到后验概率最大的簇上去。相对于KMeans等直接给出样本点的簇划分的聚类方法，GMM这种给出样本点属于每个簇的概率的聚类方法，被称为 <strong>软聚类(Soft Clustering / Soft Assignment)</strong> 。</p><h4 id="2-2-模型的训练与分析"><a href="#2-2-模型的训练与分析" class="headerlink" title="2.2 模型的训练与分析"></a>2.2 模型的训练与分析</h4><p>​    Spark的ML库提供的高斯混合模型都在<code>org.apache.spark.ml.clustering</code>包下，和其他的聚类方法类似，其具体实现分为两个类：用于抽象GMM的超参数并进行训练的<code>GaussianMixture</code>类（<code>Estimator</code>）和训练后的模型<code>GaussianMixtureModel</code>类（<code>Transformer</code>），在使用前，引入需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">GaussianMixture</span>,<span class="type">GaussianMixtureModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    我们仍采用Iris数据集进行实验。为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib的操作类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    可以通过创建一个<code>GaussianMixture</code>类，设置相应的超参数，并调用<code>fit(..)</code>方法来训练一个GMM模型<code>GaussianMixtureModel</code>，在该方法调用前需要设置一系列超参数，如下表所示：</p><ul><li>K:聚类数目，默认为2 </li><li>maxIter : 最大迭代次数，默认为100 </li><li>seed : 随机数种子，默认为随机Long值 </li><li>Tol : 对数似然函数收敛阈值，默认为0.01 </li></ul><p>其中，每一个超参数均可通过名为<code>setXXX(...)</code>（如maxIterations即为<code>setMaxIterations()</code>）的方法进行设置。这里，我们建立一个简单的<code>GaussianMixture</code>对象，设定其聚类数目为3，其他参数取默认值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> gm = <span class="keyword">new</span> <span class="type">GaussianMixture</span>().setK(<span class="number">3</span>)</span><br><span class="line">               .setPredictionCol(<span class="string">"Prediction"</span>)</span><br><span class="line">               .setProbabilityCol(<span class="string">"Probability"</span>)</span><br><span class="line"><span class="keyword">val</span> gmm = gm.fit(df)</span><br></pre></td></tr></table></figure><p>和<code>KMeans</code>等硬聚类方法不同的是，除了可以得到对样本的聚簇归属预测外，还可以得到样本属于各个聚簇的概率（这里我们存在”Probability”列中）。</p><p>​    调用<code>transform()</code>方法处理数据集之后，打印数据集，可以看到每一个样本的预测簇以及其概率分布向量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = gmm.transform(df)</span><br><span class="line">result.show(<span class="number">150</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>​    得到模型后，即可查看模型的相关参数，与KMeans方法不同，GMM不直接给出聚类中心，而是给出各个混合成分（多元高斯分布）的参数。在ML的实现中，GMM的每一个混合成分都使用一个<code>MultivariateGaussian</code>类（位于<code>org.apache.spark.ml.stat.distribution</code>包）来存储，我们可以使用<code>GaussianMixtureModel</code>类的<code>weights</code>成员获取到各个混合成分的权重，使用<code>gaussians</code>成员来获取到各个混合成分的参数（均值向量和协方差矩阵）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until gmm.getK) &#123;</span><br><span class="line">      println(<span class="string">"Component %d : weight is %f \n mu vector is %s \n sigma matrix is %s"</span> format</span><br><span class="line">      (i, gmm.weights(i), gmm.gaussians(i).mean, gmm.gaussians(i).cov))</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;Spark-ML-聚类算法&quot;&gt;&lt;a href=&quot;#Spark-ML-聚类算法&quot; class=&quot;headerlink&quot; title=&quot;Spark ML - 聚类算法&quot;&gt;&lt;/a&gt;Spark ML - 聚类算法&lt;/h2&gt;&lt;h3 id=
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML - 协同过滤</title>
    <link href="http://www.ihoge.cn/2018/ML1.html"/>
    <id>http://www.ihoge.cn/2018/ML1.html</id>
    <published>2018-05-05T17:59:21.000Z</published>
    <updated>2018-05-05T17:22:29.171Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>​    获取spark自带的MovieLens数据集，其中每行包含一个用户、一个电影、一个该用户对该电影的评分以及时间戳。我们使用默认的<code>ALS.train()</code> 方法，即显性反馈（默认<code>implicitPrefs</code> 为false）来构建推荐模型并根据模型对评分预测的均方根误差来对模型进行评估。</p><h3 id="导入需要的包："><a href="#导入需要的包：" class="headerlink" title="导入需要的包："></a>导入需要的包：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">RegressionEvaluator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.recommendation.<span class="type">ALS</span></span><br></pre></td></tr></table></figure><h3 id="根据数据结构创建读取规范"><a href="#根据数据结构创建读取规范" class="headerlink" title="根据数据结构创建读取规范"></a>根据数据结构创建读取规范</h3><p> 创建一个Rating类型，即[Int, Int, Float, Long];然后建造一个把数据中每一行转化成Rating类的函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Rating</span>(<span class="params">userId: <span class="type">Int</span>, movieId: <span class="type">Int</span>, rating: <span class="type">Float</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">parseRating</span>(<span class="params">str: <span class="type">String</span></span>)</span>: <span class="type">Rating</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> fields = str.split(<span class="string">"::"</span>)</span><br><span class="line">            assert(fields.size == <span class="number">4</span>)</span><br><span class="line">            <span class="type">Rating</span>(fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>).toInt, fields(<span class="number">2</span>).toFloat, fields(<span class="number">3</span>).toLong)</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure><h3 id="读取数据："><a href="#读取数据：" class="headerlink" title="读取数据："></a>读取数据：</h3><p> 导入implicits，读取MovieLens数据集，把数据转化成Rating类型；</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> ratings = spark.sparkContext.textFile(<span class="string">"file:///home/hduser/spark/data/mllib/als/sample_movielens_ratings.txt"</span>).map(parseRating).toDF()</span><br></pre></td></tr></table></figure><p>然后打印数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ratings.show()</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p> 把MovieLens数据集划分训练集和测试集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(training, test) = ratings.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>))</span><br></pre></td></tr></table></figure><p> 使用ALS来建立推荐模型，这里我们构建了两个模型，一个是显性反馈，一个是隐性反馈</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alsExplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setUserCol(<span class="string">"userId"</span>). setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br><span class="line"><span class="keyword">val</span> alsImplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setImplicitPrefs(<span class="literal">true</span>). setUserCol(<span class="string">"userId"</span>).setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br></pre></td></tr></table></figure><p> 在 ML 中的实现有如下的参数:</p><ul><li><code>numBlocks</code> 是用于并行化计算的用户和商品的分块个数 (默认为10)。</li><li><code>rank</code> 是模型中隐语义因子的个数（默认为10）。</li><li><code>maxIter</code> 是迭代的次数（默认为10）。</li><li><code>regParam</code> 是ALS的正则化参数（默认为1.0）。</li><li><code>implicitPrefs</code> 决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本（默认是false，即用显性反馈）。</li><li><code>alpha</code> 是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准（默认为1.0）。</li><li><p><code>nonnegative</code> 决定是否对最小二乘法使用非负的限制（默认为false）。</p><p>可以调整这些参数，不断优化结果，使均方差变小。比如：imaxIter越大，regParam越 小，均方差会越小，推荐结果较优。</p></li></ul><p>接下来，把推荐模型放在训练数据上训练：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> modelExplicit = alsExplicit.fit(training)</span><br><span class="line"><span class="keyword">val</span> modelImplicit = alsImplicit.fit(training)</span><br></pre></td></tr></table></figure><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p> 使用训练好的推荐模型对测试集中的用户商品进行预测评分，得到预测评分的数据集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> predictionsExplicit = modelExplicit.transform(test)</span><br><span class="line"><span class="keyword">val</span> predictionsImplicit = modelImplicit.transform(test)</span><br></pre></td></tr></table></figure><p> 我们把结果输出，对比一下真实结果与预测结果：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictionsExplicit.show()</span><br><span class="line">predictionsImplicit.show()</span><br></pre></td></tr></table></figure><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p> 通过计算模型的均方根误差来对模型进行评估，均方根误差越小，模型越准确：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">RegressionEvaluator</span>().setMetricName(<span class="string">"rmse"</span>).setLabelCol(<span class="string">"rating"</span>). setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line"><span class="keyword">val</span> rmseExplicit = evaluator.evaluate(predictionsExplicit)</span><br><span class="line"><span class="keyword">val</span> rmseImplicit = evaluator.evaluate(predictionsImplicit)</span><br></pre></td></tr></table></figure><p> 打印出两个模型的均方根误差 ：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">s"Explicit:Root-mean-square error = <span class="subst">$rmseExplicit</span>"</span>)</span><br><span class="line">println(<span class="string">s"Implicit:Root-mean-square error = <span class="subst">$rmseImplicit</span>"</span>)</span><br></pre></td></tr></table></figure><p> 可以看到打分的均方差值为1.69和1.80左右。由于本例的数据量很少，预测的结果和实际相比有一定的差距。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;协同过滤算法&quot;&gt;&lt;a href=&quot;#协同过滤算法&quot; class=&quot;headerlink&quot; title=&quot;协同过滤算法&quot;&gt;&lt;/a&gt;协同过滤算法&lt;/h2&gt;&lt;p&gt;​    获取spark自带的MovieLens数据集，其中每行包含一个
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理3</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing3.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing3.html</id>
    <published>2018-04-21T19:59:31.000Z</published>
    <updated>2018-04-22T14:55:12.786Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="数据预处理3"><a href="#数据预处理3" class="headerlink" title="数据预处理3"></a>数据预处理3</h1><h2 id="3-1-描述性统计"><a href="#3-1-描述性统计" class="headerlink" title="3.1 描述性统计"></a>3.1 描述性统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    (<span class="string">'custID'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'gender'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'state'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'cardholder'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'balance'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numIntTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'creditLine'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'fraudRisk'</span>, typ.IntegerType()),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([</span><br><span class="line">    typ.StructField(e[<span class="number">0</span>], e[<span class="number">1</span>], <span class="keyword">True</span>) <span class="keyword">for</span> e <span class="keyword">in</span> fields</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: integer (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.groupBy(<span class="string">'gender'</span>).count().show()</span><br></pre></td></tr></table></figure><pre><code>+------+-------+|gender|  count|+------+-------+|     1|6178231||     2|3821769|+------+-------+</code></pre><p>⚠️ 这里男女比例失衡，在实际生产场景中应该重视这个问题</p><h3 id="descrive"><a href="#descrive" class="headerlink" title=".descrive()"></a>.descrive()</h3><p>这里可以选定某些列   <code>Dataframe的.describe()方法属于，RDD不可用</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line">desc = fraud_df.describe(numerical)</span><br><span class="line">desc.show()</span><br></pre></td></tr></table></figure><pre><code>+-------+-----------------+------------------+-----------------+|summary|          balance|          numTrans|      numIntTrans|+-------+-----------------+------------------+-----------------+|  count|         10000000|          10000000|         10000000||   mean|     4109.9199193|        28.9351871|        4.0471899|| stddev|3996.847309737077|26.553781024522852|8.602970115863767||    min|                0|                 0|                0||    max|            41485|               100|               60|+-------+-----------------+------------------+-----------------+</code></pre><ul><li>所有特征成正态分布；最大值是最小值的多倍</li><li>变异系数（均值与标准差之比）非常高（接近或大于1），意味着这是一个广泛的观测数据。</li></ul><p>如何检查偏度？（这里对“平衡”特征进行检查）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.agg(&#123;<span class="string">'balance'</span>:<span class="string">'skewness'</span>&#125;).show()</span><br></pre></td></tr></table></figure><pre><code>+------------------+| skewness(balance)|+------------------+|1.1818315552995033|+------------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.agg(&#123;<span class="string">'balance'</span>:<span class="string">'stddev'</span>&#125;).show()</span><br></pre></td></tr></table></figure><pre><code>+-----------------+|  stddev(balance)|+-----------------+|3996.847309737077|+-----------------+</code></pre><p>列表中的聚合函数包括：</p><ul><li>avg()</li><li>count()</li><li>countDistinct()</li><li>first()</li><li>kurtosis()</li><li>max()</li><li>mean()</li><li>skewness()</li><li>stddev()</li><li>stddev_pop()</li><li>stddev_samp()</li><li>sum()</li><li>sunDistinct()</li><li>var_pop()</li><li>var_samp()</li><li>variance()</li></ul><h2 id="3-2-相关性"><a href="#3-2-相关性" class="headerlink" title="3.2 相关性"></a>3.2 相关性</h2><p>模型应该只包括哪些与目标高度相关的特征，因此检查特征的相关性是非常重要的。其次之间的高度相关特征（即<strong>共线（collinear）性</strong>），可能会导致模型的不可预知性为或者可能进行不必要的复杂化。</p><p>这里的<strong>.corr(…)</strong>方法支持Pearson（皮尔森）相关系数，并且只能计算两两相关性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.corr(<span class="string">'balance'</span>, <span class="string">'numTrans'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.00044523140172659576</code></pre><p>为了创建相关矩阵，可以使用以下脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line"></span><br><span class="line">corr=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(numerical)):</span><br><span class="line">    temp = [<span class="keyword">None</span>] * i</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i, len(numerical)):</span><br><span class="line">        temp.append(fraud_df.corr(numerical[i], numerical[j]))</span><br><span class="line">    corr.append(temp)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 这里令temp = [None] * i是为了填充左下角矩阵</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">corrpd = pd.DataFrame(corr)</span><br><span class="line">corrpd.columns = corrpd.index = numerical</span><br><span class="line">corrpd</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>balance</th><br>      <th>numTrans</th><br>      <th>numIntTrans</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>balance</th><br>      <td>1.0</td><br>      <td>0.000445</td><br>      <td>0.000271</td><br>    </tr><br>    <tr><br>      <th>numTrans</th><br>      <td>NaN</td><br>      <td>1.000000</td><br>      <td>-0.000281</td><br>    </tr><br>    <tr><br>      <th>numIntTrans</th><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>1.000000</td><br>    </tr><br>  </tbody><br></table><br></div><h2 id="3-3-可视化"><a href="#3-3-可视化" class="headerlink" title="3.3 可视化"></a>3.3 可视化</h2><h3 id="3-3-1-直方图"><a href="#3-3-1-直方图" class="headerlink" title="3.3.1 直方图"></a>3.3.1 直方图</h3><p>当数据量非常大时，需要先对数据进行聚合:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hists = fraud_df.select(<span class="string">'balance'</span>).rdd.flatMap(<span class="keyword">lambda</span> x: x).histogram(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'bins'</span>:hists[<span class="number">0</span>][:<span class="number">-1</span>],</span><br><span class="line">    <span class="string">'freq'</span>:hists[<span class="number">1</span>]</span><br><span class="line">&#125;</span><br><span class="line">plt.bar(data[<span class="string">'bins'</span>], data[<span class="string">'freq'</span>], width=<span class="number">2000</span>)</span><br><span class="line">plt.title(<span class="string">'Histogram of \'balance\''</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqkvrtx0f5j31000g2q41.jpg" alt=""></p><h3 id="3-3-2-散点图"><a href="#3-3-2-散点图" class="headerlink" title="3.3.2 散点图"></a>3.3.2 散点图</h3><p>因为Pyspark不支持在服务器端的任何可视化模块，并且视图同时绘制数十亿的观测数据是非常不切实际的。在这个例子中，把欺诈数据集作为一个阶层抽取0.02%大约2000个观测数据。</p><p>使用<strong>df.sampleBy(…)</strong>方法随机抽取样本集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line"></span><br><span class="line">data_sample = fraud_df.sampleBy(<span class="string">'gender'</span>, &#123;<span class="number">1</span>: <span class="number">0.001</span>, <span class="number">2</span>: <span class="number">0.001</span>&#125;).select(numerical)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_sample.describe().show()</span><br></pre></td></tr></table></figure><pre><code>+-------+------------------+------------------+-----------------+|summary|           balance|          numTrans|      numIntTrans|+-------+------------------+------------------+-----------------+|  count|              2032|              2032|             2032||   mean| 3962.410433070866|28.546751968503937|3.989665354330709|| stddev|3876.8504670568145|25.611704520599815|8.235480127222067||    min|                 0|                 0|                0||    max|             28000|               100|               60|+-------+------------------+------------------+-----------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data_sample.rdd.map(<span class="keyword">lambda</span> x: [x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]]).collect()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Scatter3D</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># data = [d1,d3,d2]</span></span><br><span class="line">range_color = [</span><br><span class="line">    <span class="string">'#313695'</span>, <span class="string">'#4575b4'</span>, <span class="string">'#74add1'</span>, <span class="string">'#abd9e9'</span>, <span class="string">'#e0f3f8'</span>, <span class="string">'#ffffbf'</span>,</span><br><span class="line">    <span class="string">'#fee090'</span>, <span class="string">'#fdae61'</span>, <span class="string">'#f46d43'</span>, <span class="string">'#d73027'</span>, <span class="string">'#a50026'</span>]</span><br><span class="line">scatter3D = Scatter3D(<span class="string">"3D 散点图示例"</span>, width=<span class="number">1200</span>, height=<span class="number">800</span>)</span><br><span class="line">scatter3D.add(<span class="string">"3D"</span>, </span><br><span class="line">              data, is_visualmap=<span class="keyword">True</span>, </span><br><span class="line">              visual_range_color=range_color, </span><br><span class="line">              grid3d_opacity=<span class="number">0.5</span>,</span><br><span class="line">              is_grid3d_rotate = <span class="keyword">True</span>,</span><br><span class="line">              xaxis3d_name=<span class="string">'balance'</span>,</span><br><span class="line">              yaxis3d_name=<span class="string">'numTrans'</span>,</span><br><span class="line">              zaxis3d_name=<span class="string">'numIntTrans'</span></span><br><span class="line">             )</span><br><span class="line">scatter3D.render()</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqkvm4tn42j31kw0szk4z.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;数据预处理3&quot;&gt;&lt;a href=&quot;#数据预处理3&quot; class=&quot;headerlink&quot; title=&quot;数据预处理3&quot;&gt;&lt;/a&gt;数据预处理3&lt;/h1&gt;&lt;h2 id=&quot;3-1-描述性统计&quot;&gt;&lt;a href=&quot;#3-1-描述性统计&quot;
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理2</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing2.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing2.html</id>
    <published>2018-04-21T19:59:21.000Z</published>
    <updated>2018-04-22T14:55:12.053Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="数据预处理2"><a href="#数据预处理2" class="headerlink" title="数据预处理2"></a>数据预处理2</h1><h2 id="2-1-RDD读取方式"><a href="#2-1-RDD读取方式" class="headerlink" title="2.1 RDD读取方式"></a>2.1 RDD读取方式</h2><p><strong>1、读取文件为RDD</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fileData = sc.textFile(<span class="string">'/pydata/ccFraud.gz'</span>)</span><br><span class="line">header = fileData.first()</span><br><span class="line"></span><br><span class="line">rddData = fileData \</span><br><span class="line">    .filter(<span class="keyword">lambda</span> row: row != header) \</span><br><span class="line">    .map(<span class="keyword">lambda</span> row: [int(elem) <span class="keyword">for</span> elem <span class="keyword">in</span> row.split(<span class="string">','</span>)])</span><br></pre></td></tr></table></figure><p><strong>2、创建schema</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [typ.StructField(h[:], typ.IntegerType(), <span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> header.split(<span class="string">','</span>)]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType(fields)</span><br></pre></td></tr></table></figure><p><strong>3、RDD创建DataFrame</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfData = spark.createDataFrame(fraud, schema)</span><br><span class="line">dfData.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- &quot;custID&quot;: integer (nullable = true) |-- &quot;gender&quot;: integer (nullable = true) |-- &quot;state&quot;: integer (nullable = true) |-- &quot;cardholder&quot;: integer (nullable = true) |-- &quot;balance&quot;: integer (nullable = true) |-- &quot;numTrans&quot;: integer (nullable = true) |-- &quot;numIntlTrans&quot;: integer (nullable = true) |-- &quot;creditLine&quot;: integer (nullable = true) |-- &quot;fraudRisk&quot;: integer (nullable = true)</code></pre><p><strong>4、创建视图</strong></p><p>这里视图sql查询失败，而在spark-shell中没问题，是什么原因？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfData.createOrReplaceTempView(<span class="string">"DataViw"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"select gender,count(*) from DataViw group by gender"</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+-------+|count()|+-------+|      0|+-------+</code></pre><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fqkmuptfisj30qo0f0gnx.jpg" alt=""></p><h2 id="2-2-DataFrame读取模式"><a href="#2-2-DataFrame读取模式" class="headerlink" title="2.2 DataFrame读取模式"></a>2.2 DataFrame读取模式</h2><p><strong>创建类型方式一</strong>：全部为IntegerType</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    <span class="string">'custID'</span>,</span><br><span class="line">    <span class="string">'gender'</span>,</span><br><span class="line">    <span class="string">'state'</span>,</span><br><span class="line">    <span class="string">'cardholder'</span>,</span><br><span class="line">    <span class="string">'balance'</span>,</span><br><span class="line">    <span class="string">'numTrans'</span>,</span><br><span class="line">    <span class="string">'numIntlTrans'</span>,</span><br><span class="line">    <span class="string">'creditLine'</span>,</span><br><span class="line">    <span class="string">'fraudRisk'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([typ.StructField(f, typ.IntegerType(), <span class="keyword">True</span>) <span class="keyword">for</span> f <span class="keyword">in</span> fields])</span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: integer (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntlTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><p><strong>创建类型方式二</strong>: 指定每一列的类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    (<span class="string">'custID'</span>, typ.StringType()),</span><br><span class="line">    (<span class="string">'gender'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'state'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'cardholder'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'balance'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numIntlTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'creditLine'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'fraudRisk'</span>, typ.IntegerType()),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([</span><br><span class="line">    typ.StructField(e[<span class="number">0</span>], e[<span class="number">1</span>], <span class="keyword">True</span>) <span class="keyword">for</span> e <span class="keyword">in</span> fields</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: string (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntlTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>+------+------+-----+----------+-------+--------+------------+----------+---------+|custID|gender|state|cardholder|balance|numTrans|numIntlTrans|creditLine|fraudRisk|+------+------+-----+----------+-------+--------+------------+----------+---------+|     1|     1|   35|         1|   3000|       4|          14|         2|        0||     2|     2|    2|         1|      0|       9|           0|        18|        0||     3|     2|    2|         1|      0|      27|           9|        16|        0||     4|     1|   15|         1|      0|      12|           0|         5|        0||     5|     1|   46|         1|      0|      11|          16|         7|        0||     6|     2|   44|         2|   5546|      21|           0|        13|        0||     7|     1|    3|         1|   2000|      41|           0|         1|        0||     8|     1|   10|         1|   6016|      20|           3|         6|        0||     9|     2|   32|         1|   2428|       4|          10|        22|        0||    10|     1|   23|         1|      0|      18|          56|         5|        0|+------+------+-----+----------+-------+--------+------------+----------+---------+only showing top 10 rows</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;数据预处理2&quot;&gt;&lt;a href=&quot;#数据预处理2&quot; class=&quot;headerlink&quot; title=&quot;数据预处理2&quot;&gt;&lt;/a&gt;数据预处理2&lt;/h1&gt;&lt;h2 id=&quot;2-1-RDD读取方式&quot;&gt;&lt;a href=&quot;#2-1-RDD读
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理1</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing1.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing1.html</id>
    <published>2018-04-21T19:59:00.000Z</published>
    <updated>2018-04-22T14:55:13.475Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Pyspark数据预处理1"><a href="#Pyspark数据预处理1" class="headerlink" title="Pyspark数据预处理1"></a>Pyspark数据预处理1</h1><h2 id="1-1-重复值"><a href="#1-1-重复值" class="headerlink" title="1.1 重复值"></a>1.1 重复值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">'M'</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>, <span class="string">'gender'</span>])</span><br></pre></td></tr></table></figure><h3 id="1-1-1-查重"><a href="#1-1-1-查重" class="headerlink" title="1.1.1 查重"></a>1.1.1 <strong>查重</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"总数据量：&#123;&#125;"</span>.format(df.count()))</span><br><span class="line">print(<span class="string">"重复数据量：&#123;&#125;"</span>.format(df.count() - df.distinct().count()))</span><br></pre></td></tr></table></figure><pre><code>总数据量：7重复数据量：1</code></pre><h3 id="1-1-2-去重"><a href="#1-1-2-去重" class="headerlink" title="1.1.2  去重"></a>1.1.2  <strong>去重</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = df.dropDuplicates()</span><br><span class="line">df1.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+| id|weight|height|age|gender|+---+------+------+---+------+|  5| 133.2|   5.7| 54|     F||  5| 129.2|   5.3| 42|     M||  1| 144.5|   5.9| 33|     M||  4| 144.5|   5.9| 33|     M||  2| 167.2|   5.4| 45|     M||  3| 124.1|   5.2| 23|     F|+---+------+------+---+------+</code></pre><h3 id="1-1-3-查询排除ID列后的重复值"><a href="#1-1-3-查询排除ID列后的重复值" class="headerlink" title="1.1.3  查询排除ID列后的重复值"></a>1.1.3  <strong>查询排除ID列后的重复值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"df1中样本量：&#123;&#125;"</span>.format(df1.count()))</span><br><span class="line">print(<span class="string">"df1去重后样本量：&#123;&#125;"</span>\</span><br><span class="line">      .format(</span><br><span class="line">      df1.select([c <span class="keyword">for</span> c <span class="keyword">in</span> df1.columns <span class="keyword">if</span> c != <span class="string">'id'</span>])</span><br><span class="line">      .distinct()</span><br><span class="line">      .count()))</span><br></pre></td></tr></table></figure><pre><code>df1中样本量：6df1去重后样本量：5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2 = df1.dropDuplicates(subset=[c <span class="keyword">for</span> c <span class="keyword">in</span> df.columns <span class="keyword">if</span> c != <span class="string">'id'</span>])</span><br><span class="line">df2.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+| id|weight|height|age|gender|+---+------+------+---+------+|  5| 133.2|   5.7| 54|     F||  4| 144.5|   5.9| 33|     M||  2| 167.2|   5.4| 45|     M||  3| 124.1|   5.2| 23|     F||  5| 129.2|   5.3| 42|     M|+---+------+------+---+------+</code></pre><h2 id="1-1-4-查看是否有重复ID"><a href="#1-1-4-查看是否有重复ID" class="headerlink" title="1.1.4 查看是否有重复ID"></a>1.1.4 查看是否有重复ID</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line">df2.agg(</span><br><span class="line">    fn.count(<span class="string">'id'</span>).alias(<span class="string">'count'</span>),</span><br><span class="line">    fn.countDistinct(<span class="string">'id'</span>).alias(<span class="string">'distinct'</span>))\</span><br><span class="line">    .show()</span><br></pre></td></tr></table></figure><pre><code>+-----+--------+|count|distinct|+-----+--------+|    5|       4|+-----+--------+</code></pre><p>这里需要给每行数据赋唯一ID。通过<code>fn.monotonically_increasing_id()</code>方法给每一条记录提供一个唯一并且递增的ID，当数据放置在大约不到10亿个分区中，每个分区的记录少于8亿条，ID就能被保证时唯一的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df3 = df2.withColumn(<span class="string">'new_id'</span>, fn.monotonically_increasing_id())</span><br><span class="line">df3.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+-------------+| id|weight|height|age|gender|       new_id|+---+------+------+---+------+-------------+|  5| 133.2|   5.7| 54|     F|  25769803776||  1| 144.5|   5.9| 33|     M| 171798691840||  2| 167.2|   5.4| 45|     M| 592705486848||  3| 124.1|   5.2| 23|     F|1236950581248||  5| 129.2|   5.3| 42|     M|1365799600128|+---+------+------+---+------+-------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df3.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- id: long (nullable = true) |-- weight: double (nullable = true) |-- height: double (nullable = true) |-- age: long (nullable = true) |-- gender: string (nullable = true) |-- new_id: long (nullable = false)</code></pre><h2 id="1-2-空缺值"><a href="#1-2-空缺值" class="headerlink" title="1.2. 空缺值"></a>1.2. 空缺值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_miss = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.6</span>, <span class="number">28</span>,   <span class="string">'M'</span>,  <span class="number">100000</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>,   <span class="string">'M'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="keyword">None</span> , <span class="number">5.2</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>,   <span class="string">'M'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>,   <span class="string">'F'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="keyword">None</span>, <span class="string">'F'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>,   <span class="string">'M'</span>,  <span class="number">76000</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>, <span class="string">'gender'</span>, <span class="string">'income'</span>])</span><br></pre></td></tr></table></figure><h3 id="1-2-1-查找缺失值"><a href="#1-2-1-查找缺失值" class="headerlink" title="1.2.1 查找缺失值"></a>1.2.1 查找缺失值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_miss.rdd.map(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x[<span class="string">'id'</span>], sum([c <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">for</span> c <span class="keyword">in</span> x]))</span><br><span class="line">).collect()</span><br></pre></td></tr></table></figure><pre><code>[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_miss.where(<span class="string">'id == 3'</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+------+| id|weight|height| age|gender|income|+---+------+------+----+------+------+|  3|  null|   5.2|null|  null|  null|+---+------+------+----+------+------+</code></pre><h3 id="1-2-2-检查每一列缺失值数据的百分比："><a href="#1-2-2-检查每一列缺失值数据的百分比：" class="headerlink" title="1.2.2 检查每一列缺失值数据的百分比："></a>1.2.2 检查每一列缺失值数据的百分比：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_miss.agg(*[</span><br><span class="line">    (<span class="number">1</span> - (fn.count(c) / fn.count(<span class="string">'*'</span>))).alias(c + <span class="string">'_missing'</span>)</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns</span><br><span class="line">]).show()</span><br></pre></td></tr></table></figure><pre><code>+----------+------------------+--------------+------------------+------------------+------------------+|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|+----------+------------------+--------------+------------------+------------------+------------------+|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|+----------+------------------+--------------+------------------+------------------+------------------+</code></pre><h3 id="1-2-3-移除缺失严重的特征"><a href="#1-2-3-移除缺失严重的特征" class="headerlink" title="1.2.3 移除缺失严重的特征"></a>1.2.3 移除缺失严重的特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一：</span></span><br><span class="line">df_miss1 = df_miss.select([</span><br><span class="line">    c <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns <span class="keyword">if</span> c != <span class="string">'income'</span></span><br><span class="line">])</span><br><span class="line">df_miss1.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+| id|weight|height| age|gender|+---+------+------+----+------+|  1| 143.5|   5.6|  28|     M||  2| 167.2|   5.4|  45|     M||  3|  null|   5.2|null|  null||  4| 144.5|   5.9|  33|     M||  5| 133.2|   5.7|  54|     F||  6| 124.1|   5.2|null|     F||  7| 129.2|   5.3|  42|     M|+---+------+------+----+------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式二： 设定阀值,阀值越大过滤越严格</span></span><br><span class="line">df_miss2 = df_miss.dropna(thresh=<span class="number">3</span>)</span><br><span class="line">df_miss2.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+------+| id|weight|height| age|gender|income|+---+------+------+----+------+------+|  1| 143.5|   5.6|  28|     M|100000||  2| 167.2|   5.4|  45|     M|  null||  4| 144.5|   5.9|  33|     M|  null||  5| 133.2|   5.7|  54|     F|  null||  6| 124.1|   5.2|null|     F|  null||  7| 129.2|   5.3|  42|     M| 76000|+---+------+------+----+------+------+</code></pre><h3 id="1-2-4-填充空缺值-很好用的方法"><a href="#1-2-4-填充空缺值-很好用的方法" class="headerlink" title="1.2.4 填充空缺值(很好用的方法)"></a>1.2.4 填充空缺值(很好用的方法)</h3><p>使用df.fillna()方法并传给它一个字典效率高</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充平均值，先创建平均值字典</span></span><br><span class="line">means = df_miss1.agg(</span><br><span class="line">        *[fn.mean(c).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss1.columns <span class="keyword">if</span> c != <span class="string">'gender'</span>])\</span><br><span class="line">        .toPandas().to_dict(<span class="string">'records'</span>)[<span class="number">0</span>]</span><br><span class="line">means[<span class="string">'gender'</span>] = <span class="string">'missing'</span></span><br><span class="line"></span><br><span class="line">df_miss1.fillna(means).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------------------+------+---+-------+| id|            weight|height|age| gender|+---+------------------+------+---+-------+|  1|             143.5|   5.6| 28|      M||  2|             167.2|   5.4| 45|      M||  3|140.28333333333333|   5.2| 40|missing||  4|             144.5|   5.9| 33|      M||  5|             133.2|   5.7| 54|      F||  6|             124.1|   5.2| 40|      F||  7|             129.2|   5.3| 42|      M|+---+------------------+------+---+-------+</code></pre><h2 id="1-3-离群值"><a href="#1-3-离群值" class="headerlink" title="1.3. 离群值"></a>1.3. 离群值</h2><h3 id="1-3-1-定义离群区间"><a href="#1-3-1-定义离群区间" class="headerlink" title="1.3.1 定义离群区间"></a>1.3.1 定义离群区间</h3><p>IQR：定义为上分位和下分位之差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_outliers = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.3</span>, <span class="number">28</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">154.2</span>, <span class="number">5.5</span>, <span class="number">45</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">342.3</span>, <span class="number">5.1</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.5</span>, <span class="number">33</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.4</span>, <span class="number">54</span>),</span><br><span class="line">        (<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.1</span>, <span class="number">21</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>])</span><br></pre></td></tr></table></figure><p>计算每个特征的上下截断点：<code>.approxQuantitle(...)方法</code></p><ul><li>第一个参数指定列名</li><li>第二个参数可以时0～1之间的任意数（0.5为指定中位数）</li><li>第三个参数指定对每个度量的容忍度（如果为1，就会计算一个度量的准确值）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cols = [<span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>]</span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    quantiles = df_outliers.approxQuantile(col, [<span class="number">0.25</span>, <span class="number">0.75</span>], <span class="number">0.01</span>)</span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [quantiles[<span class="number">0</span>] - <span class="number">0.4</span> * IQR, quantiles[<span class="number">1</span>] + <span class="number">0.4</span> * IQR]</span><br><span class="line">    </span><br><span class="line">bounds</span><br><span class="line"><span class="comment"># 根据实际业务情况进行调整</span></span><br></pre></td></tr></table></figure><pre><code>{&apos;age&apos;: [17.6, 64.4], &apos;height&apos;: [4.9399999999999995, 5.66], &apos;weight&apos;: [119.19999999999999, 164.2]}</code></pre><h3 id="1-3-2-标记离群值"><a href="#1-3-2-标记离群值" class="headerlink" title="1.3.2 标记离群值"></a>1.3.2 标记离群值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">outliers = df_outliers\</span><br><span class="line">            .select(*[<span class="string">'id'</span>]</span><br><span class="line">            + [(</span><br><span class="line">                (df_outliers[c] &lt; bounds[c][<span class="number">0</span>])|</span><br><span class="line">                (df_outliers[c] &gt; bounds[c][<span class="number">1</span>])</span><br><span class="line">            ).alias(c + <span class="string">'_o'</span>) <span class="keyword">for</span> c <span class="keyword">in</span> cols ])</span><br><span class="line">outliers.show()</span><br></pre></td></tr></table></figure><pre><code>+---+--------+--------+-----+| id|weight_o|height_o|age_o|+---+--------+--------+-----+|  7|   false|   false|false||  6|   false|   false|false||  5|   false|   false|false||  1|   false|   false|false||  3|    true|   false| true||  2|   false|   false|false||  4|   false|   false|false|+---+--------+--------+-----+</code></pre><h3 id="1-3-3-查看离群值"><a href="#1-3-3-查看离群值" class="headerlink" title="1.3.3 查看离群值"></a>1.3.3 查看离群值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bad_outlier = df_outliers.join(outliers, on=<span class="string">'id'</span>)</span><br><span class="line">bad_outlier.filter(<span class="string">'weight_o'</span>).select(<span class="string">'id'</span>, <span class="string">'weight'</span>).show()</span><br><span class="line">bad_outlier.filter(<span class="string">'age_o'</span>).select(<span class="string">'id'</span>, <span class="string">'age'</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+| id|weight|+---+------+|  3| 342.3|+---+------++---+---+| id|age|+---+---+|  3| 99|+---+---+</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Pyspark数据预处理1&quot;&gt;&lt;a href=&quot;#Pyspark数据预处理1&quot; class=&quot;headerlink&quot; title=&quot;Pyspark数据预处理1&quot;&gt;&lt;/a&gt;Pyspark数据预处理1&lt;/h1&gt;&lt;h2 id=&quot;1-1
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Strutured Streaming(Spark 2.3)</title>
    <link href="http://www.ihoge.cn/2018/StructuredStreaming.html"/>
    <id>http://www.ihoge.cn/2018/StructuredStreaming.html</id>
    <published>2018-04-19T02:59:21.000Z</published>
    <updated>2018-04-22T14:55:14.155Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark-2-3-Strutured-Streaming"><a href="#Spark-2-3-Strutured-Streaming" class="headerlink" title="Spark 2.3 Strutured Streaming"></a>Spark 2.3 Strutured Streaming</h1><p>为了继续实现 Spark 更快，更轻松，更智能的目标，Spark 2.3 在许多模块都做了重要的更新，比如 Structured Streaming 引入了低延迟的连续处理（continuous processing）；支持 stream-to-stream joins；通过改善 pandas UDFs 的性能来提升 PySpark；支持第四种调度引擎 Kubernetes clusters（其他三种分别是自带的独立模式Standalone，YARN、Mesos）。除了这些比较具有里程碑的重要功能外，Spark  2.3 还有以下几个重要的更新：<br>引入 DataSource v2 APIs<br>矢量化(Vectorized)的 ORC reader<br>Spark History Server v2 with K-V store<br>基于 Structured Streaming 的机器学习管道API模型<br>MLlib 增强<br>Spark SQL 增强<br>这篇文章将简单地介绍上面一些高级功能和改进，更多的特性请参见 <a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark 2.3 release notes</a></p><p>这里主要针对Spark2.3 对Structured Streaming的新特性进行描述。</p><p>The simplest way to perform streaming analytics is not having to reason about streaming.</p><h2 id="毫秒延迟的连续流处理"><a href="#毫秒延迟的连续流处理" class="headerlink" title="毫秒延迟的连续流处理"></a>毫秒延迟的连续流处理</h2><p>Apache Spark 2.0 的 Structured Streaming 将微批次处理(micro-batch processing)从它的高级 APIs 中解耦出去，原因有两个：首先，开发人员更容易学习这些 API，不需要考虑这些 APIs 的微批次处理情况;其次，它允许开发人员将一个流视为一个无限表，他们查询流的数据，就像他们查询静态表一样简便。<br>但是，为了给开发人员提供不同的流处理模式，社区引入了一种新的毫秒级低延迟(millisecond low-latency连续模式(continuous mode)。<br>在内部，结构化的流引擎逐步执行微批中的查询计算，执行周期由触发器间隔决定，这个延迟对大多数真实世界的流应用程序来说是可以容忍的。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241174964831.jpg" alt=""></p><h3 id="微批处理（micro-batch）"><a href="#微批处理（micro-batch）" class="headerlink" title="微批处理（micro-batch）"></a>微批处理（micro-batch）</h3><p>Structured Streaming默认使用微量批处理执行模型。这意味着Spark流引擎会定期检查流源，并对自上次批量结束后到达的新数据运行批量查询。At a high-level, it looks like this:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241491220257.jpg" alt=""></p><p>在微批处理体系结构中，驱动程序通过将记录偏移量保存到<strong>预写的日志中（write-ahead-log）</strong>来检查进度，然后用它来重新启动查询。请注意，为了获得确定性的重新执行和端对端语义，在微批处理启动之前，要在下一个微批处理中处理的范围偏移保存到日志中。因此，数据源中可用的记录可能<strong>不得不等待当前的微批次在其偏移记录之前</strong>完成，并且在下一个微批中处理它。过程看起来像这样。</p><p>In this architecture, the driver checkpoints the progress by saving the records offsets to a write-ahead-log, which may be then used to restart the query. Note that the range offsets to be processed in the next micro-batch is saved to the log before the micro-batch has started in order to get deterministic re-executions and end-to-end semantics. As a result, a record that is available at the source may have to wait for the current micro-batch to be completed before its offset is logged and the next micro-batch processes it. At the record level, the timeline looks like this.<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241495217055.jpg" alt=""><br>这会导致更多的延迟时间。</p><h3 id="连续处理（Continuous-Processing）"><a href="#连续处理（Continuous-Processing）" class="headerlink" title="连续处理（Continuous Processing）"></a>连续处理（Continuous Processing）</h3><p>而对于连续模式，流读取器连续拉取源数据并处理数据，而不是按指定的触发时间间隔读取一批数据。通过不断地查询源数据和处理数据，新的记录在到达时立即被处理，将等待时间缩短到毫秒，满足低延迟的应用程序的需求，具体如下面图所示：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241175216530.jpg" alt=""></p><p>目前连续模式支持 map-like Dataset 操作，包括投影(projections)、selections以及其他 SQL 函数，但是不支持 current_timestamp(), current_date() 以及聚合函数。它还支持将 Kafka 作为数据源和数据存储目的地(sink)，也支持 console 和 memory sink。<br>现在，开发人员可以根据延迟要求选择模式连续或微量批处理，来构建大规模实时流式传输应用程序，同时这些系统还能够享受到 Structured Streaming 提供的 fault-tolerance 和 reliability guarantees 特性。<br>简单来说，Spark 2.3 中的连续模式是实验性的，它提供了以下特性：<br>端到端的毫秒级延迟<br>至少一次语义保证<br>支持 map-like 的 Dataset 操作</p><h2 id="流与流进行Join"><a href="#流与流进行Join" class="headerlink" title="流与流进行Join"></a>流与流进行Join</h2><p>Spark 2.0 版本的 Structured Streaming 支持流 DataFrame/Dataset 和静态数据集之间的 join，但是 Spark 2.3 带来了期待已久的流和流的 Join 操作。支持内连接和外连接，可用在大量的实时场景中。<br>广告收益是流与流进行Join的典型用例。例如，展示广告流和广告点击流共享您希望进行流式分析的公共关键字(如adId)和相关数据，根据这些数据你可以分析出哪些广告更容易被点击。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241175635666.jpg" alt=""></p><p>这个例子看起来很简单，但是实现流和流的Join需要解决很多技术难题，如下：</p><ul><li><strong>缓存处理延迟数据：</strong>需要缓存延迟的数据，直到从其他流中找到匹配的事件;</li><li><strong>先置缓冲区大小：</strong>限制流式连接缓冲区大小的唯一方法是将延迟超过某个阈值的数据丢弃。此最大延迟阈值应由用户根据业务需求和系统资源限制之间的平衡进行配置。</li><li><strong>明确定义的语义</strong>：在静态连接和流式连接之间保持一致的SQL连接语义，具有或不具有上述阈值。</li></ul><p>幸运的事Spark2.3解决了所有这些问题，因此我们使用SQL连接的清晰语义来表达计算，并控制相关事件之间的延迟容忍。<br>假设有两个不同的kafka流，我们要通过两个流中的共有的<code>adID</code>属性进行连接。让我们来看看代码是怎么实现的：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241453518177.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241453673148.jpg" alt=""></p><p>与所有的结构化数据流的查询相同，代码是完全一样的。将kafka流数据当作静态的Dataframe一样去定义。当执行此查询时，结构化流式处理引擎会根据需要将点击和展示作为流状态进行缓冲。对于特定广告，只要接收到两个相关事件（即收到第二个事件后），即会生成联合输出。随着数据到达，连接的输出将逐步生成并写入查询接收器（例如，另一个Kafka）。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241457085725.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark-2-3-Strutured-Streaming&quot;&gt;&lt;a href=&quot;#Spark-2-3-Strutured-Streaming&quot; class=&quot;headerlink&quot; title=&quot;Spark 2.3 Strutu
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="strutured Streaming" scheme="http://www.ihoge.cn/tags/strutured-Streaming/"/>
    
  </entry>
  
  <entry>
    <title>流式处理框架 &amp; Spark Streaming &amp; Flume &amp; Kafka</title>
    <link href="http://www.ihoge.cn/2018/strame.html"/>
    <id>http://www.ihoge.cn/2018/strame.html</id>
    <published>2018-04-19T02:59:21.000Z</published>
    <updated>2018-04-22T14:55:14.911Z</updated>
    
    <content type="html"><![CDATA[<h1 id="流式处理框架-Spark-Streaming"><a href="#流式处理框架-Spark-Streaming" class="headerlink" title="流式处理框架 Spark Streaming"></a>流式处理框架 Spark Streaming</h1><ul><li>很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。</li><li>对于技术人员来说，可以利用数据挖掘和OLAP（On-Line Analytical Processing）分析工具从中找到对企业有价值的信息，这也是离线批处理的一般过程。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241530856455.jpg" alt=""></li></ul><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>流式处理框架基础</li><li>Flume</li><li>Kafka</li></ol><h2 id="1-流式处理框架基础"><a href="#1-流式处理框架基础" class="headerlink" title="1. 流式处理框架基础"></a>1. 流式处理框架基础</h2><h3 id="1-1-流数据一般特征"><a href="#1-1-流数据一般特征" class="headerlink" title="1.1 流数据一般特征"></a>1.1 流数据一般特征</h3><p><strong>流数据具有如下特征：</strong></p><ul><li>数据快速持续到达，潜在大小也许是无穷无尽的；<ul><li>数据来源众多，格式复杂；</li><li>数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储；</li><li>注重数据的整体价值，不过分关注个别数据；</li><li>数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序；</li></ul></li></ul><p>因此，用于静态数据存储的存储工具和批处理计算框架便无法满足流式计算、实时分析的应用需求。</p><h3 id="1-2-实时计算的核心框架"><a href="#1-2-实时计算的核心框架" class="headerlink" title="1.2 实时计算的核心框架"></a>1.2 实时计算的核心框架</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241530202093.jpg" alt=""></p><h3 id="1-3-数据源"><a href="#1-3-数据源" class="headerlink" title="1.3 数据源"></a>1.3 数据源</h3><p><strong>为何数据源在流式计算中显得异常重要</strong></p><p>所谓数据源，是指为了满足不同需求而不断输出数据的框架。不同于针对静态数据的批处理，要进行实时计算，就必须有足够“实时”且能够稳定运行的消息“投递者”，它必须能够高效的整合更低一级数据源发送的实时数据，并根据处理框架的API对数据进行规范化处理和转存，同时，对于流式处理系统而言，很多时候经过流式处理框架处理后的数据也是流数据，也可视作一种数据源。</p><p><strong>数据源的高低级之分</strong></p><p>对于流式处理框架而言，数据源的高低级之分，其实就是数据“输出”框架的高低级之分。任何数据源，要进行收集、转存和传递等工作，就需要有与之对应的处理框架，其框架越完善、功能越强大、且能够提供更便于实时处理的数据，我们就称其高级数据源，反之就称为低级数据源。</p><p><strong>数据源本身的“处理框架”</strong></p><p>这其实并不是规范的说法，通常来说，低级数据源我们会直接称呼其数据流，如文件流、套接字流等，而高级数据源，我们更多的会根据其作用，称其为日志收集系统（如Flume），或分布式消息队列（如Kafka）。对于初学者而言，我们可以将这些复杂的框架名称统一想象成一个数据的中转站，这些框架的核心作用都是收集、暂存然后投递数据。</p><p><strong>大数据分析师的知识边界</strong></p><p>一方面，我们要进行流式计算，就必须了解数据源对计算框架投递消息的方法，如此才能根据数据源设计算法，或者根据计算需求调整数据源；另一方面，我们也要区别大数据分析师与ETL工程师的区别，我们只需要了解与计算框架最近一层的数据源是如何与计算框架相互嵌套的既可，而无需再深究该数据源更低一层数据源、甚至是数据产生第一层数据源是如何工作的。</p><h2 id="2-Flume"><a href="#2-Flume" class="headerlink" title="2. Flume"></a>2. Flume</h2><h3 id="2-1-Flume介绍"><a href="#2-1-Flume介绍" class="headerlink" title="2.1 Flume介绍"></a>2.1 Flume介绍</h3><p>Flume是Cloudera公司开发的分布式、高可用的日志收集系统。现已开源给Apache。<br>目前比较有代表性的日志收集系统，除了Flume之外，还有FaceBook的Scribe。</p><p>Flume原始版本为OG，后经过整体架构的重新设计，以改名为Flume-NG。Flume发展至今，已不限于日志收集，还可以通过简单的配置，收集不同数据源的海量数据并将数据准确高效的传输到不同的处理框架或数据存储系统。<br>目前Flume可对接的主流大数据框架有Hadoop、Kafka、Hive、HBase、Spark等。</p><h3 id="2-2-Flume基本架构"><a href="#2-2-Flume基本架构" class="headerlink" title="2.2 Flume基本架构"></a>2.2 Flume基本架构</h3><p>Flume-NG采用三层架构设计，分别对应其三个核心组件：Source、Channel和Sink，其中，Source主要功能为收集上一层数据源传输过来的数据，Channel用于数据的暂时存储，Sink则用于数据的处理，即数据的传输对象和传输方式。其基本架构如下图所示：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241531282473.jpg" alt=""></p><p>其中，一个Agent代表一个Java进程，上图表示一个Event（数据）在一个Agent的传输流程。</p><ul><li><code>Event</code>：一条消息或者一个数据，具有可选的头信息，在头信息中可以设置时间戳、主机名等信息；</li><li><code>Source</code>：数据源，接收或者收集不同形式的数据源；</li><li><code>Channel</code>：Event的临时缓冲区，Source接收的Event会先发送到Channel进行缓存，在缓冲区内等待Sink的消费；</li><li><code>Sink</code>：用于处理Channel中缓存的数据，并发送至下一层（处理框架、存储中心或者下一个Agent）；</li><li><code>Agent</code>：包含了Source、Channel、Sink等组件的Flume进程；</li><li><code>Interceptor</code>：Event拦截器，在数据进入Channel之前根据配置要求，对数据进行头信息（Header）编辑，添加时间戳、主机名称等；</li><li><code>Selector</code>：Event的选择器，即决定Event流入Channel的方式，主要有<code>复制</code>（Replicating）和<code>复用</code>（Multiplexing）两种选方式；</li><li><code>Sink Processor</code>：Event Sink处理器，Flume提供了故障转移处理器和负载均衡处理器两种。</li></ul><h3 id="2-3-Flume核心组件-Source"><a href="#2-3-Flume核心组件-Source" class="headerlink" title="2.3 Flume核心组件:Source"></a>2.3 Flume核心组件:Source</h3><p><strong>Source核心功能</strong></p><ul><li>用于对接各种数据源，并将收集到的Event发送至用于临时存储的Channel中。Flume中每个Agent对应一个Source，而每个Agent的Source在启动前必须通过修改配置变量来设置其接受消息种类，此即称为Source种类</li></ul><p><strong>常用Scoure种类</strong>(依据数据类型分类)</p><ul><li><p><code>Avro Source</code>：Avro是Doug Cutting牵头开发的一个数据序列化系统，设计用于支持大批量数据交换的应用。Avro Source支持Avro协议，接收RPC请求，<code>Avro Source</code>通过监听Avro端口接收外部Avro客户端流事件，在Flume的多层架构中经常被使用接收上游Sink发送的event。</p></li><li><p><code>Kafka Source</code>：对接分布式消息队列Kafka，作为Kafka的消费者持续从Kafka中拉取数据，如果多个kafka source同时消费Kafka中同一个Topic，则其会被设置为同组id，从而保证多个Kafka Source之间不会重复拉取数据。</p></li><li><p><code>Exec Source</code>：支持Linux命令，收集标准输出数据或者通过tail -f file的方式监听指定文件。Exce Source可以实现实时的消息传输，但却不会记录已读取的文件的位置，不支持断点续传，如果Exce Source重启或者挂掉都会造成后续增加的消息无法接收，建议只在测试环境中使用。</p></li><li><p><code>Spooling Directory Source</code>：用于监听一个文件夹，收集文件夹中新文件数据，手机玩新文件数据会将文件名称的后缀改为.completed，缺点是不支持老文件（已经completed的文件）中新增数据集的收集，并且不能够对嵌套文件夹进行递归监听。</p></li><li><p><code>Taildir Source</code>：监听一个文件或文件夹，通过正则表达式匹配需要监听的数据源文件，支持文件夹嵌套递归监听，Taildir Source通过将监听文件的位置写入到文件中，从而实现断点续传，并且能够保证没有重复的数据读取。</p></li></ul><h3 id="2-4-Flume核心组件-Channel"><a href="#2-4-Flume核心组件-Channel" class="headerlink" title="2.4 Flume核心组件:Channel"></a>2.4 Flume核心组件:Channel</h3><p><strong>Channel核心功能</strong></p><ul><li>Channel是Event的<code>临时缓冲区</code>，存储Source收集但尚未被Sink读取的Event，其目标是为平衡Source收集速度和Sink读取速度，可视为Flume内部的消息队列。Channel线程的安全性较高且具有事务性，支持Source写失败重复写和Sink读失败重复读等操作。同时，我们根据Channel存储方式划分Channel种类。</li></ul><p><strong>常用Channel种类</strong> (依据存储方式分类)</p><ul><li><p>Memory Channel：<code>缓冲区所有数据都存于内存</code>，Memory Channel读写速度快，但存储量受内存限制，且当Flume进程挂掉、服务器宕机或重启时都会造成数据丢失。建议在服务器内存充足，且不关心数据丢失的场景下使用。</p></li><li><p>File Channel：缓冲区所有数据写入磁盘，File Channel存储容量大，无数据丢失风险。File Channel数据存储路径可以配置多个磁盘文件路径，通过磁盘并行写入提高其性能，在写入磁盘时是顺序写入，且单个数据文件大小可通过配置文件中maxFileSize参数进行调整，当被写入文件大小超过上限时，Flume会自动创建新文件用来存储后续Event。但数据文件数量不会无限增长，一旦旧文件被Sink读取完成，就将被删除。Flume通过设置检查点和备份检查点实现Agent重启之后快速将File Channel中的数据按顺序回放到内存中，以保证Agent在失败重启后仍能快速安全地提供服务。</p></li><li><p><code>Kafka Channel</code>：值得一提的是，Kafka可作为Flume中Channel存储方式，Kafka是分布式、可扩展、高容错、高吞吐的分布式系统，Kafka通过其优秀的架构设计充分利用磁盘顺序读写特性，在廉价的硬件条件下就能完成高效的消息发布和订阅，对比其他两种Channel，Kafka Channel在读取速度、存储量和容错性上完美的弥补了二者短板，若能合理利用Kafka性能，能够达到事半功倍的效果。</p></li></ul><h3 id="2-5-Flume核心组件-Sink"><a href="#2-5-Flume核心组件-Sink" class="headerlink" title="2.5 Flume核心组件:Sink"></a>2.5 Flume核心组件:Sink</h3><p><strong>Sink核心功能</strong></p><ul><li>用于处理Channel中缓存的数据，当数据经过Sink Processor处理后由Sink进行后续处理，Sink可将数据传输至静态数据存储中心进行数据保存，也可以将数据传输至实时处理框架进行数据处理，当然，也可以将数据传输至下一层数据源，进行进一步数据聚合、整理或推送。</li></ul><p><strong>常用Sink种类</strong> (依据投递接收方划分)</p><ul><li><p>Avro Sink：Avro Sink常用于对接下一层Arvo Source，通过发送RPC请求将Event发送到下一层的Avro Source，同时，Avro Sink提供了端到端的批量压缩数据传输，从而解决RPC传输过程中占用大量网络资源以及产生大量Socket连接等问题。</p></li><li><p>HDFS Sink：HDFS作为Hadoop生态中的最常用文件系统，具有高容错、可扩展、高性能、低成本等特点，HDFS Sink通过将Event写入HDFS进行数据存储，能够有效、长期存储大量数据。</p></li><li><p>Kafka Sink：在消息传递过程中采用Kafka框架能够从很大程度上降低系统耦合度，从而增加系统系统稳定性和容错机制，Flume可通过Kafka Sink将Event写入Kafka的Topic，其他应用通过Kafka获得数据。Flume从1.7.0开始支持Kafka 0.9及以上版本。</p></li></ul><h3 id="Interceptor（拦截器）-与选择器配合使用"><a href="#Interceptor（拦截器）-与选择器配合使用" class="headerlink" title="Interceptor（拦截器） (与选择器配合使用)"></a>Interceptor（拦截器） (与选择器配合使用)</h3><p><strong>Interceptor（拦截器）功能介绍</strong></p><ul><li>Source将Event写入Channel之前，可以用拦截器对Event进行添加头信息等简单处理，Source和Channel之间可设多个拦截器，不同拦截器可根据自身规则对Event进行简单处理，注意，拦截器属于轻量级插件，无法应对复杂数据处理工作。</li></ul><p><strong>常用Interceptor</strong></p><ul><li><p>主机拦截器（Host Interceptor）：Flume通过主机拦截器在Event头信息中添加主机名称或者IP。通过主机拦截器，Channel可以根据不同的主机信息分区存储Event，后续Sink也可根据不同主机信息对Event进行分别处理。</p></li><li><p>静态拦截器（Static Interceptor）：主要用于修改、过滤Event在此被拦截之前所设置的信息。</p></li></ul><h3 id="Selector（选择器）-与拦截器配合使用"><a href="#Selector（选择器）-与拦截器配合使用" class="headerlink" title="Selector（选择器） (与拦截器配合使用)"></a>Selector（选择器） (与拦截器配合使用)</h3><p><strong>功能介绍</strong></p><ul><li>Source发送的Event通过Channel选择器来决定以何种方式写入Channel，Flume提供了三种常用的选择器，分别是复制Channel选择器（Replicating Channel Selector）、复用Channel选择器（Multiplexing Channel Selector）和自定义选择器。</li></ul><p><strong>常用选择器</strong></p><ul><li><p>复制选择器（ Replicating Channel Selector ）：是Flume选择器的默认模式，即不对选择器进行设置时采用的模式，此时Source将以复制的方式将一个Event写入多个Channel中，不同的Sink可从不同的的Channel中获得相同的数据。复制选择器用途较多，当一个Event要做多个用途时，可考虑用复制选择器。</p></li><li><p>复用Channel选择器（Multiplexing Channel Selector）：复用选择器需要配合拦截器共同使用，复用选择器会根据Event的头信息来判断每个Event应该写入哪个Channel中。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241532271911.jpg" alt=""></p></li></ul><p>💡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关于负载均衡和故障排除的相关说明:</span><br><span class="line">Flume为了提高整体容错能力和稳定性，提供了负载均衡故障转移功能</span><br><span class="line">这两项功能配置较为简单，只要合理配置Sink组，并且在每组Sink中设置多个</span><br><span class="line">子Sink，就能够自动进行负载均衡和故障转移。</span><br></pre></td></tr></table></figure><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p>Kafka是由LinkedIn公司开源的分布式消息队列。现已加入到Apache软件基金会，并且凭借其高吞吐、可扩展、高可用、部署简单、开发接口丰富等特性，已在各大互联网公司的实际生产环境中广泛使用。<br>同时，大多数分布式处理系统都支持使用kafka，如Spark、Storm、Druid、Flume等<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241532641098.jpg" alt=""></p><h3 id="Kafka基本特点"><a href="#Kafka基本特点" class="headerlink" title="Kafka基本特点"></a>Kafka基本特点</h3><ul><li>Kafka其实就是消息“中转站”</li><li>Kafka本身不产生消息，也不对消息进行永久存储，它只是消息的“中转站”。</li></ul><ul><li>数据“中转站”至关重要</li><li>在实际工作中，我们广泛需要消息“中转站”的原因，是数据采集端种类各异，消息格式数据格式种类繁多，同时数据应用情景也非常复杂，提取数据可能为了实时计算，可能为了存储一段时间后进行批处理，也可能直接推送至数据产品前段，也可能中间夹杂各种其他过程以及各过程之间的复用。因此，我们需要“中转站”对数据进行暂存、简单处理、再次投递。</li></ul><ul><li>Kafka作为最优秀的消息“中转站”之一，并被广泛认可，完全得益于其优秀的架构。</li></ul><h3 id="Kafka构架特点"><a href="#Kafka构架特点" class="headerlink" title="Kafka构架特点"></a>Kafka构架特点</h3><ol><li>生产者和消费者不需要彼此了解</li><li>消费者的性能问题不会影响生产者</li><li>消费者受到保护，免受生产者的伤害</li><li>在处理负载方面有很大的灵活性</li><li>消息可供任何人使用 - </li><li>大量的新用例，监控，审计和故障排除</li></ol><ul><li>高吞吐率</li></ul><ol><li>Kafka利用顺序读写磁盘的设计，提供了可以和内存随机读写相匹敌的读写速度；</li><li>其灵活的客户端API设计，利用Linux操作系统提供的“零拷贝”特性，减少了消息网络传输的时间；</li><li>同时提供端到端的消息压缩传输，对同一主题下的消息采用分布式存储；</li></ol><ul><li>高容错、高可用</li></ul><p>Kafka在集群模式下允许用户对每个分区（partition）配置多个副本，且将副本均匀的分到各个节点（broker）内存储，保证同一个分区的副本不会再同一台机器上存储。同时，多副本之间采用Leader-Follower机制同步消息，只有Leader对外提供读写服务，当Leader意外失败、Broker进程关闭、服务器宕机等情况导致数据不可用时，kafka会从Follower中选择一个成为Leader继续提供读写服务。</p><ul><li>可拓展</li></ul><p>理论上Kafka的性能随着Broker的增加而增加，增加一个Broker只需为新增Broker设置一个唯一编号，配置文件编写完成后，Kafka就能通过ZooKeeper发现新的Broker，并投入使用。</p><ul><li>接藕</li></ul><p>Kafka内部能够将消息生产阶段和处理阶段分开，两个阶段互相独立，且各自能实现自己的处理逻辑，通过Kafka提供的消息写入和消费接口实现对消息的连接处理。两个阶段相互独立，不仅降低了自身复杂度，同时也实现了对外部框架提供部分服务的功能（如Flume中嵌入Kafka）。</p><ul><li><strong>峰值处理</strong></li></ul><p>实际工作中，经常会遇见数据在某个时间点爆发式增长（如双11），此时如果后台处理系统无法及时处理峰值需求，就会导致数据积压，严重时会导致系统崩溃。而若能合理使用Kafka进行数据中转，就相当于给系统接入了一个巨大的缓冲区，其既能接收持续暴增的请求，也能根据后台需求提供服务，进而提高了系统数据处理能力。</p><h3 id="Kafka基本架构"><a href="#Kafka基本架构" class="headerlink" title="Kafka基本架构"></a>Kafka基本架构</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241533218101.jpg" alt=""></p><ul><li>Producer代表消息生产者</li><li>Consumer代表消息消费者</li><li>Broker代表Kafka集群中各节点</li><li>Partition表示消息的一个分区</li><li>ZooKeeper为Kafka集群提供资源调度服务</li></ul><h3 id="Kafka基本概念"><a href="#Kafka基本概念" class="headerlink" title="Kafka基本概念"></a>Kafka基本概念</h3><ul><li><p><strong>Broker</strong>：一个Kafka的实例就是一个Broker，相当于Flume中的Agent；</p></li><li><p><strong>Topic</strong>：主题，Kafka中同一类型数据集的名称，相当于数据库中的表，Producer将同一类型数据写入同一个Topic中，同一个Consumer或Consumer Group从同一个Topic中消费数据，同时，一个Topic在物理上会被分成多分存储到不同的物理机上；</p></li><li><p><strong>Partition</strong>：分区，一个Topic可设置多个分区，相当于把一个数据集分成多分，存储到不同分区中进行存储（类似于HDFS），分区命名规则为topicname-index；</p></li><li><p><strong>Segment</strong>：段文件，Kafka中最小存储单位，一个topic包含多个Partition，一个Partition又包含多个Segment，Segment以其内部消息的起始偏移量进行索引；</p></li><li><p><strong>Offset</strong>：消息的起始偏移量，可作为Segment的索引；</p></li><li><p>Replication：副本，一个Partition可有一个或多个副本，创建Topic时可设置副本数量；</p></li><li><p>Producer：消息生产者，负责向Kafka集群中发布消息；</p></li><li><p>Consumer Group：消费者组，一个Consumer Group可包含一个或多个Consumer，当一个topic被一个消费者组消费的时候，一条消息只能由其中一位消费者消费，不会出现多位消费者消费同一条信息的情况；</p></li><li><p>Consumer：消息消费者，可从指定topic中拉取消息；</p></li><li><p>ZooKeeper：kafka需要ZooKeeper对其进行协调管理，安装Kafka过程将自带一个ZooKeeper。</p></li></ul><h2 id="实操流程"><a href="#实操流程" class="headerlink" title="实操流程"></a>实操流程</h2><h3 id="单节点演示"><a href="#单节点演示" class="headerlink" title="单节点演示"></a>单节点演示</h3><ol><li><p>启动zoopeeper服务<br> <code>bin/zookeeper-server-start.sh config/zookeeper.properties</code></p></li><li><p>启动Kafka服务<br> <code>bin/kafka-server-start.sh config/server.properties</code></p></li><li><p>创建topic取名cdatest<br> <code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic cdatest</code></p></li><li><p>查看topic<br> <code>bin/kafka-topics.sh --list --zookeeper localhost:2181</code></p></li><li><p>启动生产者指定topic，并在终端输入测试数据<br> <code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic cdatest</code></p></li><li><p>启动消费者指定topic<br> <code>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic cdatest --from-beginning</code></p></li></ol><h3 id="集群模式测试"><a href="#集群模式测试" class="headerlink" title="集群模式测试"></a>集群模式测试</h3><ol><li><p>修改参数server.properties</p></li><li><p>启动zookeeper集群服务</p></li><li><p>启动kafka集群（分别在各节点执行）<br><code>./bin/kafka-server-start.sh config/server.properties</code></p></li><li><p>任意一个节点创建topic<br><code>./bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 3 --topic kktest</code></p></li><li><p>查看topic状态<br><code>./bin/kafka-topics.sh --describe --zookeeper master:2181,slave1:2181,slave2:2181 --topic kktest</code></p></li><li><p>创建producer，手动输入测试数据（以master为例）<br><code>./bin/kafka-console-producer.sh --broker-list master:9092 --topic kktest</code></p></li><li><p>以slave1为例创建consumer消费数据<br><code>./bin/kafka-console-consumer.sh --bootstrap-server slave1:9092 --from-beginning --topic kktest</code></p></li></ol><p><strong>可以在不同节点创建多个生产者和消费者</strong></p><h2 id="使用外部数据源进行数据传输"><a href="#使用外部数据源进行数据传输" class="headerlink" title="使用外部数据源进行数据传输"></a>使用外部数据源进行数据传输</h2><ol><li><p>启动Kafka，启动prodecer和consumer</p></li><li><p>在Kafka安装路径内创建test.txt然后执行:q</p></li><li>~/kafka/bin/connect-standalone.sh ~/kafka/config/connect-standalone.properties ~/kafka/config/connect-file-source.properties ~/kafka/config/connect-file-sink.properties</li></ol><p><strong>自定义模式读取外部文件时，有两个主要的配置文件</strong><br>connect-file-source.properties<br>connect-file-sink.properties</p><p>根据需要修改文件读取模式、文件路径、Topic等</p><h2 id="Kafka于Flume联合部署"><a href="#Kafka于Flume联合部署" class="headerlink" title="Kafka于Flume联合部署"></a>Kafka于Flume联合部署</h2><p>创建flume配置文件：如kafkaSource.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent    a1.sources = r1    a1.sinks = k1    a1.channels = c1   # Describe/configure the source    a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource    a1.sources.r1.kafka.topics = kafkaTopic  a1.sources.r1.kafka.bootstrap.servers = master:9092,slave1:9092,slave2:9092  # Describe the sink    a1.sinks.k1.type = logger # Use a channel which buffers events in memory    a1.channels.c1.type = memory    a1.channels.c1.capacity = 1000    a1.channels.c1.transactionCapacity = 100   # Bind the source and sink to the channel    a1.sources.r1.channels = c1    a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>启动Flume：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/flume/bin/flume-ng agent --conf ~/flume/conf --conf-file ~/flume/conf/kafkaSource.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>创建kafkaTopic</p><p><code>~/kafka/bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 3 --topic kafkaTopic</code></p><p>创建producer，输入测试信息<br><code>~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sparkMLLib</code></p><p><strong>顺序出错的分析</strong></p><ol><li>Kafka传出来的数据顺序就已经被打乱</li><li>计算资源（虽然传输的数据很微小，但是集群的启动成本高昂）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;流式处理框架-Spark-Streaming&quot;&gt;&lt;a href=&quot;#流式处理框架-Spark-Streaming&quot; class=&quot;headerlink&quot; title=&quot;流式处理框架 Spark Streaming&quot;&gt;&lt;/a&gt;流式处理框架 Spark Streami
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://www.ihoge.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark的基本架构</title>
    <link href="http://www.ihoge.cn/2018/IntroductionToSpark.html"/>
    <id>http://www.ihoge.cn/2018/IntroductionToSpark.html</id>
    <published>2018-04-18T02:59:21.000Z</published>
    <updated>2018-04-22T14:55:15.713Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark的基本架构"><a href="#Spark的基本架构" class="headerlink" title="Spark的基本架构"></a>Spark的基本架构</h1><p>当单机没有足够的能力和资源来执行大量信息的计算（或者低延迟计算），这时就需要一个集群或一组机器将许多机器的资源集中在一起，使我们可以使用全部累积的在一起的计算和存储资源。现在只有一组机器不够强大，你需要一个框架来协调他们之间的工作。 Spark是一种工具，可以管理和协调跨计算机集群执行数据任务。<br>Spark用于执行任务的机器集群可以由Spark的Standalone，YARN或Mesos等集群管理器进行管理。然后，我们向这些集群管理器提交Spark应用程序，这些集群管理器将资源授予我们的应用程序，以便我们完成我们的工作。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240180152481.jpg" alt=""></p><h3 id="1-Spark-Application"><a href="#1-Spark-Application" class="headerlink" title="1. Spark Application"></a>1. Spark Application</h3><p>Spark应用程序由一个驱动程序进程和一组执行程序进程组成。Driver进程运行main（）函数，位于集群中的一个节点上，它负责三件事：维护Spark应用程序的相关信息;回应用户的程序或输入;分配和安排Executors之间的工作。驱动程序过程是绝对必要的 - 它是Spark应用程序的核心，并在应用程序的生命周期中保留所有相关信息。<br>Executor负责实际执行Driver分配给他们的工作。这意味着，每个Executor只有两个任务：执行由驱动程序分配给它的代码，并将该执行程序的计算状态报告给驱动程序节点。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239867627806.jpg" alt=""></p><p>群集管理器控制物理机器并为Spark应用程序分配资源。这可以是几个核心集群管理员之一：Spark的Standalone，YARN或Mesos。这意味着可以同时在群集上运行多个Spark应用程序。<br>在前面的插图中，左侧是我们的driver，右侧是四个executors。在该图中，我们删除了群集节点的概念。用户可以通过配置指定有多少执行者应该落在每个节点上。</p><ul><li>Spark有一些集群管理器，负责调度可用资源。</li><li>驱动程序进程负责执行执行程序中的驱动程序命令，以完成我们的任务。</li></ul><h3 id="2-Spark’s-Languge-APIs"><a href="#2-Spark’s-Languge-APIs" class="headerlink" title="2. Spark’s Languge APIs"></a>2. Spark’s Languge APIs</h3><p>尽管我们的executor大多会一直运行Spark代码。但我们仍然可以通过Spark的语言API用多种不同语言运行Spark代码。大多数情况下，Spark会在每种语言中提供一些核心“concepts”，并将不同语言的代码译成运行在机器集群上的Spark代码。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239873536392.jpg" alt=""></p><p> <code>Spark有两套基本的API：低级非结构化(Unstructured)API和更高级别的结构化(Structured)API。</code></p><h3 id="3-SparkSession"><a href="#3-SparkSession" class="headerlink" title="3. SparkSession"></a>3. SparkSession</h3><p>我们通过驱动程序来控制Spark应用程序。该驱动程序进程将自身作为名为SparkSession并作为唯一的接口API对象向用户开放。 SparkSession实例是Spark在群集中执行用户定义操作的方式。 SparkSession和Spark应用程序之间有一对一的对应关系。在Scala和Python中，变量在启动控制台时可用作spark。让我们看下简单的Scala和/或Python中的SparkSession。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239862105558.jpg" alt=""></p><h3 id="4-Dataframe"><a href="#4-Dataframe" class="headerlink" title="4. Dataframe"></a>4. Dataframe</h3><p>DataFrame是最常见的<code>Structured API</code>（结构化API），只是表示有类型的<code>包含行和列的数据表</code>。一个简单的比喻就是一个带有命名列的电子表格。其根本区别在于，当电子表格位于一台计算机上某个特定位置时，Spark DataFrame可以跨越数千台计算机。将数据放在多台计算机上的原因无非有两种：数据太大而无法放在一台计算机上，或者在一台计算机上执行计算所需的时间太长。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15239862709297.jpg" alt=""></p><p>DataFrame概念并不是Spark独有的。 R和Python都有相似的概念。但是，Python / R DataFrame（有一些例外）存在于一台机器上，而不是多台机器上。这限制了您可以对python和R中给定的DataFrame执行的操作与该特定机器上存在的资源进行对比。但是，由于Spark具有适用于Python和R的<code>Spark’s Language APIs</code>，因此将Pandas（Python）DataFrame转换为Spark DataFrame和R DataFrame转换为Spark DataFrame（R）非常容易。</p><p><strong>注意</strong><br>Spark有几个核心抽象：Datasets，Dadaframes，SQL Table和弹性分布式数据集（RDD）。这些抽象都表示分布式数据集合，但它们有不同的接口来处理这些数据。最简单和最有效的是DataFrames，它可以用于所有语言。<strong>以下概念适用于所有的核心抽象。</strong></p><h3 id="5-Partitions"><a href="#5-Partitions" class="headerlink" title="5. Partitions"></a>5. Partitions</h3><p>为了允许每个执行者并行执行工作，Spark将数据分解成称为分区的块。分区是位于集群中的一台物理机上的一组行。 DataFrame的分区表示数据在执行过程中如何在整个机器群中物理分布。如果你有一个分区，即使你有数千个执行者，Spark也只会有一个分区。如果有多个分区，但只有一个执行程序Spark仍然只有一个并行性，因为只有一个计算资源。<br>值得注意的是，使用DataFrames，我们不会（大部分）操作 手动分区（基于个人）。我们只需指定物理分区中数据的高级转换，并且Spark确定此工作将如何在集群上实际执行。较低级别的API确实存在（通过弹性分布式数据集接口）。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240195737335.jpg" alt=""></p><h3 id="6-Transformations"><a href="#6-Transformations" class="headerlink" title="6. Transformations"></a>6. Transformations</h3><p>在Spark中，核心数据结构是不可改变的，这意味着一旦创建它们就不能更改。起初，这可能看起来像一个奇怪的概念，如果你不能改变它，你应该如何使用它？为了“更改”DataFrame，您必须指示Spark如何修改您所需的DataFrame。这些说明被称为<code>转换</code>。<br>转换操作没有返回输出，这是因为我们只指定了一个抽象转换，并且Spark不会在转换之前采取行动，直到我们执行一个动作。Transformations是如何使用Spark来表达业务逻辑的核心。Spark有两种类型的Transformations，一种是窄依赖转换关系，一种是宽依赖转换关系。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239881561168.jpg" alt=""><br>宽依赖指输入分区对多输出分区起作用（多个孩子）。这被称为shuffle，Spark将在群集之间交换分区。对于窄依赖转换，Spark将自动执行称为流水线的操作，这意味着如果我们在DataFrame上指定了多个过滤器，它们将全部在内存中执行。当我们执行shuffle时，Spark会将结果写入磁盘。</p><h3 id="7-Lazy-Evaluation"><a href="#7-Lazy-Evaluation" class="headerlink" title="7. Lazy Evaluation"></a>7. Lazy Evaluation</h3><p>Lazy Evaluation意味着Spark将等到执行计算指令图的最后时刻。在Spark中，我们不是在表达某些操作时立即修改数据，而是建立起来应用于源数据的转换计划。Spark将把原始DataFrame转换计划编译为一个高效的物理计划，该计划将在群集中尽可能高效地运行。这为最终用户带来了巨大的好处，因为Spark可以优化整个数据流从端到端。这方面的一个例子就是所谓的“predicate pushdown” DataFrames。如果我们构建一个大的Spark作业，但在最后指定了一个过滤器，只需要我们从源数据中获取一行，则执行此操作的最有效方法就是访问我们需要的单个记录。 Spark实际上会通过自动推低滤波器来优化这一点。</p><h3 id="8-Actions"><a href="#8-Actions" class="headerlink" title="8. Actions"></a>8. Actions</h3><p>转换使我们能够建立我们的逻辑计划。为了触发计算，我们需要一个动作操作。一个动作指示Spark计算一系列转换的结果。<br>在指定我们的操作时，我们开始了一个Spark作业，它运行我们的过滤器转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区的基础上执行计数，然后一个collect将我们的结果带到各自语言的本地对象。我们可以通过检查Spark UI（<a href="http://localhost:4040）来看到所有这些，Spark" target="_blank" rel="noopener">http://localhost:4040）来看到所有这些，Spark</a> UI是一个包含在Spark中的工具，它允许我们监视集群上运行的Spark作业。</p><h3 id="9-Dataframe-amp-SQL"><a href="#9-Dataframe-amp-SQL" class="headerlink" title="9. Dataframe &amp; SQL"></a>9. Dataframe &amp; SQL</h3><p>Spark SQL是Spark为结构化和半结构化数据处理设计的最受欢迎的模块之一。 Spark SQL允许用户使用SQL或可在Java，Scala，Python和R中使用的DataFrame和Dataset API来查询Spark程序中的structured data。由于DataFrame API提供了一种统一的方法来访问各种的数据源（包括Hive datasets，Avro，Parquet，ORC，JSON和JDBC），用户能够以相同方式连接到任何数据源，并将这些多个数据源连接在一起。 Spark SQL使用Hive meta store为用户提供了与现有Hive数据，查询和UDF完全兼容的功能。用户可以无缝地 在Spark上无需修改即可运行其当前的Hive工作负载。<br>Spark SQL也可以通过spark-sql shell来访问，现有的业务工具可以通过标准的JDBC和ODBC接口进行连接。</p><p>现在我们通过一个示例并在DataFrame和SQL中进行跟踪。不管语言如何，以完全相同的方式启动相同的转换。您可以在SQL或DataFrames（R，Python，Scala或Java）中表达业务逻辑，并且在实际执行代码之前，Spark会将该逻辑编译计划优化并最终生成最优的物理计划。 Spark SQL允许您作为用户将任何DataFrame注册为表或视图（临时表），并使用纯SQL查询它。编写SQL查询或编写DataFrame代码之间没有性能差异 都“编译”到我们在DataFrame代码中指定的相同底层计划。<br>通过一个简单的方法调用就可以将任何DataFrame制作成表格或视图。</p><p><strong>With SQl</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162004942.jpg" alt=""><br><strong>With DataFrame</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162492089.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162738329.jpg" alt=""></p><p>现在有7个步骤将我们带回源数据。您可以在这些DataFrame的解释计划中看到这一点。以上图解说明了我们在“代码”中执行的一系列步骤。真正的执行计划（解释中可见的执行计划）将与下面的执行计划有所不同，因为在物理执行方面进行了优化，然而，该执行计划与任何计划一样都是起点。这个执行计划是一个有向无环图（DAG）的转换，每个转换产生一个新的不可变DataFrame，我们在这个DataFrame上调用一个动作来产生一个结果。</p><ol><li>第一步是读取数据。但是Spark实际上并没有读取它（Lazy Evaluation）</li><li>第二步是我们的分组，在技术上，当我们调用groupBy时，我们最终得到了一个RelationalGroupedDataset，它是DataFrame的一个奇特名称，该DataFrame具有指定的分组，但需要用户在可以进一步查询之前<strong>指定聚合</strong>。</li><li>因此第三步是指定聚合。我们使用总和聚合方法。这需要输入一列 表达式或简单的列名称。 sum方法调用的结果是一个新的dataFrame。你会看到它有一个新的模式，但它知道每个列的类型。（再次强调！）这里没有执行计算是非常重要的。这只是我们表达的另一种转换，Spark仅仅能够跟踪我们提供的类型信息。</li><li>第四步是简化语言，我们使用withColumnRename给原始列重新定义新名称。当然，这不会执行计算 - 这只是另一种转换！</li><li>第五步导入一个函数对数据进行排序，即desc函数。从destination_total列中找到的最大值。</li><li>第六步，我们将指定一个限制。这只是说明我们只需要五个值。这就像一个过滤器，只是它按位置而不是按值过滤。可以肯定地说，它基本上只是指定了一定大小的DataFrame。</li><li>最后一步是我们的行动！现在我们实际上开始收集上面的DataFrame结果的过程，Spark将以我们正在执行的语言返回一个列表或数组。现在我们看下它的解释计划。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240170262028.jpg" alt=""><br>虽然这个解释计划与我们确切的“概念计划”不符，但所有的部分都在那里。可以看到limit语句以及orderBy（在第一行）。你也可以看到我们的聚合是如何在partial_sum调用中的两个阶段发生的。这是因为数字列表是可交换的，并且Spark可以执行sum()并按分区进行划分。当然，我们也可以看到我们如何在DataFrame中读取数据。同时我们也可以将它写出到Spark支持的任何数据源中。例如，假设我们想要将这些信息存储在PostgreSQL等数据库中，或者将它们写入另一个文件。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark的基本架构&quot;&gt;&lt;a href=&quot;#Spark的基本架构&quot; class=&quot;headerlink&quot; title=&quot;Spark的基本架构&quot;&gt;&lt;/a&gt;Spark的基本架构&lt;/h1&gt;&lt;p&gt;当单机没有足够的能力和资源来执行大量信息
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://www.ihoge.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>分布式数据库HBase原理</title>
    <link href="http://www.ihoge.cn/2018/hbase.html"/>
    <id>http://www.ihoge.cn/2018/hbase.html</id>
    <published>2018-04-16T05:59:21.000Z</published>
    <updated>2018-04-22T14:55:17.503Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="目录："><a href="#目录：" class="headerlink" title="目录："></a>目录：</h2><ol><li>概述</li><li>HBase访问借口</li><li>HBase数据模型</li><li>HBase实现原理</li><li>HBase运行机制</li><li>HBase应用方案</li></ol><hr><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>HBase是一个高可靠、高性能、面向咧、可伸缩的分布式数据库。是谷歌<code>BigTable</code>的开源实现，主要用来存储<code>非结构化</code>和<code>半结构化</code>的松散数据。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238450244324.jpg" alt=""><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238450357302.jpg" alt=""></p><h3 id="为什么要HBase？"><a href="#为什么要HBase？" class="headerlink" title="为什么要HBase？"></a>为什么要HBase？</h3><ol><li>受限于Hadoop MR编程框架的高延迟数据处理机制，<code>无法满足大规模数据实时处理的需求</code>。</li><li>HDFS不能随机访问</li><li>传统关系型数据无法应对剧增的海量数据</li><li>传统关系型数据库在数据结构变化时需要停机维护；孔裂浪费存储空间</li></ol><p>因此，业界出现了一类面相半结构化数据存储和处理的高可扩展、低写入、查询延迟的系统。如键值数据库、文档数据库和<code>列族</code>数据库（如BitTable和HBase）。如今HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中。</p><h3 id="HBase与传统关系数据库的对比"><a href="#HBase与传统关系数据库的对比" class="headerlink" title="HBase与传统关系数据库的对比"></a>HBase与传统关系数据库的对比</h3><table><thead><tr><th>特性</th><th>传统关系数据库</th><th>HBase</th></tr></thead><tbody><tr><td>数据类型</td><td>关系模式，具有丰富的数据类型后和存储方式</td><td>采用更简洁的数据模型，吧数据存储为未经解释的字符串</td></tr><tr><td>数据操作</td><td>包含丰富的操作，涉及复杂的夺标链接</td><td>不存在复杂的表于表之间的关系，只有简单的增、删、查、清空等，避免表于表间复杂关系</td></tr><tr><td>存储模式</td><td>基于行存储</td><td>基于列存储，每个列族有几个文件保存，不同列族的文件是分离的</td></tr><tr><td>数据索引</td><td>通过针对不同列构建复杂多个索引以提高访问性能</td><td>只有一个索引（行键），访问方法为或行键访问或行键扫面，通过巧妙的设计，速度不会慢下来</td></tr><tr><td>数据维护</td><td>更新操作用最新的数据覆盖旧的</td><td>更新操作生成一个新的版本，久的版本仍然保留</td></tr><tr><td>可伸缩性</td><td>很难实现横向拓展，纵向拓展空间也有限</td><td>可以轻易通过在集群中增加或减少硬件数量实现性能伸缩</td></tr></tbody></table><h3 id="HBase访问接口"><a href="#HBase访问接口" class="headerlink" title="HBase访问接口"></a>HBase访问接口</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238467033431.jpg" alt=""></p><h2 id="HBase数据模型"><a href="#HBase数据模型" class="headerlink" title="HBase数据模型"></a>HBase数据模型</h2><ol><li>数据模型概述</li><li>数据模型相关概念</li><li>概念视图</li><li>物理视图</li><li>面向列的存储</li></ol><h3 id="数据模型概述"><a href="#数据模型概述" class="headerlink" title="数据模型概述"></a>数据模型概述</h3><ul><li>HBase是一个<code>稀疏</code>、<code>多维度</code>、排序的<code>映射表</code>，这张表的索引时<code>行键</code>、<code>列族</code>、<code>列限定符</code>、、<code>时间戳</code>。</li><li>每一个值是一个未经解释的字符串，没有数据类型。</li><li>每一行都有一个可排序的行键和任意多的列</li><li>表在水平方向由一个或者多个列族组成，一个列族可以包含任意多个列，同一个列族里面的数据存储在一起</li><li>列族支持动态扩展，轻松的添加列族或列，无预先定义列的数量和类型。所有列均以字符串形式存储，用户需自行进行数据类型转换。</li><li>HBase中执行更新操作时，生成新版本，保留旧版本，查询时默认返回最新版本。创建时可以设置最多保留版本数量。<h3 id="数据模型相关概念"><a href="#数据模型相关概念" class="headerlink" title="数据模型相关概念"></a>数据模型相关概念</h3></li><li>表：HBase采用表来组织数据，表由行和列组成，列换分为若干个列族，避免夺标链接操作，追求分析效率。</li><li>行：每个HBAse表由若干行组成，每个行由行键（Row Key）来标示。不给行键所在的列进行命名，让其拥有纵向可拓展性。</li><li>列族：一个HBase表被分组成许多“列族”的集合，它是基本的访问控制单元，也是基本存储单元。</li><li>列限定符：相当于列名</li><li>单元格：在HBaae表中，通过行、列族和列限定符确定一个“单元格”，（时间戳定义其版本）单元格中存储的数据没有数据类型，总被视为字节数组byte[]</li><li>HBase中需要根据行键、列族、列限定符和时间戳确定一个单元格，因此是一个“四维”坐标。<br>💡：上文中提到HBase数据只有一个索引（行键），</li></ul><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238479970872.jpg" alt=""></p><h3 id="概念视图"><a href="#概念视图" class="headerlink" title="概念视图"></a>概念视图</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238492197984.jpg" alt=""></p><h3 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238492407274.jpg" alt=""></p><h3 id="面向列的存储"><a href="#面向列的存储" class="headerlink" title="面向列的存储"></a>面向列的存储</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238493181117.jpg" alt=""></p><p>💡<strong>不同存储模型优劣对比</strong></p><ul><li><p>面向行存储的数据库主要采用NSM（N-ary Storage Model）存储模型，即一个元组（行）会被连续存储在磁盘页中，数据是一行行进行存储，读取也是一行行进行读取。当要选取某属性进行分析时，也需要首先扫面完整元组内容。</p><ul><li>优点：适用于联机事务性数据处理，即将分布于不同地理位置的数据利用网络进行连接，进而进行统一的存储和管理。</li><li>缺点：鄙视和分析性操作</li></ul></li><li><p>面向列存储的数据库主要采用DSM（Decompostion Storage Model）存储模型，该模型会对关系进行垂直分解，并为每个属性分配一个子关系，每个子关系单独存储。</p><ul><li>优点：在批处理和即兴查询等分析操作中能够直接定位目标列，能够有有效I/O开销；同一列数据类型相同，存储过程能够拥有很高的数据压缩旅，从而节省存储空间。</li><li>缺点：执行连续操作时要付出昂贵的元组重构代价。</li></ul></li></ul><p>💡 <strong>总结：NSM存储模型更加适合事务型应用，DSM存储模型更加适合分析性应用</strong></p><h2 id="HBase实现原理"><a href="#HBase实现原理" class="headerlink" title="HBase实现原理"></a>HBase实现原理</h2><h3 id="HBase功能组件"><a href="#HBase功能组件" class="headerlink" title="HBase功能组件"></a>HBase功能组件</h3><ul><li>库函数</li><li>一个Master主服务器</li><li>许多个Region服务器</li></ul><p>Master负责管理和维护Hbase表的分区信息，维护Region服务器列表，分配Region，负责均衡，和Namenode功能类似。</p><p>Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求，和Datanode功能类似。</p><p>客户端并不是直接从Master主服务器读取数据，而是在获得Region的存储位置后，直接从Region服务器上读取数据。</p><p>客户端并不依赖Master，而是通过<code>Zookeeper</code>来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计是的Master负载很小。</p><h3 id="表和Region"><a href="#表和Region" class="headerlink" title="表和Region"></a>表和Region</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238507508277.jpg" alt=""></p><ul><li>一个表包含多个Region</li><li>开始只有一个Region，后来不断分裂</li><li><p>Regin拆分操作非常快（开始只是修改文件指向），接近瞬间。因为拆分之后的Region读取的仍然是原存储文件，知道“合并”过程吧存储文件异步写到独立的文件之后，才会读取新文件。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238509042129.jpg" alt=""></p></li><li><p>每个Region默认大小是100MB到200MB（2006之前）</p><ul><li>每个Region的最佳大小取决于丹台服曲奇的有效处理能力</li><li>目前每个Region的最佳大小建议1G～2G（2013以后）</li></ul></li><li>每个Region不会被分拆到多个Region服务器（Region最小不可分）</li><li>每个Region服务器存储10～1000个Region<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238514793939.jpg" alt=""></li></ul><h3 id="Region的定位"><a href="#Region的定位" class="headerlink" title="Region的定位"></a>Region的定位</h3><ul><li>元数据表，又名<code>META</code>表，存储了<code>Region</code>和<code>Region</code>服务器的映射关系。</li><li>当HBase表很大时，<code>META</code>表也会被分裂成多个<code>Region</code></li><li>根数据表，有明<code>ROOT</code>表，记录所有元数据的具体位置</li><li><code>ROOT</code>表只有唯一一个<code>Region</code>，名字是在程序中被写死的</li><li><p><code>Zookeeper</code>文件记录了<code>ROOT</code>表的位置<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238589518486.jpg" alt=""></p></li><li><p>为了加快访问速度，<code>META</code>表的全部<code>Region</code>都会被保存在内存中</p></li><li>假设<code>META</code>表的每行（一个映射条目）在内存中大约占用1KB，每个<code>Region</code>限制为128MB，那么上面三层结构可以曹村的用户数据表的<code>Region</code>数目的计算方法是：<ul><li><code>ROOT</code>表能够寻址<code>META</code>表的<code>Region</code>个数 X 每个<code>META</code>表能寻址的个数</li></ul></li><li>一个<code>ROOT</code>表最多只能有一个<code>Region</code>大小，也就是最多只能有<code>128MB</code>，按照每行占用1KB内存计算，128MB空间可以容纳$128MB ÷ 1kb = 2^{17}$。也就是一个ROOT可以寻址$2^{17}$个.META表的Region。</li><li>同理每个<code>META</code>表的Region可以存之的用户数据表的Region个数也是$2^{17}$个</li></ul><p>客户访问数据时的“三级寻址”：</p><ul><li>为了加速寻址，客户端会缓存位置信息，同时需要解决缓存失效的问题</li><li>寻址过程客户端需要询问<code>Zookeeper</code>服务器，不需要链接<code>Master</code>服务器。</li></ul><h2 id="HBase运行机制"><a href="#HBase运行机制" class="headerlink" title="HBase运行机制"></a>HBase运行机制</h2><ol><li>HBase系统架构</li><li>Region服务器工作原理</li><li>Store工作原理</li><li>HLog工作原理</li></ol><h3 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title="HBase系统架构"></a>HBase系统架构</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238605952550.jpg" alt=""></p><ul><li>客户端<ul><li>客户端包含访问HBase的接口,同时在缓存中维护者已经好访问过的Region位置信息,用来加快后续数据访问的过程</li></ul></li><li>Zookeeper服务器<ul><li>Zookeeper可以帮助选出一个Master作为集群的总管,并保证在任何时刻总有唯一一个Master在运行,这就避免了Master“单点失效”问题.</li></ul></li></ul><p>💡<strong>Zookeeper是一个很好的集群管理工具,被大量用于分布式计算,提供配置维护、域名服务、分布式同步、组服务等</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238608443919.jpg" alt=""></p><ul><li>Master服务器: 主服务器主要负责和Region的管理工作<ul><li>管理用户对表的增删改查</li><li>实现不同Region服务器之间的负载均衡</li><li>在Region分裂或合并后,负责重新调整Region的分布</li><li>对发生故障时晓得Region服务器上的Region进行迁移</li></ul></li><li>Region服务器:<ul><li>Region服务器时HBase最核心的模块,负责维护分配给自己的Region,并响应用户的读写请求</li></ul></li></ul><h3 id="Region服务器的工作原理"><a href="#Region服务器的工作原理" class="headerlink" title="Region服务器的工作原理"></a>Region服务器的工作原理</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238616055354.jpg" alt=""></p><p><strong>1.用户读写数据过程</strong></p><ul><li>用户写入数据时，被分配到相应Region服务器去执行</li><li>用户数据首先被写入到MemStore和Hlog中</li><li>只有当操作写入Hlog之后，commit()调用才会将其返回给客户端</li><li>当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找</li></ul><p><strong>2.缓存的刷新</strong></p><ul><li>系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记</li><li><p>每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件</p></li><li><p>每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务</p></li></ul><p><strong>3.StoreFile的合并</strong></p><ul><li>每次刷写都生成一个新的StoreFile，数量太多，影响查找速度</li><li>调用Store.compact()把多个合并成一个</li><li>合并操作比较耗费资源，只有数量达到一个阈值才启动合并</li></ul><p>💡<code>这么做是为了尽量一次性刷到磁盘,以此提高速度.但是如果StoreFile数量太多影响查找速度</code></p><h3 id="Store工作原理"><a href="#Store工作原理" class="headerlink" title="Store工作原理"></a>Store工作原理</h3><ul><li>Store是Region服务器的核心</li><li>多个StoreFile合并成一个</li><li>单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238621060069.jpg" alt=""></li></ul><h3 id="HLog工作原理"><a href="#HLog工作原理" class="headerlink" title="HLog工作原理"></a>HLog工作原理</h3><ul><li>分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复</li><li>HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）</li><li>用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘</li><li>Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master</li><li>Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录</li><li>系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器</li><li>Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复</li><li>共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志</li></ul><h2 id="应用方案"><a href="#应用方案" class="headerlink" title="应用方案"></a>应用方案</h2><ol><li>HBase实际应用中的性能优化方法</li><li>HBase性能监视</li><li>在HBase之上构建SQL引擎</li><li>构建HBase二级索引</li></ol><h3 id="HBase实际应用中的性能优化方法"><a href="#HBase实际应用中的性能优化方法" class="headerlink" title="HBase实际应用中的性能优化方法"></a>HBase实际应用中的性能优化方法</h3><p><strong>行键</strong><br>行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。<br>举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。<br><strong>InMemory</strong><br>创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。<br><strong>Max Version</strong><br>创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。<br><strong>Time to Live</strong><br>创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 <em> 24 </em> 60 * 60)。</p><h3 id="HBase性能监视"><a href="#HBase性能监视" class="headerlink" title="HBase性能监视"></a>HBase性能监视</h3><ol><li>Master-status(自带)</li><li>Ganglia</li><li>OpenTSDB</li><li>Ambari</li></ol><p><strong>Master-status</strong><br>HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是<a href="http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面" target="_blank" rel="noopener">http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面</a><br>可以查看HBase集群的当前状态<br><strong>Ganglia</strong><br>Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能<br><strong>OpenTSDB</strong><br>OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等<br><strong>Ambari</strong><br>Ambari 的作用就是创建、管理、监视 Hadoop 的集群 (推荐HDP CDH)<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238656031566.jpg" alt=""></p><h3 id="在HBase之上构建SQL引擎"><a href="#在HBase之上构建SQL引擎" class="headerlink" title="在HBase之上构建SQL引擎"></a>在HBase之上构建SQL引擎</h3><p>NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因：</p><p>　　1.易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。<br>　　2.减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。　</p><p>方案：<br>1.Hive整合HBase<br>2.Phoenix</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;目录：&quot;&gt;&lt;a href=&quot;#目录：&quot; class=&quot;headerlink&quot; title=&quot;目录：&quot;&gt;&lt;/a&gt;目录：&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;概述&lt;/li&gt;
&lt;li&gt;HBase访问借口&lt;/li&gt;
&lt;li&gt;HBase数据模型&lt;/
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="hbase" scheme="http://www.ihoge.cn/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>免安装免配置 还免费的Spark 集群 --Databrickes Spark Clould</title>
    <link href="http://www.ihoge.cn/2018/Databrickes.html"/>
    <id>http://www.ihoge.cn/2018/Databrickes.html</id>
    <published>2018-04-15T19:59:21.000Z</published>
    <updated>2018-04-22T14:55:19.382Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>摘要：本文带你畅游<code>Databrickes Spark Clould</code>云服务。小白迅速上手大数据Spark开发环境，从此告别集群<code>Bug</code>的烦恼，彻底解放物理机负担让你随时随地想<code>Run</code>就<code>Run</code>😎。</p><h2 id="目录："><a href="#目录：" class="headerlink" title="目录："></a>目录：</h2><ol><li>Databrickes是个啥？</li><li>Databrickes注册</li><li>Databrickes快速入门</li><li>其他进阶神操作</li></ol><hr><h2 id="Databrickes是个啥？"><a href="#Databrickes是个啥？" class="headerlink" title="Databrickes是个啥？"></a>Databrickes是个啥？</h2><p><a href="https://databricks.com" target="_blank" rel="noopener">YouTube简介，自备云梯</a><br>Databricks 属于 Spark 的商业化公司，由美国伯克利大学 AMP 实验室著名的 Spark 大数据处理系统多位创始人联合创立。Databricks 致力于提供基于 Spark 的云服务，可用于数据集成，数据管道等任务。<br>Databricks 公司的云解决方案由三部分组成：Databricks 平台、Spark 和 Databricks 工作区。该产品背后的理念是提供处理数据的单独空间，不受托管环境和 Hadoop 集群管理的影响，整个过程在云中完成。该产品有几个核心概念：由 Notebooks 提供一种与数据交互并构建图形的方法，当用户了解了显示数据的方式时，就可以开始构建主控面板以监视某些类型的数据。最后，用户可以通过该平台的任务启动器来规划 Apache Spark 的运行时间。</p><p>说白了就是<code>Databricks</code>捞钱的，老是开源spark 也不收你钱，大家也要吃饭不是，干脆搞个 spark on cloud，服务懒人或小公司，你既不用搭建集群也不用维护，交点儿钱直接用他们提供的在线集群。当然还有一个作用，推广普及spark ，所以也就有了<code>Community</code>版，即免费版</p><p><code>Databricks</code>相当于给你了一个在线spark 集群，即：<code>Spark online</code>（听着怎么这么像网游 (⊙﹏⊙)b），我等尚处spark初级阶段的小白们刚好可以用他们提供的免费版来学习，省去了搭建和维护配置的过程，用来学习或温习Spark API真是完美！<br>（笔者比较懒，下面<code>Databricks Spark Cloud</code>都用<code>DSC</code>代替😂）<br>话不多说，开搞！！！</p><h2 id="注册Databricks"><a href="#注册Databricks" class="headerlink" title="注册Databricks"></a>注册Databricks</h2><p><a href="https://databricks.com" target="_blank" rel="noopener">官网直达</a></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqdwayuscdj31kw0pg1kx.jpg" alt=""><br><code>Do you want to TRY DATACRICKS FREE?</code><br><code>Yeah, click here!</code></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238167919311.jpg" alt=""></p><p>下面就是注册页面，不再啰嗦。注意的是左下角要进行人际身份验证，网络不好的童鞋请自备云梯😎。然后验证邮箱，点击链接到登录界面：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238177684915.jpg" alt=""></p><p>💡<code>If you don&#39;t want to pay for money, pelese click here!</code></p><h2 id="搭建云上Spark集群环境"><a href="#搭建云上Spark集群环境" class="headerlink" title="搭建云上Spark集群环境"></a>搭建云上Spark集群环境</h2><p><a href="https://docs.databricks.com/getting-started/index.html" target="_blank" rel="noopener">入门指南</a><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238199155320.jpg" alt=""></p><p>这货长这样～界面清爽颜值高，功能齐全效率高。</p><h2 id="基本使用流程："><a href="#基本使用流程：" class="headerlink" title="基本使用流程："></a>基本使用流程：</h2><ol><li>创建集群<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238206713516.jpg" alt=""></li></ol><p>没办法，笔者已经有一个集群在运行了，作为免费用户只能拥有一个活动集群，或者升级<code>高级用户</code>😎</p><p>2.创建nootebook<br>选择集群，选择语言，新建项目，凯撸～<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238208963616.jpg" alt=""><br>创建笔记、加载依赖资源、创建文件夹</p><p>3.创建Table<br>上传本地数据，速度有点慢，耐心等待一会～😅<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238209363742.jpg" alt=""></p><h2 id="任务流程可视化、数据可视化"><a href="#任务流程可视化、数据可视化" class="headerlink" title="任务流程可视化、数据可视化"></a>任务流程可视化、数据可视化</h2><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238214224410.jpg" alt=""><br>自动生成任务阶段流程，生成DAG。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238214940209.jpg" alt=""><br>丰富的画图工具，只需点点点即可完成高颜值图表。</p><h2 id="其他操作："><a href="#其他操作：" class="headerlink" title="其他操作："></a>其他操作：</h2><p>下面是重点！！<br>下面是重点！！<br>下面是重点！！</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238216319562.jpg" alt=""><br>这里提供了丰富的适合不同场景的训练数据、源代码及简介。<br>只需找到<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238217238654.jpg" alt=""><br>即可快速导入到工作台。只可惜自己英文太烂，啃的很慢😂😂😂</p><p><code>DSC</code>不仅提供了方便高可用的云集群服务，而且提供了丰富的学习资料，对像我等来说简直是完美的产品。感谢各位大佬给我们提供了如此便捷的学习环境和资源，<code>Life is short</code>，只争朝夕！<br><a href="https://youtu.be/fn3WeMZZcCk?t=17m19s" target="_blank" rel="noopener">油管视频自备云梯</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;摘要：本文带你畅游&lt;code&gt;Databrickes Spark Clould&lt;/code&gt;云服务。小白迅速上手大数据Spark开发环境，从此告别集群&lt;code&gt;Bug&lt;/code&gt;的烦恼，彻底解放物理机负担让你随时随地想&lt;code&gt;Ru
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="databrickes" scheme="http://www.ihoge.cn/tags/databrickes/"/>
    
  </entry>
  
  <entry>
    <title>弹性式分布数据集RDD——Pyspark基础 （二）</title>
    <link href="http://www.ihoge.cn/2018/pyspark.html"/>
    <id>http://www.ihoge.cn/2018/pyspark.html</id>
    <published>2018-04-15T09:59:21.000Z</published>
    <updated>2018-04-22T14:55:20.485Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RDD的内部运行方式"><a href="#RDD的内部运行方式" class="headerlink" title="RDD的内部运行方式"></a>RDD的内部运行方式</h2><p>RDD不仅是一组不可变的JVM（Java虚拟机）对象的分布集，而且是Spark的核心，可以让任务执行高速运算。</p><p>RDD将<code>跟踪（计入日记）应用于每个快的所有转换</code>，以加速计算速度，并在发生错误和部分数据丢失时提供回退（容错机制）。</p><p>RDD采用并行的运行方式，也就是每个转换操作并行执行，从而提高速度。<br>RDD有两种并行操作：</p><ul><li>转换操作（返回指向新的RDD的指针）</li><li>动作操作（在运行计算后向驱动程序返回值）</li></ul><p>数据集的转换通常是<code>惰性</code>的，这也意味着任何转换操作仅在调用数据集上的操作时才执行。该延迟执行会产生风多的精细查询：针对性能进行优化查询。这种优化始于Spark的DAGScheduler——面向阶段的调度器。DAGScheduler负责Stage级的调度详见:<a href="http://ihoge.cn/2018/Spark%20Scheduler.html" target="_blank" rel="noopener">Spark运行原理剖析</a></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqdbpax6jbj31kw0hntga.jpg" alt=""></p><p><strong>由于具有单独的RDD转换和动作，DAGScheduler可以在查询中执行优化。包括但不限于避免shuffle数据（最耗费资源的任务）</strong></p><h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><p>方式一： 用<code>.parallelize(...)</code>集合（元素list或array）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = sc.parallelize([(<span class="string">'a'</span>,<span class="number">1</span>),(<span class="string">'b'</span>,<span class="number">2</span>),(<span class="string">'c'</span>,<span class="number">3</span>),(<span class="string">'d'</span>,<span class="number">5</span>),(<span class="string">'e'</span>,<span class="number">5</span>)])</span><br></pre></td></tr></table></figure></p><p>方式二： 读入外部文件</p><ul><li>支持多文件系统中读取：如NTFS、FAT、HFS+（Mac OS Extended），或者如HDFS、S3、Cassandra这类的分布式文件系统，还有其他类文件系统。</li><li>指出多种数据格式：如文本、parquet、JSON、Hive tables（Hive表）以及使用JDBC驱动程序可读取的关系数据库中的数据。（注意：Spark可以自动处理压缩数据集）</li></ul><p>💡<strong>Tip1：</strong>读取的方式不同，持有对象表达方式也不同。从文件中读取的数据表示为<code>MapPartitionsRDD</code>；使用集合方法的数据表示为<code>ParallelCollectionRDD</code></p><p>💡<strong>Tip2：</strong>RDD是无schema的数据结构（和DataFrame不同），所以我们几乎可以混用任何数据结构：tuple、dict、list和spark等都能支持。如果对数据集使用<code>.collect()</code>方法，将把RDD对所有元素返回给驱动程序，驱动程序将其序列化成了一个列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_from_file = sc.textFile(<span class="string">"hdfs://master:9000/pydata/VS14MORT.txt.gz"</span>,<span class="number">4</span>) <span class="comment"># 这里表示4个分区</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractInformation</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">    selected_indices = [</span><br><span class="line">         <span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,</span><br><span class="line">         <span class="number">19</span>,<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>,<span class="number">25</span>,<span class="number">27</span>,<span class="number">28</span>,<span class="number">29</span>,<span class="number">30</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>,</span><br><span class="line">         <span class="number">36</span>,<span class="number">37</span>,<span class="number">38</span>,<span class="number">39</span>,<span class="number">40</span>,<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>,<span class="number">45</span>,<span class="number">46</span>,<span class="number">47</span>,<span class="number">48</span>,</span><br><span class="line">         <span class="number">49</span>,<span class="number">50</span>,<span class="number">51</span>,<span class="number">52</span>,<span class="number">53</span>,<span class="number">54</span>,<span class="number">55</span>,<span class="number">56</span>,<span class="number">58</span>,<span class="number">60</span>,<span class="number">61</span>,<span class="number">62</span>,<span class="number">63</span>,</span><br><span class="line">         <span class="number">64</span>,<span class="number">65</span>,<span class="number">66</span>,<span class="number">67</span>,<span class="number">68</span>,<span class="number">69</span>,<span class="number">70</span>,<span class="number">71</span>,<span class="number">72</span>,<span class="number">73</span>,<span class="number">74</span>,<span class="number">75</span>,<span class="number">76</span>,</span><br><span class="line">         <span class="number">77</span>,<span class="number">78</span>,<span class="number">79</span>,<span class="number">81</span>,<span class="number">82</span>,<span class="number">83</span>,<span class="number">84</span>,<span class="number">85</span>,<span class="number">87</span>,<span class="number">89</span></span><br><span class="line">    ]</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Input record schema</span></span><br><span class="line"><span class="string">        schema: n-m (o) -- xxx</span></span><br><span class="line"><span class="string">            n - position from</span></span><br><span class="line"><span class="string">            m - position to</span></span><br><span class="line"><span class="string">            o - number of characters</span></span><br><span class="line"><span class="string">            xxx - description</span></span><br><span class="line"><span class="string">        1. 1-19 (19) -- reserved positions</span></span><br><span class="line"><span class="string">        2. 20 (1) -- resident status</span></span><br><span class="line"><span class="string">        3. 21-60 (40) -- reserved positions</span></span><br><span class="line"><span class="string">        4. 61-62 (2) -- education code (1989 revision)</span></span><br><span class="line"><span class="string">        5. 63 (1) -- education code (2003 revision)</span></span><br><span class="line"><span class="string">        6. 64 (1) -- education reporting flag</span></span><br><span class="line"><span class="string">        7. 65-66 (2) -- month of death</span></span><br><span class="line"><span class="string">        8. 67-68 (2) -- reserved positions</span></span><br><span class="line"><span class="string">        9. 69 (1) -- sex</span></span><br><span class="line"><span class="string">        10. 70 (1) -- age: 1-years, 2-months, 4-days, 5-hours, 6-minutes, 9-not stated</span></span><br><span class="line"><span class="string">        11. 71-73 (3) -- number of units (years, months etc)</span></span><br><span class="line"><span class="string">        12. 74 (1) -- age substitution flag (if the age reported in positions 70-74 is calculated using dates of birth and death)</span></span><br><span class="line"><span class="string">        13. 75-76 (2) -- age recoded into 52 categories</span></span><br><span class="line"><span class="string">        14. 77-78 (2) -- age recoded into 27 categories</span></span><br><span class="line"><span class="string">        15. 79-80 (2) -- age recoded into 12 categories</span></span><br><span class="line"><span class="string">        16. 81-82 (2) -- infant age recoded into 22 categories</span></span><br><span class="line"><span class="string">        17. 83 (1) -- place of death</span></span><br><span class="line"><span class="string">        18. 84 (1) -- marital status</span></span><br><span class="line"><span class="string">        19. 85 (1) -- day of the week of death</span></span><br><span class="line"><span class="string">        20. 86-101 (16) -- reserved positions</span></span><br><span class="line"><span class="string">        21. 102-105 (4) -- current year</span></span><br><span class="line"><span class="string">        22. 106 (1) -- injury at work</span></span><br><span class="line"><span class="string">        23. 107 (1) -- manner of death</span></span><br><span class="line"><span class="string">        24. 108 (1) -- manner of disposition</span></span><br><span class="line"><span class="string">        25. 109 (1) -- autopsy</span></span><br><span class="line"><span class="string">        26. 110-143 (34) -- reserved positions</span></span><br><span class="line"><span class="string">        27. 144 (1) -- activity code</span></span><br><span class="line"><span class="string">        28. 145 (1) -- place of injury</span></span><br><span class="line"><span class="string">        29. 146-149 (4) -- ICD code</span></span><br><span class="line"><span class="string">        30. 150-152 (3) -- 358 cause recode</span></span><br><span class="line"><span class="string">        31. 153 (1) -- reserved position</span></span><br><span class="line"><span class="string">        32. 154-156 (3) -- 113 cause recode</span></span><br><span class="line"><span class="string">        33. 157-159 (3) -- 130 infant cause recode</span></span><br><span class="line"><span class="string">        34. 160-161 (2) -- 39 cause recode</span></span><br><span class="line"><span class="string">        35. 162 (1) -- reserved position</span></span><br><span class="line"><span class="string">        36. 163-164 (2) -- number of entity-axis conditions</span></span><br><span class="line"><span class="string">        37-56. 165-304 (140) -- list of up to 20 conditions</span></span><br><span class="line"><span class="string">        57. 305-340 (36) -- reserved positions</span></span><br><span class="line"><span class="string">        58. 341-342 (2) -- number of record axis conditions</span></span><br><span class="line"><span class="string">        59. 343 (1) -- reserved position</span></span><br><span class="line"><span class="string">        60-79. 344-443 (100) -- record axis conditions</span></span><br><span class="line"><span class="string">        80. 444 (1) -- reserve position</span></span><br><span class="line"><span class="string">        81. 445-446 (2) -- race</span></span><br><span class="line"><span class="string">        82. 447 (1) -- bridged race flag</span></span><br><span class="line"><span class="string">        83. 448 (1) -- race imputation flag</span></span><br><span class="line"><span class="string">        84. 449 (1) -- race recode (3 categories)</span></span><br><span class="line"><span class="string">        85. 450 (1) -- race recode (5 categories)</span></span><br><span class="line"><span class="string">        86. 461-483 (33) -- reserved positions</span></span><br><span class="line"><span class="string">        87. 484-486 (3) -- Hispanic origin</span></span><br><span class="line"><span class="string">        88. 487 (1) -- reserved</span></span><br><span class="line"><span class="string">        89. 488 (1) -- Hispanic origin/race recode</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">    record_split = re\</span><br><span class="line">        .compile(</span><br><span class="line">            <span class="string">r'([\s]&#123;19&#125;)([0-9]&#123;1&#125;)([\s]&#123;40&#125;)([0-9\s]&#123;2&#125;)([0-9\s]&#123;1&#125;)([0-9]&#123;1&#125;)([0-9]&#123;2&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([\s]&#123;2&#125;)([FM]&#123;1&#125;)([0-9]&#123;1&#125;)([0-9]&#123;3&#125;)([0-9\s]&#123;1&#125;)([0-9]&#123;2&#125;)([0-9]&#123;2&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([0-9]&#123;2&#125;)([0-9\s]&#123;2&#125;)([0-9]&#123;1&#125;)([SMWDU]&#123;1&#125;)([0-9]&#123;1&#125;)([\s]&#123;16&#125;)([0-9]&#123;4&#125;)'</span> +</span><br><span class="line">            <span class="string">r'([YNU]&#123;1&#125;)([0-9\s]&#123;1&#125;)([BCOU]&#123;1&#125;)([YNU]&#123;1&#125;)([\s]&#123;34&#125;)([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)'</span> +</span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;4&#125;)([0-9]&#123;3&#125;)([\s]&#123;1&#125;)([0-9\s]&#123;3&#125;)([0-9\s]&#123;3&#125;)([0-9\s]&#123;2&#125;)([\s]&#123;1&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([0-9\s]&#123;2&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([\s]&#123;36&#125;)([A-Z0-9\s]&#123;2&#125;)([\s]&#123;1&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([\s]&#123;1&#125;)([0-9\s]&#123;2&#125;)([0-9\s]&#123;1&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)([\s]&#123;33&#125;)([0-9\s]&#123;3&#125;)([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        rs = np.array(record_split.split(row))[selected_indices]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        rs = np.array([<span class="string">'-99'</span>] * len(selected_indices))</span><br><span class="line">    <span class="keyword">return</span> rs</span><br><span class="line"></span><br><span class="line">data_file = data_from_file.map(extractInformation)</span><br><span class="line">data_file.map(<span class="keyword">lambda</span> row: row).take(<span class="number">1</span>)</span><br><span class="line">data_file.cache()</span><br><span class="line">data_file.is_cached</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><h2 id="全局作用域和局部作用域"><a href="#全局作用域和局部作用域" class="headerlink" title="全局作用域和局部作用域"></a>全局作用域和局部作用域</h2><p>Spark可以在两种模式下运行：本地和集群。本地运行Spark代码时和目前使用的python没有说明不同。然而他如果将相同的代码部署到集群，便可能会导致大量的困扰，这就需要了解Spark是怎么在集群上执行工作的。这里有一篇文章介绍的很详细。<a href="http://ihoge.cn/2018/Spark%20Scheduler.html" target="_blank" rel="noopener">参考：Spark运行原理详解</a></p><p>在集群模式下，提交任务时任务发送给了Master节点。该驱动程序节点为任务创建DAG，并且决定哪一个执行者（Worker）节点运行特定的任务。然后该驱动程序知识工作者执行它们的任务，并且在结束时将结果返回给驱动程序。然而在这之前，驱动程序为每一个任务的终止做准备：驱动程序中有一组变量和方法，以变工作者在RDD上执行任务。</p><p>这组变量和方法在执行者的上下问本质上是静态的，每个执行器从驱动程序中获取的一份变量和方法的<strong>副本</strong>。这意味着运行任务时，如果执行者改变这些变量或覆盖这些方法，它不影响任何其他执行者的副本或者驱动程序的变量和方法。这可能会导致一些意想不到的行为和运行错误，这些行为和错误通常都很难被追踪到。</p><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>转换操作可以调整数据集。包括映射、筛选、链接、转换数据集中的值。</p><h3 id="map-转换"><a href="#map-转换" class="headerlink" title=".map()转换"></a>.map()转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_2014 = data_file.map(<span class="keyword">lambda</span> x: x[<span class="number">16</span>])</span><br><span class="line">data_2014.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>[&apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;-99&apos;]</code></pre><h3 id="filter-转换"><a href="#filter-转换" class="headerlink" title=".filter()转换"></a>.filter()转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_filter = data_file.filter(<span class="keyword">lambda</span> x: x[<span class="number">16</span>] == <span class="string">'2014'</span> <span class="keyword">and</span> x[<span class="number">21</span>] == <span class="string">'0'</span>)</span><br><span class="line">print(data_filter.count())</span><br><span class="line">data_file.take(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>22[array([&apos;1&apos;, &apos;  &apos;, &apos;2&apos;, &apos;1&apos;, &apos;01&apos;, &apos;M&apos;, &apos;1&apos;, &apos;087&apos;, &apos; &apos;, &apos;43&apos;, &apos;23&apos;, &apos;11&apos;,        &apos;  &apos;, &apos;4&apos;, &apos;M&apos;, &apos;4&apos;, &apos;2014&apos;, &apos;U&apos;, &apos;7&apos;, &apos;C&apos;, &apos;N&apos;, &apos; &apos;, &apos; &apos;, &apos;I64 &apos;,        &apos;238&apos;, &apos;070&apos;, &apos;   &apos;, &apos;24&apos;, &apos;01&apos;, &apos;11I64  &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;01&apos;,        &apos;I64  &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;01&apos;, &apos; &apos;,        &apos; &apos;, &apos;1&apos;, &apos;1&apos;, &apos;100&apos;, &apos;6&apos;], dtype=&apos;&lt;U40&apos;), array([&apos;1&apos;, &apos;  &apos;, &apos;2&apos;, &apos;1&apos;, &apos;01&apos;, &apos;M&apos;, &apos;1&apos;, &apos;058&apos;, &apos; &apos;, &apos;37&apos;, &apos;17&apos;, &apos;08&apos;,        &apos;  &apos;, &apos;4&apos;, &apos;D&apos;, &apos;3&apos;, &apos;2014&apos;, &apos;U&apos;, &apos;7&apos;, &apos;C&apos;, &apos;N&apos;, &apos; &apos;, &apos; &apos;, &apos;I250&apos;,        &apos;214&apos;, &apos;062&apos;, &apos;   &apos;, &apos;21&apos;, &apos;03&apos;, &apos;11I250 &apos;, &apos;61I272 &apos;, &apos;62E669 &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;03&apos;,        &apos;I250 &apos;, &apos;E669 &apos;, &apos;I272 &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;01&apos;, &apos; &apos;,        &apos; &apos;, &apos;1&apos;, &apos;1&apos;, &apos;100&apos;, &apos;6&apos;], dtype=&apos;&lt;U40&apos;)]</code></pre><h3 id="flatMap-转换"><a href="#flatMap-转换" class="headerlink" title=".flatMap()转换"></a>.flatMap()转换</h3><p><code>.flatMap()</code>方法和.map()工作类似，不同的是flatMap()返回一个扁平的结果而不是一个列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_flat = data_file.flatMap(<span class="keyword">lambda</span> x: (x[<span class="number">16</span>], int(x[<span class="number">16</span>])+<span class="number">1</span>))</span><br><span class="line">data_flat.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>[&apos;2014&apos;, 2015, &apos;2014&apos;, 2015, &apos;2014&apos;, 2015, &apos;2014&apos;, 2015, &apos;2014&apos;, 2015]</code></pre><p>.flatMap()可以用于过滤一些格式不正确的记录。在这个机制下，.flatMap()方法吧每一行看作一个列表对待，然后将所有记录简单的加入到一起，通过传递一个空列表可以丢弃格式不正确的记录。</p><h3 id="distinct-转换"><a href="#distinct-转换" class="headerlink" title=".distinct()转换"></a>.distinct()转换</h3><p>这里用该方法检查性别列表是否只包含了男性和女性验证我们是否准确解释了数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">distinct_gender = data_file.map(<span class="keyword">lambda</span> x: x[<span class="number">5</span>]).distinct()</span><br><span class="line">distinct_gender.collect()</span><br></pre></td></tr></table></figure><pre><code>[&apos;M&apos;, &apos;F&apos;, &apos;-99&apos;]</code></pre><h3 id="sample-转换"><a href="#sample-转换" class="headerlink" title=".sample() 转换"></a>.sample() 转换</h3><p>该方法返回数据集的随机样本。第一个参数<code>withReplacement</code>指定采样是否应该替换，第二个参数<code>fraction</code>定义返回数据量的百分比，第三个参数是伪随机数产生器的种子<code>seed</code>。</p><p>为了节省运算时间，这里选取愿数据千分之一的随机数据作为下面的练习数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_sample = data_file.sample(<span class="keyword">False</span>, <span class="number">0.001</span>, <span class="number">666</span>)</span><br><span class="line">data_sample.cache()</span><br></pre></td></tr></table></figure><pre><code>PythonRDD[25] at RDD at PythonRDD.scala:48</code></pre><h3 id="leftOuterJoin-转换"><a href="#leftOuterJoin-转换" class="headerlink" title=".leftOuterJoin()转换"></a>.leftOuterJoin()转换</h3><ul><li>.leftOuterJoin()： 根据两个数据集中都有得值来连接两个RDD，并返回<code>左侧</code>的RDD记录，而右边的记录副加载两个RDD匹配的地方。</li><li>.join() ：只返回两个RDD之间的关联数值</li><li>.intersection()：返回两个RDD中相等的记录</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">'a'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">4</span>), (<span class="string">'c'</span>,<span class="number">10</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>,<span class="number">4</span>), (<span class="string">'a'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">6</span>), (<span class="string">'d'</span>,<span class="number">15</span>)])</span><br><span class="line">print(<span class="string">"leftOuterJoin: "</span>,rdd1.leftOuterJoin(rdd2).collect())</span><br><span class="line">print(<span class="string">"Join: "</span>,rdd1.join(rdd2).collect())</span><br><span class="line">print(<span class="string">"intersection: "</span>, rdd1.intersection(rdd2).collect())</span><br></pre></td></tr></table></figure><pre><code>leftOuterJoin:  [(&apos;c&apos;, (10, None)), (&apos;b&apos;, (4, 6)), (&apos;a&apos;, (1, 1)), (&apos;a&apos;, (1, 4))]Join:  [(&apos;b&apos;, (4, 6)), (&apos;a&apos;, (1, 1)), (&apos;a&apos;, (1, 4))]intersection:  [(&apos;a&apos;, 1)]</code></pre><h3 id="repartition-转换"><a href="#repartition-转换" class="headerlink" title=".repartition()转换"></a>.repartition()转换</h3><p>重新对数据集进行分区，改变数据集分赛区的数量。此功能应该谨慎并且仅当真正需要的时候使用，因为它会充足数据，导致性能产生巨大的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(len(rdd2.glom().collect()))</span><br><span class="line">rdd2 = rdd2.repartition(<span class="number">4</span>)</span><br><span class="line">print(len(rdd2.glom().collect()))</span><br></pre></td></tr></table></figure><pre><code>34</code></pre><h2 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h2><h3 id="collect-动作"><a href="#collect-动作" class="headerlink" title=".collect() 动作"></a>.collect() 动作</h3><p>返回所有RDD的元素给驱动程序</p><p>💡同时常用的还有: <code>.collectAsMap()</code>方法</p><h3 id="take-动作"><a href="#take-动作" class="headerlink" title=".take() 动作"></a>.take() 动作</h3><p>可以说这事最有用的方法，返回<strong>单个数据分区</strong>的前n行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.take(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#等同于:</span></span><br><span class="line">rdd.first()</span><br></pre></td></tr></table></figure></p><h3 id="reduce-动作"><a href="#reduce-动作" class="headerlink" title=".reduce() 动作"></a>.reduce() 动作</h3><p>该方法使用指定的方法减少RDD中的元素。可以用该方法计算RDD总的元素之和：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd1.map(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br></pre></td></tr></table></figure></p><p>在每一个分区里，reduce()方法运行求和方法，将改总和返回给最终聚合所在的程序节点。</p><p>⚠️<strong>警告：</strong><br>要谨慎注意的是，reduce传递的函数需要时关联的，既满足元素顺序改变结果不变，操作符顺序改变结果不变。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([1, 2, 0.5, 0.1],1)</span><br><span class="line">rdd.reduce(lambda x, y: x / y)</span><br><span class="line"></span><br><span class="line">out: 10.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([1, 2, 0.5, 0.1],2)</span><br><span class="line">rdd.reduce(lambda x, y: x / y)</span><br><span class="line"></span><br><span class="line">out: 0.1</span><br></pre></td></tr></table></figure><p>这里我们希望输出结果是10.0，第一个只把RDD放在一个分区，输出结果符合预期。但是在第二个例子中，分了2个区，结果就不对了。因为该方法是在每个分区并行计算的。</p><h3 id="reduceByKey-动作"><a href="#reduceByKey-动作" class="headerlink" title=".reduceByKey()  动作"></a>.reduceByKey()  动作</h3><p>该方法和.reduce()方法类似，但是实在key-key基础上运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_key = sc.parallelize([(<span class="string">'a'</span>,<span class="number">3</span>), (<span class="string">'a'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">6</span>), (<span class="string">'d'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">6</span>), (<span class="string">'d'</span>,<span class="number">15</span>), (<span class="string">'d'</span>,<span class="number">3</span>), (<span class="string">'a'</span>,<span class="number">7</span>), (<span class="string">'b'</span>, <span class="number">8</span>)],<span class="number">4</span>)</span><br><span class="line">data_key.reduceByKey(<span class="keyword">lambda</span> x, y: x+y).collect()</span><br></pre></td></tr></table></figure><pre><code>[(&apos;b&apos;, 20), (&apos;a&apos;, 11), (&apos;d&apos;, 19)]</code></pre><h3 id="count-动作"><a href="#count-动作" class="headerlink" title=".count()  动作"></a>.count()  动作</h3><p>.count() 方法统计出了RDD里所有的元素数量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.count()</span><br></pre></td></tr></table></figure></p><p>.count() 方法产生入戏方法同样的结果，但不需要把整个数据集移动到驱动程序：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(rdd.collect()). # ⚠️警告：不要这样做！！</span><br></pre></td></tr></table></figure></p><h3 id="countByKey-动作"><a href="#countByKey-动作" class="headerlink" title=".countByKey() 动作"></a>.countByKey() 动作</h3><p>如果数据集是Ket-Value形式，可以使用.countByKey()方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_key.countByKey().items()</span><br></pre></td></tr></table></figure><pre><code>dict_items([(&apos;a&apos;, 3), (&apos;b&apos;, 3), (&apos;d&apos;, 3)])</code></pre><h3 id="saveAsTextFile-动作"><a href="#saveAsTextFile-动作" class="headerlink" title=".saveAsTextFile()  动作"></a>.saveAsTextFile()  动作</h3><p>该方法将RDD保存为文本文件：每个文件一个分区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_key.saveAsTextFile(<span class="string">'hdfs://master:9000/out/data_key.txt'</span>)</span><br></pre></td></tr></table></figure><p>要读取它的时候需要解析，因为所有行都被视为字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseInput</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    pattern = re.compile(<span class="string">r"\(\'([a-z]+)\',.([0-9]+)\)"</span>) <span class="comment"># 这里“+”号代表匹配一个或多个匹配字符，否则针对双位数动作操作会报错</span></span><br><span class="line">    row_split = pattern.split(row)</span><br><span class="line">    <span class="keyword">return</span> (row_split[<span class="number">1</span>], row_split[<span class="number">2</span>])</span><br><span class="line">data_key_read = sc.textFile(<span class="string">'hdfs://master:9000/out/data_key.txt'</span>)</span><br><span class="line">data_key_read.map(parseInput).collect()</span><br></pre></td></tr></table></figure><pre><code>[(&apos;a&apos;, &apos;3&apos;), (&apos;a&apos;, &apos;1&apos;), (&apos;b&apos;, &apos;6&apos;), (&apos;d&apos;, &apos;1&apos;), (&apos;b&apos;, &apos;6&apos;), (&apos;d&apos;, &apos;15&apos;), (&apos;d&apos;, &apos;3&apos;), (&apos;a&apos;, &apos;7&apos;), (&apos;b&apos;, &apos;8&apos;)]</code></pre><p>💡同时还有：</p><ul><li>rdd.saveAsHadoopDataset</li><li>rdd.saveAsSequenceFile</li><li>…<br>等方法</li></ul><h3 id="foreach-动作"><a href="#foreach-动作" class="headerlink" title=".foreach()  动作"></a>.foreach()  动作</h3><p>这个方法对RDD里的每个元素，用迭代方法应用相同的函数；和<code>.map()</code>相比，<code>.foreach()</code>方法按照一个接一个的方式，对每一条记录应用一个定义好的函数。当希望将数据曹村道PySpark本身不支持的数据库是，该方法很有用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line">rdd.foreach(f)</span><br></pre></td></tr></table></figure></p><h2 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h2><ul><li><code>RDD是Spark的核心</code>；这些无<code>schema</code>数据结构早Spark中处理的最基本的数据结构。</li><li>RDD的两种创建方式： parallelize 和 文件读取</li><li>Spark中的转化是惰性的，只在操作被调用时应用。</li><li>Scala 和 Python RDD之间一个主要的区别是速度： Python RDD 比 Scala 慢很多！</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;RDD的内部运行方式&quot;&gt;&lt;a href=&quot;#RDD的内部运行方式&quot; class=&quot;headerlink&quot; title=&quot;RDD的内部运行方式&quot;&gt;&lt;/a&gt;RDD的内部运行方式&lt;/h2&gt;&lt;p&gt;RDD不仅是一组不可变的JVM（Java虚拟机）对象的分布集，而且是Spar
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Spark运行原理剖析</title>
    <link href="http://www.ihoge.cn/2018/Spark%20Scheduler.html"/>
    <id>http://www.ihoge.cn/2018/Spark Scheduler.html</id>
    <published>2018-04-15T09:59:21.000Z</published>
    <updated>2018-05-09T12:38:03.223Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>Apache Spark是一个开源的，强大的分布式查询和处理引擎。它提供MapReduce的灵活性和可扩展性，但速度明显更高。<br><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqd8rm8qdtj30er07a74i.jpg" alt=""></p><p>Spark的核心是根据RDD来实现的，Spark Scheduler则为Spark核心实现的重要一环，其作用就是任务调度。Spark的任务调度就是如何组织任务去处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG，基于DAG划分Stage，将每个Stage中的任务发到指定节点运行。基于Spark的任务调度原理，我们可以合理规划资源利用，做到尽可能用最少的资源高效地完成任务计算。</p><h2 id="分布式运行框架"><a href="#分布式运行框架" class="headerlink" title="分布式运行框架"></a>分布式运行框架</h2><p>Spark可以部署在多种资源管理平台，例如Yarn、Mesos等，Spark本身也实现了一个简易的资源管理机制，称之为Standalone模式。以下所述均表示Spark-on-Yarn。Spark部署在Yarn上有两种运行模式，分别为yarn-client和yarn-cluster模式，它们的区别仅仅在于Spark Driver是运行在Client端还是ApplicationMater端。如下图所示为Spark部署在Yarn上，以yarn-cluster模式运行的分布式计算框架。<br><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqd9qs2x4ej31920n4n11.jpg" alt=""></p><p>其中蓝色部分是Spark里的概念，包括Client、ApplicationMaster、Driver和Executor，其中Client和ApplicationMaster主要是负责与Yarn进行交互；Driver作为Spark应用程序的总控，负责分发任务以及监控任务运行状态；Executor负责执行任务，并上报状态信息给Driver，从逻辑上来看Executor是进程，运行在其中的任务是线程，所以说Spark的任务是线程级别的。通过下面的时序图可以更清晰地理解一个Spark应用程序从提交到运行的完整流程。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqd9s947htj31am11mafh.jpg" alt=""></p><p><strong>Client</strong><br>提交一个Spark应用程序，首先通过Client向ResourceManager请求启动一个Application，同时检查是否有足够的资源满足Application的需求，如果资源条件满足，则准备ApplicationMaster的启动上下文，交给ResourceManager，并循环监控Application状态。</p><p><strong>ResourceManager</strong><br>当提交的资源队列中有资源时，ResourceManager会在某个NodeManager上启动ApplicationMaster进程，ApplicationMaster会单独启动Driver后台线程，当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。</p><p><strong>Driver</strong><br>Driver线程主要是初始化SparkContext对象，准备运行所需的上下文，然后一方面保持与ApplicationMaster的RPC连接，通过ApplicationMaster申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲Executor上。</p><p><strong>Executor</strong><br>当ResourceManager向ApplicationMaster返回Container资源时，ApplicationMaster就尝试在对应的Container上启动Executor进程，Executor进程起来后，会向Driver注册，注册成功后保持与Driver的心跳，同时等待Driver分发任务，当分发的任务执行完毕后，将任务状态上报给Driver。</p><blockquote><p>Driver把资源申请的逻辑给抽象出来，以适配不同的资源管理系统，所以才间接地通过ApplicationMaster去和Yarn打交道。</p></blockquote><p>从上述时序图可知，<strong>Client只管提交Application并监控Application的状态</strong>。对于Spark的任务调度主要是集中在两个方面: <code>资源申请</code>和<code>任务分发</code>，其主要是通过<code>ApplicationMaster</code>、<code>Driver</code>以及<code>Executor</code>之间来完成，下面详细剖析Spark任务调度每个细节。</p><h2 id="Spark任务调度总览"><a href="#Spark任务调度总览" class="headerlink" title="Spark任务调度总览"></a>Spark任务调度总览</h2><p>当Driver起来后，Driver则会根据用户程序逻辑准备任务，并根据Executor资源情况逐步分发任务。在详细阐述任务调度前，首先说明下Spark里的几个概念。一个Spark应用程序包括Job、Stage以及Task三个概念：</p><ul><li>Job是以Action方法为界，遇到一个Action方法则触发一个Job；</li><li>Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次划分；</li><li>Task是Stage的子集，以并行度(分区数)来衡量，分区数是多少，则有多少个task。</li></ul><p>Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度，总体调度流程如下图所示。<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqda2pc5qcj31dw12sn79.jpg" alt=""></p><p>Spark RDD通过其Transactions操作，形成了RDD血缘关系图，即DAG，最后通过Action的调用，触发Job并调度执行。DAGScheduler负责Stage级的调度，主要是将DAG切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度。TaskScheduler负责Task级的调度，将DAGScheduler给过来的TaskSet按照指定的调度策略分发到Executor上执行，调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多种实现，分别对接不同的资源管理系统。有了上述感性的认识后，下面这张图描述了Spark-On-Yarn模式下在任务调度期间，ApplicationMaster、Driver以及Executor内部模块的交互过程。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258693615222.jpg" alt=""><br>Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并启动SchedulerBackend以及HeartbeatReceiver。SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。下面着重剖析DAGScheduler负责的Stage调度以及TaskScheduler负责的Task调度。</p><h3 id="Stage级的调度"><a href="#Stage级的调度" class="headerlink" title="Stage级的调度"></a>Stage级的调度</h3><p>Spark的任务调度是从DAG切割开始，主要是由DAGScheduler来完成。当遇到一个Action操作后就会触发一个Job的计算，并交给DAGScheduler来提交，下图是涉及到Job提交的相关方法调用流程图。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqda8al64mj31kw0hdq7n.jpg" alt=""></p><p>Job由最终的RDD和Action方法封装而成，SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，具体划分策略是，由最终的RDD不断通过依赖回溯判断父依赖是否是款依赖，即以Shuffle为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算，如上图紫色流程部分。划分的Stages分两类，一类叫做ResultStage，为DAG最下游的Stage，由Action方法决定，另一类叫做ShuffleMapStage，为下游Stage准备数据，下面看一个简单的例子WordCount。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqda8um68bj31cw0huahy.jpg" alt=""></p><p>Job由saveAsTextFile触发，该Job由RDD-3和saveAsTextFile方法组成，根据RDD之间的依赖关系从RDD-3开始回溯搜索，直到没有依赖的RDD-0，在回溯搜索过程中，RDD-3依赖RDD-2，并且是宽依赖，所以在RDD-2和RDD-3之间划分Stage，RDD-3被划到最后一个Stage，即ResultStage中，RDD-2依赖RDD-1，RDD-1依赖RDD-0，这些依赖都是窄依赖，所以将RDD-0、RDD-1和RDD-2划分到同一个Stage，即ShuffleMapStage中，实际执行的时候，数据记录会一气呵成地执行RDD-0到RDD-2的转化。不难看出，其本质上是一个深度优先搜索算法。</p><p>一个Stage是否被提交，需要判断它的父Stage是否执行，只有在父Stage执行完毕才能提交当前Stage，如果一个Stage没有父Stage，那么从该Stage开始提交。Stage提交时会将Task信息（分区信息以及方法等）序列化并被打包成TaskSet交给TaskScheduler，一个Partition对应一个Task，另一方面监控Stage的运行状态，只有Executor丢失或者Task由于Fetch失败才需要重新提交失败的Stage以调度运行失败的任务，其他类型的Task失败会在TaskScheduler的调度过程中重试。</p><p>相对来说DAGScheduler做的事情较为简单，仅仅是在Stage层面上划分DAG，提交Stage并监控相关状态信息。TaskScheduler则相对较为复杂，下面详细阐述其细节。</p><h3 id="Task级的调度"><a href="#Task级的调度" class="headerlink" title="Task级的调度"></a>Task级的调度</h3><p>Spark Task的调度是由TaskScheduler来完成，由前文可知，DAGScheduler将Stage打包到TaskSet交给TaskScheduler，TaskScheduler会将其封装为TaskSetManager加入到调度队列中，TaskSetManager负责监控管理同一个Stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。前面也提到，TaskScheduler初始化后会启动SchedulerBackend，它负责跟外界打交道，接收Executor的注册信息，并维护Executor的状态，所以说SchedulerBackend是管“粮食”的，同时它在启动后会定期地去“询问”TaskScheduler有没有任务要运行，也就是说，它会定期地“问”TaskScheduler“我有这么余量，你要不要啊”，TaskScheduler在SchedulerBackend“问”它的时候，会从调度队列中按照指定的调度策略选择TaskSetManager去调度运行，大致方法调用流程如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqda9kfl3dj31kw0lqq8i.jpg" alt=""></p><h4 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h4><p>前面讲到，TaskScheduler会先把DAGScheduler给过来的TaskSet封装成TaskSetManager扔到任务队列里，然后再从任务队列里按照一定的规则把它们取出来在SchedulerBackend给过来的Executor上运行。这个调度过程实际上还是比较粗粒度的，是面向TaskSetManager的。</p><p>TaskScheduler是以树的方式来管理任务队列，树中的节点类型为Schdulable，叶子节点为TaskSetManager，非叶子节点为Pool，下图是它们之间的继承关系。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdaa1sdk7j313o0qc0uz.jpg" alt=""></p><p>TaskScheduler支持两种调度策略，一种是FIFO，也是默认的调度策略，另一种是FAIR。在TaskScheduler初始化过程中会实例化rootPool，表示树的根节点，是Pool类型。如果是采用FIFO调度策略，则直接简单地将TaskSetManager按照先来先到的方式入队，出队时直接拿出最先进队的TaskSetManager，其树结构大致如下图所示，TaskSetManager保存在一个FIFO队列中。<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqdaa8cgqwj316u0j2q6a.jpg" alt=""></p><p>在阐述FAIR调度策略前，先贴一段使用FAIR调度策略的应用程序代码，后面针对该代码逻辑来详细阐述FAIR调度的实现细节。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultiJobTest</span> </span>&#123;</span><br><span class="line">  <span class="comment">// spark.scheduler.mode=FAIR</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(...)</span><br><span class="line">      .map(_.split(<span class="string">"\\s+"</span>))</span><br><span class="line">      .map(x =&gt; (x(<span class="number">0</span>), x(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobExecutor = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    jobExecutor.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        spark.sparkContext.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="string">"count-pool"</span>)</span><br><span class="line">        <span class="keyword">val</span> cnt = rdd.groupByKey().count()</span><br><span class="line">        println(<span class="string">s"Count: <span class="subst">$cnt</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    jobExecutor.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        spark.sparkContext.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="string">"take-pool"</span>)</span><br><span class="line">        <span class="keyword">val</span> data = rdd.sortByKey().take(<span class="number">10</span>)</span><br><span class="line">        println(<span class="string">s"Data Samples: "</span>)</span><br><span class="line">        data.foreach &#123; x =&gt; println(x.mkString(<span class="string">", "</span>)) &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    jobExecutor.shutdown()</span><br><span class="line">    <span class="keyword">while</span> (!jobExecutor.isTerminated) &#123;&#125;</span><br><span class="line">    println(<span class="string">"Done!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述应用程序中使用两个线程分别调用了Action方法，即有两个Job会并发提交，但是不管怎样，这两个Job被切分成若干TaskSet后终究会被交到TaskScheduler这里统一管理，其调度树大致如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqdac0cfroj31kw0pf0zt.jpg" alt=""></p><p>在出队时，则会对所有TaskSetManager排序，具体排序过程是从根节点rootPool开始，递归地去排序子节点，最后合并到一个ArrayBuffer里，代码逻辑如下。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSortedTaskSetQueue</span></span>: <span class="type">ArrayBuffer</span>[<span class="type">TaskSetManager</span>] = &#123;</span><br><span class="line">    <span class="keyword">var</span> sortedTaskSetQueue = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">TaskSetManager</span>]</span><br><span class="line">    <span class="keyword">val</span> sortedSchedulableQueue = schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)</span><br><span class="line">    <span class="keyword">for</span> (schedulable &lt;- sortedSchedulableQueue) &#123;</span><br><span class="line">      sortedTaskSetQueue ++= schedulable.getSortedTaskSetQueue</span><br><span class="line">    &#125;</span><br><span class="line">    sortedTaskSetQueue</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>使用FAIR调度策略时，上面代码中的taskSetSchedulingAlgorithm的类型为FairSchedulingAlgorithm，排序过程的比较是基于Fair-share来比较的，每个要排序的对象包含三个属性: runningTasks值（正在运行的Task数）、minShare值、weight值，比较时会综合考量runningTasks值，minShare以及weight值。如果A对象的runningTasks大于它的minShare，B对象的runningTasks小于它的minShare，那么B排在A前面；如果A、B对象的runningTasks都小于它们的minShare，那么就比较runningTasks与minShare的比值，谁小谁排前面；如果A、B对象的runningTasks都大于它们的minShare，那么就比较runningTasks与weight的比值，谁小谁排前面。整体上来说就是通过minShare和weight这两个参数控制比较过程，可以做到不让资源被某些长时间Task给一直占了。</p><p>从调度队列中拿到TaskSetManager后，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。前面也提到，TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task。</p><h4 id="本地化调度"><a href="#本地化调度" class="headerlink" title="本地化调度"></a>本地化调度</h4><p>从调度队列中拿到TaskSetManager后，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。前面也提到，TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task。<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqdadrcr1pj31ai068q4f.jpg" alt=""></p><p>在TaskSetManager初始化过程中，会对Tasks按照Locality级别进行分类，Task的Locality有五种，优先级由高到低顺序：PROCESS_LOCAL(指定的Executor)，NODE_LOCAL(指定的主机节点)，NO_PREF(无所谓)，RACK_LOCAL(指定的机架)，ANY(满足不了Task的Locality就随便调度)。这五种Locality级别存在包含关系，RACK_LOCAL包含NODE_LOCAL，NODE_LOCAL包含PROCESS_LOCAL，然而ANY包含其他所有四种。初始化阶段在对Task分类时，根据Task的preferredLocations判断它属于哪个Locality级别，属于PROCESS_LOCAL的Task同时也会被加入到NODE_LOCAL、RACK_LOCAL类别中，比如，一个Task的preferredLocations指定了在Executor-2上执行，那么它属于Executor-2对应的PROCESS_LOCAL类别，同时也把他加入到Executor-2所在的主机对应的NODE_LOCAL类别，Executor-2所在的主机的机架对应的RACK_LOCAL类别中，以及ANY类别，这样在调度执行时，满足不了PROCESS_LOCAL，就逐步退化到NODE_LOCAL，RACK_LOCAL，ANY。</p><p>TaskSetManager在决定调度哪些Task时，是通过上面流程图中的resourceOffer方法来实现，为了尽可能地将Task调度到它的preferredLocations上，它采用一种延迟调度算法。resourceOffer方法原型如下，参数包括要调度任务的Executor Id、主机地址以及最大可容忍的Locality级别。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resourceOffer</span></span>(</span><br><span class="line">      execId: <span class="type">String</span>,</span><br><span class="line">      host: <span class="type">String</span>,</span><br><span class="line">      maxLocality: <span class="type">TaskLocality</span>.<span class="type">TaskLocality</span>)</span><br><span class="line">    : <span class="type">Option</span>[<span class="type">TaskDescription</span>]</span><br></pre></td></tr></table></figure><p>延迟调度算法的大致流程如下图所示。<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqdaela7nrj31kw0pvgr7.jpg" alt=""></p><p>首先看是否存在execId对应的PROCESS_LOCAL类别的任务，如果存在，取出来调度，否则根据当前时间，判断是否超过了PROCESS_LOCAL类别最大容忍的延迟，如果超过，则退化到下一个级别NODE_LOCAL，否则等待不调度。退化到下一个级别NODE_LOCAL后调度流程也类似，看是否存在host对应的NODE_LOCAL类别的任务，如果存在，取出来调度，否则根据当前时间，判断是否超过了NODE_LOCAL类别最大容忍的延迟，如果超过，则退化到下一个级别RACK_LOCAL，否则等待不调度，以此类推…..。当不满足Locatity类别会选择等待，直到下一轮调度重复上述流程，如果你比较激进，可以调大每个类别的最大容忍延迟时间，如果不满足Locatity时就会等待多个调度周期，直到满足或者超过延迟时间退化到下一个级别为止。</p><h4 id="失败重试与黑名单机制"><a href="#失败重试与黑名单机制" class="headerlink" title="失败重试与黑名单机制"></a>失败重试与黑名单机制</h4><p>除了选择合适的Task调度运行外，还需要监控Task的执行状态，前面也提到，与外部打交道的是SchedulerBackend，Task被提交到Executor启动执行后，Executor会将执行状态上报给SchedulerBackend，SchedulerBackend则告诉TaskScheduler，TaskScheduler找到该Task对应的TaskSetManager，并通知到该TaskSetManager，这样TaskSetManager就知道Task的失败与成功状态，对于失败的Task，会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的Task池子中，否则整个Application失败。</p><p>在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“黑暗”时间，“黑暗”时间是指这段时间内不要再往这个节点上调度这个Task了。</p><h4 id="推测式执行"><a href="#推测式执行" class="headerlink" title="推测式执行"></a>推测式执行</h4><p>TaskScheduler在启动SchedulerBackend后，还会启动一个后台线程专门负责推测任务的调度，推测任务是指对一个Task在不同的Executor上启动多个实例，如果有Task实例运行成功，则会干掉其他Executor上运行的实例。推测调度线程会每隔固定时间检查是否有Task需要推测执行，如果有，则会调用SchedulerBackend的reviveOffers去尝试拿资源运行推测任务。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdah2m113j31ho0gstau.jpg" alt=""></p><p>检查是否有Task需要推测执行的逻辑最后会交到TaskSetManager，TaskSetManager采用基于统计的算法，检查Task是否需要推测执行，算法流程大致如下图所示。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdaj1pwd0j31au11oq9z.jpg" alt=""><br>TaskSetManager首先会统计成功的Task数，当成功的Task数超过75%(可通过参数spark.speculation.quantile控制)时，再统计所有成功的Tasks的运行时间，得到一个中位数，用这个中位数乘以1.5(可通过参数spark.speculation.multiplier控制)得到运行时间门限，如果在运行的Tasks的运行时间超过这个门限，则对它启用推测。算法逻辑较为简单，其实就是对那些拖慢整体进度的Tasks启用推测，以加速整个TaskSet即Stage的运行。</p><h2 id="资源申请机制"><a href="#资源申请机制" class="headerlink" title="资源申请机制"></a>资源申请机制</h2><p>在前文已经提过，ApplicationMaster和SchedulerBackend起来后，SchedulerBackend通过ApplicationMaster申请资源，ApplicationMaster就是用来专门适配YARN申请Container资源的，当申请到Container，会在相应Container上启动Executor进程，其他事情就交给SchedulerBackend。Spark早期版本只支持静态资源申请，即一开始就指定用多少资源，在整个Spark应用程序运行过程中资源都不能改变，后来支持动态Executor申请，用户不需要指定确切的Executor数量，Spark会动态调整Executor的数量以达到资源利用的最大化。</p><h3 id="静态资源申请"><a href="#静态资源申请" class="headerlink" title="静态资源申请"></a>静态资源申请</h3><p>静态资源申请是用户在提交Spark应用程序时，就要提前估计应用程序需要使用的资源，包括Executor数(num_executor)、每个Executor上的core数(executor_cores)、每个Executor的内存(executor_memory)以及Driver的内存(driver_memory)。</p><p>在估计资源使用时，应当首先了解这些资源是怎么用的。任务的并行度由分区数(Partitions)决定，一个Stage有多少分区，就会有多少Task。每个Task默认占用一个Core，一个Executor上的所有core共享Executor上的内存，一次并行运行的Task数等于num_executor*executor_cores，如果分区数超过该值，则需要运行多个轮次，一般来说建议运行3～5轮较为合适，否则考虑增加num_executor或executor_cores。由于一个Executor的所有tasks会共享内存executor_memory，所以建议executor_cores不宜过大。executor_memory的设置则需要综合每个分区的数据量以及是否有缓存等逻辑。下图描绘了一个应用程序内部资源利用情况。<br><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdak8sm5bj31kw0uj16a.jpg" alt=""></p><h3 id="动态资源申请"><a href="#动态资源申请" class="headerlink" title="动态资源申请"></a>动态资源申请</h3><p>动态资源申请目前只支持到Executor，即可以不用指定num_executor，通过参数spark.dynamicAllocation.enabled来控制。由于许多Spark应用程序一开始可能不需要那么多Executor或者其本身就不需要太多Executor，所以不必一次性申请那么多Executor，根据具体的任务数动态调整Executor的数量，尽可能做到资源的不浪费。由于动态Executor的调整会导致Executor动态的添加与删除，如果删除Executor，其上面的中间Shuffle结果可能会丢失，这就需要借助第三方的ShuffleService了，如果Spark是部署在Yarn上，则可以在Yarn上配置Spark的ShuffleService，具体操作仅需做两点:</p><p>1.首先在yarn-site.xml中加上如下配置：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>7337<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>2.将Spark ShuffleService jar包<code>$SPARK_HOME/lib/spark-*-yarn-shuffle.jar</code>拷贝到每台NodeManager的<code>$HADOOP_HOME/share/hadoop/yarn/lib/</code>下，并重启所有的NodeManager。</p><p>当启用动态Executor申请时，在SparkContext初始化过程中会实例化ExecutorAllocationManager，它是被用来专门控制动态Executor申请逻辑的，动态Executor申请是一种基于当前Task负载压力实现动态增删Executor的机制。一开始会按照参数spark.dynamicAllocation.initialExecutors设置的初始Executor数申请，然后根据当前积压的Task数量，逐步增长申请的Executor数，如果当前有积压的Task，那么取积压的Task数和spark.dynamicAllocation.maxExecutors中的最小值作为Executor数上限，每次新增加申请的Executor为2的次方，即第一次增加1，第二次增加2，第三次增加4，…。另一方面，如果一个Executor在一段时间内都没有Task运行，则将其回收，但是在Remove Executor时，要保证最少的Executor数，该值通过参数spark.dynamicAllocation.minExecutors来控制，如果Executor上有Cache的数据，则永远不会被Remove，以保证中间数据不丢失。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本文详细阐述了Spark的任务调度，着重讨论Spark on Yarn的部署调度，剖析了从应用程序提交到运行的全过程。Spark Schedule算是Spark中的一个大模块，它负责任务下发与监控等，基本上扮演了Spark大脑的角色。了解Spark Schedule有助于帮助我们清楚地认识Spark应用程序的运行轨迹，同时在我们实现其他系统时，也可以借鉴Spark的实现。</p><p>转载请注明出处，本文永久链接：<a href="http://sharkdtu.com/posts/spark-scheduler.html" target="_blank" rel="noopener">http://sharkdtu.com/posts/spark-scheduler.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;Apache Spark是一个开源的，强大的分布式查询和处理引擎。它提供MapReduce的灵活性和可扩展性，但速度明显更高。&lt;br&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fq
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://www.ihoge.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkSQL DataFrame进阶篇</title>
    <link href="http://www.ihoge.cn/2018/Sparksql0.html"/>
    <id>http://www.ihoge.cn/2018/Sparksql0.html</id>
    <published>2018-04-14T09:59:21.000Z</published>
    <updated>2018-04-22T14:54:42.223Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="1-创建SparkSession【2-0】和-SQLContext实例【1-x】"><a href="#1-创建SparkSession【2-0】和-SQLContext实例【1-x】" class="headerlink" title="1.创建SparkSession【2.0】和 SQLContext实例【1.x】"></a>1.创建SparkSession【2.0】和 SQLContext实例【1.x】</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>创建<span class="type">SparkSession</span>【<span class="number">2.0</span>】</span><br><span class="line"><span class="comment">///spark2.0后，用sparksession代替sparkcontext和sqlcontext的创建</span></span><br><span class="line"><span class="keyword">val</span> spark= <span class="type">SparkSession</span>.builder().appName(<span class="string">"SparkSQLTest"</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> numbers=spark.range(<span class="number">1</span> ,<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">numbers.show()</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>创建<span class="type">SQLContext</span>实例【<span class="number">1.</span>x】</span><br><span class="line"><span class="keyword">val</span> conf= <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SQL_Advanced_case"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"><span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">import</span>.sqlContext.implicits._</span><br></pre></td></tr></table></figure><h1 id="2-创建DataFrame"><a href="#2-创建DataFrame" class="headerlink" title="2.创建DataFrame"></a>2.创建DataFrame</h1><h2 id="方式1-不创建RDD"><a href="#方式1-不创建RDD" class="headerlink" title="方式1:不创建RDD"></a>方式1:不创建RDD</h2><p>使用createDataFram方法，直接基于列表List创建DataFrame.缺点是创建出来的DataFrame没有列名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val customerData=List((&quot;Alex&quot;,&quot;浙江&quot;,39,230.00), (&quot;Bob&quot;,&quot;北京&quot;, 18, 170.00), (&quot;Chris&quot;, &quot;江苏&quot;, 45, 529.95), (&quot;Dave&quot;, &quot;北京&quot;, 25, 99.99), (&quot;Ellie&quot;, &quot;浙江&quot;, 23, 1299.95), (&quot;Fred&quot;, &quot;北京&quot;, 21, 1099.00))</span><br><span class="line">val customerDF1=sqlContext.createDataFrame(customerData)</span><br><span class="line">customerDF1.printSchema</span><br></pre></td></tr></table></figure></p><h2 id="方式2-不创建RDD"><a href="#方式2-不创建RDD" class="headerlink" title="方式2:不创建RDD"></a>方式2:不创建RDD</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用createDataFram方法，直接基于列表<span class="type">List</span>创建<span class="type">DataFrame</span>.即便定义了样例类，但基于createDataFrame创建出来的<span class="type">DataFrame</span>缺点是创建出来的<span class="type">DataFrame</span>没有列名</span><br><span class="line"><span class="keyword">val</span> customerData=<span class="type">List</span>((<span class="string">"Alex"</span>,<span class="string">"浙江"</span>,<span class="number">39</span>,<span class="number">230.00</span>), (<span class="string">"Bob"</span>,<span class="string">"北京"</span>, <span class="number">18</span>, <span class="number">170.00</span>), (<span class="string">"Chris"</span>, <span class="string">"江苏"</span>, <span class="number">45</span>, <span class="number">529.95</span>), (<span class="string">"Dave"</span>, <span class="string">"北京"</span>, <span class="number">25</span>, <span class="number">99.99</span>), (<span class="string">"Ellie"</span>, <span class="string">"浙江"</span>, <span class="number">23</span>, <span class="number">1299.95</span>), (<span class="string">"Fred"</span>, <span class="string">"北京"</span>, <span class="number">21</span>, <span class="number">1099.00</span>))</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerInfo</span>(<span class="params">customer:<span class="type">String</span>, province:<span class="type">String</span>, age: <span class="type">Int</span>, total:<span class="type">Double</span> </span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">cdf=cDataRDD</span>.<span class="title">map</span>(<span class="params">x=&gt;x.split(","</span>)).<span class="title">map</span>(<span class="params">x=&gt;<span class="type">CustomerInfo</span>(x(0</span>),<span class="title">x</span>(<span class="params">1</span>),<span class="title">x</span>(<span class="params">2</span>).<span class="title">toInt</span>,<span class="title">x</span>(<span class="params">3</span>).<span class="title">toDouble</span>)).<span class="title">toDF</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">customerDF1=sqlContext</span>.<span class="title">createDataFrame</span>(<span class="params">customerData</span>)</span></span><br><span class="line"><span class="class"><span class="title">customerDF1</span>.<span class="title">printSchema</span></span></span><br></pre></td></tr></table></figure><h2 id="方式3：传统方式"><a href="#方式3：传统方式" class="headerlink" title="方式3：传统方式"></a>方式3：传统方式</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">先创建<span class="type">RDD</span>和样例类，然后通过toDF创建<span class="type">DataFrame</span>。此时<span class="type">DataFrame</span>中包含列信息。</span><br><span class="line"><span class="keyword">val</span> customerData=<span class="type">Array</span>(<span class="string">"Alex,浙江,39,230.00"</span>,<span class="string">"Bob,北京,18,170.00"</span>,<span class="string">"Chris,江苏,45,529.95"</span>,<span class="string">"Dave,北京,25,99.99"</span>,<span class="string">"Ellie,浙江,23,1299.95"</span>,<span class="string">"Fred,北京,21,1099.00"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerInfo</span>(<span class="params">customer:<span class="type">String</span>, province:<span class="type">String</span>, age: <span class="type">Int</span>, total:<span class="type">Double</span> </span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">customerRDD=sc</span>.<span class="title">makeRDD</span>(<span class="params">customerData</span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">customerDF1=customerRDD</span>.<span class="title">map</span>(<span class="params">x=&gt;x.split(","</span>)).<span class="title">map</span>(<span class="params">x=&gt;<span class="type">CustomerInfo</span>(x(0</span>),<span class="title">x</span>(<span class="params">1</span>),<span class="title">x</span>(<span class="params">2</span>).<span class="title">toInt</span>,<span class="title">x</span>(<span class="params">3</span>).<span class="title">toDouble</span>)).<span class="title">toDF</span></span></span><br><span class="line"><span class="class"><span class="title">customerDF1</span>.<span class="title">printSchema</span></span></span><br></pre></td></tr></table></figure><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="修改列名"><a href="#修改列名" class="headerlink" title="修改列名"></a>修改列名</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> customerDF=customerDF1.withColumnRenamed(<span class="string">"_1"</span>,<span class="string">"customer"</span>).withColumnRenamed(<span class="string">"_2"</span>,<span class="string">"province"</span>).withColumnRenamed(<span class="string">"_3"</span>, <span class="string">"age"</span>).withColumnRenamed(<span class="string">"_4"</span>, <span class="string">"total"</span>)</span><br></pre></td></tr></table></figure><h3 id="查看表模式"><a href="#查看表模式" class="headerlink" title="查看表模式"></a>查看表模式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.printSchema</span><br></pre></td></tr></table></figure><h3 id="数值型数据基本的统计与分析"><a href="#数值型数据基本的统计与分析" class="headerlink" title="数值型数据基本的统计与分析"></a>数值型数据基本的统计与分析</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.describe().show</span><br></pre></td></tr></table></figure><h1 id="3-DataFrame方法"><a href="#3-DataFrame方法" class="headerlink" title="3.DataFrame方法"></a>3.DataFrame方法</h1><h2 id="select"><a href="#select" class="headerlink" title="select"></a>select</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">customerDF.select(customerDF.col(<span class="string">"customer"</span>)).show</span><br><span class="line">customerDF.select(customerDF(<span class="string">"customer"</span>)).show</span><br><span class="line">customerDF.select(<span class="string">"customer"</span>, <span class="string">"province"</span>).show</span><br><span class="line">customerDF.select($<span class="string">"customer"</span>, $<span class="string">"province"</span>).show</span><br><span class="line">customerDF.select(col(<span class="string">"customer"</span>), col(<span class="string">"province"</span>)).show</span><br><span class="line">customerDF.select(customerDF(<span class="string">"customer"</span>), col(<span class="string">"province"</span>)).show</span><br><span class="line">customerDF.select(<span class="string">"customer"</span>, $<span class="string">"province"</span>).show<span class="comment">//错误,字符串与$不能混用。</span></span><br><span class="line">customerDF.select(col(<span class="string">"customer"</span>), $<span class="string">"province"</span>).show<span class="comment">//是否正确？正确！！</span></span><br></pre></td></tr></table></figure><h3 id="使用表达式"><a href="#使用表达式" class="headerlink" title="使用表达式"></a>使用表达式</h3><p>（Column对象中的方法）<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF. select($<span class="string">"customer"</span>,( $<span class="string">"age"</span>*<span class="number">2</span>)+<span class="number">10</span>, $<span class="string">"province"</span>===<span class="string">"浙江"</span>).show <span class="comment">//计算$"province"==="浙江"这一关系表达式的值</span></span><br></pre></td></tr></table></figure></p><h3 id="as、alias列的重命名"><a href="#as、alias列的重命名" class="headerlink" title="as、alias列的重命名"></a>as、alias列的重命名</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF. select($<span class="string">"customer"</span> as <span class="string">"name"</span>,( $<span class="string">"age"</span>*<span class="number">2</span>)+<span class="number">10</span> alias <span class="string">"newAge"</span>, $<span class="string">"province"</span>===<span class="string">"浙江"</span> as <span class="string">"isZJ"</span>).show</span><br></pre></td></tr></table></figure><h3 id="lit添加列"><a href="#lit添加列" class="headerlink" title="lit添加列"></a>lit添加列</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.sql.functions中的方法lit</span><br><span class="line"><span class="keyword">val</span> cdf1=customerDF.select($<span class="string">"customer"</span>, $<span class="string">"age"</span>, when($<span class="string">"age"</span>&lt;<span class="number">20</span>,<span class="number">1</span>).when($<span class="string">"age"</span>&lt;<span class="number">30</span>, <span class="number">2</span>).otherwise(<span class="number">3</span>) as <span class="string">"ageGroup"</span>, lit(<span class="literal">false</span>) as <span class="string">"trusted"</span>)</span><br><span class="line">cdf1.show</span><br><span class="line">cdf1.printSchema</span><br></pre></td></tr></table></figure><h3 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cdf2=cdf1.drop(<span class="string">"trusted"</span>)</span><br><span class="line">cdf2.show</span><br><span class="line">cdf2.printSchema</span><br></pre></td></tr></table></figure><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.select($<span class="string">"province"</span>).distinct.show</span><br></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">customerDF.filter($<span class="string">"age"</span>&gt;<span class="number">30</span>).show</span><br><span class="line">customerDF.filter(<span class="string">"age&gt;30"</span>).show</span><br><span class="line">customerDF.filter($<span class="string">"age"</span>&lt;=<span class="number">30</span> and $<span class="string">"province"</span>===<span class="string">"浙江"</span> )</span><br><span class="line">customerDF.filter(<span class="string">"age&lt;=30 and province =’浙江’"</span>)</span><br></pre></td></tr></table></figure><h2 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h2><h3 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h3><p>向已有的DataFrame添加一个新列，不删除之前的列<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> customerAgeGroupDF=customerDF.withColumn(<span class="string">"agegroup"</span>, when($<span class="string">"age"</span>&lt;<span class="number">20</span>, <span class="number">1</span>).when($<span class="string">"age"</span>&lt;<span class="number">30</span>, <span class="number">2</span>).otherwise(<span class="number">3</span>))</span><br><span class="line">customerAgeGroupDF.show</span><br></pre></td></tr></table></figure></p><h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><p>操作返回GroupedData对象【2.0中为RelationalGroupedDataSet】<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//其中封装了大量聚合方法。</span></span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>).max().show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>,<span class="string">"province"</span>).count().show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>).min(<span class="string">"age"</span>, <span class="string">"total"</span>).show()</span><br></pre></td></tr></table></figure></p><h3 id="agg"><a href="#agg" class="headerlink" title="agg"></a>agg</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>).agg(sum($<span class="string">"total"</span>), min($<span class="string">"total"</span>)).show()</span><br></pre></td></tr></table></figure><h3 id="pivot"><a href="#pivot" class="headerlink" title="pivot"></a>pivot</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">customerAgeGroupDF.groupBy(<span class="string">"province"</span>).pivot(<span class="string">"agegroup"</span>).sum(<span class="string">"total"</span>).show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"province"</span>).pivot(<span class="string">"agegroup"</span>,<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>)). agg(<span class="string">"total"</span>).show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"province"</span>).pivot(<span class="string">"agegroup"</span>,<span class="type">Seq</span>(<span class="number">2</span>,<span class="number">3</span>)).agg(sum($<span class="string">"total"</span>), min($<span class="string">"total"</span>)).filter($<span class="string">"provice"</span>=!=<span class="string">"北京"</span>).show</span><br></pre></td></tr></table></figure><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">customerDF.orderBy(<span class="string">"age"</span>).show</span><br><span class="line">customerDF.orderBy($<span class="string">"age"</span>).show</span><br><span class="line">customerDF.orderBy(desc(<span class="string">"age"</span>)).show()</span><br><span class="line"><span class="comment">/*此处orderBy方法一定要用在所有聚合函数之后，因为groupBy方法返回的是GroupedData类型数据,</span></span><br><span class="line"><span class="comment">该类型数据中的聚合方法返回DateFrame类型对象，而orderBy是DataFrame中的方法，所以用在groupBy</span></span><br><span class="line"><span class="comment">之后会提示错误：orderBy不是GroupedData的成员方法。*/</span></span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>,<span class="string">"province"</span>).count().orderBy($<span class="string">"agegroup"</span>.desc).show()</span><br><span class="line">students.sort($<span class="string">"age"</span>.asc).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;1-创建SparkSession【2-0】和-SQLContext实例【1-x】&quot;&gt;&lt;a href=&quot;#1-创建SparkSession【2-0】和-SQLContext实例【1-x】&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="sparksql" scheme="http://www.ihoge.cn/tags/sparksql/"/>
    
  </entry>
  
  <entry>
    <title>SparkSQL DataFrame基础篇</title>
    <link href="http://www.ihoge.cn/2018/SparkSQl.html"/>
    <id>http://www.ihoge.cn/2018/SparkSQl.html</id>
    <published>2018-04-14T05:59:21.000Z</published>
    <updated>2018-04-22T14:55:25.956Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>#SparkSQL DataFrame基础篇</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Spark</span> <span class="number">2.2</span>及以后的<span class="type">SparkSession</span>替换了<span class="type">Spark</span>以前版本中的<span class="type">SparkContext</span>和<span class="type">SQLContext</span>，为<span class="type">Spark</span>集群提供了唯一的入口点。</span><br><span class="line"><span class="keyword">val</span> spark =<span class="type">SparkSession</span>.builder().</span><br><span class="line">appName(“<span class="type">SparkExample</span>”).</span><br><span class="line">getOrCreate()</span><br><span class="line">为了向后兼容，<span class="type">SparkSession</span>对象包含<span class="type">SparkContext</span>和<span class="type">SQLContext</span>对象。当使用交互式<span class="type">Spark</span> shell时，创建一个<span class="type">SparkSession</span>类型对象名为spark。</span><br><span class="line"></span><br><span class="line">因此该文档里所有的<span class="type">SQLContext</span>在spark2<span class="number">.2</span>+中都可以替换成spark</span><br></pre></td></tr></table></figure><h2 id="基于反射机制创建DataFrame"><a href="#基于反射机制创建DataFrame" class="headerlink" title="基于反射机制创建DataFrame"></a>基于反射机制创建DataFrame</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//students.txt</span><br><span class="line">160201,Michael,17</span><br><span class="line">160101,Andy,23</span><br><span class="line">160301,justin,23</span><br><span class="line">160202,John,22</span><br><span class="line">160102,Herry,17</span><br><span class="line">160203,Brewster,18</span><br><span class="line">160302,Brice,20</span><br><span class="line">160303,Justin,25</span><br><span class="line">160103,Jerry,22</span><br><span class="line">160304,Tom,24</span><br></pre></td></tr></table></figure><p>不结合hive，使用spark实例(适用于spark1+)<br>val spark=new org.apache.spark.sql.spark(sc)<br>import spark.implicits._</p><p>结合hive后，访问hive时使用HiveContext实例<br>val hiveContext=new org.apache.spark.sql.hive.HiveContext(sc)<br>import hiveContext.implicits._</p><p><code>这里与Spark2.2+有所不同</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br><span class="line">students.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line"><span class="keyword">val</span> youngstudents=spark.sql(<span class="string">"SELECT name FROM tb_students WHERE age&gt;=19 AND age&lt;=22"</span>)</span><br><span class="line">youngstudents.show</span><br></pre></td></tr></table></figure><h2 id="基于编程创建DataFrame"><a href="#基于编程创建DataFrame" class="headerlink" title="基于编程创建DataFrame"></a>基于编程创建DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._   </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._         </span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> students=sc.textFile(<span class="string">"hdfs://master:9000/sqldata/students.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schemaString=<span class="string">"id name age"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema=<span class="type">StructType</span>(schemaString.split(<span class="string">" "</span>).map(fieldname=&gt; <span class="type">StructField</span>(fieldname, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rowRDD=students.map(_.split(<span class="string">","</span>)).map(p=&gt;<span class="type">Row</span>(p(<span class="number">0</span>), p(<span class="number">1</span>), p(<span class="number">2</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> studentsDataFrame=spark.createDataFrame(rowRDD, schema) </span><br><span class="line"></span><br><span class="line">studentsDataFrame.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> names=spark.sql(<span class="string">"SELECT name FROM tb_students"</span>)</span><br><span class="line"></span><br><span class="line">names.show</span><br></pre></td></tr></table></figure><h2 id="基于DataFrame创建Json文件"><a href="#基于DataFrame创建Json文件" class="headerlink" title="基于DataFrame创建Json文件"></a>基于DataFrame创建Json文件</h2><h3 id="1-创建DataFrame"><a href="#1-创建DataFrame" class="headerlink" title="1.创建DataFrame"></a>1.创建DataFrame</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br></pre></td></tr></table></figure><h3 id="2-另存为json类型文件"><a href="#2-另存为json类型文件" class="headerlink" title="2.另存为json类型文件"></a>2.另存为json类型文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//students.save("hdfs://master:9000/sqldata/students.json")//无法执行</span></span><br><span class="line"></span><br><span class="line">spark1<span class="number">.4</span>及以后，dataframe中的save方法不建议使用，有的直接被弃用，</span><br><span class="line">使用<span class="type">DataFrame</span>中的write方法返回一个<span class="type">DataFrameWriter</span>类型对象，再使用里面的save方法、format().save()、parquet()等方法</span><br><span class="line"></span><br><span class="line">students.write.save(<span class="string">"hdfs://master:9000/sqldata/students.json"</span>)</span><br><span class="line">students.write.format(<span class="string">"json"</span>).save(<span class="string">"hdfs://master:9000/sqldata/s1.json"</span>)<span class="comment">//Spark 1.4 DataFrameWriter中方法format、save</span></span><br><span class="line">students.write.json(<span class="string">"hdfs://master:9000/sqldata/s2.json"</span>)<span class="comment">//Spark 1.4 DataFrameWriter中方法json</span></span><br></pre></td></tr></table></figure><h2 id="基于DataFrame创建Parquet文件"><a href="#基于DataFrame创建Parquet文件" class="headerlink" title="基于DataFrame创建Parquet文件"></a>基于DataFrame创建Parquet文件</h2><h3 id="1-创建DataFrame-1"><a href="#1-创建DataFrame-1" class="headerlink" title="1.创建DataFrame"></a>1.创建DataFrame</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br></pre></td></tr></table></figure><h3 id="2-另存为parquet类型文件"><a href="#2-另存为parquet类型文件" class="headerlink" title="2.另存为parquet类型文件"></a>2.另存为parquet类型文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//students.save("hdfs://master:9000/sqldata/students.parquet")//无法执行</span></span><br><span class="line"></span><br><span class="line">spark1<span class="number">.4</span>及以后，dataframe中的save方法不建议使用，有的直接被弃用，</span><br><span class="line">使用<span class="type">DataFrame</span>中的write方法返回一个<span class="type">DataFrameWriter</span>类型对象，再使用里面的save方法、format().save()、parquet()等方法</span><br><span class="line"></span><br><span class="line">students.write.save(<span class="string">"hdfs://master:9000/sqldata/students.parquet"</span>)</span><br><span class="line">students.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master:9000/sqldata/s1.parquet"</span>)</span><br><span class="line">students.write.parquet(<span class="string">"hdfs://master:9000/sqldata/s2.parquet"</span>)</span><br></pre></td></tr></table></figure><h2 id="基于Json创建DataFrame"><a href="#基于Json创建DataFrame" class="headerlink" title="基于Json创建DataFrame"></a>基于Json创建DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//read方法是spark中的方法，返回值类型是DataFrameReader,以往的jsonFile方法已经不建议使用 </span></span><br><span class="line"><span class="keyword">val</span> students=spark.read.json(<span class="string">"hdfs://master:9000/sqldata/s2.json"</span>)<span class="comment">//read.json()是最通用的一种方法s1.json,s2.json都可读     </span></span><br><span class="line">students.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from tb_students"</span>).show</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">val students=spark.read.load("hdfs://master:9000/sqldata/students.json")//可执行，但换做读s1.json,s2.json不可读</span></span><br><span class="line"><span class="comment">val students=spark.read.load("hdfs://master:9000/sqldata/s1.json") //不可执行，load默认读取parquet类型文件</span></span><br><span class="line"><span class="comment">val students=spark.jsonFile("hdfs://master:9000/sqldata/students.json") //jsonFile不可用</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="基于Parquet创建DataFrame"><a href="#基于Parquet创建DataFrame" class="headerlink" title="基于Parquet创建DataFrame"></a>基于Parquet创建DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//read方法是spark中的方法，返回值类型是DataFrameReader,以往的parquetFile方法已经不建议使用 </span></span><br><span class="line"><span class="keyword">val</span> students=spark.read.parquet(<span class="string">"hdfs://master:9000/sqldata/students.parquet"</span>) <span class="comment">//最通用的一种方法,students.parquet,s1.parquet,s2.parquet都可读</span></span><br><span class="line">students.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from tb_students"</span>).show</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">val students=spark.read.load("hdfs://master:9000/sqldata/students.parquet")</span></span><br><span class="line"><span class="comment">val students=spark.load.parquet("hdfs://master:9000/sqldata/s1.parquet")//此处不可执行因为load非SparkSession中方法，Spark1.X中load方法可执行，默认读取parquet文件</span></span><br><span class="line"><span class="comment">val students=spark.parquetFile("hdfs://master:9000/sqldata/students.parquet")//parquetFile不可用</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="DataFrame的其他操作"><a href="#DataFrame的其他操作" class="headerlink" title="DataFrame的其他操作"></a>DataFrame的其他操作</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">students.head    </span><br><span class="line"></span><br><span class="line">students.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">students.show(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">students.columns</span><br><span class="line"></span><br><span class="line">students.dtypes</span><br><span class="line"></span><br><span class="line">students.printSchema</span><br><span class="line"><span class="comment">//更详细的Schema信息</span></span><br></pre></td></tr></table></figure><h3 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.withColumn(<span class="string">"bonus"</span>, students(<span class="string">"age"</span>)*<span class="number">50</span>).show</span><br><span class="line">students.withColumn(<span class="string">"bonus"</span>, $<span class="string">"age"</span>*<span class="number">50</span>).show<span class="comment">//可以运行</span></span><br><span class="line">students.withColumn(<span class="string">"bonus"</span>, <span class="string">"age"</span>*<span class="number">50</span>).show<span class="comment">//不可运行</span></span><br></pre></td></tr></table></figure><h3 id="withColumnRenamed"><a href="#withColumnRenamed" class="headerlink" title="withColumnRenamed"></a>withColumnRenamed</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newstudents=students.withColumnRenamed(<span class="string">"age"</span>, <span class="string">"newage"</span>)</span><br><span class="line"></span><br><span class="line">newstudents.printSchema</span><br></pre></td></tr></table></figure><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br><span class="line"></span><br><span class="line">students.select(<span class="string">"age"</span>, <span class="string">"name"</span>).show</span><br><span class="line">students.select( $<span class="string">"age"</span>, $<span class="string">"name"</span>).show</span><br><span class="line">students.selectExpr(<span class="string">"age+1"</span>, <span class="string">"name"</span>, <span class="string">"abs(age)"</span>).show</span><br><span class="line">students.selectExpr(<span class="string">"age+1 as newage1"</span>, <span class="string">"name as newName"</span>, <span class="string">"sqrt(age) as newage2"</span>).show</span><br></pre></td></tr></table></figure><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">students.filter(<span class="string">"age&gt;20"</span>).show</span><br><span class="line">students.filter($<span class="string">"age"</span>&gt;<span class="number">20</span>).show</span><br><span class="line">students.filter($<span class="string">"age"</span>&gt;<span class="number">23</span> &amp;&amp; $<span class="string">"name"</span>===<span class="string">"Justin"</span>).show  <span class="comment">//可以执行</span></span><br><span class="line">students.filter(<span class="symbol">'age</span>&gt;<span class="number">20</span>).show <span class="comment">//可以执行</span></span><br><span class="line">students.filter(<span class="string">"age&gt;23 &amp;&amp; name===Justin"</span>).show        <span class="comment">//不可以执行，字符串中不能用逻辑运算符</span></span><br></pre></td></tr></table></figure><h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">students.where(<span class="symbol">'age</span>&gt;<span class="number">20</span>).show   </span><br><span class="line">students.where($<span class="string">"age"</span>&gt;<span class="number">20</span>).show </span><br><span class="line">students.where($<span class="string">"age"</span>&gt;<span class="number">23</span> &amp;&amp; $<span class="string">"name"</span>===<span class="string">"Justin"</span>).show</span><br><span class="line">students.where(<span class="symbol">'age</span>&gt;<span class="number">23</span> &amp;&amp; <span class="symbol">'name</span>===<span class="string">"Justin"</span>).show</span><br><span class="line">students.where(<span class="string">"age&gt;23 &amp;&amp; name===Justin"</span>).show      <span class="comment">//不可以执行,字符串中不能用逻辑运算符</span></span><br></pre></td></tr></table></figure><h3 id="orderBy"><a href="#orderBy" class="headerlink" title="orderBy"></a>orderBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.orderBy(<span class="string">"age"</span>, <span class="string">"id"</span>).show(<span class="number">5</span>)</span><br><span class="line">students.orderBy(students(<span class="string">"age"</span>)).show(<span class="number">5</span>)</span><br><span class="line">students.orderBy($<span class="string">"age"</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">newstudents.groupBy(&apos;course_id).mean(&quot;score&quot;).orderBy(&apos;course_id).show</span><br><span class="line"></span><br><span class="line">max(&quot;col&quot;)</span><br><span class="line">min(&quot;col&quot;)</span><br><span class="line">mean(&quot;col&quot;)</span><br><span class="line">sum(&quot;col&quot;)</span><br><span class="line">该四种方法只适用于数值型的GroupedData对象</span><br></pre></td></tr></table></figure><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.sort(<span class="string">"age"</span>).show(<span class="number">5</span>)</span><br><span class="line">students.sort($<span class="string">"age"</span>.desc).show(<span class="number">5</span>)</span><br><span class="line">students.sort(<span class="string">"age"</span>.desc).show(<span class="number">5</span>)<span class="comment">//不可执行</span></span><br></pre></td></tr></table></figure><h3 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newstudents=students.toDF(<span class="string">"newid"</span>, <span class="string">"newage"</span>, <span class="string">"newname"</span>)</span><br><span class="line">newstudents.printSchema</span><br></pre></td></tr></table></figure><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Score</span>(<span class="params">id:<span class="type">String</span>,course_id:<span class="type">String</span>,score:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">scores_rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/scores.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Score</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">scores</span> </span>= scores_rdd.toDF()</span><br><span class="line">students.join(scores, students(<span class="string">"id"</span> )===scores(<span class="string">"id"</span>), <span class="string">"outer"</span>).show</span><br></pre></td></tr></table></figure><h2 id="案例练习"><a href="#案例练习" class="headerlink" title="案例练习"></a>案例练习</h2><h3 id="Spark-SQL基本案例1"><a href="#Spark-SQL基本案例1" class="headerlink" title="Spark SQL基本案例1"></a>Spark SQL基本案例1</h3><p><strong>scores.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scores 学号、课程编号、成绩</span><br><span class="line">160201,1,60</span><br><span class="line">160101,2,90</span><br><span class="line">160301,1,70</span><br><span class="line">160202,3,70</span><br><span class="line">160102,3,50</span><br><span class="line">160102,4,95</span><br><span class="line">160302,1,98</span><br><span class="line">160303,2,57</span><br><span class="line">160103,3,64</span><br><span class="line">160304,3,50</span><br><span class="line">160201,2,77</span><br><span class="line">160101,3,57</span><br><span class="line">160301,3,72</span><br><span class="line">160202,2,80</span><br><span class="line">160102,2,58</span><br><span class="line">160102,3,97</span><br><span class="line">160302,4,91</span><br><span class="line">160303,1,67</span><br><span class="line">160103,2,62</span><br><span class="line">160304,4,71</span><br></pre></td></tr></table></figure></p><p><strong>students.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">160201,Michael,17</span><br><span class="line">160101,Andy,23</span><br><span class="line">160301,justin,23</span><br><span class="line">160202,John,22</span><br><span class="line">160102,Herry,17</span><br><span class="line">160203,Brewster,18</span><br><span class="line">160302,Brice,20</span><br><span class="line">160303,Justin,25</span><br><span class="line">160103,Jerry,22</span><br><span class="line">160304,Tom,24</span><br></pre></td></tr></table></figure></p><p>1.利用反射机制创建students.txt对应的DataFrame，其中包含id、name、age三个字段。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd=sc.textFile(&quot;hdfs://master:9000/sqldata/students.txt&quot;).map(_.split(&quot;,&quot;)).map(p=&gt;Student(p(0),p(1),p(2).toInt))</span><br><span class="line">students = rdd.toDF</span><br><span class="line">students.show</span><br></pre></td></tr></table></figure></p><p>2.查看所有学生的姓名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">students.select(&apos;name).show</span><br></pre></td></tr></table></figure></p><p>3.查询所有学生的年龄，并按照年龄降序排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">students.select(&apos;age).show</span><br><span class="line">students.sort(&quot;age&quot;.desc).show()</span><br></pre></td></tr></table></figure></p><p>4.查询年龄小于19或年龄大于21的所有学生。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">students.where($&quot;age&quot;&gt;21 || $&quot;age&quot;&lt;19).show</span><br></pre></td></tr></table></figure></p><p>5.添加scholarship字段，每个学生的scholarship项是其龄*20。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val newstudents = students.withColumn(&quot;scholarship&quot;,$&quot;age&quot;*20)</span><br><span class="line">newstudents.show(5)</span><br></pre></td></tr></table></figure></p><p>6.将添加scholarship字段后的DataFrame分别以Parquet和JSON格式保存至HDFS上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">newstudents.write.format(&quot;json&quot;).save(&quot;hdfs://master:9000/sqldata/newstudents.json&quot;)</span><br><span class="line">newstudents.write.format(&quot;parquet&quot;).save(&quot;hdfs://master:9000/sqldata/newstudents.parquet&quot;)</span><br></pre></td></tr></table></figure></p><p>7.利用SQL语句实现案例1中2-4。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.registerTempTable(&quot;tb_students&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;SELECT name FROM tb_students WHERE age&gt;=19 AND age&lt;=21&quot;)</span><br></pre></td></tr></table></figure></p><p>8.利用自定义接口创建scores.txt对应的DataFrame，其中包含id、course_id、score三个字段。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val scores_rdd=sc.textFile(&quot;hdfs://master:9000/sqldata/scores.txt&quot;).map(_.split(&quot;,&quot;)).map(p =&gt; Score(p(0), p(1), p(2).trim.toInt))</span><br><span class="line"></span><br><span class="line">val scores = scores.toDF()</span><br><span class="line">val newstudents = students.join(scores, students(&quot;id&quot; )==scores(&quot;id&quot;), &quot;outer&quot;)</span><br><span class="line">newstudents.show</span><br></pre></td></tr></table></figure></p><p>9.按照课程编号分组，查看每门课的平均成绩，并按课程编号升序排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newstudents.groupBy(&apos;course_id).mean(&quot;score&quot;).orderBy(&apos;course_id).show</span><br></pre></td></tr></table></figure></p><p>10.按照学生编号分组，查看个学生的姓名和其所有课程的平均成绩，并在统计结果中筛选出平均成绩大于72的同学。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val newstudents = students.join(scores, students(&quot;id&quot; )==scores(&quot;id&quot;), &quot;outer&quot;)</span><br><span class="line">newstudents.groupBy($&quot;name&quot;).mean(&quot;score&quot;).where($&quot;avg(score)&quot;&gt;72).orderBy(&quot;name&quot;).show</span><br></pre></td></tr></table></figure></p><h3 id="Spark-SQL基本案例2"><a href="#Spark-SQL基本案例2" class="headerlink" title="Spark SQL基本案例2"></a>Spark SQL基本案例2</h3><p><strong>students.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">学号/姓名/性别/年龄/学年/系别/</span><br><span class="line">160201,Michael,male,37,2012,2,2</span><br><span class="line">160101,Rose,female,33,2011,1,1</span><br><span class="line">160301,justin,male,23,2013,3,3</span><br><span class="line">160202,John,male,22,2012,2,2</span><br><span class="line">160102,Lucy,female,27,2011,2,1</span><br><span class="line">160203,Brewster,male,37,2012,1,2</span><br><span class="line">160302,Susan,female,30,2013,1,3</span><br><span class="line">160303,Justin,male,23,2013,3,3</span><br><span class="line">160103,John,male,22,2011,2,1</span><br><span class="line">160304,Lucy,female,27,2013,3,3</span><br></pre></td></tr></table></figure></p><p><strong>departments.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Computer,1</span><br><span class="line">Math,2</span><br><span class="line">Art,3</span><br></pre></td></tr></table></figure></p><p><strong>projects.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">学号/创新项目编号/学年/</span><br><span class="line">160201,XC2014001,2014,16,64,2</span><br><span class="line">160101,XC201213,2012,32,48,3</span><br><span class="line">160301,RW201103,2011,32,48,3</span><br><span class="line">160202,XC2014002,2014,16,64,1</span><br><span class="line">160102,XC2013002,2013,16,64,2</span><br><span class="line">160102,XC2012011,2012,16,48,2</span><br><span class="line">160302,RW201401,2014,32,32,2</span><br><span class="line">160304,RW201503,2015,16,32,1</span><br></pre></td></tr></table></figure></p><p><strong>scores.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">160201,90</span><br><span class="line">160101,83</span><br><span class="line">160301,80</span><br><span class="line">160202,70</span><br><span class="line">160102,67</span><br><span class="line">160102,89</span><br><span class="line">160302,91</span><br><span class="line">160303,58</span><br><span class="line">160103,64</span><br></pre></td></tr></table></figure></p><p><strong>scholarship.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">160201,2013,2000</span><br><span class="line">160201,2014,3000</span><br><span class="line">160102,2013,2000</span><br><span class="line">160101,2013,2000</span><br><span class="line">160301,2014,2000</span><br><span class="line">160302,2014,2000</span><br><span class="line">160302,2015,3000</span><br><span class="line">160302,2016,4000</span><br></pre></td></tr></table></figure></p><p>1.查询创建的五个表的概要信息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>2.查询各院系学生总数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>3.查询各院系学生奖学金的总和并排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>4.查询各院系学生的平均学分绩值并排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>5.统计各学院每年学生参与创新项目所获得的创新学分数总数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;#SparkSQL DataFrame基础篇&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="sparksql" scheme="http://www.ihoge.cn/tags/sparksql/"/>
    
  </entry>
  
  <entry>
    <title>使用aconda3-5.1.0（Python3.6.4） 搭建pyspark远程部署</title>
    <link href="http://www.ihoge.cn/2018/anacondaPyspark.html"/>
    <id>http://www.ihoge.cn/2018/anacondaPyspark.html</id>
    <published>2018-04-13T17:59:21.000Z</published>
    <updated>2018-04-22T14:55:33.612Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>首次安装的环境搭配是这样的：<br> jdk8<br> hadoop2.6.5<br> spark2.1<br> scala2.12.4<br> Anaconda3-5.1.0<br>一连串的报错让人惊喜无限，尽管反复调整配置始终无法解决。<br><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqavez8o4aj310q094jsn.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqavfnihv5j316g0katdf.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqavgahjl2j315y0e00vm.jpg" alt=""></p><p>坑了一整天后最后最终发现是<strong>版本不兼容！！</strong>再次提醒自己一定要重视各组件版本的问题。这里最主要的是spark和Anaconda版本的兼容问题，为了兼容python3尽量用新版的spark。最终解决方案的版本搭配如下：<br> jdk8<br> hadoop2.7.5<br> spark2.3.0<br> scala2.11.12<br> Anaconda3-5.1.0</p><h3 id="一、VM安装Ubuntu16-04虚拟机"><a href="#一、VM安装Ubuntu16-04虚拟机" class="headerlink" title="一、VM安装Ubuntu16.04虚拟机"></a>一、VM安装Ubuntu16.04虚拟机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vim</span><br><span class="line">sudo apt-get install openssh-server</span><br><span class="line"></span><br><span class="line"># 配置ssh免密登陆</span><br><span class="line">ssh localhost</span><br><span class="line">ssh-keygen -t rsa //一路回车</span><br><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line"></span><br><span class="line">sudo vi /etc/hosts //添加各个节点ip</span><br><span class="line">192.168.221.132 master</span><br><span class="line">192.168.221.133 slave1</span><br><span class="line">192.168.221.134 slave2</span><br><span class="line"></span><br><span class="line"># sudo vi /etc/hostname</span><br><span class="line">master</span><br></pre></td></tr></table></figure><h3 id="二、配置profile环境变量"><a href="#二、配置profile环境变量" class="headerlink" title="二、配置profile环境变量"></a>二、配置profile环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#Java</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_161</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jar</span><br><span class="line">#Hadoop</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">#Scala</span><br><span class="line">export SCALA_HOME=/home/hadoop/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br><span class="line">#Anaconda</span><br><span class="line">export PATH=/home/hadoop/anaconda3/bin:$PATH</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=/home/hadoop/anaconda3/bin/jupyter</span><br><span class="line">export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot;</span><br><span class="line">export PYSPARK_PYTHON=/home/hadoop/anaconda3/bin/python</span><br><span class="line">#Spark</span><br><span class="line">export SPARK_HOME=/home/hadoop/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="三、hadoop-六个配置文件"><a href="#三、hadoop-六个配置文件" class="headerlink" title="三、hadoop 六个配置文件"></a>三、hadoop 六个配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/home/hadoop/hadoop/jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"># core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/home/hadoop/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:50090&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># slaves</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="三、spark两个配置文件"><a href="#三、spark两个配置文件" class="headerlink" title="三、spark两个配置文件"></a>三、spark两个配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># spark-env.sh</span><br><span class="line">#java</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_161</span><br><span class="line">#scala</span><br><span class="line">export SCALA_HOME=/home/hadoop/scala</span><br><span class="line">#hadoop</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=/home/hadoop/hadoop/etc/hadoop</span><br><span class="line">#spark</span><br><span class="line">export SPARK_HOME=/home/hadoop/spark</span><br><span class="line">export SPARK_LOCAL_DIRS=/home/hadoop/spark</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/home/hadoop/hadoop/bin/hadoop classpath)</span><br><span class="line">export SPARK_WORKER_CORES=1</span><br><span class="line">export SPARK_WORKER_INSTANCES=1</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line">export SPARK_MASTER_IP=master</span><br><span class="line">export SPARK_LIBRARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native</span><br><span class="line"></span><br><span class="line"># slaves</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="四、解压缩文件"><a href="#四、解压缩文件" class="headerlink" title="四、解压缩文件"></a>四、解压缩文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scp jdk-8u161-linux-x64.tar hadoop@master:~</span><br><span class="line">scp Anaconda3-5.1.0-Linux-x86_64.sh hadoop@master:~</span><br><span class="line">scp -r hadoop/ hadoop@master:~</span><br><span class="line">scp -r scala/ hadoop@master:~</span><br><span class="line">scp -r spark/ hadoop@master:~</span><br><span class="line"></span><br><span class="line">tar -xvf jdk-8u161-linux-x64.tar -C ./</span><br><span class="line"></span><br><span class="line">source ~/.profile</span><br><span class="line">分别查看jdk版本、hadoop版本、scala版本</span><br><span class="line"></span><br><span class="line"># 集群模式启动spark查看jps</span><br><span class="line">spark-shell --master spark://master:7077 --executor-memory 512m --total-executor-cores 2</span><br></pre></td></tr></table></figure><h3 id="五、安装Anaconda"><a href="#五、安装Anaconda" class="headerlink" title="五、安装Anaconda"></a>五、安装Anaconda</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-5.1.0-Linux-x86_64.sh -b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建配置jupyter_notebook_config.py</span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br><span class="line"></span><br><span class="line">c = get_config()</span><br><span class="line">c.IPKernelApp.pylab = &apos;inline&apos;</span><br><span class="line">c.NotebookApp.ip = &apos;*&apos; </span><br><span class="line">c.NotebookApp.open.browser = False</span><br><span class="line">c.NotebookApp.password = u&apos;&apos;</span><br><span class="line">c.NotebookApp.port = 8888</span><br></pre></td></tr></table></figure><h3 id="六、关机后克隆出两个新节点并配置相关内容"><a href="#六、关机后克隆出两个新节点并配置相关内容" class="headerlink" title="六、关机后克隆出两个新节点并配置相关内容"></a>六、关机后克隆出两个新节点并配置相关内容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">sudo vi /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="七、远程测试pyspark集群"><a href="#七、远程测试pyspark集群" class="headerlink" title="七、远程测试pyspark集群"></a>七、远程测试pyspark集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 服务器端启动集群</span><br><span class="line">start-all.sh</span><br><span class="line">spark/sbin/start-all.sh</span><br><span class="line"></span><br><span class="line"># hadoop和spark的进程都显示正常后开始启动pyspark</span><br><span class="line">1、local模式运行</span><br><span class="line">pyspark</span><br><span class="line"></span><br><span class="line">2、Stand Alone运行模式</span><br><span class="line">MASTER=spark://master:7077 pyspark --num-executors 1 --total-executor-cores 3 --executor-memory 512m</span><br><span class="line"></span><br><span class="line">3、Hadoop YARN-client 模式</span><br><span class="line">HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop pyspark --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>然后在远程Web端输入192.168.221.132:8888<br>页面打开后需要输入验证信息（第一次验证即可）：<br>输入上图<code>token</code>后面的字符串和用户密码<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqbhup77mqj31fu0j0wu2.jpg" alt=""><br>输入<code>sc</code>测试<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqbhwgb3wdj310m0dowfj.jpg" alt=""></p><p>至此，aconda3-5.1.0（Python3.6.4） 搭建pyspark远程服务器部署成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;首次安装的环境搭配是这样的：&lt;br&gt; jdk8&lt;br&gt; hadoop2.6.5&lt;br&gt; spark2.1
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="environment" scheme="http://www.ihoge.cn/tags/environment/"/>
    
  </entry>
  
  <entry>
    <title>Hive集成Spark+Sql</title>
    <link href="http://www.ihoge.cn/2018/HiveSpark.html"/>
    <id>http://www.ihoge.cn/2018/HiveSpark.html</id>
    <published>2018-04-13T03:59:21.000Z</published>
    <updated>2018-04-22T14:55:36.971Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、Hive安装"><a href="#一、Hive安装" class="headerlink" title="一、Hive安装"></a>一、Hive安装</h2><h3 id="1-Hive简介"><a href="#1-Hive简介" class="headerlink" title="1.Hive简介"></a>1.Hive简介</h3><p>​    Hive是Facebook开发的构建于Hadoop集群之上的数据仓库应用，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。</p><p>​    Hive是一个可以提供有效的、合理的且直观的组织和使用数据的模型，即使对于经验丰富的Java开发工程师来说，将这些常见的数据运算对应到底层的MapReduce Java API也是令人敬畏的。Hive可以帮用户做这些工作，用户就可以集中精力关注查询本身了。Hive可以将大多数的查询转换为MapReduce任务。Hive最适合于数据仓库应用程序，使用该应用程序进行相关的静态数据分析，不需要快速响应给出结果，而且数据本身也不会频繁变化。</p><p>​    Hive不是一个完整的数据库。Hadoop以及HDFS的设计本身约束和局限性限制了Hive所能胜任的工作。最大的限制就是Hive不支持记录级别的更新、插入或者删除。用户可以通过查询生成新表或将查询结果导入到文件中去。因为，Hadoop是一个面向批处理的系统，而MapReduce启动任务启动过程需要消耗很长时间，所以Hive延时也比较长。Hive还不支持事务。因此，Hive不支持联机事务处理（OLTP），更接近于一个联机分析技术（OLAP）工具，但是，目前还没有满足“联机”部分。</p><p>​    Hive提供了一系列的工具，可以用来进行数据提取转化加载(ETL)，其中，ETL是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。因此，Hive是最适合数据仓库应用程序的，它可以维护海量数据，而且可以对数据进行挖掘，然后形成意见和报告等。</p><p>​    因为大多数的数据仓库应用程序是基于SQL的关系数据库现实的，所以，Hive降低了将这些应用程序移植到Hadoop上的障碍。如果用户懂得SQL，那么学习使用Hive会很容易。因为Hive定义了简单的类SQL 查询语言——HiveQL，这里值得一提的是，与SQLServer、Oracle相比，HiveQL和MySQL提供的SQL语言更接近。同样的，相对于其他的Hadoop语言和工具来说，Hive也使得开发者将基于SQL的应用程序移植到Hadoop变得更加容易。</p><h3 id="2-Hive安装"><a href="#2-Hive安装" class="headerlink" title="2.Hive安装"></a>2.Hive安装</h3><p>​    接下来，开始Hive的安装，安装Hive之前，首先需要装好Hadoop和Spark。在<strong><a href="https://hive.apache.org/" target="_blank" rel="noopener">Hive官网</a></strong>可下载最新版本Hive，并且能够查阅版本改动说明，本次课程采用1.2.2版本进行安装。可以采用WinSCP传输apache-hive-1.2.2-bin.tar至虚拟机“下载”文件夹中，再进行后续安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载                                              # 进入下载文件夹</span><br><span class="line">sudo tar -zxf apache-hive-1.2.2-bin.tar.gz -C /usr/local    # 安装至/usr/local文件夹内</span><br><span class="line">cd /usr/local                                         # 进入/usr/local文件夹</span><br><span class="line">sudo mv ./apache-hive-1.2.2-bin/ ./hive               # 更名为hive</span><br><span class="line">sudo chown -R hadoop ./hive                           # 修改hive权限</span><br><span class="line">mkdir -p /usr/local/hive/warehouse                    # 创建元数据存储文件夹</span><br><span class="line">sudo chmod a+rwx /usr/local/hive/warehouse            # 修改文件权限</span><br></pre></td></tr></table></figure><p>然后添加Hive安装路径至系统环境变量</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure><p>添加下述路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Hive</span></span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>并使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure><p>修改hive读取spark的jar包地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/bin</span><br><span class="line">vim hive</span><br></pre></td></tr></table></figure><p>修改为</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add Spark assembly jar to the classpath</span></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$SPARK_HOME</span>"</span> ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  sparkAssemblyPath=`ls <span class="variable">$&#123;SPARK_HOME&#125;</span>/jars/*.jar`</span><br><span class="line">  CLASSPATH=<span class="string">"<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;sparkAssemblyPath&#125;</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>然后采用hive默认配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/conf</span><br><span class="line">cp hive-default.xml.template hive-default.xml</span><br></pre></td></tr></table></figure><p>尝试启动Hive，此时启动是以本地模式进行启动，能正常启动则说明安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>​    若出现jline等jar包错误，则需要进入到hadoop安装目录下的share/hadoop/yarn/lib下删除jline-0.9.94.jar文件，再启动hive即可（因为高版本的Hadoop对Hive有捆绑）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/share/hadoop/yarn/lib</span><br><span class="line">rm -rf jline-0.9.94.jar</span><br></pre></td></tr></table></figure><p>###3. Hive的基本配置</p><p>​    在安装Hive时，默认情况下，元数据存储在Derby数据库中。Derby是一个完全用Java编写的数据库，所以可以跨平台，但需要在JVM中运行 。因为多用户和系统可能需要并发访问元数据存储，所以默认的内置数据库并不适用于生产环境。任何一个适用于JDBC进行连接的数据库都可用作元数据库存储，这里我们把MySQL作为存储元数据的数据库。接下来，我们分别对这两种方式进行介绍，即使用Derby数据库的方式和使用MySQL数据库的方式。</p><h4 id="3-1-使用Derby作为元数据库"><a href="#3-1-使用Derby作为元数据库" class="headerlink" title="3.1 使用Derby作为元数据库"></a>3.1 使用Derby作为元数据库</h4><p>​    本地模式中，用户的“表”等元数据信息，都默认存储在file://user/hive/warehouse，对于其他模式默认存储路径是hdfs://namenode_server/user/hive/warehouse。使用如下命令编辑hive-site.xml文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>在hive-site.xml文件添加以下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:derby:;databaseName=/usr/local/hive/metastore_db;create=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                           </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​    若要以伪分布式模式和分布式模式配置Hive，只需根据Hadoop配置文件core-site.xml中fs.defaultFS的值对hive.metastore.warehouse.dir 进行相应修改即可。配置完成之后即可启动Hive，然后尝试使用HiveQL命令创建表。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> derby;</span><br><span class="line"><span class="keyword">use</span> derby;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> x(a <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> x;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> x;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure><h4 id="3-2-使用MySQL作为元数据库"><a href="#3-2-使用MySQL作为元数据库" class="headerlink" title="3.2 使用MySQL作为元数据库"></a>3.2 使用MySQL作为元数据库</h4><h5 id="3-2-1-安装MySQL"><a href="#3-2-1-安装MySQL" class="headerlink" title="3.2.1 安装MySQL"></a>3.2.1 安装MySQL</h5><p>首先，查看并卸载系统自带的MySQL相关安装包（或之前安装过MySQL），命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install rpm</span><br><span class="line">rpm -qa | grep mysql</span><br></pre></td></tr></table></figure><p>若没有安装rpm工具，系统会有提示，按照提示安装即可。接下来查看是否有系统自带的MySQL相关安装包，若有，按下面命令删除：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -e --nodeps mysql-libs-xxxxxx</span><br></pre></td></tr></table></figure><p>注：xxxxx是已经安装的mysql的版本号，然后进行MySQL的安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install mysql-server</span><br></pre></td></tr></table></figure><p>安装完成后，启动设置MySQL服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><p>当然，还可使用下列命令进行额外设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chkconfig mysql on                              # 设置开机自动启动</span><br><span class="line">sudo /usr/bin/mysqladmin -u root password '123'      # 设置root用户密码</span><br></pre></td></tr></table></figure><p>接下来，创建hive用户及其数据库等，用于存放Hive的元数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/mysql/my.cnf</span><br><span class="line">注释掉：bind-address            = 127.0.0.1</span><br><span class="line"></span><br><span class="line">create database hive;</span><br><span class="line">grant all on *.* to hive@localhost identified by &apos;hive&apos;; </span><br><span class="line">flush privileges;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure><p>切换hive用户登陆</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u hive -p hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure><p>若能看到hive数据库存在，则说明创建成功。</p><h5 id="3-2-2-修改Hive配置"><a href="#3-2-2-修改Hive配置" class="headerlink" title="3.2.2 修改Hive配置"></a>3.2.2 修改Hive配置</h5><p>接下来，修改hive-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>输入下列信息</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>或者指定元数据文件夹</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive;createDatebaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                           </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword <span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后将JDBC文件放到hive的lib文件夹内，JDBC包的下载参考前述部分</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载</span><br><span class="line">cp mysql-connector-java-5.1.26-bin.jar /usr/local/hive/lib</span><br><span class="line">mkdir -p /usr/local/hive/tmp</span><br><span class="line">sudo chmod a+rwx /usr/local/hive/tmp</span><br></pre></td></tr></table></figure><p>也可从官网直接下载最新版jdbc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz</span><br></pre></td></tr></table></figure><p>然后进行解压安装。当然，如果之前删除了jline-0.9.94.jar，此时需要把hive对应的jar包放进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/hive/lib/jline-2.12.jar  /usr/local/hadoop/share/hadoop/yarn/lib</span><br></pre></td></tr></table></figure><p>然后尝试启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line">start-all.sh</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>成启动后，即可输入hive –help查看hive常用命令。</p><h2 id="二、Hive使用"><a href="#二、Hive使用" class="headerlink" title="二、Hive使用"></a>二、Hive使用</h2><h3 id="1-Hive基本数据类型"><a href="#1-Hive基本数据类型" class="headerlink" title="1.Hive基本数据类型"></a>1.Hive基本数据类型</h3><p>首先，我们简单叙述一下HiveQL的基本数据类型。</p><p>Hive支持基本数据类型和复杂类型, 基本数据类型主要有数值类型(INT、FLOAT、DOUBLE ) 、布尔型和字符串, 复杂类型有三种:ARRAY、MAP 和 STRUCT。</p><h4 id="1-1-基本数据类型"><a href="#1-1-基本数据类型" class="headerlink" title="1.1 基本数据类型"></a>1.1 基本数据类型</h4><ul><li>TINYINT: 1个字节</li><li>SMALLINT: 2个字节</li><li>INT: 4个字节</li><li>BIGINT: 8个字节</li><li>BOOLEAN: TRUE/FALSE </li><li>FLOAT: 4个字节，单精度浮点型</li><li>DOUBLE: 8个字节，双精度浮点型STRING 字符串</li></ul><h4 id="1-2-复杂数据类型"><a href="#1-2-复杂数据类型" class="headerlink" title="1. 2 复杂数据类型"></a>1. 2 复杂数据类型</h4><ul><li>ARRAY: 有序字段</li><li>MAP: 无序字段</li><li>STRUCT: 一组命名的字段</li></ul><h3 id="2-常用的HiveQL操作命令"><a href="#2-常用的HiveQL操作命令" class="headerlink" title="2.常用的HiveQL操作命令"></a>2.常用的HiveQL操作命令</h3><p>​    Hive常用的HiveQL操作命令主要包括：数据定义、数据操作。接下来详细介绍一下这些命令即用法（想要了解更多请参照《Hive编程指南》一书）。</p><h4 id="2-1-数据定义"><a href="#2-1-数据定义" class="headerlink" title="2.1 数据定义"></a>2.1 数据定义</h4><p>主要用于创建修改和删除数据库、表、视图、函数和索引。</p><ul><li><p>创建、修改和删除数据库</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists hive;       #创建数据库</span><br><span class="line">show databases;                           #查看Hive中包含数据库</span><br><span class="line">show databases like 'h.*';                #查看Hive中以h开头数据库</span><br><span class="line">describe databases;                       #查看hive数据库位置等信息</span><br><span class="line">alter database hive set dbproperties;     #为hive设置键值对属性</span><br><span class="line">use hive;                                 #切换到hive数据库下</span><br><span class="line">drop database if exists hive;             #删除不含表的数据库</span><br><span class="line">drop database if exists hive cascade;     #删除数据库和它中的表</span><br></pre></td></tr></table></figure><p>注意，除 dbproperties属性外，数据库的元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置，没有办法删除或重置数据库属性。</p></li><li><p>创建、修改和删除表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#创建内部表（管理表）</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'username'</span>,</span><br><span class="line">      pwd <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'password'</span>,</span><br><span class="line">      address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      <span class="keyword">comment</span>  <span class="string">'home address'</span>,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt; <span class="keyword">comment</span> <span class="string">'number,sex'</span>) </span><br><span class="line">      <span class="keyword">comment</span> <span class="string">'description of the table'</span>  </span><br><span class="line">     tblproperties(<span class="string">'creator'</span>=<span class="string">'me'</span>,<span class="string">'time'</span>=<span class="string">'2016.1.1'</span>); </span><br><span class="line">#创建外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> usr2(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">      pwd <span class="keyword">string</span>,</span><br><span class="line">  address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt;) </span><br><span class="line">      <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">     location <span class="string">'/usr/local/hive/warehouse/hive.db/usr'</span>; </span><br><span class="line">#创建分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> usr3(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">      pwd <span class="keyword">string</span>,</span><br><span class="line">      address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt;) </span><br><span class="line">      partitioned <span class="keyword">by</span>(city <span class="keyword">string</span>,state <span class="keyword">string</span>);    </span><br><span class="line">#复制usr表的表模式  </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr1 <span class="keyword">like</span> hive.usr;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">in</span> hive;  </span><br><span class="line">show tables 'u.*';        #查看hive中以u开头的表</span><br><span class="line">describe hive.usr;        #查看usr表相关信息</span><br><span class="line">alter table usr rename to custom;      #重命名表 </span><br><span class="line">#为表增加一个分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> </span><br><span class="line">     <span class="keyword">partition</span>(city=”beijing”,state=”China”) </span><br><span class="line">     location <span class="string">'/usr/local/hive/warehouse/usr2/China/beijing'</span>; </span><br><span class="line">#修改分区路径</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">partition</span>(city=”beijing”,state=”China”)</span><br><span class="line">     <span class="keyword">set</span> location <span class="string">'/usr/local/hive/warehouse/usr2/CH/beijing'</span>;</span><br><span class="line">#删除分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">drop</span> <span class="keyword">if</span> <span class="keyword">exists</span>  <span class="keyword">partition</span>(city=”beijing”,state=”China”)</span><br><span class="line">#修改列信息</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr <span class="keyword">change</span> <span class="keyword">column</span> pwd <span class="keyword">password</span> <span class="keyword">string</span> <span class="keyword">after</span> address;</span><br><span class="line">alter table usr add columns(hobby string);                  #增加列</span><br><span class="line">alter table usr replace columns(uname string);              #删除替换列</span><br><span class="line">alter table usr set tblproperties('creator'='liming');      #修改表属性</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">partition</span>(city=”beijing”,state=”China”)    #修改存储属性</span><br><span class="line"><span class="keyword">set</span> fileformat sequencefile;             </span><br><span class="line">use hive;                                                   #切换到hive数据库下</span><br><span class="line">drop table if exists usr1;                                  #删除表</span><br><span class="line">drop database if exists hive cascade;                       #删除数据库和它中的表</span><br></pre></td></tr></table></figure></li><li><p>视图和索引的创建、修改和删除</p><p>基本语法格式</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create view view_name as....;                #创建视图</span><br><span class="line">alter view view_name set tblproperties(…);   #修改视图</span><br></pre></td></tr></table></figure><p>因为视图是只读的，所以 对于视图只允许改变元数据中的 tblproperties属性。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#删除视图</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> <span class="keyword">if</span> <span class="keyword">exists</span> view_name;</span><br><span class="line">#创建索引</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> index_name <span class="keyword">on</span> <span class="keyword">table</span> table_name(partition_name/column_name)  </span><br><span class="line"><span class="keyword">as</span> <span class="string">'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'</span> <span class="keyword">with</span> <span class="keyword">deferred</span> rebuild....;</span><br></pre></td></tr></table></figure><p>这里’org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’是一个索引处理器，即一个实现了索引接口的Java类，另外Hive还有其他的索引实现。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter index index_name on table table_name partition(...)  rebulid;   #重建索引</span><br></pre></td></tr></table></figure><p>如果使用 deferred rebuild，那么新索引成空白状态，任何时候可以进行第一次索引创建或重建。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show formatted index on table_name;                       #显示索引</span><br><span class="line">drop index if exists index_name on table table_name;      #删除索引</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2-数据操作"><a href="#2-2-数据操作" class="headerlink" title="2.2 数据操作"></a>2.2 数据操作</h4><p>主要实现的是将数据装载到表中（或是从表中导出），并进行相应查询操作</p><ul><li><p>向表中装载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.course(cid <span class="built_in">int</span>,<span class="keyword">sid</span> <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p>向表中装载数据有两种方法：从文件中导入和通过查询语句插入。</p><ul><li><p>从文件中导入</p><p>假如这个表中的记录存储于文件stu.txt中，该文件的存储路径为usr/local/hadoop/examples/stu.txt，内容如下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 Hello</span><br><span class="line">2 World</span><br><span class="line">3 CDA</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/usr/local/hadoop/examples/stu.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure></li><li><p>通过查询语句插入</p><p>使用如下命令，创建stu1表，它和stu表属性相同，我们要把从stu表中查询得到的数据插入到stu1中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu1 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>上面是创建表，并直接向新表插入数据；若表已经存在，向表中插入数据需执行以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu1 <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu <span class="keyword">where</span>（条件）;</span><br></pre></td></tr></table></figure><p>这里关键字overwrite的作用是替换掉表（或分区）中原有数据，换成into关键字，直接追加到原有内容后。</p></li></ul></li><li><p>写入临时文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/usr/local/hadoop/tmp/stu'</span>  <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure></li><li><p>查询操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,</span><br><span class="line">  <span class="keyword">case</span> </span><br><span class="line">  <span class="keyword">when</span> <span class="keyword">id</span>=<span class="number">1</span> <span class="keyword">then</span> <span class="string">'first'</span> </span><br><span class="line">  <span class="keyword">when</span> <span class="keyword">id</span>=<span class="number">2</span> <span class="keyword">then</span> <span class="string">'second'</span></span><br><span class="line">  <span class="keyword">else</span> <span class="string">'third'</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-3-连接"><a href="#2-3-连接" class="headerlink" title="2.3 连接"></a>2.3 连接</h4><p>​    连接（join）是将两个表中在共同数据项上相互匹配的那些行合并起来, HiveQL 的连接分为内连接、左向外连接、右向外连接、全外连接和半连接 5 种。</p><ul><li><p>内连接(等值连接)</p><p>内连接使用比较运算符根据每个表共有的列的值匹配两个表中的行。</p><p>首先，我们先把以下内容插入到course表中（自行完成）。</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 3</span><br><span class="line">2 1</span><br><span class="line">3 1</span><br></pre></td></tr></table></figure><p>​    下面, 查询stu和course表中学号相同的所有行，命令如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>左连接</p><p>​    左连接的结果集包括“LEFT OUTER”子句中指定的左表的所有行, 而不仅仅是连接列所匹配的行。如果左表的某行在右表中没有匹配行, 则在相关联的结果集中右表的所有选择列均为空值，命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>右连接</p><p>​    右连接是左向外连接的反向连接,将返回右表的所有行。如果右表的某行在左表中没有匹配行,则将为左表返回空值。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">right</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>全连接</p><p>​    全连接返回左表和右表中的所有行。当某行在另一表中没有匹配行时,则另一个表的选择列表包含空值。如果表之间有匹配行,则整个结果集包含基表的数据值。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>半连接</p><p>​    半连接是 Hive 所特有的, Hive 不支持 in 操作,但是拥有替代的方案; left semi join, 称为半连接, 需要注意的是连接的表不能在查询的列中,只能出现在 on 子句中。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.* <span class="keyword">from</span> stu <span class="keyword">left</span> semi <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><h2 id="三、Spark与Hive集成"><a href="#三、Spark与Hive集成" class="headerlink" title="三、Spark与Hive集成"></a>三、Spark与Hive集成</h2><h3 id="1-安装Spark"><a href="#1-安装Spark" class="headerlink" title="1.安装Spark"></a>1.安装Spark</h3><p>​    为了让Spark能够访问Hive，必须为Spark添加Hive支持。Spark官方提供的预编译版本，通常是不包含Hive支持的，需要采用源码编译，编译得到一个包含Hive支持的Spark版本。首先测试一下电脑上已经安装的Spark版本是否支持Hive</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>这样就启动进入了spark-shell，然后输入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br></pre></td></tr></table></figure><p>如果报错，则说明spark无法识别org.apache.spark.sql.hive.HiveContext，这时我们就需要采用源码编译方法得到支持hive的spark版本。</p><ul><li><p>下载源码文件</p><p>​    进入官网后，可以按照下图配置选择“2.1.0(Dec 28, 2016)”和“SourceCode”，然后，在图中红色方框内，有个“Download Spark: spark-2.1.0.tgz”的下载链接，点击该链接就可以下载Spark源码文件了。</p></li></ul><ul><li><p>编译过程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/spark-2.0.2</span><br><span class="line">./dev/make-distribution.sh —tgz —name h27hive -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.1 -Phive -Phive-thriftserver -DskipTests</span><br></pre></td></tr></table></figure><p>或可选择直接安装已编译好的版本，把下好的<code>spark-2.0.2-bin-h27hive.tgz</code>放到下载文件夹内</p></li><li><p>Spark解压安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载                                              # 进入下载文件夹</span><br><span class="line">sudo tar -zxf spark-2.0.2-bin-h27hive.tgz -C /usr/local   # 安装至/usr/local文件夹内</span><br><span class="line">cd /usr/local                                         # 进入/usr/local文件夹</span><br><span class="line">sudo mv ./spark-1.4.0-bin-hadoop2.4/ ./spark          # 更名为spark</span><br><span class="line">sudo chown -R hadoop ./spark                          # 修改sqoop权限</span><br></pre></td></tr></table></figure></li><li><p>添加环境变量</p><p>注，如果电脑上已经装了另一个spark，此处可不增设环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure><p>添加spark安装路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">spark</span></span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>并保存修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure></li><li><p>修改Spark配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf                              # 进入spark配置文件夹</span><br><span class="line">sudo cp spark-env.sh.template spark-env.sh            # 复制spark-env临时文件为配置文件</span><br><span class="line">vim spark-env.sh                                      # 编辑spark配置文件</span><br></pre></td></tr></table></figure><p>添加下述配置信息</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></table></figure><p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。在伪分布式模式下仅测试是否安装成功时，其他配置暂时可不做修改。</p></li><li><p>运行样例程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">bin/run-example SparkPi 2&gt;&amp;1 | grep "Pi is"</span><br></pre></td></tr></table></figure></li><li><p>放置Hive配置文件</p><p>为了让Spark能够访问Hive，需要把Hive的配置文件hive-site.xml拷贝到Spark的conf目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf</span><br><span class="line">cp /usr/local/hive/conf/hive-site.xml .</span><br><span class="line">ll</span><br></pre></td></tr></table></figure></li><li><p>测试是否集成成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>然后输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-在Hive中创建数据库和表"><a href="#2-在Hive中创建数据库和表" class="headerlink" title="2.在Hive中创建数据库和表"></a>2.在Hive中创建数据库和表</h3><p>首先启动MySQL数据库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure><p>​    由于Hive是基于Hadoop的数据仓库，使用HiveQL语言撰写的查询语句，最终都会被Hive自动解析成MapReduce任务由Hadoop去具体执行，因此，需要启动Hadoop，然后再启动Hive。<br>然后执行以下命令启动Hadoop：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>Hadoop启动成功以后，可以再启动Hive：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>然后在hive命令提示符内进行操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> sparktest;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> sparktest.student(</span><br><span class="line">&gt; <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">&gt; <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">&gt; gender <span class="keyword">string</span>,</span><br><span class="line">&gt; age <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">use</span> sparktest;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'Xueqian'</span>,<span class="string">'F'</span>,<span class="number">23</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'Weiliang'</span>,<span class="string">'M'</span>,<span class="number">24</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>通过上面操作，我们就在Hive中创建了sparktest.student表，这个表有两条数据。</p><h3 id="3-连接Hive读写数据"><a href="#3-连接Hive读写数据" class="headerlink" title="3.连接Hive读写数据"></a>3.连接Hive读写数据</h3><p>​    现在我们看如何使用Spark读写Hive中的数据。注意，操作到这里之前，你一定已经按照前面的各个操作步骤，启动了Hadoop、Hive、MySQL和spark-shell（包含Hive支持）。在进行编程之前，我们需要做一些准备工作，我们需要修改“/usr/local/sparkwithhive/conf/spark-env.sh”这个配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf/</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><p>这样就使用vim编辑器打开了spark-env.sh这个文件，输入下面内容：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$CLASSPATH</span>:/usr/<span class="built_in">local</span>/hive/lib</span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/usr/<span class="built_in">local</span>/hive/conf</span><br><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/usr/<span class="built_in">local</span>/hive/lib/mysql-connector-java-5.1.26-bin.jar</span><br></pre></td></tr></table></figure><p>保存并推出，然后启动spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>然后在shell界面中输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="string">"spark-warehouse"</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark Hive Example"</span>).config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line">sql(<span class="string">"SELECT * FROM sparktest.student"</span>).show()</span><br></pre></td></tr></table></figure><p>然后再开一个命令行界面，启动hive界面，查看spark-shell中对hive表插入数据的结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>然后输入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> sparktest;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>然后在spark-shell中进行数据插入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="comment">//下面我们设置两条数据表示两个学生信息</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="string">"3 Rongcheng M 26"</span>,<span class="string">"4 Guanhua M 27"</span>)).map(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment">//下面要设置模式信息</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"gender"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)))</span><br><span class="line"><span class="comment">//下面创建Row对象，每个Row对象都是rowRDD中的一行</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt))</span><br><span class="line"><span class="comment">//建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来</span></span><br><span class="line"><span class="keyword">val</span> studentDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"><span class="comment">//查看studentDF</span></span><br><span class="line">studentDF.show()</span><br><span class="line"><span class="comment">//下面注册临时表</span></span><br><span class="line">studentDF.registerTempTable(<span class="string">"tempTable"</span>)</span><br><span class="line">sql(<span class="string">"insert into sparktest.student select * from tempTable"</span>)</span><br></pre></td></tr></table></figure><p>然后切换到hive窗口，查看数据库内容变化</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>能够查询到新增数据结果，则说明操作成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、Hive安装&quot;&gt;&lt;a href=&quot;#一、Hive安装&quot; class=&quot;headerlink&quot; title=&quot;一、Hive安装&quot;&gt;&lt;/a&gt;一、Hive安装&lt;/h2&gt;&lt;h3 id=&quot;1-Hive简介&quot;&gt;&lt;a href=&quot;#1-H
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="environment" scheme="http://www.ihoge.cn/tags/environment/"/>
    
  </entry>
  
</feed>
