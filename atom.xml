<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Student</title>
  
  <subtitle>Life is short</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.ihoge.cn/"/>
  <updated>2018-08-05T10:14:15.900Z</updated>
  <id>http://www.ihoge.cn/</id>
  
  <author>
    <name>刘知行</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>EM 算法(Expectation Maximization Algorithm)</title>
    <link href="http://www.ihoge.cn/2018/EM.html"/>
    <id>http://www.ihoge.cn/2018/EM.html</id>
    <published>2018-08-04T10:20:21.000Z</published>
    <updated>2018-08-05T10:14:15.900Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EM-算法-Expectation-Maximization-Algorithm"><a href="#EM-算法-Expectation-Maximization-Algorithm" class="headerlink" title="EM 算法(Expectation Maximization Algorithm)"></a>EM 算法(Expectation Maximization Algorithm)</h1><hr><ul><li><ol><li>EM 算法简介</li></ol></li><li><ol start="2"><li>准备知识<ul><li>2.1 极大似然估计</li><li>2.2 Jensen 不等式</li></ul></li></ol></li><li><ol start="3"><li>EM 算法详解<ul><li>3.1 问题描述</li><li>3.2 EM 算法推倒</li><li>3.3 EM 算法流程</li></ul></li></ol></li><li><ol start="4"><li>EM 算法小结</li></ol></li></ul><hr><h2 id="1-EM-算法简介"><a href="#1-EM-算法简介" class="headerlink" title="1. EM 算法简介"></a>1. EM 算法简介</h2><ul><li><p>EM算法是一种迭代优化策略，由于它的计算方法中每一次迭代都分两步，其中一个为期望步（E步），另一个为极大步（M步），所以算法被称为EM算法（Expectation Maximization Algorithm）。EM算法受到缺失思想影响，最初是为了解决数据缺失情况下的参数估计问题，其算法基础和收敛有效性等问题在Dempster，Laird和Rubin三人于1977年所做的文章Maximum likelihood from incomplete data via the EM algorithm中给出了详细的阐述。其基本思想是：首先根据己经给出的观测数据，估计出模型参数的值；然后再依据上一步估计出的参数值估计缺失数据的值，再根据估计出的缺失数据加上之前己经观测到的数据重新再对参数值进行估计，然后反复迭代，直至最后收敛，迭代结束。 </p></li><li><p>EM算法作为一种数据添加算法，在近几十年得到迅速的发展，主要源于当前科学研究以及各方面实际应用中数据量越来越大的情况下，经常存在数据缺失或者不可用的的问题，这时候直接处理数据比较困难，而数据添加办法有很多种，常用的有神经网络拟合、添补法、卡尔曼滤波法等等，但是EM算法之所以能迅速普及主要源于它算法简单，稳定上升的步骤能非常可靠地找到“最优的收敛值”。随着理论的发展，EM算法己经不单单用在处理缺失数据的问题，运用这种思想，它所能处理的问题更加广泛。有时候缺失数据并非是真的缺少了，而是为了简化问题而采取的策略，这时EM算法被称为数据添加技术，所添加的数据通常被称为“潜在数据”，复杂的问题通过引入恰当的潜在数据，能够有效地解决我们的问题。</p></li></ul><h2 id="2-准备知识"><a href="#2-准备知识" class="headerlink" title="2. 准备知识"></a>2. 准备知识</h2><ul><li>介绍EM算法之前，我们需要介绍极大似然估计以及Jensen不等式。 </li></ul><h3 id="2-1-极大似然估计"><a href="#2-1-极大似然估计" class="headerlink" title="2.1 极大似然估计"></a>2.1 极大似然估计</h3><p>（1）举例说明：经典问题——学生身高问题 </p><ul><li>我们需要调查我们学校的男生和女生的身高分布。 假设你在校园里随便找了100个男生和100个女生。他们共200个人。将他们按照性别划分为两组，然后先统计抽样得到的100个男生的身高。假设他们的身高是服从正态分布的。但是这个分布的均值 $\mu $ 和方差 $\sigma^2$ 未知， 这连个参数就是我们要估计的。记作$\theta=[\mu, \sigma]^T$。</li><li>问题：我们知道样本所服从的概率分布和模型一些样本，需要求解该模型的参数。<br><img src="media/image1-1.png" alt="image1"></li><li>我们已知的有两个：样本服从的分布模型、随机抽取的样本；我们未知的有一个：模型的参数。根据已知条件，通过极大似然估计，求出未知参数。总的来说：极大似然估计就是用来估计模型参数的统计学方法。 </li></ul><p>（2）如何估计</p><ul><li>问题数学化：设样本集 $X=x_1,x_2,…,x_N$其中 N=100 ，  $p(x_i|\theta)$ 为概率。由于 100 个样本之间独立同分布，所以我同时抽到这100个男生的概率就是他们各自概率的乘积，也就是样本集X<br>中各个样本的联合概率，用下式表示：<br>$$L(\theta) = L(x_1,…,x_n;\theta)=\prod _{i=1}^np(x_i;\theta),\theta\in \Theta$$</li><li>这个概率反映了，在概率密度函数的参数是 $\theta$ 时，得到$X$这组样本的概率。 我们需要找到一个参数 $\theta$，使得抽到$X$这组样本的概率最大，也就是说需要其对应的似然函数$L(\theta)$最大。满足条件的$\theta$叫做$\theta$的最大似然估计量，记为$$\hat\theta=arg\max L(\theta) $$</li></ul><p>（3）求最大似然函数估计值的一般步骤 </p><ul><li>首先，写出似然函数：$$L(\theta)=L(x_1,…x_n;\theta)=\prod^n_{i=1}p(x_i;\theta),\theta \in \Theta$$</li><li>然后对似然函数取对数：$$H(\theta)=\ln\prod^n_{i=1}p(x_i;\theta)=\sum \ln p(x_i;\theta)$$</li><li>接着对上式求导，另导数为0，得到似然方程；</li><li>最后，求解似然方程，得到的参数 $\theta$ 即为所求。 </li></ul><h3 id="2-2-Jensen-不等式"><a href="#2-2-Jensen-不等式" class="headerlink" title="2.2 Jensen 不等式"></a>2.2 Jensen 不等式</h3><ul><li>设$f$是定义域为实数的函数，如果对于所有的实数$x$， $f(x)$的二次导数大雨等于0，那么$f$是<strong>凸函数</strong>。</li><li>Jensen不等式表述如下：如果$f$是凸函数，$X$是随机变量，那么：$E[f(X)]≥f(E[X])$。当且仅当$X$是常量时，上式取等号。其中，$E[X]$表示$X$的数学期望。</li><li>例如：下图中，$f$是凸函数，$X$是随机变量，有0.5的概率是$a$，有0.5的概率是$b$。$X$的期望值就是$a$和$b$的中值了。下图可以看出$E[f(X)]≥f(E[X])$成立。</li><li>⚠️注：<ol><li>Jensen不等式应用于凹函数时，不等号方向反向。当且仅当X<br>是常量时，Jensen不等式等号成立。 </li><li>关于凸函数，百度百科中是这样解释的——“对于实数集上的凸函数，一般的判别方法是求它的二阶导数，如果其二阶导数在区间上非负，就称为凸函数（向下凸）”。关于函数的凹凸性，百度百科中是这样解释的——“中国数学界关于函数凹凸性定义和国外很多定义是反的。国内教材中的凹凸，是指曲线，而不是指函数，图像的凹凸与直观感受一致，却与函数的凹凸性相反。只要记住“函数的凹凸性与曲线的凹凸性相反”就不会把概念搞乱了”。关于凹凸性这里，确实解释不统一，博主暂时以函数的二阶导数大于零定义凸函数，此处不会过多影响EM算法的理解，只要能够确定何时$E[F(X)]≥F(E[X])$或者$E[F(X)]≤F(E[X])$就可以。<br><img src="media/jensen.jpg" alt="jensen"></li></ol></li></ul><h2 id="3-EM-算法详解"><a href="#3-EM-算法详解" class="headerlink" title="3. EM 算法详解"></a>3. EM 算法详解</h2><h3 id="3-1-问题描述"><a href="#3-1-问题描述" class="headerlink" title="3.1 问题描述"></a>3.1 问题描述</h3><p>我们目前有100个男生和100个女生的身高，共200个数据，但是我们不知道这200个数据中哪个是男生的身高，哪个是女生的身高。假设男生、女生的身高分别服从正态分布，则每个样本是从哪个分布抽取的，我们目前是不知道的。这个时候，对于每一个样本，就有两个方面需要猜测或者估计： 这个身高数据是来自于男生还是来自于女生？男生、女生身高的正态分布的参数分别是多少？EM算法要解决的问题正是这两个问题：<br><img src="media/EM.png" alt="E"></p><h3 id="3-2-EM-算法推导"><a href="#3-2-EM-算法推导" class="headerlink" title="3.2 EM 算法推导"></a>3.2 EM 算法推导</h3><p>样本集$X=(x_1,…,x_m)$，包含$m$各独立的样本；每个样本对应的类别$z_i$是未知的（即上文中每个样本属于哪个分布是未知的）；我们需要以及概率模型$p(x,z)$的参数 $\theta$，即需要找到适合的 $\theta$ 让 $:(\theta)$ 最大。根据上文 <strong>2.1 极大似然估计</strong> 中的似然函数取对数所得$\log L(\theta)$，可以得到如下式：</p><p>$$\sum \log p(x_i;\theta)=\sum <em>i\log \sum</em>{z_i}p(x_i,z_i;\theta)$$<br>$$=\sum_i\log \sum_{z_i}Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}$$<br>$$≥\sum_i\sum_{z_i}Q_i(z_i)\log \frac{p(x_i,z_i;\theta)}{Q(z_i)}$$</p><p>其中，第一步是根据$x_i$的边缘概率计算得来，第二步是分子分母同乘一个数得到，第三步是根据Jensen不等式得到。<br>这里对第三步进行展开：由于$\sum Q_i(z_i)[\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}]$为$\frac{p(x_i,z_i;\theta)}{Q(z_i)}$的期望，且$\log (x)$为凹函数，根据Jensen不等式（<em>当$f$为凹函数时:$E[f(X)]≤f(E[X])$</em>）。<a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html" target="_blank" rel="noopener">The EM Algorithm</a></p><p>上述过程可以看作是对$\log L(\theta)$(即$L(\theta)$)求下界。对于$Q_i(z_i)$的钻则，有很多可能，那么哪种更好呢？ 假设$\theta$已经给定，那么$\log L(\theta)$的值就取决于$Q_i(z_i)$和$p(x_i,z_i;\theta)$了。我们可以通过调整这两个概率使下界不断上升，以逼近$\log L(\theta)$的真实值，那么什么时候算事调整好了呢？当不等式变成等式时说明我们调整后的概率能够等价于$\log L(\theta)$了。按照这个思路，我们要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到：<br>$$\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}=c $$<br>$c$ 为常数，不依赖于 $z_i$。对此式作进一步推到：由于$\sum_{z_i}Q_i(z_i)=1$，则有 $\sum_{z_i}p(x_i,z_i;\theta)=c$（多个等式分子分母相加不变，则认为每个样例的两个概率比值都是$c$），因此得到下式：<br>$$Q_i(z_i)=\frac{p(z_i,z_i;\theta)}{\sum _zp(x_i,z;\theta)}$$<br>$$=\frac {p(x_i,z_i;\theta)}{p(z_i;\theta)}$$<br>$$=p(z_i|x_i;\theta)$$<br>至此，我们推出了在固定其他参数$\theta$后，$Q_i(z_i)$的计算公式就是后验概率，解决了$Q_i(z_i)$如何选择的问题。这一步就是 E 步，建立$\log L(\theta)$的下界。接下来的 M 步，就是在给定 $Q_i(z_i)$后，调整 $\theta$，取极大化 $\log L(\theta)$ 的下界（在固定$Q_i(z_i)$后， 下界还可以调整的更大）。这里可以参考 <a href="https://wenku.baidu.com/view/3396bb4d6294dd88d0d26bee.html" target="_blank" rel="noopener">EM算法</a></p><h3 id="3-3-EM-算法流程"><a href="#3-3-EM-算法流程" class="headerlink" title="3.3 EM 算法流程"></a>3.3 EM 算法流程</h3><ul><li>初始化分布参数 $\theta$；重复 E、M 步骤直至收敛；</li><li>E步骤：根据参数 $\theta$ 初始值或上一次迭代所得参数值来计算出隐形变量的后验概率（即隐形变量的期望），作为隐形变量的现估计值：$$Q_i(z_i):=p(z_i|x_i;\theta)$$</li><li>M 步骤：将似然函数最大化以获得新的参数值：$$\theta:=\arg\max_\theta\sum_i\sum_{z_i}\log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)} $$</li></ul><h2 id="4-EM-算法总结"><a href="#4-EM-算法总结" class="headerlink" title="4. EM 算法总结"></a>4. EM 算法总结</h2><ul><li>优点：EM的收敛性证明方法确实很厉害，能够利用log的凹函数性质，还能够想到利用创造下界，拉平函数下界，优化下界的方法来逐步逼近极大值。而且每一步迭代都能保证是单调的。最重要的是证明的数学公式非常精妙，硬是分子分母都乘以z的概率变成期望来套上Jensen不等式。</li><li><p>缺点：<strong>对初始值敏感</strong>。 EM算法需要初始化参数$\theta$，而参数$\theta$<br>的选择直接影响收敛效率以及能否得到全局最优解。 </p></li><li><p>EM 算法的应用：K-means 算法是 EM 算法思想的体现，E 为聚类过程，M 为更新类簇中心。 GMM（高丝混合模型）也是 EM 算法的一个应用。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EM-算法-Expectation-Maximization-Algorithm&quot;&gt;&lt;a href=&quot;#EM-算法-Expectation-Maximization-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;EM 算法(Expect
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.ihoge.cn/2018/HMM.html"/>
    <id>http://www.ihoge.cn/2018/HMM.html</id>
    <published>2018-08-04T04:48:50.835Z</published>
    <updated>2018-08-05T10:20:51.543Z</updated>
    
    <content type="html"><![CDATA[<!---title: 隐马尔可夫模型date: 2018-8-4 12:20:21comments: truecategories:  - 机器学习tags:  - 算法---><a id="more"></a><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>###1.1 隐马尔可夫的定义<br>隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻。</p><p>隐马尔可夫模型有初始概率分布、状态转移概率分布一级观测概率分布确定。其形式定义如下：</p><p>设 Q 是所有可能的状态的集合，V 是所有可能的观测的集合。</p><p>$$Q=(q_1, q_2, … q_N), V=(v_1, v_2,…, v_M)$$</p><p>其中，N 是可能的状态数， M是可能的观测数。</p><p>I 是长度为 T 的状态序列， O 是对应的观测序列。<br>$$I=(i_i,i_2,…,i_T), O=(o_1,o_2,…o_T)$$</p><p>A 是状态转移概率矩阵：</p><p>$$A=[a_{ij}]_{N\times N}$$</p><p>其中，$a_{ij}=P(i_{t+1}=q_j|i_t=q_i), i=1,2,…,N;j=1,2,…,N$</p><p>是在时刻 t 处于状态 $q_i$ 的条件下在时刻 t+1 转移到状态 $q_j$ 的概率。</p>]]></content>
    
    <summary type="html">
    
      &lt;!---
title: 隐马尔可夫模型
date: 2018-8-4 12:20:21
comments: true
categories:
  - 机器学习
tags:
  - 算法
---&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用BF神经网络进行基于影评的语义分析</title>
    <link href="http://www.ihoge.cn/2018/moviestatis.html"/>
    <id>http://www.ihoge.cn/2018/moviestatis.html</id>
    <published>2018-07-25T04:20:21.000Z</published>
    <updated>2018-08-05T10:14:28.415Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、准备数据"><a href="#一、准备数据" class="headerlink" title="一、准备数据"></a>一、准备数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame([])</span><br><span class="line"></span><br><span class="line">data[<span class="string">'reviews'</span>] = pd.read_table(<span class="string">'reviews.txt'</span>,header=<span class="keyword">None</span>,names=[<span class="string">'reviews'</span>])</span><br><span class="line">data[<span class="string">'labels'</span>] = pd.read_table(<span class="string">'labels.txt'</span>,header=<span class="keyword">None</span>,names=[<span class="string">'labels'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pos_data = data[data[<span class="string">'labels'</span>] == <span class="string">'positive'</span>].reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line">neg_data = data[data[<span class="string">'labels'</span>] == <span class="string">'negative'</span>].reset_index(drop=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pos_data.iloc[:,<span class="number">0</span>][:<span class="number">3</span>]</span><br><span class="line">list(neg_data.iloc[:,<span class="number">0</span>][:<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>[&apos;story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  &apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Pie, Style</span><br><span class="line"></span><br><span class="line">pie = Pie(<span class="string">"正负样本占比"</span>, title_pos=<span class="string">'left'</span>)</span><br><span class="line">style = Style()</span><br><span class="line">pie_style = style.add(label_pos = <span class="string">'left'</span>,</span><br><span class="line">                      is_label_show = <span class="keyword">True</span>,</span><br><span class="line">                      label_text_color = <span class="keyword">True</span>,</span><br><span class="line">                      label_text_size = <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">pie.add(<span class="string">"样例条数"</span>, [<span class="string">"positive"</span>,<span class="string">"negative"</span>],[len(pos_data), len(neg_data)], **pie_style)</span><br><span class="line">pie</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/屏幕快照 2018-07-26 下午5.16.54.png" alt="屏幕快照 2018-07-26 下午5.16.54"></p><h2 id="二、理论验证"><a href="#二、理论验证" class="headerlink" title="二、理论验证"></a>二、理论验证</h2><p>通过观察发现正面评论里常出现些赞美的词汇，负面评论出现批判的词汇。考虑能否根据这个特征来对正负评论进行预测？</p><p>根据这个想法进行一次快速的验证</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[<span class="number">1</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>&apos;story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  &apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">t0 = time()</span><br><span class="line"><span class="comment"># 创建计数器，分别把正负面评论进行词频统计并把它放在三个计数器里。</span></span><br><span class="line"></span><br><span class="line">positive_counts = Counter()</span><br><span class="line">negative_counts = Counter()</span><br><span class="line">total_counts = Counter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> words <span class="keyword">in</span> pos_data[<span class="string">'reviews'</span>]:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words.split(<span class="string">" "</span>):</span><br><span class="line">        positive_counts[word] += <span class="number">1</span></span><br><span class="line">        total_counts[word] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> words <span class="keyword">in</span> neg_data[<span class="string">'reviews'</span>]:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words.split(<span class="string">" "</span>):</span><br><span class="line">        negative_counts[word] += <span class="number">1</span></span><br><span class="line">        total_counts[word] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">print(<span class="string">"处理完毕\n耗时&#123;&#125;秒。"</span>.format(time() - t0))</span><br></pre></td></tr></table></figure><pre><code>处理完毕耗时5.25712513923645秒。</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># positive_counts.most_common()</span></span><br><span class="line"><span class="comment"># negative_counts.most_common()</span></span><br></pre></td></tr></table></figure><p>通过观察发现无论是正样本还是负样本，词频最高的词汇大多是常用词汇如 a、the、and、of … 等单词。下面把正负计算器汇总起来一起观察</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里计算的是词汇分别在好评中出现的次数与差评出现次数的比例然后取对数。</span></span><br><span class="line"></span><br><span class="line">pos_neg_ratios = Counter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> term,cnt <span class="keyword">in</span> total_counts.most_common():</span><br><span class="line"><span class="comment">#     if cnt &gt; 100:</span></span><br><span class="line">        pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 加 1 为了避免分母为负</span></span><br><span class="line">        pos_neg_ratios[term] = pos_neg_ratio</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> word,ratio <span class="keyword">in</span> pos_neg_ratios.most_common():</span><br><span class="line">    <span class="keyword">if</span>(ratio &gt; <span class="number">1</span>):</span><br><span class="line">        pos_neg_ratios[word] = np.log(ratio)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pos_neg_ratios[word] = -np.log((<span class="number">1</span> / (ratio + <span class="number">0.01</span>)))</span><br><span class="line"><span class="comment"># 如果比例小于 1 说明差评中词汇含量大，这里计算比例倒数的对数。加 0.01 同样为了避免分母为0。</span></span><br><span class="line"><span class="comment"># 最终得出的结果表示，某个单词的值越大则说明该词汇所能代表的正负面情绪越明显。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正面词汇</span></span><br><span class="line">pos_neg_ratios.most_common()[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><pre><code>[(&apos;edie&apos;, 4.6913478822291435), (&apos;antwone&apos;, 4.477336814478207), (&apos;din&apos;, 4.406719247264253), (&apos;gunga&apos;, 4.189654742026425), (&apos;goldsworthy&apos;, 4.174387269895637)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 负面词汇</span></span><br><span class="line">list(reversed(pos_neg_ratios.most_common()))[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure><pre><code>[(&apos;whelk&apos;, -4.605170185988092), (&apos;pressurized&apos;, -4.605170185988092), (&apos;bellwood&apos;, -4.605170185988092), (&apos;mwuhahahaa&apos;, -4.605170185988092), (&apos;insulation&apos;, -4.605170185988092)]</code></pre><p>通过以上观测，初步验证了我们的想法是有效可行的。某些词汇在好评和差评中出现的次数明显差距很大</p><h2 id="三、构建神经网络原型"><a href="#三、构建神经网络原型" class="headerlink" title="三、构建神经网络原型"></a>三、构建神经网络原型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">review = <span class="string">"This is a horrible, terrible movie."</span></span><br><span class="line"></span><br><span class="line">Image(filename=<span class="string">'sentiment_network.png'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是我们基于以上思想构建的简单神经网络原型</span></span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/output_17_0.png" alt="output_17_0"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">review = <span class="string">'The movie was excellent.'</span></span><br><span class="line"></span><br><span class="line">Image(filename=<span class="string">'sentiment_network_pos.png'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/output_18_0.png" alt="output_18_0"></p><h2 id="四、将每条评论转换为输入向量"><a href="#四、将每条评论转换为输入向量" class="headerlink" title="四、将每条评论转换为输入向量"></a>四、将每条评论转换为输入向量</h2><p>这里为了简单起见只是做了对每个出现的单词做个词频统计，但是实际情况往往比这个复杂，事实上大量无关单词的出现严重扰乱了数据的属性。针对这个情况后面会介绍一种简单的处理方式。更优化的处理方式可以参考TF—IDF。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab = set(total_counts.keys())</span><br><span class="line">vocab_size = len(vocab)</span><br><span class="line">print(<span class="string">"输入向量的维数："</span>,vocab_size)</span><br></pre></td></tr></table></figure><pre><code>输入向量的维数： 74074</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个容器，这里注意是二维数据集</span></span><br><span class="line">layer_0 = np.zeros((<span class="number">1</span>, vocab_size))</span><br><span class="line">layer_0</span><br></pre></td></tr></table></figure><pre><code>array([[0., 0., 0., ..., 0., 0., 0.]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个带索引的词汇字典</span></span><br><span class="line">word2index = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(vocab):</span><br><span class="line">    word2index[word] = i</span><br><span class="line"><span class="comment"># word2index</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_input_layer</span><span class="params">(review)</span>:</span></span><br><span class="line">    layer_0 = np.zeros((<span class="number">1</span>, vocab_size))    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">        layer_0[<span class="number">0</span>][word2index[word]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> layer_0    </span><br><span class="line">layer_0 = update_input_layer(reviews[<span class="number">0</span>]) <span class="comment"># 测试reviews[0]这条评论转换成数值向量</span></span><br><span class="line">layer_0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了计算效率高，可以考虑直接进行归一化操作</span></span><br></pre></td></tr></table></figure><pre><code>array([[18.,  0.,  0., ...,  0.,  0.,  0.]])</code></pre><p><strong>创建标签</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_target_for_label</span><span class="params">(label)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> label == <span class="string">'POSITIVE'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># X = layer_1   </span></span><br><span class="line"><span class="comment"># y = [*map(get_target_for_label, labels)]</span></span><br></pre></td></tr></table></figure><h2 id="五、创建神经网络"><a href="#五、创建神经网络" class="headerlink" title="五、创建神经网络"></a>五、创建神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, reviews,labels,hidden_nodes = <span class="number">10</span>, learning_rate = <span class="number">0.1</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        np.random.seed(<span class="number">1</span>)</span><br><span class="line">        self.pre_process_data(reviews, labels)</span><br><span class="line">        self.init_network(len(self.review_vocab),hidden_nodes, <span class="number">1</span>, learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_process_data</span><span class="params">(self, reviews, labels)</span>:</span></span><br><span class="line">        </span><br><span class="line">        review_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">                review_vocab.add(word)</span><br><span class="line"></span><br><span class="line">        self.review_vocab = list(review_vocab)</span><br><span class="line">        label_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            label_vocab.add(label)</span><br><span class="line">        self.label_vocab = list(label_vocab)</span><br><span class="line">        self.review_vocab_size = len(self.review_vocab)</span><br><span class="line">        self.label_vocab_size = len(self.label_vocab)        </span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.review_vocab):</span><br><span class="line">            self.word2index[word] = i</span><br><span class="line">        </span><br><span class="line">        self.label2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(self.label_vocab):</span><br><span class="line">            self.label2index[label] = i</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(self, input_nodes, hidden_nodes, output_nodes, learning_rate)</span>:</span></span><br><span class="line">        self.input_nodes = input_nodes</span><br><span class="line">        self.hidden_nodes = hidden_nodes</span><br><span class="line">        self.output_nodes = output_nodes</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))</span><br><span class="line">        self.weights_1_2 = np.random.normal(<span class="number">0.0</span>, self.output_nodes**<span class="number">-0.5</span>, </span><br><span class="line">                                                (self.hidden_nodes, self.output_nodes))</span><br><span class="line">        self.layer_0 = np.zeros((<span class="number">1</span>,input_nodes))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_input_layer</span><span class="params">(self,review)</span>:</span></span><br><span class="line">        self.layer_0 *= <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">            <span class="keyword">if</span>(word <span class="keyword">in</span> self.word2index.keys()):</span><br><span class="line">                self.layer_0[<span class="number">0</span>][self.word2index[word]] += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_for_label</span><span class="params">(self,label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>(label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_output_2_derivative</span><span class="params">(self,output)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_reviews, training_labels)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span>(len(training_reviews) == len(training_labels))</span><br><span class="line">        correct_so_far = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(training_reviews)):</span><br><span class="line">            </span><br><span class="line">            review = training_reviews[i]</span><br><span class="line">            label = training_labels[i]</span><br><span class="line">            </span><br><span class="line">            self.update_input_layer(review)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Hidden layer</span></span><br><span class="line">            layer_1 = self.layer_0.dot(self.weights_0_1)</span><br><span class="line">            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Output error</span></span><br><span class="line">            layer_2_error = layer_2 - self.get_target_for_label(label) <span class="comment"># Output layer error is the difference between desired target and actual output.</span></span><br><span class="line">            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backpropagated error</span></span><br><span class="line">            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) <span class="comment"># errors propagated to the hidden layer</span></span><br><span class="line">            layer_1_delta = layer_1_error <span class="comment"># hidden layer gradients - no nonlinearity so it's the same as the error</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update the weights</span></span><br><span class="line">            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate <span class="comment"># update hidden-to-output weights with gradient descent step</span></span><br><span class="line">            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate <span class="comment"># update input-to-hidden weights with gradient descent step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(layer_2 &gt;= <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span>(layer_2 &lt; <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'NEGATIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(training_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct_so_far) + <span class="string">" #Trained:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Training Accuracy:"</span> + str(correct_so_far * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">            <span class="keyword">if</span>(i % <span class="number">2500</span> == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">""</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, testing_reviews, testing_labels)</span>:</span></span><br><span class="line"></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testing_reviews)):</span><br><span class="line">            pred = self.run(testing_reviews[i])</span><br><span class="line">            <span class="keyword">if</span>(pred == testing_labels[i]):</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(testing_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct) + <span class="string">" #Tested:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Testing Accuracy:"</span> + str(correct * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, review)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.update_input_layer(review.lower())</span><br><span class="line">        layer_1 = self.layer_0.dot(self.weights_0_1)</span><br><span class="line">        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))</span><br><span class="line">        <span class="keyword">if</span>(layer_2[<span class="number">0</span>] &gt;= <span class="number">0.5</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"POSITIVE"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"NEGATIVE"</span></span><br></pre></td></tr></table></figure><h2 id="六、初步测试"><a href="#六、初步测试" class="headerlink" title="六、初步测试"></a>六、初步测试</h2><p>1、首先训练之前先看下测试效果是否为50%</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], learning_rate=<span class="number">0.1</span>)</span><br><span class="line">mlp.test(reviews[<span class="number">-1000</span>:],labels[<span class="number">-1000</span>:])</span><br></pre></td></tr></table></figure><pre><code>Progress:99.9% Speed(reviews/sec):1553. #Correct:500 #Tested:1000 Testing Accuracy:50.0%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):367.6 #Correct:1251 #Trained:2501 Training Accuracy:50.0%Progress:20.8% Speed(reviews/sec):366.4 #Correct:2501 #Trained:5001 Training Accuracy:50.0%Progress:31.2% Speed(reviews/sec):367.6 #Correct:3751 #Trained:7501 Training Accuracy:50.0%Progress:41.6% Speed(reviews/sec):368.7 #Correct:5001 #Trained:10001 Training Accuracy:50.0%Progress:52.0% Speed(reviews/sec):368.5 #Correct:6251 #Trained:12501 Training Accuracy:50.0%Progress:62.5% Speed(reviews/sec):368.6 #Correct:7501 #Trained:15001 Training Accuracy:50.0%Progress:72.9% Speed(reviews/sec):368.6 #Correct:8751 #Trained:17501 Training Accuracy:50.0%Progress:83.3% Speed(reviews/sec):368.5 #Correct:10001 #Trained:20001 Training Accuracy:50.0%Progress:93.7% Speed(reviews/sec):368.7 #Correct:11251 #Trained:22501 Training Accuracy:50.0%Progress:99.9% Speed(reviews/sec):368.4 #Correct:12000 #Trained:24000 Training Accuracy:50.0%</code></pre><p>2、正式开始训练，发现准确率一直是50%没有提升，考虑是否因为学习率太高造成无法收敛。</p><p>然后调小学习率重新测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], learning_rate=<span class="number">0.01</span>)</span><br><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):334.6 #Correct:1248 #Trained:2501 Training Accuracy:49.9%Progress:20.8% Speed(reviews/sec):334.0 #Correct:2498 #Trained:5001 Training Accuracy:49.9%Progress:31.2% Speed(reviews/sec):340.9 #Correct:3748 #Trained:7501 Training Accuracy:49.9%Progress:41.6% Speed(reviews/sec):348.0 #Correct:4998 #Trained:10001 Training Accuracy:49.9%Progress:52.0% Speed(reviews/sec):348.3 #Correct:6248 #Trained:12501 Training Accuracy:49.9%Progress:62.5% Speed(reviews/sec):346.7 #Correct:7490 #Trained:15001 Training Accuracy:49.9%Progress:72.9% Speed(reviews/sec):348.8 #Correct:8746 #Trained:17501 Training Accuracy:49.9%Progress:83.3% Speed(reviews/sec):348.0 #Correct:9996 #Trained:20001 Training Accuracy:49.9%Progress:93.7% Speed(reviews/sec):346.8 #Correct:11246 #Trained:22501 Training Accuracy:49.9%Progress:99.9% Speed(reviews/sec):347.0 #Correct:11995 #Trained:24000 Training Accuracy:49.9%</code></pre><p>3、继续调小学习率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], learning_rate=<span class="number">0.001</span>)</span><br><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):318.7 #Correct:1263 #Trained:2501 Training Accuracy:50.4%Progress:20.8% Speed(reviews/sec):319.5 #Correct:2615 #Trained:5001 Training Accuracy:52.2%Progress:31.2% Speed(reviews/sec):319.9 #Correct:4035 #Trained:7501 Training Accuracy:53.7%Progress:41.6% Speed(reviews/sec):320.5 #Correct:5566 #Trained:10001 Training Accuracy:55.6%Progress:52.0% Speed(reviews/sec):320.3 #Correct:7047 #Trained:12501 Training Accuracy:56.3%Progress:62.5% Speed(reviews/sec):320.1 #Correct:8658 #Trained:15001 Training Accuracy:57.7%Progress:72.9% Speed(reviews/sec):319.8 #Correct:10202 #Trained:17501 Training Accuracy:58.2%Progress:83.3% Speed(reviews/sec):319.5 #Correct:11889 #Trained:20001 Training Accuracy:59.4%Progress:93.7% Speed(reviews/sec):319.3 #Correct:13525 #Trained:22501 Training Accuracy:60.1%Progress:99.9% Speed(reviews/sec):319.2 #Correct:14574 #Trained:24000 Training Accuracy:60.7%</code></pre><p>4、增加隐藏层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], hidden_nodes=<span class="number">15</span>, learning_rate=<span class="number">0.0003</span>)</span><br><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):269.4 #Correct:1304 #Trained:2501 Training Accuracy:52.1%Progress:20.8% Speed(reviews/sec):269.0 #Correct:2765 #Trained:5001 Training Accuracy:55.2%Progress:31.2% Speed(reviews/sec):268.6 #Correct:4395 #Trained:7501 Training Accuracy:58.5%Progress:41.6% Speed(reviews/sec):268.5 #Correct:6083 #Trained:10001 Training Accuracy:60.8%Progress:52.0% Speed(reviews/sec):267.9 #Correct:7774 #Trained:12501 Training Accuracy:62.1%Progress:62.5% Speed(reviews/sec):267.2 #Correct:9483 #Trained:15001 Training Accuracy:63.2%Progress:72.9% Speed(reviews/sec):266.0 #Correct:11199 #Trained:17501 Training Accuracy:63.9%Progress:83.3% Speed(reviews/sec):265.0 #Correct:13023 #Trained:20001 Training Accuracy:65.1%Progress:93.7% Speed(reviews/sec):264.3 #Correct:14854 #Trained:22501 Training Accuracy:66.0%Progress:99.9% Speed(reviews/sec):263.7 #Correct:15993 #Trained:24000 Training Accuracy:66.6%</code></pre><p><strong>第一次训练总结：</strong></p><p>通过以上训练效果发现，首先学习率过大时模型无法收敛。当取值为 0.001 时模型开始缓慢的提升。正常情况下模型在刚开始提升的速度很快到后面越来越慢。</p><p>从以上效果来看，当模型准确率提升到60%时速度开始放缓，因此即使增加更多的迭代次数对效果的提升也不会很明显。因此我们考虑时哪些原因可能会造成这种情况。</p><p><strong>需要解决的问题：</strong></p><ul><li>训练速度太慢</li><li>准确度不高</li></ul><p><strong>可能的原因</strong>：</p><ul><li>模型隐藏层节点过少，过于简单</li><li>数据本身存在噪音对模型影响较大</li></ul><p>经过第4步的测试，发现增加隐藏层为20对效果并没有提升，反而效果更差。考虑继续调小学习率到 0.0003 模型准确率得以提升。<br>发现：<em>增加节点数的同时需要调小学习率</em></p><p>如果把神经网络比如成挖掘机，我们的目的时从数据里挖掘出有价值的金子。往往刚开始的时候很难挖到金子，可能并不是因为挖掘机挖的不够深而在于我们挖掘的位置或者操纵它的方式不对。所以我们从新回到数据集上考虑噪音和信号的问题。</p><h2 id="七、关于噪音的分析"><a href="#七、关于噪音的分析" class="headerlink" title="七、关于噪音的分析"></a>七、关于噪音的分析</h2><p><strong>关于噪音</strong></p><p>通过观察数据，发现在每一条评论中的空格字符，以及类似 a、the、at…这些字符占据了大多数甚至时几十个。这样的话放到模型里意味着我们给这些跟情绪不相关的词相当大的权重，而真正有价值的词被淹没了。有价值的情绪词汇出现的频率大多知识出现了1次。我们考虑一个最简单的方式是在评论转换为数值向量的时候不去累加词频而是简单的赋值为1，如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_input_layer</span><span class="params">(self,review)</span>:</span></span><br><span class="line">    self.layer_0 *= <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">        <span class="keyword">if</span>(word <span class="keyword">in</span> self.word2index.keys()):</span><br><span class="line">            self.layer_0[<span class="number">0</span>][self.word2index[word]] = <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>下面我们重新验证下我们的思路：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, reviews,labels,hidden_nodes = <span class="number">10</span>, learning_rate = <span class="number">0.1</span>)</span>:</span></span><br><span class="line">        </span><br><span class="line">        np.random.seed(<span class="number">1</span>)</span><br><span class="line">        self.pre_process_data(reviews, labels)</span><br><span class="line">        self.init_network(len(self.review_vocab),hidden_nodes, <span class="number">1</span>, learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_process_data</span><span class="params">(self, reviews, labels)</span>:</span></span><br><span class="line">        </span><br><span class="line">        review_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">                review_vocab.add(word)</span><br><span class="line"></span><br><span class="line">        self.review_vocab = list(review_vocab)</span><br><span class="line">        label_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            label_vocab.add(label)</span><br><span class="line">        self.label_vocab = list(label_vocab)</span><br><span class="line">        self.review_vocab_size = len(self.review_vocab)</span><br><span class="line">        self.label_vocab_size = len(self.label_vocab)        </span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.review_vocab):</span><br><span class="line">            self.word2index[word] = i</span><br><span class="line">        </span><br><span class="line">        self.label2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(self.label_vocab):</span><br><span class="line">            self.label2index[label] = i</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(self, input_nodes, hidden_nodes, output_nodes, learning_rate)</span>:</span></span><br><span class="line">        self.input_nodes = input_nodes</span><br><span class="line">        self.hidden_nodes = hidden_nodes</span><br><span class="line">        self.output_nodes = output_nodes</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))</span><br><span class="line">        self.weights_1_2 = np.random.normal(<span class="number">0.0</span>, self.output_nodes**<span class="number">-0.5</span>, </span><br><span class="line">                                                (self.hidden_nodes, self.output_nodes))</span><br><span class="line">        self.layer_0 = np.zeros((<span class="number">1</span>,input_nodes))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_input_layer</span><span class="params">(self,review)</span>:</span></span><br><span class="line">        self.layer_0 *= <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">            <span class="keyword">if</span>(word <span class="keyword">in</span> self.word2index.keys()):</span><br><span class="line">                self.layer_0[<span class="number">0</span>][self.word2index[word]] = <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_for_label</span><span class="params">(self,label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>(label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,x)</span>:</span></span><br><span class="line"><span class="comment">#         return 1 / (1 + np.exp(-x))</span></span><br><span class="line">        <span class="keyword">return</span> (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_output_2_derivative</span><span class="params">(self,output)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_reviews, training_labels)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span>(len(training_reviews) == len(training_labels))</span><br><span class="line">        correct_so_far = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(training_reviews)):</span><br><span class="line">            </span><br><span class="line">            review = training_reviews[i]</span><br><span class="line">            label = training_labels[i]</span><br><span class="line">            </span><br><span class="line">            self.update_input_layer(review)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Hidden layer</span></span><br><span class="line">            layer_1 = self.layer_0.dot(self.weights_0_1)</span><br><span class="line">            layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Output error</span></span><br><span class="line">            layer_2_error = layer_2 - self.get_target_for_label(label) <span class="comment"># Output layer error is the difference between desired target and actual output.</span></span><br><span class="line">            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backpropagated error</span></span><br><span class="line">            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) <span class="comment"># errors propagated to the hidden layer</span></span><br><span class="line">            layer_1_delta = layer_1_error <span class="comment"># hidden layer gradients - no nonlinearity so it's the same as the error</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update the weights</span></span><br><span class="line">            self.weights_1_2 -= layer_1.T.dot(layer_2_delta) * self.learning_rate <span class="comment"># update hidden-to-output weights with gradient descent step</span></span><br><span class="line">            self.weights_0_1 -= self.layer_0.T.dot(layer_1_delta) * self.learning_rate <span class="comment"># update input-to-hidden weights with gradient descent step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(layer_2 &gt;= <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span>(layer_2 &lt; <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'NEGATIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(training_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct_so_far) + <span class="string">" #Trained:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Training Accuracy:"</span> + str(correct_so_far * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">            <span class="keyword">if</span>(i % <span class="number">2500</span> == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">""</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, testing_reviews, testing_labels)</span>:</span></span><br><span class="line"></span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testing_reviews)):</span><br><span class="line">            pred = self.run(testing_reviews[i])</span><br><span class="line">            <span class="keyword">if</span>(pred == testing_labels[i]):</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(testing_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct) + <span class="string">" #Tested:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Testing Accuracy:"</span> + str(correct * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, review)</span>:</span></span><br><span class="line">        </span><br><span class="line">        self.update_input_layer(review.lower())</span><br><span class="line">        layer_1 = self.layer_0.dot(self.weights_0_1)</span><br><span class="line">        layer_2 = self.sigmoid(layer_1.dot(self.weights_1_2))</span><br><span class="line">        <span class="keyword">if</span>(layer_2[<span class="number">0</span>] &gt;= <span class="number">0.5</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"POSITIVE"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"NEGATIVE"</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], hidden_nodes=<span class="number">15</span>, learning_rate=<span class="number">0.0003</span>)</span><br><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):206.7 #Correct:1909 #Trained:2501 Training Accuracy:76.3%Progress:20.8% Speed(reviews/sec):207.2 #Correct:3917 #Trained:5001 Training Accuracy:78.3%Progress:31.2% Speed(reviews/sec):206.2 #Correct:5979 #Trained:7501 Training Accuracy:79.7%Progress:41.6% Speed(reviews/sec):204.5 #Correct:8068 #Trained:10001 Training Accuracy:80.6%Progress:52.0% Speed(reviews/sec):203.6 #Correct:10165 #Trained:12501 Training Accuracy:81.3%Progress:62.5% Speed(reviews/sec):203.1 #Correct:12236 #Trained:15001 Training Accuracy:81.5%Progress:72.9% Speed(reviews/sec):202.6 #Correct:14312 #Trained:17501 Training Accuracy:81.7%Progress:83.3% Speed(reviews/sec):202.2 #Correct:16455 #Trained:20001 Training Accuracy:82.2%Progress:93.7% Speed(reviews/sec):202.0 #Correct:18598 #Trained:22501 Training Accuracy:82.6%Progress:99.9% Speed(reviews/sec):201.8 #Correct:19894 #Trained:24000 Training Accuracy:82.8%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], hidden_nodes=<span class="number">10</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):264.1 #Correct:1812 #Trained:2501 Training Accuracy:72.4%Progress:20.8% Speed(reviews/sec):264.1 #Correct:3802 #Trained:5001 Training Accuracy:76.0%Progress:31.2% Speed(reviews/sec):264.2 #Correct:5896 #Trained:7501 Training Accuracy:78.6%Progress:41.6% Speed(reviews/sec):264.2 #Correct:8045 #Trained:10001 Training Accuracy:80.4%Progress:52.0% Speed(reviews/sec):264.2 #Correct:10172 #Trained:12501 Training Accuracy:81.3%Progress:62.5% Speed(reviews/sec):264.2 #Correct:12319 #Trained:15001 Training Accuracy:82.1%Progress:72.9% Speed(reviews/sec):264.0 #Correct:14438 #Trained:17501 Training Accuracy:82.4%Progress:83.3% Speed(reviews/sec):263.7 #Correct:16615 #Trained:20001 Training Accuracy:83.0%Progress:93.7% Speed(reviews/sec):263.5 #Correct:18796 #Trained:22501 Training Accuracy:83.5%Progress:99.9% Speed(reviews/sec):263.5 #Correct:20117 #Trained:24000 Training Accuracy:83.8%</code></pre><ul><li>看下在测试集的效果</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp.test(reviews[<span class="number">-1000</span>:],labels[<span class="number">-1000</span>:])</span><br></pre></td></tr></table></figure><pre><code>Progress:99.9% Speed(reviews/sec):1620. #Correct:849 #Tested:1000 Testing Accuracy:84.9%</code></pre><h2 id="八、关于网络计算效率低的分析"><a href="#八、关于网络计算效率低的分析" class="headerlink" title="八、关于网络计算效率低的分析"></a>八、关于网络计算效率低的分析</h2><p>通过上面的数据优化大大提升了模型的准确率，但是训练的速度还是很慢。所以我们考虑下这么才能提升训练的速度呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Image(filename=<span class="string">'sentiment_network.png'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/output_44_0.png" alt="output_44_0"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_input_layer</span><span class="params">(review)</span>:</span></span><br><span class="line">    layer_0 = np.zeros((<span class="number">1</span>, vocab_size))    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">        layer_0[<span class="number">0</span>][word2index[word]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> layer_0    </span><br><span class="line">layer_0 = update_input_layer(reviews[<span class="number">0</span>]) <span class="comment"># 测试reviews[0]这条评论转换成数值向量</span></span><br><span class="line">layer_0.sum()</span><br></pre></td></tr></table></figure><pre><code>93.0</code></pre><p>再次考虑我们的模型，这里我们的输入层节点个数为 vocab_size(74074) 个，在 <code>reviews[0]</code> 中有数值的词汇只有93个，是个极度稀疏的向量。其中大多数都为 0 ，在输入值为 0 的时候乘以权重得出结果仍然为0，这不仅没有任何意义还大大增加的计算量。 所以我们考虑用什么方式可以优化这个问题呢？</p><p><strong>解决思路：</strong><br>我们记录下非 0 元素的索引，然后在计算输出层到隐藏层的时候，只在非 0 元素索引的位置乘以权重然后进行求和，这样就大大节省了计算量。</p><p>示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer_0 = np.zeros(<span class="number">10</span>)</span><br><span class="line">layer_0[<span class="number">4</span>] = <span class="number">1</span></span><br><span class="line">layer_0[<span class="number">9</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">weight_0_1 = np.random.randn(<span class="number">10</span>,<span class="number">5</span>)</span><br><span class="line">indexs = [<span class="number">4</span>,<span class="number">9</span>]</span><br><span class="line">layer_1 = np.zeros(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indexs:</span><br><span class="line">    layer_1 += layer_0[index] * weight_0_1[index]</span><br><span class="line"></span><br><span class="line">layer_1.sum()</span><br></pre></td></tr></table></figure><pre><code>-2.131818189044033</code></pre><p>进一步考虑，这里我们的输入值都为 1， 因此 1 乘以权值这个计算步骤也可以省略，只需要对索引位置的权值进行求和就好了。</p><p>下面我们来实现这一思路：</p><ul><li>不再让隐藏层中做乘以 0 的步骤</li><li>不再让隐藏层做权重乘以 1 的步骤</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, reviews,labels,hidden_nodes = <span class="number">10</span>, learning_rate = <span class="number">0.1</span>)</span>:</span></span><br><span class="line">        np.random.seed(<span class="number">1</span>)</span><br><span class="line">        self.pre_process_data(reviews, labels)</span><br><span class="line">        self.init_network(len(self.review_vocab),hidden_nodes, <span class="number">1</span>, learning_rate)</span><br><span class="line">        <span class="comment"># 输入层节点个数为词汇库长度</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_process_data</span><span class="params">(self, reviews, labels)</span>:</span></span><br><span class="line">        <span class="comment"># 提取出所有词汇</span></span><br><span class="line">        review_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">                review_vocab.add(word)</span><br><span class="line">        self.review_vocab = list(review_vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取标签</span></span><br><span class="line">        label_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            label_vocab.add(label)</span><br><span class="line">        self.label_vocab = list(label_vocab)</span><br><span class="line">        </span><br><span class="line">        self.review_vocab_size = len(self.review_vocab)</span><br><span class="line">        self.label_vocab_size = len(self.label_vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建词汇库字典</span></span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.review_vocab):</span><br><span class="line">            self.word2index[word] = i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建标签索引字典</span></span><br><span class="line">        self.label2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(self.label_vocab):</span><br><span class="line">            self.label2index[label] = i</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化模型，输入节点为词汇库长度，隐藏层节点默认为10，输出节点默认为1，学习率默认为0.1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(self, input_nodes, hidden_nodes, output_nodes, learning_rate)</span>:</span></span><br><span class="line">        self.input_nodes = input_nodes</span><br><span class="line">        self.hidden_nodes = hidden_nodes</span><br><span class="line">        self.output_nodes = output_nodes</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line"></span><br><span class="line">        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))</span><br><span class="line">        self.weights_1_2 = np.random.normal(<span class="number">0.0</span>, self.output_nodes**<span class="number">-0.5</span>, </span><br><span class="line">                                                (self.hidden_nodes, self.output_nodes))</span><br><span class="line">        </span><br><span class="line">        self.layer_1 = np.zeros((<span class="number">1</span>,hidden_nodes))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义标签转换函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_for_label</span><span class="params">(self,label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>(label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_output_2_derivative</span><span class="params">(self,output)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#=======================训练函数=============================    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_reviews_raw, training_labels)</span>:</span></span><br><span class="line"></span><br><span class="line">        training_reviews = list()</span><br><span class="line">        <span class="keyword">for</span> review <span class="keyword">in</span> training_reviews_raw:</span><br><span class="line">            indices = set()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">                <span class="keyword">if</span>(word <span class="keyword">in</span> self.word2index.keys()):</span><br><span class="line">                    indices.add(self.word2index[word])</span><br><span class="line">            training_reviews.append(list(indices))</span><br><span class="line">        <span class="comment"># 以上代码记录下每条评论的每个词汇在对应字典中的位置索引，以此作为输入数据。</span></span><br><span class="line">        <span class="comment"># 最终这里生成的training_reviews里面存放的是每条影评的各个词汇在词汇字典里的索引。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(len(training_reviews) == len(training_labels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义一个容器计算正确的个数用以计算准确率</span></span><br><span class="line">        correct_so_far = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(training_reviews)):</span><br><span class="line">            review = training_reviews[i]</span><br><span class="line">            label = training_labels[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 输入层</span></span><br><span class="line">            self.layer_1 *= <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> review:</span><br><span class="line">                self.layer_1 += self.weights_0_1[index]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层</span></span><br><span class="line">            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 输出层误差和梯度</span></span><br><span class="line">            layer_2_error = layer_2 - self.get_target_for_label(label) <span class="comment"># Output layer error is the difference between desired target and actual output.</span></span><br><span class="line">            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播误差和梯度</span></span><br><span class="line">            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) <span class="comment"># errors propagated to the hidden layer</span></span><br><span class="line">            layer_1_delta = layer_1_error <span class="comment"># hidden layer gradients - no nonlinearity so it's the same as the error</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新权值</span></span><br><span class="line">            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate <span class="comment"># update hidden-to-output weights with gradient descent step</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> review:</span><br><span class="line">                self.weights_0_1[index] -= layer_1_delta[<span class="number">0</span>] * self.learning_rate <span class="comment"># update input-to-hidden weights with gradient descent step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(layer_2 &gt;= <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span>(layer_2 &lt; <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'NEGATIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(training_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct_so_far) + <span class="string">" #Trained:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Training Accuracy:"</span> + str(correct_so_far * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">            <span class="keyword">if</span>(i % <span class="number">2500</span> == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">""</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, testing_reviews, testing_labels)</span>:</span></span><br><span class="line">        </span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testing_reviews)):</span><br><span class="line">            pred = self.run(testing_reviews[i])</span><br><span class="line">            <span class="keyword">if</span>(pred == testing_labels[i]):</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(testing_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct) + <span class="string">" #Tested:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Testing Accuracy:"</span> + str(correct * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, review)</span>:</span></span><br><span class="line">        self.layer_1 *= <span class="number">0</span></span><br><span class="line">        unique_indices = set()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> review.lower().split(<span class="string">" "</span>):</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> self.word2index.keys():</span><br><span class="line">                unique_indices.add(self.word2index[word])</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> unique_indices:</span><br><span class="line">            self.layer_1 += self.weights_0_1[index]</span><br><span class="line">        </span><br><span class="line">        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(layer_2[<span class="number">0</span>] &gt;= <span class="number">0.5</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"POSITIVE"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"NEGATIVE"</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>], hidden_nodes=<span class="number">15</span>, learning_rate=<span class="number">0.003</span>)</span><br><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):1871. #Correct:1974 #Trained:2501 Training Accuracy:78.9%Progress:20.8% Speed(reviews/sec):1753. #Correct:4037 #Trained:5001 Training Accuracy:80.7%Progress:31.2% Speed(reviews/sec):1741. #Correct:6163 #Trained:7501 Training Accuracy:82.1%Progress:41.6% Speed(reviews/sec):1696. #Correct:8320 #Trained:10001 Training Accuracy:83.1%Progress:52.0% Speed(reviews/sec):1692. #Correct:10490 #Trained:12501 Training Accuracy:83.9%Progress:62.5% Speed(reviews/sec):1695. #Correct:12636 #Trained:15001 Training Accuracy:84.2%Progress:72.9% Speed(reviews/sec):1696. #Correct:14773 #Trained:17501 Training Accuracy:84.4%Progress:83.3% Speed(reviews/sec):1675. #Correct:16959 #Trained:20001 Training Accuracy:84.7%Progress:93.7% Speed(reviews/sec):1678. #Correct:19150 #Trained:22501 Training Accuracy:85.1%Progress:99.9% Speed(reviews/sec):1674. #Correct:20467 #Trained:24000 Training Accuracy:85.2%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp.test(reviews[<span class="number">-1000</span>:], labels[<span class="number">-1000</span>:])</span><br></pre></td></tr></table></figure><pre><code>Progress:99.9% Speed(reviews/sec):2611. #Correct:857 #Tested:1000 Testing Accuracy:85.7%</code></pre><h2 id="九、进一步降低降低噪音"><a href="#九、进一步降低降低噪音" class="headerlink" title="九、进一步降低降低噪音"></a>九、进一步降低降低噪音</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 正面评论经常出现的词汇</span></span><br><span class="line">pos_neg_ratios.most_common()</span><br></pre></td></tr></table></figure><pre><code>[(&apos;edie&apos;, 4.6913478822291435), (&apos;antwone&apos;, 4.477336814478207), (&apos;din&apos;, 4.406719247264253), (&apos;gunga&apos;, 4.189654742026425), (&apos;goldsworthy&apos;, 4.174387269895637), (&apos;gypo&apos;, 4.0943445622221), (&apos;yokai&apos;, 4.0943445622221), (&apos;paulie&apos;, 4.07753744390572), (&apos;visconti&apos;, 3.9318256327243257), (&apos;flavia&apos;, 3.9318256327243257), (&apos;blandings&apos;, 3.871201010907891), (&apos;kells&apos;, 3.871201010907891), (&apos;brashear&apos;, 3.8501476017100584), ...]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 负面评论常出现词汇</span></span><br><span class="line">list(reversed(pos_neg_ratios.most_common()))[<span class="number">0</span>:<span class="number">30</span>]</span><br></pre></td></tr></table></figure><pre><code>[(&apos;whelk&apos;, -4.605170185988092), (&apos;pressurized&apos;, -4.605170185988092), (&apos;bellwood&apos;, -4.605170185988092), (&apos;mwuhahahaa&apos;, -4.605170185988092), (&apos;insulation&apos;, -4.605170185988092), (&apos;hoodies&apos;, -4.605170185988092), (&apos;yaks&apos;, -4.605170185988092), (&apos;deamon&apos;, -4.605170185988092), (&apos;ziller&apos;, -4.605170185988092), (&apos;lagomorph&apos;, -4.605170185988092), (&apos;marinaro&apos;, -4.605170185988092), (&apos;accelerant&apos;, -4.605170185988092), (&apos;yez&apos;, -4.605170185988092), (&apos;superhu&apos;, -4.605170185988092), (&apos;fastidiously&apos;, -4.605170185988092), (&apos;spotlessly&apos;, -4.605170185988092), (&apos;dahlink&apos;, -4.605170185988092), (&apos;rebanished&apos;, -4.605170185988092), (&apos;unmated&apos;, -4.605170185988092), (&apos;wushu&apos;, -4.605170185988092), (&apos;nix&apos;, -4.605170185988092), (&apos;echance&apos;, -4.605170185988092), (&apos;vannet&apos;, -4.605170185988092), (&apos;hodet&apos;, -4.605170185988092), (&apos;francie&apos;, -4.605170185988092), (&apos;vivisects&apos;, -4.605170185988092), (&apos;degeneration&apos;, -4.605170185988092), (&apos;lowlight&apos;, -4.605170185988092), (&apos;slackly&apos;, -4.605170185988092), (&apos;unrurly&apos;, -4.605170185988092)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bokeh.models <span class="keyword">import</span> ColumnDataSource, LabelSet</span><br><span class="line"><span class="keyword">from</span> bokeh.plotting <span class="keyword">import</span> figure, show, output_file</span><br><span class="line"><span class="keyword">from</span> bokeh.io <span class="keyword">import</span> output_notebook</span><br><span class="line">output_notebook()</span><br></pre></td></tr></table></figure><pre><code>&lt;div class=&quot;bk-root&quot;&gt;    &lt;a href=&quot;https://bokeh.pydata.org&quot; target=&quot;_blank&quot; class=&quot;bk-logo bk-logo-small bk-logo-notebook&quot;&gt;&lt;/a&gt;    &lt;span id=&quot;98fe141d-97bd-4335-a540-473361f947a4&quot;&gt;Loading BokehJS ...&lt;/span&gt;&lt;/div&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hist, edges = np.histogram(list(map(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],</span><br><span class="line">                           pos_neg_ratios.most_common())), </span><br><span class="line">                           density=<span class="keyword">True</span>, bins=<span class="number">100</span>, normed=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">p = figure(tools=<span class="string">"pan,wheel_zoom,reset,save"</span>,</span><br><span class="line">           toolbar_location=<span class="string">"above"</span>,</span><br><span class="line">           title=<span class="string">"Word Positive/Negative Affinity Distribution"</span>)</span><br><span class="line">p.quad(top=hist, bottom=<span class="number">0</span>, left=edges[:<span class="number">-1</span>], right=edges[<span class="number">1</span>:], </span><br><span class="line">       line_color=<span class="string">"#555555"</span>)</span><br><span class="line">show(p)</span><br></pre></td></tr></table></figure><div class="bk-root"><br>    <div class="bk-plotdiv" id="40753858-7472-4975-9133-4441fbe5c51e"></div><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">frequency_frequency = Counter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word, cnt <span class="keyword">in</span> total_counts.most_common():</span><br><span class="line">    frequency_frequency[cnt] += <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hist, edges = np.histogram(list(map(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],frequency_frequency.most_common())), density=<span class="keyword">True</span>, bins=<span class="number">100</span>, normed=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">p = figure(tools=<span class="string">"pan,wheel_zoom,reset,save"</span>,</span><br><span class="line">           toolbar_location=<span class="string">"above"</span>,</span><br><span class="line">           title=<span class="string">"The frequency distribution of the words in our corpus"</span>)</span><br><span class="line">p.quad(top=hist, bottom=<span class="number">0</span>, left=edges[:<span class="number">-1</span>], right=edges[<span class="number">1</span>:], line_color=<span class="string">"#555555"</span>)</span><br><span class="line">show(p)</span><br></pre></td></tr></table></figure><div class="bk-root"><br>    <div class="bk-plotdiv" id="9a042fd8-c91e-401f-9639-5c7a92465777"></div><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, reviews,labels,min_count = <span class="number">10</span>,polarity_cutoff = <span class="number">0.1</span>,hidden_nodes = <span class="number">10</span>, learning_rate = <span class="number">0.1</span>)</span>:</span></span><br><span class="line">        np.random.seed(<span class="number">1</span>)</span><br><span class="line">        self.pre_process_data(reviews, labels, polarity_cutoff, min_count)</span><br><span class="line">        self.init_network(len(self.review_vocab),hidden_nodes, <span class="number">1</span>, learning_rate)</span><br><span class="line">        <span class="comment"># 输入层节点个数为词汇库长度</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_process_data</span><span class="params">(self, reviews, labels, polarity_cutoff, min_count)</span>:</span>        <span class="comment"># 提取出所有词汇</span></span><br><span class="line">        <span class="comment">#---------------进一步去除噪音-------------------</span></span><br><span class="line">        positive_counts = Counter()</span><br><span class="line">        negative_counts = Counter()</span><br><span class="line">        total_counts = Counter()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(reviews)):</span><br><span class="line">            <span class="keyword">if</span>(labels[i] == <span class="string">'POSITIVE'</span>):</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> reviews[i].split(<span class="string">" "</span>):</span><br><span class="line">                    positive_counts[word] += <span class="number">1</span></span><br><span class="line">                    total_counts[word] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> reviews[i].split(<span class="string">" "</span>):</span><br><span class="line">                    negative_counts[word] += <span class="number">1</span></span><br><span class="line">                    total_counts[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        pos_neg_ratios = Counter()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> term,cnt <span class="keyword">in</span> list(total_counts.most_common()):</span><br><span class="line">            <span class="keyword">if</span>(cnt &gt;= <span class="number">50</span>):</span><br><span class="line">                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+<span class="number">1</span>)</span><br><span class="line">                pos_neg_ratios[term] = pos_neg_ratio</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> word,ratio <span class="keyword">in</span> pos_neg_ratios.most_common():</span><br><span class="line">            <span class="keyword">if</span>(ratio &gt; <span class="number">1</span>):</span><br><span class="line">                pos_neg_ratios[word] = np.log(ratio)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pos_neg_ratios[word] = -np.log((<span class="number">1</span> / (ratio + <span class="number">0.01</span>)))</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="comment"># populate review_vocab with all of the words in the given reviews</span></span><br><span class="line">        review_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">                <span class="comment">## New for Project 6: only add words that occur at least min_count times</span></span><br><span class="line">                <span class="comment">#                     and for words with pos/neg ratios, only add words</span></span><br><span class="line">                <span class="comment">#                     that meet the polarity_cutoff</span></span><br><span class="line">                <span class="keyword">if</span>(total_counts[word] &gt; min_count):</span><br><span class="line">                    <span class="keyword">if</span>(word <span class="keyword">in</span> pos_neg_ratios.keys()):</span><br><span class="line">                        <span class="keyword">if</span>((pos_neg_ratios[word] &gt;= polarity_cutoff) <span class="keyword">or</span> (pos_neg_ratios[word] &lt;= -polarity_cutoff)):</span><br><span class="line">                            review_vocab.add(word)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        review_vocab.add(word)</span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment">## </span></span><br><span class="line">        <span class="comment">## ----------------------------------------</span></span><br><span class="line">        </span><br><span class="line">        self.review_vocab = list(review_vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取标签</span></span><br><span class="line">        label_vocab = set()</span><br><span class="line">        <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">            label_vocab.add(label)</span><br><span class="line">        self.label_vocab = list(label_vocab)</span><br><span class="line">        </span><br><span class="line">        self.review_vocab_size = len(self.review_vocab)</span><br><span class="line">        self.label_vocab_size = len(self.label_vocab)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建词汇库字典</span></span><br><span class="line">        self.word2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(self.review_vocab):</span><br><span class="line">            self.word2index[word] = i</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建标签索引字典</span></span><br><span class="line">        self.label2index = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(self.label_vocab):</span><br><span class="line">            self.label2index[label] = i</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化模型，输入节点为词汇库长度，隐藏层节点默认为10，输出节点默认为1，学习率默认为0.1</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_network</span><span class="params">(self, input_nodes, hidden_nodes, output_nodes, learning_rate)</span>:</span></span><br><span class="line">        self.input_nodes = input_nodes</span><br><span class="line">        self.hidden_nodes = hidden_nodes</span><br><span class="line">        self.output_nodes = output_nodes</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line"></span><br><span class="line">        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))</span><br><span class="line">        self.weights_1_2 = np.random.normal(<span class="number">0.0</span>, self.output_nodes**<span class="number">-0.5</span>, </span><br><span class="line">                                                (self.hidden_nodes, self.output_nodes))</span><br><span class="line">        </span><br><span class="line">        self.layer_1 = np.zeros((<span class="number">1</span>,hidden_nodes))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义标签转换函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_target_for_label</span><span class="params">(self,label)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>(label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_output_2_derivative</span><span class="params">(self,output)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> output * (<span class="number">1</span> - output)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#=======================训练函数=============================    </span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training_reviews_raw, training_labels)</span>:</span></span><br><span class="line"></span><br><span class="line">        training_reviews = list()</span><br><span class="line">        <span class="keyword">for</span> review <span class="keyword">in</span> training_reviews_raw:</span><br><span class="line">            indices = set()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> review.split(<span class="string">" "</span>):</span><br><span class="line">                <span class="keyword">if</span>(word <span class="keyword">in</span> self.word2index.keys()):</span><br><span class="line">                    indices.add(self.word2index[word])</span><br><span class="line">            training_reviews.append(list(indices))</span><br><span class="line">        <span class="comment"># 以上代码记录下每条评论的每个词汇在对应字典中的位置索引，以此作为输入数据。</span></span><br><span class="line">        <span class="comment"># 最终这里生成的training_reviews里面存放的是每条影评的各个词汇在词汇字典里的索引。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(len(training_reviews) == len(training_labels))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义一个容器计算正确的个数用以计算准确率</span></span><br><span class="line">        correct_so_far = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(training_reviews)):</span><br><span class="line">            review = training_reviews[i]</span><br><span class="line">            label = training_labels[i]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 输入层</span></span><br><span class="line">            self.layer_1 *= <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> review:</span><br><span class="line">                self.layer_1 += self.weights_0_1[index]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层</span></span><br><span class="line">            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))            </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 输出层误差和梯度</span></span><br><span class="line">            layer_2_error = layer_2 - self.get_target_for_label(label) <span class="comment"># Output layer error is the difference between desired target and actual output.</span></span><br><span class="line">            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播误差和梯度</span></span><br><span class="line">            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) <span class="comment"># errors propagated to the hidden layer</span></span><br><span class="line">            layer_1_delta = layer_1_error <span class="comment"># hidden layer gradients - no nonlinearity so it's the same as the error</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新权值</span></span><br><span class="line">            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate <span class="comment"># update hidden-to-output weights with gradient descent step</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> review:</span><br><span class="line">                self.weights_0_1[index] -= layer_1_delta[<span class="number">0</span>] * self.learning_rate <span class="comment"># update input-to-hidden weights with gradient descent step</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(layer_2 &gt;= <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'POSITIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span>(layer_2 &lt; <span class="number">0.5</span> <span class="keyword">and</span> label == <span class="string">'NEGATIVE'</span>):</span><br><span class="line">                correct_so_far += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(training_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct_so_far) + <span class="string">" #Trained:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Training Accuracy:"</span> + str(correct_so_far * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">            <span class="keyword">if</span>(i % <span class="number">2500</span> == <span class="number">0</span>):</span><br><span class="line">                print(<span class="string">""</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, testing_reviews, testing_labels)</span>:</span></span><br><span class="line">        </span><br><span class="line">        correct = <span class="number">0</span></span><br><span class="line">        start = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(testing_reviews)):</span><br><span class="line">            pred = self.run(testing_reviews[i])</span><br><span class="line">            <span class="keyword">if</span>(pred == testing_labels[i]):</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            elapsed_time = float(time.time() - start)</span><br><span class="line">            reviews_per_second = i / elapsed_time <span class="keyword">if</span> elapsed_time &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            sys.stdout.write(<span class="string">"\rProgress:"</span> + str(<span class="number">100</span> * i/float(len(testing_reviews)))[:<span class="number">4</span>] \</span><br><span class="line">                             + <span class="string">"% Speed(reviews/sec):"</span> + str(reviews_per_second)[<span class="number">0</span>:<span class="number">5</span>] \</span><br><span class="line">                             + <span class="string">" #Correct:"</span> + str(correct) + <span class="string">" #Tested:"</span> + str(i+<span class="number">1</span>) \</span><br><span class="line">                             + <span class="string">" Testing Accuracy:"</span> + str(correct * <span class="number">100</span> / float(i+<span class="number">1</span>))[:<span class="number">4</span>] + <span class="string">"%"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, review)</span>:</span></span><br><span class="line">        self.layer_1 *= <span class="number">0</span></span><br><span class="line">        unique_indices = set()</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> review.lower().split(<span class="string">" "</span>):</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> self.word2index.keys():</span><br><span class="line">                unique_indices.add(self.word2index[word])</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> unique_indices:</span><br><span class="line">            self.layer_1 += self.weights_0_1[index]</span><br><span class="line">        </span><br><span class="line">        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(layer_2[<span class="number">0</span>] &gt;= <span class="number">0.5</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"POSITIVE"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"NEGATIVE"</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlp = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>],</span><br><span class="line">                       min_count=<span class="number">20</span>,polarity_cutoff=<span class="number">0.6</span>,</span><br><span class="line">                       learning_rate=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp.test(reviews[<span class="number">-1000</span>:], labels[<span class="number">-1000</span>:])</span><br></pre></td></tr></table></figure><pre><code>Progress:99.9% Speed(reviews/sec):5572. #Correct:845 #Tested:1000 Testing Accuracy:84.5%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_most_similar_words</span><span class="params">(focus = <span class="string">"horrible"</span>)</span>:</span></span><br><span class="line">    most_similar = Counter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> mlp_full.word2index.keys():</span><br><span class="line">        most_similar[word] = np.dot(mlp_full.weights_0_1[mlp_full.word2index[word]],mlp_full.weights_0_1[mlp_full.word2index[focus]])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> most_similar.most_common()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp_full = SentimentNetwork(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>],min_count=<span class="number">0</span>,polarity_cutoff=<span class="number">0</span>,learning_rate=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mlp_full.train(reviews[:<span class="number">-1000</span>],labels[:<span class="number">-1000</span>])</span><br></pre></td></tr></table></figure><pre><code>Progress:0.0% Speed(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%Progress:10.4% Speed(reviews/sec):1712. #Correct:1962 #Trained:2501 Training Accuracy:78.4%Progress:20.8% Speed(reviews/sec):1646. #Correct:4002 #Trained:5001 Training Accuracy:80.0%Progress:31.2% Speed(reviews/sec):1545. #Correct:6120 #Trained:7501 Training Accuracy:81.5%Progress:41.6% Speed(reviews/sec):1571. #Correct:8271 #Trained:10001 Training Accuracy:82.7%Progress:52.0% Speed(reviews/sec):1560. #Correct:10431 #Trained:12501 Training Accuracy:83.4%Progress:62.5% Speed(reviews/sec):1573. #Correct:12565 #Trained:15001 Training Accuracy:83.7%Progress:72.9% Speed(reviews/sec):1567. #Correct:14670 #Trained:17501 Training Accuracy:83.8%Progress:83.3% Speed(reviews/sec):1531. #Correct:16833 #Trained:20001 Training Accuracy:84.1%Progress:93.7% Speed(reviews/sec):1512. #Correct:19015 #Trained:22501 Training Accuracy:84.5%Progress:99.9% Speed(reviews/sec):1504. #Correct:20335 #Trained:24000 Training Accuracy:84.7%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get_most_similar_words("excellent")</span></span><br><span class="line"><span class="comment"># get_most_similar_words("terrible")</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.colors <span class="keyword">as</span> colors</span><br><span class="line"></span><br><span class="line">words_to_visualize = list()</span><br><span class="line"><span class="keyword">for</span> word, ratio <span class="keyword">in</span> pos_neg_ratios.most_common(<span class="number">500</span>):</span><br><span class="line">    <span class="keyword">if</span>(word <span class="keyword">in</span> mlp_full.word2index.keys()):</span><br><span class="line">        words_to_visualize.append(word)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> word, ratio <span class="keyword">in</span> list(reversed(pos_neg_ratios.most_common()))[<span class="number">0</span>:<span class="number">500</span>]:</span><br><span class="line">    <span class="keyword">if</span>(word <span class="keyword">in</span> mlp_full.word2index.keys()):</span><br><span class="line">        words_to_visualize.append(word)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">pos = <span class="number">0</span></span><br><span class="line">neg = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">colors_list = list()</span><br><span class="line">vectors_list = list()</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> words_to_visualize:</span><br><span class="line">    <span class="keyword">if</span> word <span class="keyword">in</span> pos_neg_ratios.keys():</span><br><span class="line">        vectors_list.append(mlp_full.weights_0_1[mlp_full.word2index[word]])</span><br><span class="line">        <span class="keyword">if</span>(pos_neg_ratios[word] &gt; <span class="number">0</span>):</span><br><span class="line">            pos+=<span class="number">1</span></span><br><span class="line">            colors_list.append(<span class="string">"#00ff00"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            neg+=<span class="number">1</span></span><br><span class="line">            colors_list.append(<span class="string">"#000000"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line">tsne = TSNE(n_components=<span class="number">2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">words_top_ted_tsne = tsne.fit_transform(vectors_list)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">p = figure(tools=<span class="string">"pan,wheel_zoom,reset,save"</span>,</span><br><span class="line">           toolbar_location=<span class="string">"above"</span>,</span><br><span class="line">           title=<span class="string">"vector T-SNE for most polarized words"</span>)</span><br><span class="line"></span><br><span class="line">source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,<span class="number">0</span>],</span><br><span class="line">                                    x2=words_top_ted_tsne[:,<span class="number">1</span>],</span><br><span class="line">                                    names=words_to_visualize,</span><br><span class="line">                                    color=colors_list))</span><br><span class="line"></span><br><span class="line">p.scatter(x=<span class="string">"x1"</span>, y=<span class="string">"x2"</span>, size=<span class="number">8</span>, source=source, fill_color=<span class="string">"color"</span>)</span><br><span class="line"></span><br><span class="line">word_labels = LabelSet(x=<span class="string">"x1"</span>, y=<span class="string">"x2"</span>, text=<span class="string">"names"</span>, y_offset=<span class="number">6</span>,</span><br><span class="line">                  text_font_size=<span class="string">"8pt"</span>, text_color=<span class="string">"#555555"</span>,</span><br><span class="line">                  source=source, text_align=<span class="string">'center'</span>)</span><br><span class="line">p.add_layout(word_labels)</span><br><span class="line"></span><br><span class="line">show(p)</span><br><span class="line"></span><br><span class="line"><span class="comment"># green indicates positive words, black indicates negative words</span></span><br></pre></td></tr></table></figure><div class="bk-root"><br>    <div class="bk-plotdiv" id="d3918d6b-4bbe-44b0-9b84-9c4239966e87"></div><br></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、准备数据&quot;&gt;&lt;a href=&quot;#一、准备数据&quot; class=&quot;headerlink&quot; title=&quot;一、准备数据&quot;&gt;&lt;/a&gt;一、准备数据&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="案例" scheme="http://www.ihoge.cn/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>Pyspark连接读取Hive和Mysql数据库</title>
    <link href="http://www.ihoge.cn/2018/Pyspark_Hive_Mysql.html"/>
    <id>http://www.ihoge.cn/2018/Pyspark_Hive_Mysql.html</id>
    <published>2018-06-09T07:59:21.000Z</published>
    <updated>2018-06-27T09:20:02.934Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、前期准备"><a href="#一、前期准备" class="headerlink" title="一、前期准备__"></a>一、前期准备__</h2><p>这里默认spark版本支持hive且与hadoop本本相互兼容。<br>这里使用的版本是：</p><ul><li>Hadoop 2.7.5</li><li>Spark 2.3</li><li>Anaconda 3.5</li><li>Hive 1.2</li></ul><p>1）安装mysql</p><p>2）创建hive用户并赋予权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create user 'hive'@'*' identified by 'hive';</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这里允许所有用户连接可以以hive身份登陆mysql，并赋予hive用户操作所有数据的权限。下面读写mysql时候需要这个权限</span></span><br><span class="line">grant all privileges on *.* to 'hive'@'%' identified by 'hive';</span><br><span class="line"></span><br><span class="line">grant all privileges on *.* to 'root'@'%' identified by '1';</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>3）修改mysql相关配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1. 添加以下内容，目的是增加等待时常，避免因默认连接时间较短造成的连接失败</span></span><br><span class="line">[mysqld]</span><br><span class="line"><span class="meta">#</span><span class="bash"></span></span><br><span class="line">wait_timeout=86400</span><br><span class="line">interactive_timeout=7200</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2. 修改一下配置，目的是允许所有IP访问本地mysql，这个很重要否则只能通过locaohost访问</span></span><br><span class="line"></span><br><span class="line">bind-address            = 0.0.0.0</span><br></pre></td></tr></table></figure><p>4）配置hive-site.xml文件</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>master,slave1,slave2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>5）初始化hive元数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure><p>6）将mysql驱动器分别拷贝到<code>hive/lib</code>和<code>spark/jars/</code></p><p>7）将<code>hive/conf/hive-site.xml</code>拷贝到<code>spark/conf</code></p><h2 id="二、Pyspark读写Hive数据库"><a href="#二、Pyspark读写Hive数据库" class="headerlink" title="二、Pyspark读写Hive数据库"></a>二、Pyspark读写Hive数据库</h2><p><strong>初始化实例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext</span><br><span class="line"></span><br><span class="line">hive = HiveContext(sc)</span><br></pre></td></tr></table></figure><p><strong>读取Hive数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">st_ds = hive.sql(<span class="string">"select * from sparktest.student limit 5"</span>)</span><br><span class="line">st_ds.show()</span><br></pre></td></tr></table></figure><pre><code>+---+-------+------+---+| id|   name|gender|age|+---+-------+------+---+|  1|Xueqian|     F| 23||  1|Xueqian|     F| 23||  1|Xueqian|     F| 23||  1|Xueqian|     F| 23||  1|Xueqian|     F| 23|+---+-------+------+---+</code></pre><p><strong>写入Hive数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">st_ds.createTempView(<span class="string">"test"</span>)</span><br><span class="line">hive.sql(<span class="string">"insert into sparktest.student select * from test"</span>)</span><br></pre></td></tr></table></figure><pre><code>DataFrame[]</code></pre><h2 id="三、Pyspark-读写Mysql数据库"><a href="#三、Pyspark-读写Mysql数据库" class="headerlink" title="三、Pyspark 读写Mysql数据库"></a>三、Pyspark 读写Mysql数据库</h2><p>疑问：当pyspark 以本地模式启动时可以正常读取mysql，以yarn-client模式启动时无法读取mysql数据库</p><p>解决：初始化连接驱动时使用<code>jdbc:mysql://master:3306</code>即可，前提是修改mysql配置<code>bind-address = 0.0.0.0</code></p><p><strong>读取Mysql数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql_df = spark.read.format(<span class="string">'jdbc'</span>).options(url=<span class="string">'jdbc:mysql://master:3306/hive'</span>,</span><br><span class="line">                                             dbtable=<span class="string">'VERSION'</span>, user=<span class="string">'hive'</span>, </span><br><span class="line">                                             password=<span class="string">'hive'</span>).load()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql_df.show(truncate=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><pre><code>+------+--------------+---------------------------------------+|VER_ID|SCHEMA_VERSION|VERSION_COMMENT                        |+------+--------------+---------------------------------------+|1     |1.2.0         |Set by MetaStore hadoop@192.168.221.138|+------+--------------+---------------------------------------+</code></pre><p><strong>写入Mysql数据库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里使用上面hive读取的数据</span></span><br><span class="line">st_ds.write.jdbc(<span class="string">"jdbc:mysql://master:3306/test?user=hive&amp;password=hive"</span>,<span class="string">"t1"</span>, <span class="string">"overwrite"</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、前期准备&quot;&gt;&lt;a href=&quot;#一、前期准备&quot; class=&quot;headerlink&quot; title=&quot;一、前期准备__&quot;&gt;&lt;/a&gt;一、前期准备__&lt;/h2&gt;&lt;p&gt;这里默认spark版本支持hive且与hadoop本本相互兼容。
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>使用 Keras 分析 IMDB 电影数据</title>
    <link href="http://www.ihoge.cn/2018/Keras2.html"/>
    <id>http://www.ihoge.cn/2018/Keras2.html</id>
    <published>2018-06-07T04:20:21.000Z</published>
    <updated>2018-08-05T10:20:19.272Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="使用-Keras-分析-IMDB-电影数据"><a href="#使用-Keras-分析-IMDB-电影数据" class="headerlink" title="使用 Keras 分析 IMDB 电影数据"></a>使用 Keras 分析 IMDB 电影数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h2 id="1-加载数据"><a href="#1-加载数据" class="headerlink" title="1. 加载数据"></a>1. 加载数据</h2><p>该数据集预先加载了 Keras，所以一个简单的命令就会帮助我们划分训练和测试数据。 这里有一个我们想看多少单词的参数。 这里已将它设置为1000，也可以随时尝试设置为其他数字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (it's preloaded in Keras)</span></span><br><span class="line"><span class="comment"># 这里下载数据集需要翻墙</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(x_test.shape)</span><br><span class="line"></span><br><span class="line">print(y_train.shape)</span><br><span class="line">print(y_test.shape)</span><br></pre></td></tr></table></figure><pre><code>(25000,)(25000,)</code></pre><h2 id="2-检查数据"><a href="#2-检查数据" class="headerlink" title="2. 检查数据"></a>2. 检查数据</h2><p>请注意，数据已经过预处理，其中所有单词都包含数字，评论作为向量与评论中包含的单词一起出现。 例如，如果单词’the’是我们词典中的第一个单词，并且评论包含单词’the’，那么在相应的向量中有 1。</p><p>输出结果是 1 和 0 的向量，其中 1 表示正面评论，0 是负面评论。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x_train[<span class="number">0</span>])</span><br><span class="line">print(y_train[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]1</code></pre><h2 id="3-输出的-One-hot-编码"><a href="#3-输出的-One-hot-编码" class="headerlink" title="3. 输出的 One-hot 编码"></a>3. 输出的 One-hot 编码</h2><p>在这里，我们将输入向量转换为 (0,1)-向量。 例如，如果预处理的向量包含数字 14，则在处理的向量中，第 14 个输入将是 1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># One-hot encoding the output into vector mode, each of length 1000</span></span><br><span class="line">tokenizer = Tokenizer(num_words=<span class="number">1000</span>)</span><br><span class="line">X_train = tokenizer.sequences_to_matrix(x_train, mode=<span class="string">'binary'</span>)</span><br><span class="line">X_test = tokenizer.sequences_to_matrix(x_test, mode=<span class="string">'binary'</span>)</span><br><span class="line"><span class="comment"># print(x_train[0])</span></span><br></pre></td></tr></table></figure><p>同时我们将对输出进行 one-hot 编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">2</span></span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br><span class="line">y_test = keras.utils.to_categorical(y_test, num_classes)</span><br></pre></td></tr></table></figure><h2 id="4-模型构建"><a href="#4-模型构建" class="headerlink" title="4. 模型构建"></a>4. 模型构建</h2><p>使用 sequential 在这里构建模型。 可以随意尝试不同的层和大小！ 此外，你可以尝试添加 dropout 层以减少过拟合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.shape</span><br></pre></td></tr></table></figure><pre><code>(25000, 1000)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Build the model architecture</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">'softmax'</span>, input_dim=<span class="number">1000</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># model.add(Dense(64, activation='softmax'))</span></span><br><span class="line"><span class="comment"># model.add(Dropout(0.2))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model.add(Dense(32, activation='softmax'))</span></span><br><span class="line"><span class="comment"># model.add(Dropout(0.2))</span></span><br><span class="line"></span><br><span class="line">model.add(Dense(<span class="number">8</span>, activation=<span class="string">'softsign'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">model.add(Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Compile the model using a loss function and an optimizer.</span></span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, </span><br><span class="line">              optimizer=<span class="string">'Adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_97 (Dense)             (None, 128)               128128    _________________________________________________________________dropout_60 (Dropout)         (None, 128)               0         _________________________________________________________________dense_98 (Dense)             (None, 8)                 1032      _________________________________________________________________dropout_61 (Dropout)         (None, 8)                 0         _________________________________________________________________dense_99 (Dense)             (None, 2)                 18        =================================================================Total params: 129,178Trainable params: 129,178Non-trainable params: 0_________________________________________________________________</code></pre><h2 id="5-训练模型"><a href="#5-训练模型" class="headerlink" title="5. 训练模型"></a>5. 训练模型</h2><p>运行模型。 你可以尝试不同的 batch_size 和 epoch 数量！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, epochs=<span class="number">30</span>, batch_size=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/3025000/25000 [==============================] - 1s 45us/step - loss: 0.2456 - acc: 0.6180Epoch 2/3025000/25000 [==============================] - 0s 13us/step - loss: 0.0914 - acc: 0.8818Epoch 29/3025000/25000 [==============================] - 0s 15us/step - loss: 0.0909 - acc: 0.8830Epoch 30/3025000/25000 [==============================] - 0s 17us/step - loss: 0.0907 - acc: 0.8834</code></pre><h2 id="6-评估模型"><a href="#6-评估模型" class="headerlink" title="6. 评估模型"></a>6. 评估模型</h2><p>你可以在测试集上评估模型，这将为你提供模型的准确性。你得出的结果可以大于 85% 吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_score = model.evaluate(X_train, y_train, verbose=<span class="number">0</span>)</span><br><span class="line">test_score = model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train_Accuracy: "</span>, train_score[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"test_Accuracy: "</span>, test_score[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>train_Accuracy:  0.90024test_Accuracy:  0.86136</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;使用-Keras-分析-IMDB-电影数据&quot;&gt;&lt;a href=&quot;#使用-Keras-分析-IMDB-电影数据&quot; class=&quot;headerlink&quot; title=&quot;使用 Keras 分析 IMDB 电影数据&quot;&gt;&lt;/a&gt;使用 Ke
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="案例" scheme="http://www.ihoge.cn/tags/%E6%A1%88%E4%BE%8B/"/>
    
      <category term="keras" scheme="http://www.ihoge.cn/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>Keras初探</title>
    <link href="http://www.ihoge.cn/2018/Keras.html"/>
    <id>http://www.ihoge.cn/2018/Keras.html</id>
    <published>2018-06-06T04:20:21.000Z</published>
    <updated>2018-06-21T15:30:05.924Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h1><h2 id="简单示例"><a href="#简单示例" class="headerlink" title="简单示例"></a>简单示例</h2><p>Keras 层就像神经网络层。有全连接层、最大池化层和激活层。你可以使用模型的 add() 函数添加层。例如，简单的模型可以如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation, Flatten</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建序列模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment">#第一层 - 添加有128个节点的全连接层以及32个节点的输入层</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, input_dim=<span class="number">32</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#第二层 - 添加 softmax 激活层</span></span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#第三层 - 添加全连接层</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#第四层 - 添加 Sigmoid 激活层</span></span><br><span class="line">model.add(Activation(<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure><p>Keras 将根据第一层自动推断后续所有层的形状。这意味着，你只需为第一层设置输入维度。</p><p>上面的第一层 model.add(Dense(input_dim=32)) 将维度设为 32（表示数据来自 32 维空间）。第二层级获取第一层级的输出，并将输出维度设为 128 个节点。这种将输出传递给下一层级的链继续下去，直到最后一个层级（即模型的输出）。可以看出输出维度是 10。</p><p>构建好模型后，我们就可以用以下命令对其进行编译。我们将损失函数指定为我们一直处理的 categorical_crossentropy。我们还可以指定优化程序，稍后我们将了解这一概念，暂时将使用 adam。最后，我们可以指定评估模型用到的指标。我们将使用准确率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">"categorical_crossentropy"</span>, optimizer=<span class="string">"adam"</span>, metrics = [<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看网络结构</span></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_3 (Dense)              (None, 128)               4224      _________________________________________________________________activation_3 (Activation)    (None, 128)               0         _________________________________________________________________dense_4 (Dense)              (None, 10)                1290      _________________________________________________________________activation_4 (Activation)    (None, 10)                0         =================================================================Total params: 5,514Trainable params: 5,514Non-trainable params: 0_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.fit(X, y, epochs=1000, verbose=0)</span></span><br></pre></td></tr></table></figure><h2 id="小练习"><a href="#小练习" class="headerlink" title="小练习"></a>小练习</h2><p>构建一个简单的多层前向反馈神经网络以解决 XOR 问题。</p><ol><li>将第一层设为 Dense() 层，并将节点数设为8，且 input_dim 设为 2。</li><li>在第二层之后使用 softmax 激活函数。</li><li>将输出层节点设为 2，因为输出只有 2 个类别。</li><li>在输出层之后使用 softmax 激活函数。</li><li>对模型运行 10 个 epoch。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># Using TensorFlow 1.0.0; use tf.python_io in later versions</span></span><br><span class="line"><span class="comment"># tf.python.control_flow_ops = tf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set random seed</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our data</span></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>]]).astype(<span class="string">'float32'</span>)</span><br><span class="line">y = np.array([[<span class="number">0</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">0</span>]]).astype(<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial Setup for Keras</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Activation, Flatten</span><br><span class="line"></span><br><span class="line">y = np_utils.to_categorical(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Building the model</span></span><br><span class="line">xor = Sequential()</span><br><span class="line"><span class="comment"># Add required layers</span></span><br><span class="line">xor.add(Dense(<span class="number">8</span>, input_dim=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">xor.add(Activation(<span class="string">"softmax"</span>))</span><br><span class="line"></span><br><span class="line">xor.add(Dense(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">xor.add(Activation(<span class="string">"softmax"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify loss as "binary_crossentropy", optimizer as "adam",</span></span><br><span class="line"><span class="comment"># and add the accuracy metric</span></span><br><span class="line">xor.compile(loss=<span class="string">"binary_crossentropy"</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment this line to print the model architecture</span></span><br><span class="line">xor.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Fitting the model</span></span><br><span class="line">history = xor.fit(X, y, epochs=<span class="number">100</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Scoring the model</span></span><br><span class="line">score = xor.evaluate(X, y)</span><br><span class="line">print(<span class="string">"\nAccuracy: "</span>, score[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Checking the predictions</span></span><br><span class="line">print(<span class="string">"\nPredictions:"</span>)</span><br><span class="line">print(xor.predict_proba(X))</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_74 (Dense)             (None, 8)                 24        _________________________________________________________________activation_70 (Activation)   (None, 8)                 0         _________________________________________________________________dense_75 (Dense)             (None, 2)                 18        _________________________________________________________________activation_71 (Activation)   (None, 2)                 0         =================================================================Total params: 42Trainable params: 42Non-trainable params: 0_________________________________________________________________4/4 [==============================] - 0s 44ms/stepAccuracy:  0.75Predictions:[[0.51535237 0.4846476 ] [0.49666327 0.5033367 ] [0.49206728 0.5079328 ] [0.49131778 0.5086822 ]]</code></pre><h2 id="利用神经网络的-Kera-预测学生录取情况"><a href="#利用神经网络的-Kera-预测学生录取情况" class="headerlink" title="利用神经网络的 Kera 预测学生录取情况"></a>利用神经网络的 Kera 预测学生录取情况</h2><p>在该 notebook 中，我们基于以下三条数据预测了加州大学洛杉矶分校的研究生录取情况：</p><ul><li><p>GRE 分数（测试）即 GRE Scores (Test)</p></li><li><p>GPA 分数（成绩）即 GPA Scores (Grades)</p></li><li><p>评级（1-4）即 Class rank (1-4)</p></li></ul><p>数据集来源：<a href="http://www.ats.ucla.edu/" target="_blank" rel="noopener">http://www.ats.ucla.edu/</a></p><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>为了加载数据并很好地进行格式化，我们将使用两个非常有用的包，即 Pandas 和 Numpy。 你可以在这里此文档：</p><ul><li><a href="https://pandas.pydata.org/pandas-docs/stable/" target="_blank" rel="noopener">https://pandas.pydata.org/pandas-docs/stable/</a></li><li><a href="https://docs.scipy.org/" target="_blank" rel="noopener">https://docs.scipy.org/</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'student_data.csv'</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15283837029595.jpg" alt=""></p><h3 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Importing matplotlib</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to help us plot</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_points</span><span class="params">(data)</span>:</span></span><br><span class="line">    X = np.array(data[[<span class="string">"gre"</span>,<span class="string">"gpa"</span>]])</span><br><span class="line">    y = np.array(data[<span class="string">"admit"</span>])</span><br><span class="line">    admitted = X[np.argwhere(y==<span class="number">1</span>)]</span><br><span class="line">    rejected = X[np.argwhere(y==<span class="number">0</span>)]</span><br><span class="line">    plt.scatter([s[<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> rejected], [s[<span class="number">0</span>][<span class="number">1</span>] <span class="keyword">for</span> s <span class="keyword">in</span> rejected], s = <span class="number">25</span>, color = <span class="string">'red'</span>, edgecolor = <span class="string">'k'</span>)</span><br><span class="line">    plt.scatter([s[<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> admitted], [s[<span class="number">0</span>][<span class="number">1</span>] <span class="keyword">for</span> s <span class="keyword">in</span> admitted], s = <span class="number">25</span>, color = <span class="string">'cyan'</span>, edgecolor = <span class="string">'k'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Test (GRE)'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Grades (GPA)'</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Plotting the points</span></span><br><span class="line">plot_points(data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15283834240973.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Separating the ranks</span></span><br><span class="line">data_rank1 = data[data[<span class="string">"rank"</span>]==<span class="number">1</span>]</span><br><span class="line">data_rank2 = data[data[<span class="string">"rank"</span>]==<span class="number">2</span>]</span><br><span class="line">data_rank3 = data[data[<span class="string">"rank"</span>]==<span class="number">3</span>]</span><br><span class="line">data_rank4 = data[data[<span class="string">"rank"</span>]==<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plotting the graphs</span></span><br><span class="line">plot_points(data_rank1)</span><br><span class="line">plt.title(<span class="string">"Rank 1"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plot_points(data_rank2)</span><br><span class="line">plt.title(<span class="string">"Rank 2"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plot_points(data_rank3)</span><br><span class="line">plt.title(<span class="string">"Rank 3"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plot_points(data_rank4)</span><br><span class="line">plt.title(<span class="string">"Rank 4"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15283834482119.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15283834792298.jpg" alt=""></p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p><strong>对评级（rank）进行独热编码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">one_hot_rank = pd.get_dummies(data[<span class="string">'rank'</span>], prefix=<span class="string">'rank'</span>)</span><br><span class="line">data = pd.concat([data, one_hot_rank], axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">del</span> data[<span class="string">'rank'</span>]</span><br></pre></td></tr></table></figure><p><strong>缩放数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.gre = data.gre / np.max(data.gre)</span><br><span class="line">data.gpa = data.gpa / np.max(data.gpa)</span><br></pre></td></tr></table></figure><p><strong>生成数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(data)[:,<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># y = np_utils.to_categorical(np.array(data['admit']))</span></span><br><span class="line">y = np.array(pd.get_dummies(data[<span class="string">'admit'</span>]),dtype=<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure><h3 id="构建模型架构"><a href="#构建模型架构" class="headerlink" title="构建模型架构"></a>构建模型架构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</span><br><span class="line"></span><br><span class="line">model = Sequential()</span><br><span class="line"><span class="comment"># 第一层</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, input_dim=<span class="number">6</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">.3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层</span></span><br><span class="line">model.add(Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(Dropout(<span class="number">.1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 第三层</span></span><br><span class="line"><span class="comment"># model.add(Dense(32, activation='relu'))</span></span><br><span class="line"><span class="comment"># model.add(Dropout(.1))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层</span></span><br><span class="line">model.add(Dense(<span class="number">2</span>))</span><br><span class="line">model.add(Activation(<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>,</span><br><span class="line">              optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================dense_153 (Dense)            (None, 128)               896       _________________________________________________________________dropout_26 (Dropout)         (None, 128)               0         _________________________________________________________________dense_154 (Dense)            (None, 64)                8256      _________________________________________________________________dropout_27 (Dropout)         (None, 64)                0         _________________________________________________________________dense_155 (Dense)            (None, 2)                 130       _________________________________________________________________activation_113 (Activation)  (None, 2)                 0         =================================================================Total params: 9,282Trainable params: 9,282Non-trainable params: 0_________________________________________________________________</code></pre><p><strong>训练模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X, y, epochs=<span class="number">1000</span>, batch_size=<span class="number">100</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;keras.callbacks.History at 0x11f4905c0&gt;</code></pre><p><strong>评估模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">score = model.evaluate(X, y)</span><br><span class="line">print(<span class="string">"\nAccuracy: "</span>, score[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Checking the predictions</span></span><br><span class="line">print(<span class="string">"\nPredictions:"</span>)</span><br><span class="line"><span class="comment"># print(model.predict_proba(X)[:5])</span></span><br></pre></td></tr></table></figure><pre><code>400/400 [==============================] - 1s 2ms/stepAccuracy:  0.48Predictions:</code></pre><h3 id="拆分训练及测试集"><a href="#拆分训练及测试集" class="headerlink" title="拆分训练及测试集"></a>拆分训练及测试集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">666</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, epochs=<span class="number">200</span>, batch_size=<span class="number">100</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;keras.callbacks.History at 0x11b578f28&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">test_score = model.evaluate(X_test, y_test)</span><br><span class="line">train_score = model.evaluate(X_train, y_train)</span><br><span class="line">print(<span class="string">"\ntest_Accuracy: "</span>, test_score[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"\ntrain_Accuracy: "</span>, train_score[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># # Checking the predictions</span></span><br><span class="line">print(<span class="string">"\nPredictions:"</span>)</span><br><span class="line">print(model.predict_proba(X_test)[:<span class="number">5</span>])</span><br><span class="line">print(<span class="string">"\nLabel:\n"</span>, y_test[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><pre><code>80/80 [==============================] - 0s 5ms/step320/320 [==============================] - 0s 58us/steptest_Accuracy:  0.7train_Accuracy:  0.709375Predictions:[[0.6729823  0.3270178 ] [0.7473139  0.252686  ] [0.76458406 0.23541589] [0.8641033  0.1358967 ] [0.6661182  0.3338818 ]]Label: [[1. 0.] [1. 0.] [0. 1.] [1. 0.] [0. 1.]]</code></pre><h3 id="玩转参数"><a href="#玩转参数" class="headerlink" title="玩转参数"></a>玩转参数</h3><p>你可以看到我们在训练过程中做了几个规定。 例如，对图层的数量，图层的大小，epoch 的数量等有所规定。<br>现在轮到你来玩转参数！ 你能提高准确性吗？ 以下是针对这些参数的其他建议。 我们将在课程后面学习如下概念的定义：</p><ul><li>激活函数 (Activation function)：relu 和 sigmoid</li><li>损失函数 (Loss function)：categorical_crossentropy，mean_squared_error</li><li>优化器 (Optimizer)：rmsprop，adam，ada</li></ul><p><strong>几种优化器简介</strong></p><p><strong>SGD</strong><br>这是随机梯度下降。它使用了以下参数：</p><ul><li>学习速率</li><li>动量（获取前几步的加权平均值，以便获得动量而不至于陷在局部最低点）。</li><li>Nesterov 动量（当最接近解决方案时，它会减缓梯度）。</li></ul><p><strong>Adam</strong><br>Adam (Adaptive Moment Estimation) 使用更复杂的指数衰减，不仅仅会考虑平均值（第一个动量），并且会考虑前几步的方差（第二个动量）。</p><p><strong>RMSProp</strong><br>RMSProp (RMS 表示均方根误差）通过除以按指数衰减的平方梯度均值来减小学习速率。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Keras&quot;&gt;&lt;a href=&quot;#Keras&quot; class=&quot;headerlink&quot; title=&quot;Keras&quot;&gt;&lt;/a&gt;Keras&lt;/h1&gt;&lt;h2 id=&quot;简单示例&quot;&gt;&lt;a href=&quot;#简单示例&quot; class=&quot;header
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="keras" scheme="http://www.ihoge.cn/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>BF神经网络（二）预测共享单车使用情况</title>
    <link href="http://www.ihoge.cn/2018/BFNN.html"/>
    <id>http://www.ihoge.cn/2018/BFNN.html</id>
    <published>2018-06-01T04:20:21.000Z</published>
    <updated>2018-08-05T10:16:14.071Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>传送门：<br><a href="http://ihoge.cn/2018/DL_BFNN.html" target="_blank" rel="noopener">BF神经网络（一）算法原理与实现</a></p><h1 id="BF神经网络预测共享单车使用情况"><a href="#BF神经网络预测共享单车使用情况" class="headerlink" title="BF神经网络预测共享单车使用情况"></a>BF神经网络预测共享单车使用情况</h1><h2 id="BF实现算法基础"><a href="#BF实现算法基础" class="headerlink" title="BF实现算法基础"></a>BF实现算法基础</h2><p>BF神经网络算法的原理及实现参考前一篇博客：<a href="http://ihoge.cn/2018/DL_BFNN.html" target="_blank" rel="noopener">http://ihoge.cn/2018/DL_BFNN.html</a></p><h2 id="加载和准备数据"><a href="#加载和准备数据" class="headerlink" title="加载和准备数据"></a>加载和准备数据</h2><p>构建神经网络的关键一步是正确地准备数据。不同尺度级别的变量使网络难以高效地掌握正确的权重。我们在下方已经提供了加载和准备数据的代码。你很快将进一步学习这些代码！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_path = <span class="string">'Bike-Sharing-Dataset/hour.csv'</span></span><br><span class="line"></span><br><span class="line">rides = pd.read_csv(data_path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rides.head()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278638061796.jpg" alt=""></p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><p>此数据集包含的是从 2011 年 1 月 1 日到 2012 年 12 月 31 日期间每天每小时的骑车人数。骑车用户分成临时用户和注册用户，cnt 列是骑车用户数汇总列。你可以在上方看到前几行数据。</p><p>下图展示的是数据集中前 10 天左右的骑车人数（某些天不一定是 24 个条目，所以不是精确的 10 天）。你可以在这里看到每小时租金。这些数据很复杂！周末的骑行人数少些，工作日上下班期间是骑行高峰期。我们还可以从上方的数据中看到温度、湿度和风速信息，所有这些信息都会影响骑行人数。你需要用你的模型展示所有这些数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rides[:<span class="number">24</span>*<span class="number">10</span>].plot(x=<span class="string">'dteday'</span>, y=<span class="string">'cnt'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278473558848.jpg" alt=""></p><h3 id="虚拟变量（哑变量）"><a href="#虚拟变量（哑变量）" class="headerlink" title="虚拟变量（哑变量）"></a>虚拟变量（哑变量）</h3><p>下面是一些分类变量，例如季节、天气、月份。要在我们的模型中包含这些数据，我们需要创建二进制虚拟变量。用 Pandas 库中的 <code>get_dummies()</code> 就可以轻松实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dummy_fields = [<span class="string">'season'</span>, <span class="string">'weathersit'</span>, <span class="string">'mnth'</span>, <span class="string">'hr'</span>, <span class="string">'weekday'</span>]</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> dummy_fields:</span><br><span class="line">    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=<span class="keyword">False</span>)</span><br><span class="line">    rides = pd.concat([rides, dummies], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fields_to_drop = [<span class="string">'instant'</span>, <span class="string">'dteday'</span>, <span class="string">'season'</span>, <span class="string">'weathersit'</span>, </span><br><span class="line">                  <span class="string">'weekday'</span>, <span class="string">'atemp'</span>, <span class="string">'mnth'</span>, <span class="string">'workingday'</span>, <span class="string">'hr'</span>]</span><br><span class="line">data = rides.drop(fields_to_drop, axis=<span class="number">1</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278639194549.jpg" alt=""></p><h3 id="调整目标变量"><a href="#调整目标变量" class="headerlink" title="调整目标变量"></a>调整目标变量</h3><p>为了更轻松地训练网络，我们将对每个连续变量标准化，即转换和调整变量，使它们的均值为 0，标准差为 1。</p><p>我们会保存换算因子，以便当我们使用网络进行预测时可以还原数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">quant_features = [<span class="string">'casual'</span>, <span class="string">'registered'</span>, <span class="string">'cnt'</span>, <span class="string">'temp'</span>, <span class="string">'hum'</span>, <span class="string">'windspeed'</span>]</span><br><span class="line"><span class="comment"># Store scalings in a dictionary so we can convert back later</span></span><br><span class="line">scaled_features = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> quant_features:</span><br><span class="line">    mean, std = data[each].mean(), data[each].std()</span><br><span class="line">    scaled_features[each] = [mean, std]</span><br><span class="line">    data.loc[:, each] = (data[each] - mean)/std</span><br></pre></td></tr></table></figure><h3 id="将数据拆分为训练、测试和验证数据集"><a href="#将数据拆分为训练、测试和验证数据集" class="headerlink" title="将数据拆分为训练、测试和验证数据集"></a>将数据拆分为训练、测试和验证数据集</h3><p>我们将大约最后 21 天的数据保存为测试数据集，这些数据集会在训练完网络后使用。我们将使用该数据集进行预测，并与实际的骑行人数进行对比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save data for approximately the last 21 days </span></span><br><span class="line">test_data = data[<span class="number">-21</span>*<span class="number">24</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now remove the test data from the data set </span></span><br><span class="line">data = data[:<span class="number">-21</span>*<span class="number">24</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate the data into features and targets</span></span><br><span class="line">target_fields = [<span class="string">'cnt'</span>, <span class="string">'casual'</span>, <span class="string">'registered'</span>]</span><br><span class="line">features, targets = data.drop(target_fields, axis=<span class="number">1</span>), data[target_fields]</span><br><span class="line">test_features, test_targets = test_data.drop(target_fields, axis=<span class="number">1</span>), test_data[target_fields]</span><br></pre></td></tr></table></figure><p>我们将数据拆分为两个数据集，一个用作训练，一个在网络训练完后用来验证网络。因为数据是有时间序列特性的，所以我们用历史数据进行训练，然后尝试预测未来数据（验证数据集）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hold out the last 60 days or so of the remaining data as a validation set</span></span><br><span class="line">train_features, train_targets = features[:<span class="number">-60</span>*<span class="number">24</span>], targets[:<span class="number">-60</span>*<span class="number">24</span>]</span><br><span class="line">val_features, val_targets = features[<span class="number">-60</span>*<span class="number">24</span>:], targets[<span class="number">-60</span>*<span class="number">24</span>:]</span><br></pre></td></tr></table></figure><h2 id="开始构建网络"><a href="#开始构建网络" class="headerlink" title="开始构建网络"></a>开始构建网络</h2><p>下面你将构建自己的网络。我们已经构建好结构和反向传递部分。你将实现网络的前向传递部分。还需要设置超参数：学习速率、隐藏单元的数量，以及训练传递数量。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15278473911244.jpg" alt=""></p><p>该网络有两个层级，一个隐藏层和一个输出层。隐藏层级将使用 S 型函数作为激活函数。输出层只有一个节点，用于递归，节点的输出和节点的输入相同。即激活函数是 $f(x)=x$。这种函数获得输入信号，并生成输出信号，但是会考虑阈值，称为激活函数。我们完成网络的每个层级，并计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。这一流程叫做前向传播（forward propagation）。</p><p>我们在神经网络中使用权重将信号从输入层传播到输出层。我们还使用权重将错误从输出层传播回网络，以便更新权重。这叫做反向传播（backpropagation）。</p><blockquote><p><strong>提示</strong>：你需要为反向传播实现计算输出激活函数 ($f(x) = x$) 的导数。如果你不熟悉微积分，其实该函数就等同于等式 $y = x$。该等式的斜率是多少？也就是导数 $f(x)$。</p></blockquote><p>你需要完成以下任务：</p><ol><li>实现 S 型激活函数。将 <code>__init__</code> 中的 <code>self.activation_function</code>  设为你的 S 型函数。</li><li>在 <code>train</code> 方法中实现前向传递。</li><li>在 <code>train</code> 方法中实现反向传播算法，包括计算输出错误。</li><li>在 <code>run</code> 方法中实现前向传递。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">( self, input_nodes, hidden_nodes, output_nodes, learning_rate )</span>:</span></span><br><span class="line">        <span class="comment"># Set the number of nodes in input, hidden and output layers</span></span><br><span class="line">        self.input_nodes = input_nodes</span><br><span class="line">        self.hidden_nodes = hidden_nodes</span><br><span class="line">        self.output_nodes = output_nodes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the learning rate</span></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize weights</span></span><br><span class="line">        self.weights_input_to_hidden = np.random.normal( <span class="number">0.0</span>, self.input_nodes**<span class="number">-0.5</span>, (self.input_nodes, self.hidden_nodes) )</span><br><span class="line">        self.weights_hidden_to_output = np.random.normal( <span class="number">0.0</span>, self.hidden_nodes**<span class="number">-0.5</span>, (self.hidden_nodes, self.output_nodes) )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Activation function</span></span><br><span class="line">        self.activation_function = <span class="keyword">lambda</span> x : <span class="number">1</span> / ( <span class="number">1</span> + np.exp(-x) )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, features, targets)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Arguments</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        features: 2D array, each row is one data record, each column is a feature</span></span><br><span class="line"><span class="string">        targets: 1D array of target values</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        n_record = features.shape[<span class="number">0</span>]</span><br><span class="line">        delta_weights_i_h = np.zeros( self.weights_input_to_hidden.shape )</span><br><span class="line">        delta_weights_h_o = np.zeros( self.weights_hidden_to_output.shape )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> zip( features, targets ):</span><br><span class="line">            hidden_inputs = np.dot( X, self.weights_input_to_hidden )</span><br><span class="line">            hidden_outputs = self.activation_function( hidden_inputs )</span><br><span class="line"></span><br><span class="line">            final_inputs = np.dot( hidden_outputs, self.weights_hidden_to_output )</span><br><span class="line">            final_outputs = final_inputs</span><br><span class="line"></span><br><span class="line">            error = y - final_outputs</span><br><span class="line"></span><br><span class="line">            output_error_term = error <span class="comment"># error * 1</span></span><br><span class="line"></span><br><span class="line">            hidden_error = np.dot( self.weights_hidden_to_output, output_error_term )</span><br><span class="line"></span><br><span class="line">            hidden_error_term = hidden_error * hidden_outputs * (<span class="number">1</span> - hidden_outputs) <span class="comment"># f'(hidden_input)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Weight step</span></span><br><span class="line">            delta_weights_i_h += hidden_error_term * X[:,<span class="keyword">None</span>]</span><br><span class="line">            </span><br><span class="line">            delta_weights_h_o += output_error_term * hidden_outputs[:,<span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the weights</span></span><br><span class="line">        self.weights_input_to_hidden += self.lr * delta_weights_i_h/n_record</span><br><span class="line">        self.weights_hidden_to_output += self.lr * delta_weights_h_o/n_record</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        hidden_inputs = np.dot( features, self.weights_input_to_hidden )</span><br><span class="line">        hidden_output = self.activation_function( hidden_inputs )</span><br><span class="line"></span><br><span class="line">        final_inputs = np.dot( hidden_output, self.weights_hidden_to_output )</span><br><span class="line">        final_outputs = final_inputs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MSE</span><span class="params">(y, Y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean((y-Y)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>运行这些单元测试，检查你的网络实现是否正确。这样可以帮助你确保网络已正确实现，然后再开始训练网络。这些测试必须成功才能通过此项目。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> unittest</span><br><span class="line"></span><br><span class="line">inputs = np.array([[<span class="number">0.5</span>, <span class="number">-0.2</span>, <span class="number">0.1</span>]])</span><br><span class="line">targets = np.array([[<span class="number">0.4</span>]])</span><br><span class="line">test_w_i_h = np.array([[<span class="number">0.1</span>, <span class="number">-0.2</span>],</span><br><span class="line">                       [<span class="number">0.4</span>, <span class="number">0.5</span>],</span><br><span class="line">                       [<span class="number">-0.3</span>, <span class="number">0.2</span>]])</span><br><span class="line">test_w_h_o = np.array([[<span class="number">0.3</span>],</span><br><span class="line">                       [<span class="number">-0.1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestMethods</span><span class="params">(unittest.TestCase)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line">    <span class="comment"># Unit tests for data loading</span></span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_data_path</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test that file path to dataset has been unaltered</span></span><br><span class="line">        self.assertTrue(data_path.lower() == <span class="string">'bike-sharing-dataset/hour.csv'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_data_loaded</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test that data frame loaded</span></span><br><span class="line">        self.assertTrue(isinstance(rides, pd.DataFrame))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line">    <span class="comment"># Unit tests for network functionality</span></span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_activation</span><span class="params">(self)</span>:</span></span><br><span class="line">        network = NeuralNetwork(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0.5</span>)</span><br><span class="line">        <span class="comment"># Test that the activation function is a sigmoid</span></span><br><span class="line">        self.assertTrue(np.all(network.activation_function(<span class="number">0.5</span>) == <span class="number">1</span>/(<span class="number">1</span>+np.exp(<span class="number">-0.5</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_train</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test that weights are updated correctly on training</span></span><br><span class="line">        network = NeuralNetwork(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0.5</span>)</span><br><span class="line">        network.weights_input_to_hidden = test_w_i_h.copy()</span><br><span class="line">        network.weights_hidden_to_output = test_w_h_o.copy()</span><br><span class="line">        </span><br><span class="line">        network.train(inputs, targets)</span><br><span class="line">        self.assertTrue(np.allclose(network.weights_hidden_to_output, </span><br><span class="line">                                    np.array([[ <span class="number">0.37275328</span>], </span><br><span class="line">                                              [<span class="number">-0.03172939</span>]])))</span><br><span class="line">        self.assertTrue(np.allclose(network.weights_input_to_hidden,</span><br><span class="line">                                    np.array([[ <span class="number">0.10562014</span>, <span class="number">-0.20185996</span>], </span><br><span class="line">                                              [<span class="number">0.39775194</span>, <span class="number">0.50074398</span>], </span><br><span class="line">                                              [<span class="number">-0.29887597</span>, <span class="number">0.19962801</span>]])))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test correctness of run method</span></span><br><span class="line">        network = NeuralNetwork(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0.5</span>)</span><br><span class="line">        network.weights_input_to_hidden = test_w_i_h.copy()</span><br><span class="line">        network.weights_hidden_to_output = test_w_h_o.copy()</span><br><span class="line"></span><br><span class="line">        self.assertTrue(np.allclose(network.run(inputs), <span class="number">0.09998924</span>))</span><br><span class="line"></span><br><span class="line">suite = unittest.TestLoader().loadTestsFromModule(TestMethods())</span><br><span class="line">unittest.TextTestRunner().run(suite)</span><br></pre></td></tr></table></figure><pre><code>.....----------------------------------------------------------------------Ran 5 tests in 0.008sOK&lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt;</code></pre><h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>现在你将设置网络的超参数。策略是设置的超参数使训练集上的错误很小但是数据不会过拟合。如果网络训练时间太长，或者有太多的隐藏节点，可能就会过于针对特定训练集，无法泛化到验证数据集。即当训练集的损失降低时，验证集的损失将开始增大。</p><p>你还将采用随机梯度下降 (SGD) 方法训练网络。对于每次训练，都获取随机样本数据，而不是整个数据集。与普通梯度下降相比，训练次数要更多，但是每次时间更短。这样的话，网络训练效率更高。稍后你将详细了解 SGD。</p><h3 id="选择迭代次数"><a href="#选择迭代次数" class="headerlink" title="选择迭代次数"></a>选择迭代次数</h3><p>也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。</p><h3 id="选择学习速率"><a href="#选择学习速率" class="headerlink" title="选择学习速率"></a>选择学习速率</h3><p>速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。</p><h3 id="选择隐藏节点数量"><a href="#选择隐藏节点数量" class="headerlink" title="选择隐藏节点数量"></a>选择隐藏节点数量</h3><p>隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment">### Set the hyperparameters here ###</span></span><br><span class="line">iterations = <span class="number">2000</span></span><br><span class="line">learning_rate = <span class="number">0.7</span></span><br><span class="line">hidden_nodes = <span class="number">15</span></span><br><span class="line">output_nodes = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">N_i = train_features.shape[<span class="number">1</span>]</span><br><span class="line">network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)</span><br><span class="line"></span><br><span class="line">losses = &#123;<span class="string">'train'</span>:[], <span class="string">'validation'</span>:[]&#125;</span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(iterations):</span><br><span class="line">    <span class="comment"># Go through a random batch of 128 records from the training data set</span></span><br><span class="line">    batch = np.random.choice(train_features.index, size=<span class="number">128</span>)</span><br><span class="line">    X, y = train_features.ix[batch].values, train_targets.ix[batch][<span class="string">'cnt'</span>]</span><br><span class="line">                             </span><br><span class="line">    network.train(X, y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Printing out the training progress</span></span><br><span class="line">    train_loss = MSE(network.run(train_features).T, train_targets[<span class="string">'cnt'</span>].values)</span><br><span class="line">    val_loss = MSE(network.run(val_features).T, val_targets[<span class="string">'cnt'</span>].values)</span><br><span class="line">    sys.stdout.write(<span class="string">"\rProgress: &#123;:2.1f&#125;"</span>.format(<span class="number">100</span> * ii/float(iterations)) \</span><br><span class="line">                     + <span class="string">"% ... Training loss: "</span> + str(train_loss)[:<span class="number">5</span>] \</span><br><span class="line">                     + <span class="string">" ... Validation loss: "</span> + str(val_loss)[:<span class="number">5</span>])</span><br><span class="line">    sys.stdout.flush()</span><br><span class="line">    </span><br><span class="line">    losses[<span class="string">'train'</span>].append(train_loss)</span><br><span class="line">    losses[<span class="string">'validation'</span>].append(val_loss)</span><br></pre></td></tr></table></figure><pre><code>Progress: 100.0% ... Training loss: 0.092 ... Validation loss: 0.186</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(losses[<span class="string">'train'</span>], label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(losses[<span class="string">'validation'</span>], label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">_ = plt.ylim()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278475026241.jpg" alt=""></p><h2 id="检查预测结果"><a href="#检查预测结果" class="headerlink" title="检查预测结果"></a>检查预测结果</h2><p>使用测试数据看看网络对数据建模的效果如何。如果完全错了，请确保网络中的每步都正确实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">mean, std = scaled_features[<span class="string">'cnt'</span>]</span><br><span class="line">predictions = network.run(test_features).T*std + mean</span><br><span class="line">ax.plot(predictions[<span class="number">0</span>], label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.plot((test_targets[<span class="string">'cnt'</span>]*std + mean).values, label=<span class="string">'Data'</span>)</span><br><span class="line">ax.set_xlim(right=len(predictions))</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">dates = pd.to_datetime(rides.ix[test_data.index][<span class="string">'dteday'</span>])</span><br><span class="line">dates = dates.apply(<span class="keyword">lambda</span> d: d.strftime(<span class="string">'%b %d'</span>))</span><br><span class="line">ax.set_xticks(np.arange(len(dates))[<span class="number">12</span>::<span class="number">24</span>])</span><br><span class="line">_ = ax.set_xticklabels(dates[<span class="number">12</span>::<span class="number">24</span>], rotation=<span class="number">45</span>)</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278475263318.jpg" alt=""></p><h2 id="可选：思考下你的结果（我们不会评估这道题的答案）"><a href="#可选：思考下你的结果（我们不会评估这道题的答案）" class="headerlink" title="可选：思考下你的结果（我们不会评估这道题的答案）"></a>可选：思考下你的结果（我们不会评估这道题的答案）</h2><p>请针对你的结果回答以下问题。模型对数据的预测效果如何？哪里出现问题了？为何出现问题呢？</p><blockquote><p><strong>注意</strong>：你可以通过双击该单元编辑文本。如果想要预览文本，请按 Control + Enter</p></blockquote><h4 id="请将你的答案填写在下方"><a href="#请将你的答案填写在下方" class="headerlink" title="请将你的答案填写在下方"></a>请将你的答案填写在下方</h4><p>问题1:Dec 24 开始至Dec 28这四天的预测误差比较大的原因？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;传送门：&lt;br&gt;&lt;a href=&quot;http://ihoge.cn/2018/DL_BFNN.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BF神经网络（一）算法原理与实现&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;BF神
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="案例" scheme="http://www.ihoge.cn/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>BF神经网络（一）算法原理与实现</title>
    <link href="http://www.ihoge.cn/2018/DL_BFNN.html"/>
    <id>http://www.ihoge.cn/2018/DL_BFNN.html</id>
    <published>2018-05-30T04:20:21.000Z</published>
    <updated>2018-08-05T10:16:04.207Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>传送门：<br><a href="http://ihoge.cn/2018/DL_BFNN.html" target="_blank" rel="noopener">BF神经网络（二）预测共享单车使用情况</a></p><h2 id="一、反向传播算法基本原理"><a href="#一、反向传播算法基本原理" class="headerlink" title="一、反向传播算法基本原理"></a>一、反向传播算法基本原理</h2><p>如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。以一个两层神经网络为例，可以使用链式法则计算输入层-隐藏层间权重的误差。</p><p>要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。</p><p>例如，输出层每个输出节点$k$的误差是$\delta^o_k$，隐藏节点$j$的误差即为输出误差乘以输出层-隐藏层间的权重矩阵以及梯度。</p><p>$$\delta ^h_j=\sum W_{jk}\delta^o_kf’(h_j)$$</p><p>然后，梯度下降与之前相同，只是用新的误差：<br>$$\Delta w_{ij}=\eta \delta^h_jx_i $$</p><p>其中 $w_{ij}$ 是输入和隐藏层之间的权重， $x_i$ 是输入值。这个形式可以表示任意层数。权重更新步长等于步长乘以层输出误差再乘以该层的输入值。<br>$$\Delta w_{qp}=\eta\delta_{output}V_{input}$$<br>现在，你有了输出误差，$\delta_{output}$，便可以反向传播这些误差了。$V_{input}$ 是该层的输入，比如经过隐藏层激活函数的输出值。</p><h2 id="二、范例"><a href="#二、范例" class="headerlink" title="二、范例"></a>二、范例</h2><h3 id="1-单节点误差项"><a href="#1-单节点误差项" class="headerlink" title="1. 单节点误差项"></a>1. 单节点误差项</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278487531883.jpg" alt=""><br>假设我们试着训练一些二进制数据，目标值是 $y=1$。我们从正向传播开始，首先计算输入到隐藏层节点：</p><p>$h=\sum_iw_ix_i=0.1 \times 0.4 - 0.2\times 0.3=-0.02 $</p><p>以及隐藏层节点的输出：</p><p>$a=f(h)=sigmoid(-0.02)=0.495$</p><p>然后将其作为输出节点的输入，该神经网络的输出可表示为：</p><p>$\hat y=f(W \cdot a)=sigmoid(0.1 \times0.495)=0.512$</p><p>基于该神经网络的输出，就可以使用反向传播来更新各层的权重了。sigmoid 函数的导数$\hat y=f(W \cdot a)(1-f(W\cdot a))$，输出节点的误差项(error term)可表示为：</p><p>$\delta^o=(y-\hat y)f’(W\cdot a)=(1-0.512)\times0.512\times (1-0.512)=0.122 $</p><p>现在我们要通过反向传播来计算隐藏节点的误差项。这里我们把输出节点的误差项与隐藏层到输出层的权重 $W$ 相乘。隐藏节点的误差项$\delta ^h_j=\sum W_{jk}\delta^o_kf’(h_j)$，因为该案例只有一个隐藏节点，这就比较简单了</p><p>$\delta ^h=W\delta^of’(h)=0.1 \times 0.122 \times 0.495 \times(1-0.495)=0.003$</p><p>有了误差，就可以计算梯度下降步长了。隐藏层-输出层权重更新步长是学习速率乘以输出节点误差再乘以隐藏节点激活值。</p><p>$\Delta W=\eta\delta^oa=0.5\times0.122\times 0.495=0.0302 $</p><p>然后，输入-隐藏层权重 $w_i$ 是学习速率乘以隐藏节点误差再乘以输入值。</p><p>$\Delta w_i=\eta\delta^ox_i=(0.5\times0.003\times0.1，0.5\times0.003\times0.3)=(0.00015，0.00045)$</p><p>从这个例子中你可以看到 sigmoid 做激活函数的一个缺点。sigmoid 函数导数的最大值是 0.25，因此输出层的误差被减少了至少 75%，隐藏层的误差被减少了至少 93.75%！如果你的神经网络有很多层，使用 sigmoid 激活函数会很快把靠近输入层的权重步长降为很小的值，该问题称作<strong>梯度消失</strong>。后面的课程中你会学到在这方面表现更好，也被广泛用于最新神经网络中的其它激活函数。</p><h3 id="2-多节点误差项时的情况"><a href="#2-多节点误差项时的情况" class="headerlink" title="2. 多节点误差项时的情况"></a>2. 多节点误差项时的情况</h3><p>大多数时候我们需要创建多个隐藏层多几个节点，因此针对这种情况，现在在更新权重时，我们需要考虑隐藏层 每个节点 的误差 $\delta _j$</p><p>$\Delta w_{ij}=\eta\delta_jx_i$</p><p>首先，会有不同数量的输入和隐藏节点，所以试图把误差与输入当作行向量来乘会报错。</p><p>另外，现在$w_{ij}$是一个矩阵，所以右侧对应也应该有跟左侧同样的维度。幸运的是，NumPy 这些都能搞定。如果你用一个列向量数组和一个行向量数组相乘，它会把列向量的第一个元素与行向量的每个元素相乘，组成一个新的二维数组的第一行。列向量的每一个元素依次重复该过程，最后你会得到一个二维数组，形状是 <code>(len(column_vector)</code>,<code>len(row_vector))</code>。</p><pre><code>hidden_error*inputs[:,None]array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])</code></pre><p>这正好是我们计算权重更新的步长的方式。跟以前一样，如果你的输入是一个一行的二维数组，也可以用 <code>hidden_error*inputs.T，</code>但是如果 inputs 是一维数组，就不行了（这时可以使用上述方法<code>hidden_error*inputs[:,None]</code>）。</p><h2 id="三、反向传播练习"><a href="#三、反向传播练习" class="headerlink" title="三、反向传播练习"></a>三、反向传播练习</h2><p>接下来你将用代码来实现一次两个权重的反向传播更新。我们提供了正向传播的代码，你来实现反向传播的部分。</p><p>要做的事</p><ul><li>计算网络输出误差</li><li>计算输出层误差项</li><li>用反向传播计算隐藏层误差项</li><li>计算反向传播误差的权重更新步长</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate sigmoid</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.2</span>])</span><br><span class="line">target = <span class="number">0.6</span></span><br><span class="line">learnrate = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">weights_input_hidden = np.array([[<span class="number">0.5</span>, <span class="number">-0.6</span>],</span><br><span class="line">                                 [<span class="number">0.1</span>, <span class="number">-0.2</span>],</span><br><span class="line">                                 [<span class="number">0.1</span>, <span class="number">0.7</span>]])</span><br><span class="line"></span><br><span class="line">weights_hidden_output = np.array([<span class="number">0.1</span>, <span class="number">-0.3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## Forward pass</span></span><br><span class="line">hidden_layer_input = np.dot(x, weights_input_hidden)</span><br><span class="line">hidden_layer_output = sigmoid(hidden_layer_input)</span><br><span class="line"></span><br><span class="line">output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)</span><br><span class="line">output = sigmoid(output_layer_in)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Backwards pass</span></span><br><span class="line"><span class="comment">## <span class="doctag">TODO:</span> Calculate output error</span></span><br><span class="line">error = target - output</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate error term for output layer</span></span><br><span class="line">output_error_term = error * output * (<span class="number">1</span> - output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate error term for hidden layer</span></span><br><span class="line">hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \</span><br><span class="line">                    hidden_layer_output * (<span class="number">1</span> - hidden_layer_output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate change in weights for hidden layer to output layer</span></span><br><span class="line">delta_w_h_o = learnrate * output_error_term * hidden_layer_output</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate change in weights for input layer to hidden layer</span></span><br><span class="line">delta_w_i_h = learnrate * hidden_error_term * x[:, <span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Change in weights for hidden layer to output layer:'</span>)</span><br><span class="line">print(delta_w_h_o)</span><br><span class="line">print(<span class="string">'Change in weights for input layer to hidden layer:'</span>)</span><br><span class="line">print(delta_w_i_h)</span><br></pre></td></tr></table></figure><pre><code>Change in weights for hidden layer to output layer:[0.00804047 0.00555918]Change in weights for input layer to hidden layer:[[ 1.77005547e-04 -5.11178506e-04] [ 3.54011093e-05 -1.02235701e-04] [-7.08022187e-05  2.04471402e-04]]Nice job!  That&apos;s right!</code></pre><h2 id="四、实现反向传播算法"><a href="#四、实现反向传播算法" class="headerlink" title="四、实现反向传播算法"></a>四、实现反向传播算法</h2><p>现在我们知道输出层的误差是</p><p>$$\delta _k=(y_k-\hat y_k)f’(a_k)$$</p><p>隐藏层误差为：</p><p>$$\delta <em>j=\sum[w</em>{jk}\delta_k]f’(h_j)$$<br>现在我们只考虑一个简单神经网络，它只有一个隐藏层和一个输出节点。这是通过反向传播更新权重的算法概述：</p><p>1) 初始化权重。为了简单起见，这里初始化为0</p><ul><li>输入到隐藏册的权重$\Delta w_{ij}=0$</li><li>隐藏层到输出层权重$\Delta W_j=0$</li></ul><p>2) 对训练数据中的每个点执行</p><ul><li>让它正向通过网络，计算出$\hat y$</li><li>计算输出节点的误差梯度 $\delta^o=(y-\hat y)f’(z)$这里$z=\sum _jW_ja_j$是输出节点的输入。</li><li>误差传播到隐藏层 $\delta^h_j=\delta^oW_jf’(h_j)$</li><li>更新权重步长：<ul><li>$\Delta W_j=\Delta W_j+\delta^oa_j$</li><li>$\Delta w_{ij}=\Delta w_{ij}+\delta_j^ha_i$</li></ul></li></ul><p>3) 更新权重，其中$\eta $是学习率，$m$ 是数据点的数量：</p><ul><li>$W_j=W_j+\eta\Delta W_j/m$</li><li>$w_{ij}=w_{ij}+\eta\Delta w_{ij}/m$</li></ul><p>4) 重复以上过程$e$代。</p><p><strong>代码实现例子</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">admissions = pd.read_csv(<span class="string">'binary.csv'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.concat([admissions, pd.get_dummies(admissions[<span class="string">'rank'</span>], prefix=<span class="string">'rank'</span>)], axis=<span class="number">1</span>)</span><br><span class="line">data = data.drop(<span class="string">'rank'</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> field <span class="keyword">in</span> [<span class="string">'gre'</span>, <span class="string">'gpa'</span>]:</span><br><span class="line">    mean, std = data[field].mean(), data[field].std()</span><br><span class="line">    data.loc[:,field] = (data[field]-mean)/std</span><br><span class="line">    </span><br><span class="line">np.random.seed(<span class="number">21</span>)</span><br><span class="line">sample = np.random.choice(data.index, size=int(len(data)*<span class="number">0.9</span>), replace=<span class="keyword">False</span>)</span><br><span class="line">data, test_data = data.ix[sample], data.drop(sample)</span><br><span class="line"></span><br><span class="line">features, targets = data.drop(<span class="string">'admit'</span>, axis=<span class="number">1</span>), data[<span class="string">'admit'</span>]</span><br><span class="line">features_test, targets_test = test_data.drop(<span class="string">'admit'</span>, axis=<span class="number">1</span>), test_data[<span class="string">'admit'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#=================================</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate sigmoid</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">n_hidden = <span class="number">2</span>  <span class="comment"># number of hidden units</span></span><br><span class="line">epochs = <span class="number">900</span></span><br><span class="line">learnrate = <span class="number">0.005</span></span><br><span class="line"></span><br><span class="line">n_records, n_features = features.shape</span><br><span class="line">last_loss = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">weights_input_hidden = np.random.normal(scale=<span class="number">1</span> / n_features ** <span class="number">.5</span>,</span><br><span class="line">                                        size=(n_features, n_hidden))</span><br><span class="line">weights_hidden_output = np.random.normal(scale=<span class="number">1</span> / n_features ** <span class="number">.5</span>,</span><br><span class="line">                                         size=n_hidden)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    del_w_input_hidden = np.zeros(weights_input_hidden.shape)</span><br><span class="line">    del_w_hidden_output = np.zeros(weights_hidden_output.shape)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(features.values, targets):</span><br><span class="line"></span><br><span class="line">        hidden_input = np.dot(x, weights_input_hidden)</span><br><span class="line">        hidden_output = sigmoid(hidden_input)</span><br><span class="line"></span><br><span class="line">        output = sigmoid(np.dot(hidden_output,</span><br><span class="line">                                weights_hidden_output))</span><br><span class="line"></span><br><span class="line">        error = y - output</span><br><span class="line"></span><br><span class="line">        output_error_term = error * output * (<span class="number">1</span> - output)</span><br><span class="line"></span><br><span class="line">        hidden_error = np.dot(output_error_term, weights_hidden_output)</span><br><span class="line"></span><br><span class="line">        hidden_error_term = hidden_error * hidden_output * (<span class="number">1</span> - hidden_output)</span><br><span class="line"></span><br><span class="line">        del_w_hidden_output += output_error_term * hidden_output</span><br><span class="line">        del_w_input_hidden += hidden_error_term * x[:, <span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">    weights_input_hidden += learnrate * del_w_input_hidden / n_records</span><br><span class="line">    weights_hidden_output += learnrate * del_w_hidden_output / n_records</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> e % (epochs / <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">        hidden_output = sigmoid(np.dot(x, weights_input_hidden))</span><br><span class="line">        out = sigmoid(np.dot(hidden_output,</span><br><span class="line">                             weights_hidden_output))</span><br><span class="line">        loss = np.mean((out - targets) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> last_loss <span class="keyword">and</span> last_loss &lt; loss:</span><br><span class="line">            print(<span class="string">"Train loss: "</span>, loss, <span class="string">"  WARNING - Loss Increasing"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Train loss: "</span>, loss)</span><br><span class="line">        last_loss = loss</span><br><span class="line"></span><br><span class="line">hidden = sigmoid(np.dot(features_test, weights_input_hidden))</span><br><span class="line">out = sigmoid(np.dot(hidden, weights_hidden_output))</span><br><span class="line">predictions = out &gt; <span class="number">0.5</span></span><br><span class="line">accuracy = np.mean(predictions == targets_test)</span><br><span class="line">print(<span class="string">"Prediction accuracy: &#123;:.3f&#125;"</span>.format(accuracy))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;传送门：&lt;br&gt;&lt;a href=&quot;http://ihoge.cn/2018/DL_BFNN.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BF神经网络（二）预测共享单车使用情况&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法（1）</title>
    <link href="http://www.ihoge.cn/2018/clustering.html"/>
    <id>http://www.ihoge.cn/2018/clustering.html</id>
    <published>2018-05-28T04:20:21.000Z</published>
    <updated>2018-08-05T10:16:41.844Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><hr><p>本文内容包含：<br>    一、基本概念<br>    二、K-means<br>    三、凝聚层次聚类<br>    四、DBSCAN密度聚类<br>    五、簇评估</p><hr><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>聚类分析仅根据数据内部发现的描述对象及其关系信息将数据对象分组。组件的相似性（如凝聚度）越大，组件差别越大（如分离度），聚类就越好。</p><h3 id="1-不同的聚类类型"><a href="#1-不同的聚类类型" class="headerlink" title="1. 不同的聚类类型"></a>1. 不同的聚类类型</h3><h4 id="1-1-层次的和划分的"><a href="#1-1-层次的和划分的" class="headerlink" title="1.1 层次的和划分的"></a>1.1 层次的和划分的</h4><p>其区别是簇的集合是嵌套的还是非嵌套的。</p><ul><li><strong>层次的</strong>： 适用于小数据。可以形成相似度层次图谱，便于直观地确定类之间的划分。  </li><li><strong>划分的</strong>： 适用于大数据。将观测分为预先指定不重叠的类。但不能提供类相似度信息，需要实现决定<code>聚类个数</code>和<code>初始位置</code>。（如K-means）</li></ul><h4 id="1-2-互斥的、重叠的与模糊的"><a href="#1-2-互斥的、重叠的与模糊的" class="headerlink" title="1.2 互斥的、重叠的与模糊的"></a>1.2 互斥的、重叠的与模糊的</h4><p><strong>互斥</strong>： 互斥表示一个对象最多只属于一个类。<br><strong>重叠</strong>： 重叠表示一个对象可以被指定为多个类。没个对象以 1（绝对属于）和 0（绝对不属于）的概率属于任何一个集合，且隶属概率之和为1。</p><h4 id="1-3-完全的与部分的"><a href="#1-3-完全的与部分的" class="headerlink" title="1.3 完全的与部分的"></a>1.3 完全的与部分的</h4><p><strong>完全的</strong>： 将每个对象指派到一个簇<br><strong>部分的</strong>： 忽略掉不感兴趣的背景（噪音或者离群点）</p><h3 id="2-部分聚类的方法"><a href="#2-部分聚类的方法" class="headerlink" title="2. 部分聚类的方法"></a>2. 部分聚类的方法</h3><p><strong>K-means</strong>： 是基于原型的、划分的聚类技术。适用于大数据。将观测分为预先指定不重叠的类。但不能提供类相似度信息，需要实现决定<code>聚类个数</code>和<code>初始位置</code>。<br><strong>凝聚的层次聚类</strong>： 开始每个点作为单个簇；然后重复地合并两个最靠近的簇直到产生单个的、包含所有点的簇。<br><strong>两步聚类</strong>： 适用于大样本数据。以 <code>BIRCH</code> 方法为代表，是层次聚类和 k-means 的结合。具有运算速度快、不需要大量递归运算，节省存储空间的优点。<br><strong>基于密度的聚类</strong>： 适用于大样本数据，对噪音不敏感。基于划分的方法适用于样本形态为球状簇时的情况，当分布不规则时，需要使用该类方法。最常见的是 <code>BDSCAN</code>聚类。</p><h2 id="二、K-means"><a href="#二、K-means" class="headerlink" title="二、K-means"></a>二、K-means</h2><h3 id="1-算法步骤"><a href="#1-算法步骤" class="headerlink" title="1. 算法步骤"></a>1. 算法步骤</h3><blockquote><p><1> 选择$K$个点作为初始质心</1></p><p><2> Repeat：</2></p><p><3> 将每个点指派到最近的质心，形成$K$个簇</3></p><p><4> 重新计算每个簇的质心</4></p><p><5> Until: 质心不发生变化终止</5></p></blockquote><h3 id="2-距离的度量"><a href="#2-距离的度量" class="headerlink" title="2. 距离的度量"></a>2. 距离的度量</h3><p><a href="http://ihoge.cn/2018/KNN1.html" target="_blank" rel="noopener"><strong>闵可夫斯基距离</strong></a></p><p>闵可夫斯基距离不是一种距离，而是一类距离的定义。对于 n 维空间中的两个点 $x(x_1,x_2,x_3,…,x_n)$和$y(y_1,y_2,y_3,…,y_n)$，那么$x$和$y$亮点之间的闵可夫斯基距离为：$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{p}}}$$<br>其中p是一个可变参数：</p><ul><li>当p=1时，被称为<strong>曼哈顿距离</strong>；</li><li>当p=2时，被称为<strong>欧式距离</strong>；</li><li>当p=$\infty $时，被称为切比雪夫距离。</li></ul><p><strong>余弦相似度</strong>：</p><p>$$cos(\Theta )=\frac {a^Tb}{|a|*|b|}$$</p><ul><li>$a,b$表示两个向量，$|a|$和$|b|$表示向量的模。 余弦相似度一般衡量两个向量的相似情况，常用与文本处理。余弦角越小越相似。 </li></ul><p><strong>杰卡德（Jaccard）相似系数</strong> </p><p>$$J(A,B)=\frac {|A \bigcap B|}{|A \bigcup B|}$$</p><ul><li>这里，$A、B$表示集合，$A \bigcap B$表示两个集合公共元素的个数，$A \bigcup B$表示两个集合并集元素的个数。 Jaccard 相似系数适用于度量两个集合的相似程度，取值在 0～1 之间，越大越相似。在推荐系统中常用衡量客户或商品的相似度。</li></ul><h3 id="3-变量标准化"><a href="#3-变量标准化" class="headerlink" title="3. 变量标准化"></a>3. 变量标准化</h3><p>在聚类前，通常需要对个连续变量进行标准化，因为方差大的变量比方差晓得变量对距离或相似度的影响更大，从而对聚类结果的影响更大。</p><p>常用的方法有：</p><p><strong>正态标准化</strong>：$x_i=\frac {x_i-mean(X)}{std(X}$<br><strong>归一化</strong>：$x_i=\frac {x_i-min(X)}{max(X)-min(X)}$</p><h3 id="4-变量的维度分析"><a href="#4-变量的维度分析" class="headerlink" title="4. 变量的维度分析"></a>4. 变量的维度分析</h3><p>假设一组变量中，一个维度有5个变量，二另一个维度只有1个变量，则第一个维度的权重被明显提高了。一般情况下，每个维度上使用的变量个数应该是一样的，不过分析人员要结合具体场景不同维度提供不同数量的变量个数，这相当于加大了一些维度的权重。</p><p>除了机遇业务定义进行变量的选择，另一种常用的方法是在聚类之前进行主成分分析。</p><h3 id="5-质心的目标函数"><a href="#5-质心的目标函数" class="headerlink" title="5. 质心的目标函数"></a>5. 质心的目标函数</h3><h4 id="5-1-SSE-误差平方和"><a href="#5-1-SSE-误差平方和" class="headerlink" title="5.1 SSE 误差平方和"></a>5.1 SSE 误差平方和</h4><p>聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或点到簇的质心的临近性；<br>如，考虑临近性度量为欧几里得距离的数据，我们使用<strong>误差平方和（SSE）</strong>作为度量聚类质量的目标函数，即<strong>最小化簇中点到质心的距离平方和</strong>。 SSE也称散布（scatter），定义如下：<br>$$SSE=∑^K_{i=1}∑_{x\in C_i}dist(c_i,x)^2$$<br>其中，$dist$是欧几里得空间中两个对象之间的标准欧几里得距离。</p><p>给定这些假设，实际上可以证明：对 SSE 求导，另导数为 0 求解 $c_k$<strong>使簇的 SSE 最小的质心是均值</strong>；</p><p>$$\frac {\partial }{\partial c_k}SSE =\frac {\partial }{\partial c_k}∑^K_{i=1}∑_{x\in C_i}(c_i,x)^2=0 $$</p><p>最终得到： $$∑_{x\in C_k}2(c_k-x_k)=0\Longrightarrow m_kc_k=∑_{x\in C_k}x_k \Longrightarrow c_k = \frac 1{m_k}∑_{x\in C_k}x_k$$</p><p><strong>文档数据</strong></p><p>考虑文档数据和余弦相似性度量。这里我们假定文档数据用文档——词矩阵表示，我们的目标是最大化簇中文档与簇的质心的相似性；该量乘坐簇的<code>凝聚度（cohesion）</code>。对于该目标，可以证明，与欧几里得数据一样，簇的质心是均值。总 SSE 的类似量是总凝聚度（total cohesion）：<br>$$Total Cohesion=∑^K_{i=1}∑_{x\in C_i}cosine(c_i,x)$$</p><p>关于凝聚度的知识，会在下文模型评估里面详细介绍</p><h4 id="5-2-SAE-绝对误差和"><a href="#5-2-SAE-绝对误差和" class="headerlink" title="5.2 SAE 绝对误差和"></a>5.2 SAE 绝对误差和</h4><p>为了表明$K$均值可以用各种不同的目标函数，我们考虑如何将数据分成$K$个簇，使得<strong>点到其簇中心的曼哈顿距离之和最小</strong>。如下式绝对误差和（SAE）</p><p>$$SAE = ∑^K_{i=1}∑_{x \in C_i}|c_i-x|$$</p><p>$$\frac {\partial }{\partial c_k}SAE =\frac {\partial }{\partial c_k}∑^K_{i=1}∑_{x\in C_i}|c_i-x|=0 $$</p><p>最终得到： $$∑_{x\in C_k}\frac {\partial }{\partial c_k}|c_k-x|=0\Longrightarrow ∑_{x\in C_k}sign(x-c_k )=0 \Longrightarrow c_k=median{x\in C_k}$$<br>即簇中点的中位数。一组点的中位数的计是直截了当的，并且减少受离群值的影响。</p><h4 id="5-3-常见的邻近度、质心和目标函数组合"><a href="#5-3-常见的邻近度、质心和目标函数组合" class="headerlink" title="5.3 常见的邻近度、质心和目标函数组合"></a>5.3 常见的邻近度、质心和目标函数组合</h4><table><thead><tr><th>邻近度函数</th><th>质心</th><th>目标函数</th></tr></thead><tbody><tr><td>曼哈顿距离</td><td>中位数</td><td>最小化对象与质心的绝对误差和SAE</td></tr><tr><td>平方欧几里得距离</td><td>均值</td><td>最小化对象与质心的误差平方和SSE</td></tr><tr><td>余弦</td><td>均值</td><td>最大化对象与质心的余弦相似度和</td></tr><tr><td>Bregman散度</td><td>均值</td><td>最小化对象到质心的Bregman散度和</td></tr></tbody></table><p><code>Bregman散度</code>实际上是一类紧邻性度量，包括平方欧几里得距离。<code>Bregman散度函数</code>的重要性在于，任意这类函数都可以用作以均值为质心的 K-means 类型的聚类算法的基础。</p><h3 id="6-选择初始质心"><a href="#6-选择初始质心" class="headerlink" title="6. 选择初始质心"></a>6. 选择初始质心</h3><p>当质心随机初始化时，K-means 将产生不同的总 SEE。选择适当的初始质心是基本 K-menas 过程的关键步骤。常见的是随机选取，但这种情况下簇的质量常常很差。考虑什么情况下选择的初始质心能找到最优解？答案是：每个簇刚好分到一个质心。事实证明发生这种情况的概率是非常非常低的。</p><p>常见一种技术是：多次运行，然后选取具有最小 SEE 的簇集。该策略虽然简单，但是效果可能不太好，依然是概率事件。</p><p>另一种有效的技术是：取一个样本，并使用层次聚类技术对他聚类。从层次聚类中提取 $K$ 个簇，并用这些簇的质心作为初始质心。该方法虽然有效，但仅对下列情况有效：（1）样本相对较小，例如数百到数千（层次聚类开销较大）；（2）$K$ 相对与样本大小较小。</p><p>还有一种方法是：随机选择第一个点或者所有点到质心作为第一个点。然后对于每个候机初始质心，选择里已经选取的初始质心最远的点，并且把该方法应用与点样本。 这样可以大大缓解可能会选择离群点作为质心的可能，并且大大减小计算量。</p><p>另外，我们也可以采用对初始化问题不太敏感的 K-means 的变种，<strong>二分K-means</strong>、使用后处理来“修补”<br>所产生的簇集</p><h3 id="7-时间复杂性和空间复杂性"><a href="#7-时间复杂性和空间复杂性" class="headerlink" title="7. 时间复杂性和空间复杂性"></a>7. 时间复杂性和空间复杂性</h3><ul><li><p>所需空间：$O((m+K)n)$，m 是点数， n 是属性数</p></li><li><p>所需时间：$O(I<em>K</em>m*n)$，$I$ 是收敛所需迭代次数，通常很小，可以是有界的。</p></li></ul><h3 id="8-K-means-其他问题"><a href="#8-K-means-其他问题" class="headerlink" title="8. K-means 其他问题"></a>8. K-means 其他问题</h3><h4 id="8-1-处理空簇"><a href="#8-1-处理空簇" class="headerlink" title="8.1 处理空簇"></a>8.1 处理空簇</h4><p>K-means 存在的问题之一是：如果所有的点在指派的步骤都为分配到某个簇，就会得到空簇。这种情况下需要选择一个替补质心，否则误差将会偏大。</p><ul><li>方法一： 选择一个距离当前任何质心最远的点</li><li>方法二： 从具有最大 SSE 的簇中选择一个替补质心。浙江分裂簇并降低聚类的总 SSE。</li></ul><h4 id="8-2-离群点"><a href="#8-2-离群点" class="headerlink" title="8.2 离群点"></a>8.2 离群点</h4><p>当然我们想到的第一反应是删除离群点，但是有些聚类应用，不能删除离群点。在某些情况下（财经分析），明显的离群点可能是最令人感兴趣的点。</p><p>那么问题来了，如何识别离群点？</p><ul><li>方法一：聚类前删除离群点</li><li>方法二：后处理离群点。如删除那些具有不寻常影响的点（尤其是多次运行算法时），另外也可以删除那些很小的簇，他们尝尝代表离群点的组。</li></ul><h4 id="8-3-后处理降低-SSE"><a href="#8-3-后处理降低-SSE" class="headerlink" title="8.3 后处理降低 SSE"></a>8.3 后处理降低 SSE</h4><ul><li><p><strong>增加簇个数</strong></p><ul><li>分裂一个簇：通常选择具有最大 SSE 的簇，页可以分裂在特定属性上具有最大标准差的簇。</li><li>引进一个新的质心：通常选择离所有质心最远的点。</li></ul></li><li><p><strong>减少簇个数</strong></p><ul><li>拆散一个簇： 通常选择拆散使总 SSE 增加最少的簇， 删除对应的质心</li><li>合并两个簇： 通常选择合并质心最接近的两个簇，或者合并两个导致总 SSE 增加最少的簇。这两种方法与层次聚类使用的方法相同，分别乘坐质心方法和 Ward 方法。</li></ul></li></ul><h3 id="9-二分-K-means"><a href="#9-二分-K-means" class="headerlink" title="9. 二分 K-means"></a>9. 二分 K-means</h3><p>二分 K-means 算法时基于 K-means 算法的直接扩充，它基于一种简单想法：为了得到 K 个簇，将所有点的集合分裂成两个簇，从这些簇中选取一个继续分裂，如此下去，知道产生 K 个簇。</p><p>算法实现步骤：</p><blockquote><p><1> 初始化簇表，是指包含有所有的点组成的簇。</1></p><p><2> <strong>Repeat</strong>：</2></p><blockquote><p><3>   从簇表中取出一个簇</3></p></blockquote><p><4>   对选出的簇进行多次二分“实验”</4></p><p><5>   for i = 1 to 试验次数 do：</5></p><p><6>       使用基本 K-means，二分选定的簇</6></p><p><7>   end for</7></p><p><8>   从二分实验中选择具有最小 SSE 的两个簇</8></p><p><9>   将这两个簇添加到簇表中</9></p></blockquote><blockquote><p><10>  <strong>Until</strong> 簇表包含 K 个簇。  </10></p></blockquote><p>待分裂的簇有许多不同的选择方法。可以选择最大的簇，选择具有最大 SSE 的簇，或者使用一个基于大小和 SSE 的标准进行选择。不同的选择导致不同的簇。</p><p>我们通常使用结果簇的质心作为基本 K-means 的初始质心，对结果逐步求精。 因为尽管 K-means 可以保证找到使 SSE 局部最小的聚类，但是自二分 K-means 算法中，我们“局部地”使用了 K-means ，即二分个体簇。因此，最终的簇集并不代表使 SSE 局部最小的聚类。</p><h3 id="10-K-means优缺点"><a href="#10-K-means优缺点" class="headerlink" title="10. K-means优缺点"></a>10. K-means优缺点</h3><h4 id="10-1-优点"><a href="#10-1-优点" class="headerlink" title="10.1 优点"></a>10.1 优点</h4><ul><li>简单并且可以用于各种数据类型；</li><li>具备适合的空间复杂度和计算负责度，适用于大样本数据；</li><li><p>K-means 某些变种甚至更有效 （二分K-means）且不受初始化问题影响。</p><h4 id="10-2-缺点"><a href="#10-2-缺点" class="headerlink" title="10.2 缺点"></a>10.2 缺点</h4></li><li><p>不能处理非球形簇、不同尺寸和不同密度的簇；</p></li><li>对离群点敏感；</li><li>K-means 仅限于具有中心（质心）概念的数据。有一种相关的 K-中心点聚类技术没有这种限制，但是开销更大。</li></ul><h2 id="三、凝聚层次聚类"><a href="#三、凝聚层次聚类" class="headerlink" title="三、凝聚层次聚类"></a>三、凝聚层次聚类</h2><p>有两种产生层次聚类的基本方法：</p><ul><li>凝聚的： 从点作为个体簇开始，每一步合并两个最接近的簇</li><li>分裂的： 从包含所有的点某个簇开始，每一步分裂一个簇，知道今生下单点簇</li></ul><p>到目前为之，凝聚层次聚类最常见，这里只讨论这类方法。</p><h3 id="1-基本聚类层次聚类算法"><a href="#1-基本聚类层次聚类算法" class="headerlink" title="1. 基本聚类层次聚类算法"></a>1. 基本聚类层次聚类算法</h3><blockquote><p><1> 如果需要，计算邻近度矩阵</1></p><p><2> <strong>Repeat:</strong></2></p><blockquote><p><3> 合并最接近的两个簇</3></p><p><4> 更新临近性矩阵，以反映新的簇与原来簇之间的临近性</4></p></blockquote></blockquote><blockquote><p><5> <strong>Until：</strong> 仅剩下一个簇</5></p></blockquote><h3 id="2-定义簇之间的临近性"><a href="#2-定义簇之间的临近性" class="headerlink" title="2. 定义簇之间的临近性"></a>2. 定义簇之间的临近性</h3><ul><li><strong>MIN</strong>：MIN定义簇的邻近度为不同簇的两个最近点之间的距离，也叫做<strong>单链（sigle link）</strong></li><li><strong>MAX</strong>：MAX定义簇的邻近度为不同簇的两个最远点之间的距离，也叫做<strong>全链（complete link）</strong></li><li><strong>组平均</strong>：它定义簇邻近度为取自不同簇的所有点对邻近度的平均值。</li></ul><p>如果我们取基于原型的观点，簇用质心代表，则不同簇邻近度定义就更加自然。</p><ul><li><strong>簇质心间邻近度</strong>：两个簇质心之间的距离</li><li><strong>Ward方法</strong>：同样嘉定簇用气质心代表，但他使用合并两个簇导致 SSE 的增量来度量两个簇之间的临近性。像 K-means 一样， Ward 方法也是图最小化点到其簇质心的距离平方和。</li></ul><h3 id="3-时间和空间复杂性"><a href="#3-时间和空间复杂性" class="headerlink" title="3. 时间和空间复杂性"></a>3. 时间和空间复杂性</h3><ul><li><strong>空间复杂度</strong>： $O(m^2)$</li><li><strong>时间复杂度</strong>： $O(m^2)$时间计算邻近度矩阵，基于以上算法所需总时间为$O(m^2log(m))$</li></ul><p>基本层次聚类使用邻近度矩阵，这需要存储$m^2/2$个邻近度（假定邻近度矩阵是对称的），其中$m$是数据点的个数。记录簇所需要的空间正比于簇的个数$m-1$，不包括单点簇。因此空间复杂度是$O(m^2)$。</p><p>上面步骤中<3>和<4>涉及$m-1$次迭代，因为开始有$m$个簇，每次迭代合并两个簇。如果邻近度矩阵采用线性搜索，则对于第$i$次迭代，步骤3需要$O(m-i+1)^2$的时间，步骤<4>只需要$O(m-i+1)$时间，在合并两个簇后更新邻近度矩阵（对于我们考虑的技术，簇合并至影响$O(m-i+1)$个邻近度）不做修改，时间负责度将为$O(m^3)$。如果某个簇到其他所有簇的距离存放在一个有序表或堆中，则查找两个最近簇的开销可能降低到$O(m-i+1)$，然而，由于维护有序表或堆的附加开销，基于以上算法的层次聚类所需总时间是$O(m^2log(m))$。</4></4></3></p><p>由此可见，层次聚类算法需要消耗大量的存储和计算资源，严重限制了它所能处理的数据集的大小。</p><h3 id="4-层次聚类的主要问题"><a href="#4-层次聚类的主要问题" class="headerlink" title="4. 层次聚类的主要问题"></a>4. 层次聚类的主要问题</h3><h4 id="4-1-缺乏全局目标函数"><a href="#4-1-缺乏全局目标函数" class="headerlink" title="4.1 缺乏全局目标函数"></a>4.1 缺乏全局目标函数</h4><p>也就是说凝聚层次聚类技术使用各种标准，在每一步局部地确定哪些簇应当合并（或分裂，对于分裂方法）。这种方法产生的聚类算法避开了解决困难的组合优化能力。此外这样的方法没有局部极小问题或很难选择初始化的问题。当然，在许多情况下$O(m^2)$的空间复杂度和$O(m^2log(m))$的事件负责度也阻碍了它的应用。</p><h4 id="4-2-处理不同大小簇的能力"><a href="#4-2-处理不同大小簇的能力" class="headerlink" title="4.2 处理不同大小簇的能力"></a>4.2 处理不同大小簇的能力</h4><p>对于如何处理待合并的簇对的相对大小（鄙人理解为权值，该讨论仅适用于涉及求和的簇临近性方法，如质心、Ward方法和组平均）有两种方法：</p><ul><li><strong>加权</strong>：平等的对待所有簇，赋予不同大小的簇中点不同的权值</li><li><strong>非加权</strong>： 赋予不同大小簇中点相同的权值 </li></ul><h4 id="4-3-合并不可逆"><a href="#4-3-合并不可逆" class="headerlink" title="4.3 合并不可逆"></a>4.3 合并不可逆</h4><p>对于合并两个簇，凝聚层次聚类算法去相遇作出有好的局部决策。然而，一旦做出合并决策，以后就不能撤销。这种方法阻碍了局部最优标准变成全局最优标准。</p><p>有一些技术是图克服“合并不可逆”这一限制，一种通过移动树的分支以改善全局目标函数；另一种使用划分聚类技术（如K-means）来创建许多小簇，然后从这些小簇出发进行层次聚类。<br>看似第二种更可取</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>该算法缺点上面描述的已经很详细了，通常使用这类算法是因为基本应用需要层次结构，此外有些研究表明这些算法能产生高质量的聚类。然而就计算量和存储量而言，凝聚层次聚类算法是昂贵的，合并时不可逆的，对于噪声、高维数据（如文档数据），这也可能造成问题。</p><p>首先使用其他技术（K-均值）进行部分聚类，这两个问题都可以在某种程度上加以解决。</p><h2 id="四、DBSCAN-基于密度的聚类"><a href="#四、DBSCAN-基于密度的聚类" class="headerlink" title="四、DBSCAN 基于密度的聚类"></a>四、DBSCAN 基于密度的聚类</h2><p> 基于密度的聚类寻找被低密度区域分离的高密度区域。DBSCAN 是一种简单、有效的基于密度的聚类算法。</p><h3 id="1-基本名词"><a href="#1-基本名词" class="headerlink" title="1. 基本名词"></a>1. 基本名词</h3><ul><li><strong>核心点</strong>： 如果该点满足给定的邻域内（半径为$Eps$的范围内）的点的个数超过给定的阈值$Minpts$，则该点为满足该条件下的核心点。</li><li><strong>边界点</strong>： 边界点落在某个核心点的邻域内，同时边界点可能落在多个核心点的邻域内。</li><li><strong>噪声点</strong>： 噪声点既非核心点，也不是边界点</li></ul><h3 id="2-SBSCAN算法步骤"><a href="#2-SBSCAN算法步骤" class="headerlink" title="2. SBSCAN算法步骤"></a>2. SBSCAN算法步骤</h3><blockquote><p><1> 将所有点标记为核心点、边界点或噪音点</1></p><p><2> 删除噪音点</2></p><p><3> 为距离在 $Eps$ 之内的所有核心点之间赋予一条边 </3></p><p><4> 每组联通的核心点形成一个簇</4></p><p><5> 将每个边界点指派到一个与之关联的核心点的簇中</5></p></blockquote><h3 id="3-时间和空间复杂度"><a href="#3-时间和空间复杂度" class="headerlink" title="3. 时间和空间复杂度"></a>3. 时间和空间复杂度</h3><ul><li><strong>时间复杂度</strong>： $O(m <em> 找出邻域中的点所需要的时间)$，在最坏的情况下时间复杂度为$O(m^2)$；当数据结构为 KD树，可以有效地检索特定点给顶距离内的所有点，时间复杂度降低到$O(m</em>log(m))$</li><li><strong>空间复杂度</strong>： $O(m)$，每个点只需维护少量数据，即簇标号和每个点是核心点、边界点还是噪音点的标示。</li></ul><h3 id="4-选择-DBSCAN-参数"><a href="#4-选择-DBSCAN-参数" class="headerlink" title="4. 选择 DBSCAN 参数"></a>4. 选择 DBSCAN 参数</h3><ul><li><p><strong>$k-$距离</strong>：如何选择 $Eps$ 和 $MinPts$ 参数，基本的方法是观察点到它的第 $k$ 个最近邻的距离。</p><p>考虑下，如何 $k$ 不大于簇个数的话， $k-距离$ 相对较小，反之对于不咋簇中的点（噪音点或异常值）则$k距离$较大。因此对于参数的选取我们可以利用这点进行作图：先选取一个 $k$ (一般为4)，计算所有点的$k-距离$，并递增排序，画出曲线图，则我们会看到$k-距离的变化$，并依照此图选择出合适的 $MinPts$ 参数，即对应拐点的位置。</p></li></ul><h3 id="5-优点和缺点"><a href="#5-优点和缺点" class="headerlink" title="5. 优点和缺点"></a>5. 优点和缺点</h3><ul><li>对噪声不敏感，而且能处理任意形状和大小的簇， DBSCAN 可以发现使用 K 均值不能发现的许多簇。</li><li>当簇的密度变化太大时， DBSCAN 就会有麻烦。 对于高维数据也会有问题，因为对于这样的数据，密度定义更困难。最后，当邻近计算需要计算所有的点对邻近度时（对于高维数据，常常如此），DBSCAN 的开销可能是很大的。</li></ul><h2 id="五、簇评估"><a href="#五、簇评估" class="headerlink" title="五、簇评估"></a>五、簇评估</h2><h3 id="1-簇评估的基本目标和度量"><a href="#1-簇评估的基本目标和度量" class="headerlink" title="1. 簇评估的基本目标和度量"></a>1. 簇评估的基本目标和度量</h3><p>簇评估的一些重要问题包含：</p><ol><li>确定数据集的<strong>聚类趋势</strong>，即识别数据中是否实际存在非随机结构</li><li>确定正确的簇个数</li><li>不引用附加的信息，评估聚类分析结果对数据的拟合情况</li><li>将聚类分析结果与已知客观结果相比较</li><li>比较两个簇集，确定哪个更好</li></ol><p>用于评估簇的各方面的评估度量或指标一般分成如下三类：</p><ul><li><strong>非监督的</strong>：非监督度量通常称为<strong>内部指标</strong>，仅使用出现在数据集中的信息。如 SSE <ul><li><em>凝聚性</em>：度量簇中对象的密切关系，越大越好</li><li><em>分离性</em>：度量簇与簇之间的分离度，也是越大越好</li></ul></li><li><strong>监督的</strong>：例如监督指标的 熵，它度量簇表好与外部提供标号的匹配程度。通常称为<strong>外部指标</strong></li><li><strong>相对的</strong>：不是一种单独的簇评估度量类型，而是度量的一种具体使用。如使用 SSE 和 熵 对两个 K-means 聚类进行比较。</li></ul><h3 id="2-划分聚类的非监督簇评估"><a href="#2-划分聚类的非监督簇评估" class="headerlink" title="2.划分聚类的非监督簇评估"></a>2.划分聚类的非监督簇评估</h3><h4 id="2-1-凝聚的和分离度"><a href="#2-1-凝聚的和分离度" class="headerlink" title="2.1 凝聚的和分离度"></a>2.1 凝聚的和分离度</h4><p>通常，将 K 个簇的集合的总体有效性表示为个体簇有效性的加权和：</p><p>$$overall_validity=∑^K_{i=1}w_i*validity(C_i)$$</p><p>其中，$validity$函数可以是凝聚度、分离度，或者这些量的组合。权值将因簇有效性的度量而异。在某些情况下，全职可以简单地取 1 或者簇的大小；而在其他情况下，他们反应更复杂的性质。如凝聚度的平方根。如果有效性函数是凝聚度，则值越高越好。如果是分离度，则簇与簇之间越大越好；簇中点越小越好。</p><p>凝聚度和分离度是分两种情况下考虑的，一个是基于图的观点。</p><p><strong>基于图的观点：</strong></p><p>该情况下度量的取值需要簇中所有的点加入运算求出平均距离或者总距离来对凝聚程度和分离程度作出度量。（一个簇的话是计算簇内两两之间距离，两个簇的话计算其中一个簇内每个点到另一个簇内所有点的距离，因此这种情况下计算量往往比较大）<br>$$基于图的凝聚度SSE=\frac 1{2m}∑_{x\in C_i}∑_{y\in C_i}dist(x,y)^2$$<br>$$基于原型的凝聚度SSE= ∑_{x\in C_i}dist(c_i,x)^2$$<br><strong>基于原型的观点：</strong></p><p>这个比较好理解，就是取每个簇的质心（或中心点）作为该簇的代表。簇内凝聚度计算的是簇内每个点到质心的距离；分离性度量的是每个簇质心到总体质心的距离或者每个簇两两之间质心的距离。</p><p>$$总分离度 SSB=∑_{i=1}^Km_idist(c_i,c)^2=\frac 1{2K}∑^K_{i=1}∑^K_{j=1}\frac mKdist(c_i,c_j)^2$$<br>其中 $c_i$ 是第 $i$ 个簇的均值，$c$ 是总均值。总 $SSB$ 越高，簇之间的分离性越好。</p><p><strong>凝聚度和分离度之间的关系：</strong></p><p>可以证明总 $SSE$ 和总 $SSB$ 之和是一个常数，它等于总平方和（$TSS$，即每个点到总体数据均值距离的平方和）：</p><p>$$TSS=SSE+SSB$$<br>因此，<strong>最小化SSW(凝聚度)等价于最大化SSB（分离度）</strong>。</p><h4 id="2-2-轮廓系数"><a href="#2-2-轮廓系数" class="headerlink" title="2.2 轮廓系数"></a>2.2 轮廓系数</h4><p>流行的轮廓系数方法结合了凝聚度和分离度，该方法分三部分组成：</p><blockquote><p><1>    对于第 $i$ 个对象，计算它到簇中所有其他对象的平均距离，记作$a_i$。</1></p><p><2>    对于第 $i$ 个对象和不包含该对象的任意簇，计算该对象到给定簇中所有对象的平均距离。关于所有的簇，找出最小值，记作 $b_i$。</2></p><p><3>    对于第 $i$ 个对象，轮廓系数是 $s_i=\frac {(b_i-a_i)}{max(a_i,b_i)}$。</3></p></blockquote><p>轮廓系数取值在 -1 到 1 之间变化，但我们不希望取负值因为负值代表着簇中平均距离大于到其他簇的最小平均距离是不合理的。我们希望的是 $b_i&gt;a_i即上面的max(a_i,b_i)=b_i$，并且 $a_i$ 越接近 0 越好，因为当 $a_i=0$ 时轮廓系数取其最大值1。</p><p>我们可以简单地取簇中点的轮廓系数取平均值，计算簇的平均轮廓系数。通过计算所有点的平均轮廓系数，可以得到聚类优良性的总度量。</p><h4 id="2-3-邻近度矩阵"><a href="#2-3-邻近度矩阵" class="headerlink" title="2.3 邻近度矩阵"></a>2.3 邻近度矩阵</h4><p>数据挖掘导论参考<p337></p337></p><h3 id="3-层次聚类的非监督度量"><a href="#3-层次聚类的非监督度量" class="headerlink" title="3. 层次聚类的非监督度量"></a>3. 层次聚类的非监督度量</h3><p><strong>共性分类距离</strong>：凝聚层次聚类技术首次将对象放在同一个簇时的邻近度。即，如果在凝聚层次聚类进行的某个时刻，两个合并的簇之间的最小距离时 0.1，则其中一个簇中的所有点关于另一个簇中各点的共性分类距离都是 0.1。</p><p><strong>共性相关系数</strong>：是该矩阵与原来的相异度矩阵的项之间的相关度，是（特定类型的）层次聚类对数据拟合程度的标准度量。该度量最常见的应用是评估对于特定的数据类型，那种类型的层次聚类最好。</p><p><strong>霍普金斯(Hopkins)统计量</strong></p><p>对于该方法度量的是<strong>聚类趋势</strong>。我们随机产生 p 个随机分布在数据空间上的点，并且也抽取 p 个世纪数据点。 对于这两个点集没我们找出每个点到原数据集的最近邻距离。 设 $u_i$是人工产生的点的最近邻距离，$w_i$是样本点到源数据集的最近邻距离。则 Hopkins 统计量：<br>$$H=\frac {∑^p_{i=1}w_i}{∑^p_{i=1}u_i+∑^p_{i=1}w_i} $$</p><p>如果随机产生的点与样本点具有大致相同的最近邻距离，则 $H$ 将在 0.5 左右。$H$ 越接近 0 或者 1 分别表明数据是高度聚类的和数据在数据空间是有规律分布的。</p><p><strong>簇有效性面向分类的度量</strong></p><ul><li><code>熵</code>：每个簇由单个累的对象组成的程度。对于每个簇，首先计算数据的类分布，即对于簇 $i$，计算成员属于类 $j$ 的概率 $P_{ij}=m_{ij}/m_i$，其中$m_i$是簇 $i$ 中对象的个数，而 $m_{ij}$是簇 $i$ 中类 $j$ 的对象个数。使用类分布，用标准公式 $e_i=-∑^L_{j-1}P_{ij}log_2P_{ij}$ 计算每个簇的熵。 其中$L$是类的个数。 簇集合的总熵用每个簇的熵的加权和计算，即$e=∑^K_{i=1}\frac {m_i}{m}e_i$，其中$K$是簇的个数，而$m$是数据点的总数。</li><li><code>纯度</code>：簇包含单个累的对象的另一种度量程序。使用前面的属于，簇 $i$ 的纯度是 $P_i=\max_j P_{ij}$，而聚类的总纯度是$purity=∑^K_{i=1}=\frac {m_i}mPi$</li><li><code>精度</code>：簇中一个特定的对象所占的比例。簇$i$关于类$j$的精度是$precision(i,j)=P_{ij}$</li><li><code>召回率</code>：簇包含一个特地昂累的所有对象的程度。簇$i$关于类的召回率是$recall(i,j)=m_{ij}/m_j$，其中$m_j$是类$j$的对象个数。</li><li><strong>F度量</strong>：精度和召回率的组合，度量在多大程度上，簇只包含一个特定类型累的对象和包含该累的所有对象。簇 $i$关于类$j$的$F$度量是$F(i,j)=2 \frac{精度*召回率}{精度+召回率}$</li></ul><p>《未完待续…》</p><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;hr&gt;
&lt;p&gt;本文内容包含：&lt;br&gt;    一、基本概念&lt;br&gt;    二、K-means&lt;br&gt;    三、凝聚层次聚类&lt;br&gt;    四、DBSCAN密度聚类&lt;br&gt;    五、簇评估&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;一、基本概念&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习——Tensorflow（1）</title>
    <link href="http://www.ihoge.cn/2018/tensorflow1.html"/>
    <id>http://www.ihoge.cn/2018/tensorflow1.html</id>
    <published>2018-05-26T04:20:21.000Z</published>
    <updated>2018-08-05T10:16:31.444Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、Tensorflow简单示例"><a href="#一、Tensorflow简单示例" class="headerlink" title="一、Tensorflow简单示例"></a>一、Tensorflow简单示例</h2><h3 id="1-基本运算"><a href="#1-基本运算" class="headerlink" title="1. 基本运算"></a>1. 基本运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个常量</span></span><br><span class="line">a = tf.constant([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment">#定义一个变量</span></span><br><span class="line">x = tf.Variable([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个加法op</span></span><br><span class="line">add = tf.add(a,x)</span><br><span class="line"><span class="comment">#定义一个减法</span></span><br><span class="line">sub = tf.subtract(a,x)</span><br><span class="line"><span class="comment">#定义一个乘法op</span></span><br><span class="line">mul = tf.multiply(a,x)</span><br><span class="line"><span class="comment">#定义初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment">#定义多个操作</span></span><br><span class="line">add2 = tf.add(a,add)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(<span class="string">"加法："</span>,sess.run(add)) <span class="comment">#执行加法</span></span><br><span class="line">    print(<span class="string">"减法："</span>,sess.run(sub)) <span class="comment">#执行减法</span></span><br><span class="line">    print(<span class="string">"乘法："</span>,sess.run(mul)) <span class="comment">#执行乘法</span></span><br><span class="line">    <span class="comment">#同时执行乘法op和加法op</span></span><br><span class="line">    result = sess.run([add,add2,sub,mul])</span><br><span class="line">    print(<span class="string">"执行多个："</span>,result)</span><br></pre></td></tr></table></figure><h3 id="2-使用占位符"><a href="#2-使用占位符" class="headerlink" title="2. 使用占位符"></a>2. 使用占位符</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Feed：先定义占位符，等需要的时候再传入数据</span></span><br><span class="line"><span class="comment">#创建占位符</span></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment">#定义乘法op</span></span><br><span class="line">output = tf.multiply(input1,input2)</span><br><span class="line">add = tf.add(input1,input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#feed的数据以字典的形式传入</span></span><br><span class="line">    print(sess.run(add, feed_dict=&#123;input1:[<span class="number">8.</span>],input2:[<span class="number">2.</span>]&#125;))</span><br></pre></td></tr></table></figure><h2 id="二、Tensorflow简单回归模型"><a href="#二、Tensorflow简单回归模型" class="headerlink" title="二、Tensorflow简单回归模型"></a>二、Tensorflow简单回归模型</h2><h3 id="1-最简单的线性回归模型"><a href="#1-最简单的线性回归模型" class="headerlink" title="1. 最简单的线性回归模型"></a>1. 最简单的线性回归模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用numpy生成100个随机点</span></span><br><span class="line"><span class="comment">#样本点</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line">y_data = x_data*<span class="number">0.1</span> + <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#构造一个线性模型</span></span><br><span class="line">d = tf.Variable(<span class="number">1.1</span>)</span><br><span class="line">k = tf.Variable(<span class="number">0.5</span>)</span><br><span class="line">y = k*x_data + d</span><br><span class="line"></span><br><span class="line"><span class="comment">#二次代价函数&lt;均方差&gt;</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y_data,y)</span><br><span class="line"><span class="comment">#定义一个梯度下降法来进行训练的优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#最小化代价函数</span></span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train)</span><br><span class="line">        <span class="keyword">if</span> step%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            print(step,sess.run([k,d]))</span><br></pre></td></tr></table></figure><h3 id="2-非线性回归的问题"><a href="#2-非线性回归的问题" class="headerlink" title="2. 非线性回归的问题"></a>2. 非线性回归的问题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用numpy生成200个随机点</span></span><br><span class="line">x_data = np.linspace(<span class="number">-0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.015</span>,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义两个placeholder，列数为1，行数未知</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络结构：1-20-1，一个输入一个输出一个隐藏层包含20个神经元</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络中间层</span></span><br><span class="line">Weights_L1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">20</span>])) <span class="comment"># 初始化1行20列权值</span></span><br><span class="line">biases_L1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">20</span>])) <span class="comment"># 初始化1行20列偏置</span></span><br><span class="line">Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1 <span class="comment"># 计算神经元信号</span></span><br><span class="line">L1 = tf.nn.tanh(Wx_plus_b_L1) <span class="comment"># 使用激活函数计算神经元输出信号</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络输出层</span></span><br><span class="line">Weights_L2 = tf.Variable(tf.random_normal([<span class="number">20</span>,<span class="number">1</span>]))</span><br><span class="line">biases_L2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction = tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"><span class="comment"># prediction = Wx_plus_b_L2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#二次代价函数</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line"><span class="comment">#使用梯度下降法最小化代价函数训练</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#变量初始化</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="comment">#获得预测值</span></span><br><span class="line">    prediction_value = sess.run(prediction,feed_dict=&#123;x:x_data&#125;)</span><br><span class="line">    <span class="comment">#画图</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data,y_data)</span><br><span class="line">    plt.plot(x_data,prediction_value,<span class="string">'r-'</span>,lw=<span class="number">5</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><pre><code>这里思考下中间层和输出层的激活函数的选取问题。1. 中间层和输出层的激活函数均采用tanh函数，当迭代1000次时，数据拟合效果良好；输出层激活函数换成恒等函数时，效果会更好一点。2. 这里使用sigmoid函数或者softmax函数，当迭代1000次时，无法拟合。事实证明在这个数据集里sigmoid函数和softmax函数均不能作为输出层的激活函数。当输出层激活函数为softmax时预测值恒为1这个很好理解；同理sigmoid此类函数收到输出值域的限制，在该数据里是无法用来作为输出激活函数的。3. 经过有限次的测试发现，很对该数据情况下输出层的激活函数可以使用tanh、softsign和恒等函数；其中恒等激活函数表现最好（个人考虑是因为该数据非常简单）。4. 经过有限次的测试发现，sigmoid、softmax、softsign、tanh均可作为该数据情况下的中间层激活函数（恒等函数除外）。其中tanh和softsign拟合的最快但softsign效果不好；sigmoid和softmax函数拟合较慢。随着迭代次数增加到20000次，最终都能很好地拟合数据。5. sigmoid作为激活函数对神经炎要求的数量一般情况下要比tanh高。</code></pre><h2 id="三、Tensorflow分类模型"><a href="#三、Tensorflow分类模型" class="headerlink" title="三、Tensorflow分类模型"></a>三、Tensorflow分类模型</h2><p>本节用到Tensorflow自带的 mnist 数据集。这里使用独热编码将多元回归的问题转换成10个数值的二元分类问题。使用softmax作为输出层激活函数的意义在于将输出的概率数组归一化并凸显概率最大的值。当然这里也可以使用sigmoid或其他作为输出层激活函数。</p><h3 id="1-简单的MNIST数据集分类"><a href="#1-简单的MNIST数据集分类" class="headerlink" title="1. 简单的MNIST数据集分类"></a>1. 简单的MNIST数据集分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义两个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个简单的神经网络</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"></span><br><span class="line"><span class="comment">#二次代价函数</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction) </span><br><span class="line"><span class="comment">#使用梯度下降法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中。</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率。</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment">#epoch：所有数据训练一次，就是一个epoch周期</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">        <span class="comment">#batch：一般为32，64个数据</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        </span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print(<span class="string">"Iter "</span> + str(epoch) + <span class="string">",Testing Accuracy "</span> + str(acc))</span><br></pre></td></tr></table></figure><h3 id="2-过拟合解决及梯度下降优化器"><a href="#2-过拟合解决及梯度下降优化器" class="headerlink" title="2. 过拟合解决及梯度下降优化器"></a>2. 过拟合解决及梯度下降优化器</h3><p>Dropout采用随机的方式“做空”神经元的权重，L1正则化采用的是“做空”贡献非常小的神经元权重，L2正则化是消弱每个神经元的权重让每个都有少许的贡献。<br>在神经网络中它们之间也可以结合使用，dropout应用较多些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义三个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 784-1000-500-10</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,<span class="number">1000</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">1000</span>])+<span class="number">0.1</span>)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob) </span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([<span class="number">1000</span>,<span class="number">500</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">500</span>])+<span class="number">0.1</span>)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob) </span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([<span class="number">500</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#同样这里也可以使用正则项</span></span><br><span class="line"><span class="comment">#l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + #tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + tf.nn.l2_loss(W3) + #tf.nn.l2_loss(b3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"></span><br><span class="line"><span class="comment">#正则后的交叉熵代价函数</span></span><br><span class="line"><span class="comment">#loss = tf.losses.softmax_cross_entropy(y,prediction) + #0.0005*l2_loss #这里0.0005为学习率</span></span><br><span class="line"><span class="comment">#使用梯度下降法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 使用优化器的梯度下降，同时还有其他很多种基于梯度下降的优化。这里的学习率取值比传统的梯度下降法要小</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">31</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">0.5</span>&#125;) <span class="comment">#这里keep_prob:0.5 表示保留50%的神经元，这里把另它为1的时候保留所有神经元测试结果准确率提高了2个百分点，同时相对应的计算量也增大了</span></span><br><span class="line">        </span><br><span class="line">        test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"Iter "</span> + str(epoch) + <span class="string">",Testing Accuracy "</span> + str(test_acc) +<span class="string">",Training Accuracy "</span> + str(train_acc))</span><br></pre></td></tr></table></figure><h3 id="3-神经网络优化"><a href="#3-神经网络优化" class="headerlink" title="3. 神经网络优化"></a>3. 神经网络优化</h3><p>这里的优化方式是不断减小学习率，使得在极小值附近迭代速度放缓，解决因学习率过大反复震荡无法拟合的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义三个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line">lr = tf.Variable(<span class="number">0.001</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 784-500-300-10</span></span><br><span class="line"><span class="comment">#创建一个神经网络</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,<span class="number">500</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">500</span>])+<span class="number">0.1</span>)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob) </span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([<span class="number">500</span>,<span class="number">300</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">300</span>])+<span class="number">0.1</span>)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob) </span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([<span class="number">300</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">        sess.run(tf.assign(lr, <span class="number">0.001</span> * (<span class="number">0.95</span> ** epoch)))</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        </span><br><span class="line">        learning_rate = sess.run(lr)</span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">", Testing Accuracy= "</span> + str(acc) + <span class="string">", Learning Rate= "</span> + str(learning_rate))</span><br></pre></td></tr></table></figure><h2 id="四、CNN卷积神经网络"><a href="#四、CNN卷积神经网络" class="headerlink" title="四、CNN卷积神经网络"></a>四、CNN卷积神经网络</h2><p>以上的案例采用的都是BP神经网络。考虑一张图片像素为100*100，则需要一万个输入神经元，若隐藏层也有一万个神经元则需要训练一亿个参数，这不仅需要更多计算昂还需要大量额训练样本用来“求解”。因此下面我们考虑用卷积神经网络来解决这个问题。</p><ul><li>CNN通过<strong>局部感受野</strong>和<strong>权值共享</strong>减少了神经网络需要训练的参数（权值）的个数。</li><li><p>卷积核/滤波器<br><img src="media/15273889821782.jpg" alt=""></p></li><li><p>卷积Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li><li><p>池化</p><ul><li>max-pooling 提取卷积后特征的最大值也就是最重要的特征，进一步压缩参数</li><li>mean-pooling </li><li>随机-pooling<br><img src="media/15273890234010.jpg" alt=""></li></ul></li><li><p>池化Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li></ul><p>下面看一个 CNN 卷积神经网络用于 MINIST 数据的分类问题。在CPU上运行比较耗时，16G内存的Mac-Pro大概两三分钟一个周期。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"><span class="comment">#定义两个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])<span class="comment">#28*28</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化权值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape,stddev=<span class="number">0.1</span>)<span class="comment">#生成一个截断的正态分布</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化偏置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>,shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">#卷积层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x,W)</span>:</span></span><br><span class="line">    <span class="comment">#x input tensor of shape `[batch, in_height, in_width, in_channels]`</span></span><br><span class="line">    <span class="comment">#W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]</span></span><br><span class="line">    <span class="comment">#`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长</span></span><br><span class="line">    <span class="comment">#padding: A `string` from: `"SAME", "VALID"`</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x,W,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#池化层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment">#ksize [1,x,y,1]</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#改变x的格式转为4D的格式[batch, in_height, in_width, in_channels]`</span></span><br><span class="line">x_image = tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第一个卷积层的权值和偏置</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])<span class="comment">#5*5的采样窗口，32个卷积核从1个平面抽取特征</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])<span class="comment">#每一个卷积核一个偏置值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)<span class="comment">#进行max-pooling</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第二个卷积层的权值和偏置</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])<span class="comment">#5*5的采样窗口，64个卷积核从32个平面抽取特征</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])<span class="comment">#每一个卷积核一个偏置值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)<span class="comment">#进行max-pooling</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14</span></span><br><span class="line"><span class="comment">#第二次卷积后为14*14，第二次池化后变为了7*7</span></span><br><span class="line"><span class="comment">#进过上面操作后得到64张7*7的平面</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第一个全连接层的权值</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>,<span class="number">1024</span>])<span class="comment">#上一层有7*7*64个神经元，全连接层有1024个神经元</span></span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])<span class="comment">#1024个节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把池化层2的输出扁平化为1维</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2,[<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line"><span class="comment">#求第一个全连接层的输出</span></span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#keep_prob用来表示神经元的输出概率</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第二个全连接层</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>,<span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算输出</span></span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">cross_entropy = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"><span class="comment">#使用AdamOptimizer进行优化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"><span class="comment">#结果存放在一个布尔列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(prediction,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">0.7</span>&#125;)</span><br><span class="line"></span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">", Testing Accuracy= "</span> + str(acc))</span><br></pre></td></tr></table></figure><h2 id="五、RNN递归神经网络"><a href="#五、RNN递归神经网络" class="headerlink" title="五、RNN递归神经网络"></a>五、RNN递归神经网络</h2><p>RNN 一个重要的用法就是通过递归的调用神经元，利用之前的信息来决策当前的问题。如下图所示：</p><p><img src="media/15273903144616.jpg" alt=""></p><p>RNN 递归的次数由多少批次（上图中的$t$来确定）。本质上还是类似 BP 神经网络，因此随着不断的迭代会存在梯度衰减的问题。 RNN 发明出来的时候还没有出现 Relu 函数，因此针对梯度衰减的问题推出了一种优化的模型 ：长段时间记忆模型（LSTM）。<br><img src="media/15273909839848.jpg" alt=""></p><h2 id="六、LSTM长段时间记忆"><a href="#六、LSTM长段时间记忆" class="headerlink" title="六、LSTM长段时间记忆"></a>六、LSTM长段时间记忆</h2><p>LSTM 原理是通过增加输入控制（Input Gate）、遗忘控制（Forget Gate）和输出控制（Output Gate）来快速遗忘不重要的信息持久化重要的信息，具体控制方式如下图所示：<br><img src="media/15273902768081.jpg" alt=""><br><img src="media/15273917640640.jpg" alt=""><br><img src="media/15273917751199.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入图片是28*28</span></span><br><span class="line">n_inputs = <span class="number">28</span> <span class="comment">#输入一行，一行有28个数据</span></span><br><span class="line">max_time = <span class="number">28</span> <span class="comment">#一共28行</span></span><br><span class="line">lstm_size = <span class="number">100</span> <span class="comment">#隐层单元</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># 10个分类</span></span><br><span class="line">batch_size = <span class="number">50</span> <span class="comment">#每批次50个样本</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size <span class="comment">#计算一共有多少个批次</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里的none表示第一个维度可以是任意的长度</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line"><span class="comment">#正确的标签</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化权值</span></span><br><span class="line">weights = tf.Variable(tf.truncated_normal([lstm_size, n_classes], stddev=<span class="number">0.1</span>))</span><br><span class="line"><span class="comment">#初始化偏置值</span></span><br><span class="line">biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[n_classes]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义RNN网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RNN</span><span class="params">(X,weights,biases)</span>:</span></span><br><span class="line">    <span class="comment"># inputs=[batch_size, max_time, n_inputs]</span></span><br><span class="line">    inputs = tf.reshape(X,[<span class="number">-1</span>,max_time,n_inputs])</span><br><span class="line">    <span class="comment">#定义LSTM</span></span><br><span class="line">    lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_size)</span><br><span class="line"><span class="comment">#    final_state[state, batch_size, cell.state_size]</span></span><br><span class="line"><span class="comment">#    final_state[0]是cell state</span></span><br><span class="line"><span class="comment">#    final_state[1]是hidden_state</span></span><br><span class="line"><span class="comment">#    outputs: The RNN output `Tensor`.</span></span><br><span class="line"><span class="comment">#       If time_major == False (default), this will be a `Tensor` shaped:</span></span><br><span class="line"><span class="comment">#         `[batch_size, max_time, cell.output_size]`.</span></span><br><span class="line"><span class="comment">#       If time_major == True, this will be a `Tensor` shaped:</span></span><br><span class="line"><span class="comment">#         `[max_time, batch_size, cell.output_size]`.</span></span><br><span class="line">    outputs,final_state = tf.nn.dynamic_rnn(lstm_cell,inputs,dtype=tf.float32)</span><br><span class="line">    results = tf.nn.softmax(tf.matmul(final_state[<span class="number">1</span>],weights) + biases)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#计算RNN的返回结果</span></span><br><span class="line">prediction= RNN(x, weights, biases)  </span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"><span class="comment">#使用AdamOptimizer进行优化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))<span class="comment">#把correct_prediction变为float32类型</span></span><br><span class="line"><span class="comment">#初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">11</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        </span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">", Testing Accuracy= "</span> + str(acc))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、Tensorflow简单示例&quot;&gt;&lt;a href=&quot;#一、Tensorflow简单示例&quot; class=&quot;headerlink&quot; title=&quot;一、Tensorflow简单示例&quot;&gt;&lt;/a&gt;一、Tensorflow简单示例&lt;/h2
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>提升树（Boosting tree）算法总结</title>
    <link href="http://www.ihoge.cn/2018/boosting.html"/>
    <id>http://www.ihoge.cn/2018/boosting.html</id>
    <published>2018-05-20T04:20:21.000Z</published>
    <updated>2018-08-05T10:16:52.918Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>本文是综合了之前的以往多个笔记汇总而成，内容包含：</p><pre><code>一、Boosting基本概念二、前向分步加法模型    1. 加法模型    2. 前向分步算法三、AdaBoost    1. 算法解析    2. 模型构建    3. 算法缺点四、二叉分类树五、回归分类树    1. 算法解析    2. 模型构建六、梯度提升树（GBDT）    1. 算法解析    2. 模型构建七、XGBoost    1. 原理详解    2. 目标函数    3. 学习过程    4. 损失函数    5. 正则化    6. 决策树的构建    7. 流程步骤    8. 优缺点八、总结    1. Boosting家族    2. AdaBoost    3. 回归提升树和AdaBoost    4. GBDT和回归提升树       5. XGBoost和GBDT    6. 参考文献</code></pre><p>提升(Boosting)是集成学习方法里的一个重要方法，其主要思想是将弱分类器组装成一个强分类器。在 PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p><p>提升树模型实际采用加法模型（即基函数的线性组合）与前向分步算法，以决策树为基函数的提升方法称为提升树（Boosting Tree）。</p><p><strong>对分类问题决策树是二叉分类树</strong>，<strong>对回归问题决策树是二叉回归树</strong>。</p><p>提升树模型可以表示为决策树的加法模型：<br>$$f_M(x)=∑^M_{i=1}T(x;\Theta _m)$$其中$T(x;\Theta _m)$表示决策树；$\Theta_m$表示决策树的参数；$M$为树的个数。</p><p>不同问题的提升树学习算法，其主要区别在于损失函数不同。<strong>平方损失函数常用于回归问题，用指数损失函数用于分类问题，以及绝对损失函数用于决策问题</strong>。</p><blockquote><p>由于树的线性组合可以很好的拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。</p></blockquote><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>提升（Boosting）方法是一类应用广泛且非常有效的统计学习方法。</p><p><strong>它基于这样一种思想</strong>：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠顶个诸葛亮”的道理。</p><p><strong>强可学习</strong>：如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的</p><p><strong>弱可学习</strong>：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的</p><p><strong>AdaBoost算法</strong>：那么如何将弱学习算法提升为强学习算法呢？关于提升方法的研究很多，有很多算法被提出。最具代表性的是AdaBoost算法（AdaBoost algorithm）</p><p><strong>Boosting算法的两个核心问题</strong></p><ol><li><p><em>在每一轮如何改变训练数据的权值或概率分布</em></p><p> <code>通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。</code>。</p></li><li><p><em>如何将弱分类器组合成一个强分类器</em></p><p> <code>通过加法模型将弱分类器进行线性组合，比如 AdaBoost 通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。</code>。</p></li></ol><blockquote><p>AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里。<br>AdaBoost算法是损失函数为指数函数时的Boosting算法</p></blockquote><h2 id="二、前向分步加法模型（Forward-Stagewise-Additive-Modeling）"><a href="#二、前向分步加法模型（Forward-Stagewise-Additive-Modeling）" class="headerlink" title="二、前向分步加法模型（Forward Stagewise Additive Modeling）"></a>二、前向分步加法模型（Forward Stagewise Additive Modeling）</h2><h3 id="1-加法模型"><a href="#1-加法模型" class="headerlink" title="1. 加法模型"></a>1. 加法模型</h3><p>（形为$Y=I+U+T+K$的模型为加法模型）<br>$$f(x)=∑^M_{m=1}\beta _mb(x;\gamma  _m)$$ 其中，$b(x;\gamma_m)$为基函数，$\beta_m$为基函数的系数。</p><h3 id="2-前向分步算法"><a href="#2-前向分步算法" class="headerlink" title="2. 前向分步算法"></a>2. 前向分步算法</h3><p>在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$称为经验风险极小化，即损失函数极小化的问题：<br>$$min_{(\beta_m,\gamma_m)}∑^N_{i=1}L(y_i,∑^M_{m=1}\beta _mb(x_i;\gamma_m )) $$</p><p>通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式$f(x)=∑^M_{m=1}\beta _mb(x;\gamma  _m)$，那么就可以简化优化的复杂度。</p><p>具体地，每步只需优化如下损失函数:$$min_{\beta, \gamma}∑^M_{i=1}L(y_i,\beta b(x_i;\gamma))$$</p><p><strong>前向分布算法步骤</strong>如下：</p><blockquote><p><strong>输入</strong>：训练数据集$D={(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_N,y_N)}$;损失函数$L(y,f(x))$；基函数集$(\beta (x;\gamma))$;<br><strong>输出</strong>：加法模型$f(x)$  </p><p>（1）初始化$f_0(x)=0$  </p><p>（2）对于$k=1,2,…,K$<br>（a）极小化损失函数，得到$\beta_m ,\gamma_m$:<br>    $$(\beta _m,\gamma_m)=argmin_{(\beta,\gamma)}∑^N_{i=1}L(y_i,f_{m-1}(x_i)+\beta b(x_i,\gamma))$$<br>（b）更新<br>    $$f_m(x)=f_{m-1}(x)+\beta _mb(x;\gamma_m)$$    </p><p>（3）得到加法模型<br>$$f(x)=f_M(x)=∑^M_{m=1}\beta_mb(x;\gamma_m)$$<br>这样。前向分步算法将同时求解从$m=1$到$m=M$所有参数$\beta_m,\gamma_m$的优化问题简化为逐次求解$\beta_m,\gamma_m$的优化问题。<br>前向分布算法学习的是加法模型，当基函数为基本分类器是，该加法模型等价于Adaboost的最终分类器。（AdaBoost算法参数迭代公式就是由此而来）</p><p><strong>基学习器</strong>：基函数为同一类型；反之称为<strong>组件学习器</strong>或直接成个体学习器</p></blockquote><h2 id="三、AdaBoost"><a href="#三、AdaBoost" class="headerlink" title="三、AdaBoost"></a>三、AdaBoost</h2><p>AdaBoost算法是前向分步算法的特例，其模型是由基本分类器组成的加法模型，损失函数是指数函数。</p><h3 id="1-AdaBoost算法解析"><a href="#1-AdaBoost算法解析" class="headerlink" title="1. AdaBoost算法解析"></a>1. AdaBoost算法解析</h3><p> <strong>AdaBoost模型是弱分类器的线性组合：</strong><br>    $$f(x)=∑^M_{m=1}\alpha _mG_m(x)$$</p><ul><li>$M$表示该提升树共有$M$个弱分类器组成</li><li>$G_m(x)$表示第$m$个弱分类器</li><li>$\alpha_m$为第$m$个弱分类器的参数（反应该分类器的重要性）</li></ul><p>Adaboost算法在分类问题中的主要特点：通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。 AdaBoost－算法描述（伪代码）如下：</p><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in \chi ⊆R^n,y_i\in Y={-1,1}$，弱学习算法$G_m(x)$;<br><strong>输出：</strong>最终强化算法分类器$G(x)$<br>（1）初始化训练数据总和为1的权值分布：（初始权重为归一化后的均值既$\frac 1N$）<br>$$D_1=(w_{11},…,w_{1i},…w_{1N}),w_{1i}=\frac 1N, i=1,2,…N$$<br>（2）对$m=1,2,…M$：（弱分类器的个数）<br>（a）使用具有权值分布的$D_m$的训练数据集学习，得到基本分类器：(数据集$X$到{-1,1}的映射)<br>$$G_m(x):X-&gt;{-1,1}$$<br>（b）计算$Gm(x)$在训练数据集上的分类误差率：（公式不够简洁明了，其实总结下来非常好理解：误差率$e_m$=误分类样本的权值之和）<br>$$e_m=∑^N_{i=1}P(G_m(x_i)≠y_i)=∑^N_{i=1}w_{mi}I(G_m(x_i)≠y_i)$$<br>我们来考虑下误差$e_m$的取值空间：由于训练集权制之和为1，因此误差$0≤e_m≤1$。但是这样还不够。因为我们在选择分裂阈值的时候会选择一个最优或局部最优的取值来分裂，且当$e_m=0.5$是表明该分裂阈值对预测无贡献。因此最终得到的$e_m$的实际取值应小于$e_m≤0.5$。<br>所以最终：$0≤e_m≤0.5$，且每次迭代误差$e_m$递减。这点对下面的参数理解很重要。<br>（c）计算$G_m(x)$的系数:(这里对数为自然对数)<br>$$\alpha_m=\frac 12log\frac{1-e_m}{e_m} $$<br>那么问题来了，为什么要用这个公式来计算更新每个基分类器的参数？我们先画个图出来观察下这个函数。（其中y轴为$\alpha _m$，x轴为误差$e_m$）</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266077461675.jpg" alt=""></p><p>由（2-b）我们得到误差$e_m$的取值范围为$0≤e_m&lt;0.5$，结合该图可以可知$0&lt;\alpha_m&lt;+\infty $。<br>另外可以发现，通过该函数的转换，弱分类器$G_m(x)$的误差的越小，参数$\alpha_m$越大。即实现了<strong><code>给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</code></strong><br>（d）更新训练数据集的权值分布：（该权值决定数据集的重要性，并让误差的计算变得简单）<br>$$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…w_{m+1,N})$$<br>$$w_{m+1,i}=\frac {w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x-i)),i=1,2,…N$$<br>这里$y_i={-1,1} $为真实值，$G_m(x_i)={-1,1}$为预测值。当预测正确时$y_iG_m(x_i)$为1，反之为-1。<br>令$\delta_{m_i}=\alpha_my_iG_m(x_i)$，$\theta_{mi}=\frac {w_{mi}}{Z_m}$(把它看作一个用于归一化权值的加权平均常数)。权重$w_{m+1,i}$的更新函数可以简化为$$w_{m+1,i}=\theta_{mi}exp(\delta <em>{mi}),i=1,2,…N$$画出$y=w</em>{m+1,i}=exp(\delta_{mi})$的图形来看一下：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266159564936.jpg" alt=""></p><p>由于$0&lt;\alpha_m&lt;1$，所以$-1&lt;\delta_{m,i }&lt;1$。且<strong>使得预测错误的数据集样本点更新后的权重变大，预测正确的权值变小，然后对所有权值进行归一化</strong>。这就是该函数实现的作用。(图中y=1是当$\alpha$无限接近于0时的情况：解释为，当$\alpha_m$权值越大，权重$w_{m+1,i}$更新改变的效果越明显。)<br>这里，$Z_m$是规范化因子，目的是使各数据集的权重进行归一化。理解为$Z_m$=更新后的各数据集权重之和。<br>$$Z_m=∑^N_{i=1}w_{mi}exp(-\alpha_my_iG_m(x_i))$$<br>（3）构建基本分类器的新型组合$f(x)=∑^M_{m=1}\alpha_mG_m(x)$，即：<br>$$G(x)=sign(f(x))=sign(∑^M_{m=1}\alpha_mG_m(x))$$<br>函数$sign()$的意义是将正数判别为1，负数判别为-1，最终达到分类的目的。如图：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266164023757.jpg" alt=""></p></blockquote><h3 id="2-AdaBoost构建步骤"><a href="#2-AdaBoost构建步骤" class="headerlink" title="2. AdaBoost构建步骤"></a>2. AdaBoost构建步骤</h3><blockquote><p>上面解释了AdaBoost算法的具体内容。这里写出它的分布实现步骤再对上文算法加深下理解：<br>（1）假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$。<br>（2）AdaBoost反复学习基本分类器，在每一轮$m＝1,2,…,M$顺次地执行下列操作：<br>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$<br>（b）计算基本分类器$G_m(x)$再加权训练数据集上的分类误差率（即误分类样本的权值之和。这里要注意$w_{mi}$表示第$m$轮中第$i$个实例的权值，且权值之和为1，即$∑^N_{i=1}w_{mi}=1$）：<br>$$e_m=P(G_m(x_i)≠y_i)=∑_{G_m(x_i)≠y_i}w_{mi}$$<br>（c）计算基本分类器$G_m (x)$的系数$\alpha_m$。$alpha_m$表示$G_m(x)$在最终分类器中的重要性。由上面（2-c）可知，<strong>当$e_m≤1/2$时，$alpha_m≥0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的分类器在最终分类器中的作用越大。</strong><br>（d）更新训练数据的权值分布为下一轮作准备。式（2-d）的权重更新函数可以写成：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266248077288.jpg" alt="">    </p><p> 由此可知，被基本分类器$G_m (x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大$e^{(2\alpha_m)}=\frac{e_m}{1-e_m} $倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。<br>（3）线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$ 表示了基本分类器$G_m (x)$的重要性，这里，所有$\alpha_m$ 之和并不为1。$f(x)$的符号决定实例x的类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p></blockquote><h3 id="3-AdaBoost算法缺点"><a href="#3-AdaBoost算法缺点" class="headerlink" title="3. AdaBoost算法缺点"></a>3. AdaBoost算法缺点</h3><p><strong>1. 常规AdaBoost算法只能处理二分类问题</strong></p><p>MultiBoost工具的侧重点不同于XGBoost，是Adaboost算法的多分类版本实现，更偏向于解决multi-class / multi-label / multi-task的分类问题。</p><p><strong>2. 对异常值敏感</strong></p><p>指数损失存在的一个问题是不断增加误分类样本的权重（指数上升）。如果数据样本是异常点（outlier），会极大的干扰后面基本分类器学习效果；</p><p><strong>3. 模型无法用于概率估计</strong></p><p>对于取值为$\hat y \in {-1,1}$的随机变量说，$e^{-\hat yf}$不是任何概率密度函数的对数形式，模型$f(x)$的结果无法用概率解释。<br>MLAPP中的原话：$e^{-\hat y f}$is not the logarithm of any pmf for binary variables $\hat y \in {-1,1}$; consequently we cannot recover probability estimate from $f(x)$.”</p><h2 id="四、二叉分类提升树（如AdaBoost）"><a href="#四、二叉分类提升树（如AdaBoost）" class="headerlink" title="四、二叉分类提升树（如AdaBoost）"></a>四、二叉分类提升树（如AdaBoost）</h2><p>对于二类分类问题，提升树算法只需要将AdaBoost算法例子中的基本分类器限制为二叉分类树即可，可以说<strong>此时的决策树算法时AdaBoost算法的特殊情况</strong>。</p><p><strong>二叉分类树中用基尼指数作为最优特征选择的度量标准。</strong></p><p>在实际操作中，通过遍历所有特征（如果是连续值，需做离散化）及其取值，选择基尼指数最小所对应的特征和特征值。</p><h2 id="五、二叉回归提升树"><a href="#五、二叉回归提升树" class="headerlink" title="五、二叉回归提升树"></a>五、二叉回归提升树</h2><p><strong>二叉回归树采用平方误差最小化作为特征选择和切分点选择的依据</strong>。</p><p>下面要解决的问题是：如何划分特征空间？</p><p>一个启发式的方式就是选择特征空间中第$m$个特征$f_m$和它的取值$s$,作为划分特征和划分点，然后寻找最优划分特征$f_m$和最优划分点$s$。</p><p><strong>具体操作就是遍历所有未划分过的特征集合和对应的取值（集合）</strong>求解得出另损失函数最小的参数$f_m和s$。</p><h3 id="1-回归问题提升树算法解析"><a href="#1-回归问题提升树算法解析" class="headerlink" title="1. 回归问题提升树算法解析"></a>1. 回归问题提升树算法解析</h3><p>对于二类分类问题，提升树算法只需将AdaBoost算法中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况，这里不再细述。</p><p>已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$ \chi $为输入空间，$y_i \in Y ⊆ R$，$Y$为输出空间。如果将输入空间$\chi $划分为$J$个互不相交的区域$R_1,R_2,…R_J$，并且在每个区域上确定输出的敞亮$c_j$，那么树可以表示为：$$T(x; \Theta)=∑^J_{j=1}c_jI(x \in R_j)$$</p><p>其中参数$\Theta ={(R_1,c_1),(R_3,c_2),…,(R_J,c_J)}$表示树的区域划分和各区域上的常数。$J$是回归树的复杂度即叶节点的个数。</p><blockquote><p>$f_0(x)=0$<br>$f_m(x)=f_{m-1}(x)+T(x;\Theta_m),m=1,2,…,M$<br>$f(x)=f_M(x)=∑^M_{m=1}T(x;\Theta_m)$</p><p>在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x) $，需求解：</p><p>$Pred (\Theta_m)$<br>$=argmin_{(\Theta_m)}∑^N_{i=1}L(y_i,f_m(x_i))$<br>$=argmin_{(\Theta_m)}∑^N_{i=1}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m ))$<br>得到$Pred(\Theta _m)$，即第$m$棵树的参数。</p><p>当采用平方误差损失函数$L(y,f(x))=(y-f(x))^2$时，其损失变为：</p><p>$L(y,f_{m-1}(x)+T(x;\Theta_m ) )$<br>$=[y-f_{m-1}(x)-T(x;\Theta_m )]^2$<br>$=[\gamma -T(x;\Theta_m)]^2$</p><p>这里$\gamma=y-f_{m-1}(x)$<strong>是当前模型拟合数据的残差（这点很重要）</strong>。所以，对回归问题的提升算法来说，只需要简单地拟合当前模型的残差。这样算法是很简单地。</p></blockquote><h3 id="2-回归提升树构建步骤"><a href="#2-回归提升树构建步骤" class="headerlink" title="2. 回归提升树构建步骤"></a>2. 回归提升树构建步骤</h3><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：回归提升树$f(x)$;<br>（1）初始化$f_0(x)=0 $</p><p>（2）对$m=1,2,…,M$</p><p>（a）计算或更新残差<br>$$\gamma _{mi}=y_i-f_{m-1}(x_i),i=1,2,…,N$$<br>（b）拟合残差$\gamma_{mi}$学习一个回归树，得到$T(x; \Theta_m)$<br>（c）更新$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$<br>（d）重复步骤（a～c）知道满足终止条件</p><p>（3）得到回归问题提升树<br>$$f(x)=f_M(x)=∑^M_{m=1}T(x;\Theta_m)$$</p></blockquote><h2 id="六、梯度提升树（GBDT）"><a href="#六、梯度提升树（GBDT）" class="headerlink" title="六、梯度提升树（GBDT）"></a>六、梯度提升树（GBDT）</h2><h3 id="1-梯度提升树-GBDT-算法解析"><a href="#1-梯度提升树-GBDT-算法解析" class="headerlink" title="1. 梯度提升树(GBDT)算法解析"></a>1. 梯度提升树(GBDT)算法解析</h3><p><strong>Gradient boosting</strong> 就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高。之所以称为 Gradient，是因为在添加新模型时使用了梯度下降算法来最小化的损失。<br><strong>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。</strong></p><p>损失函数的负梯度为：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15267353730899.jpg" alt=""></p><p>也就是说梯度提升树（GBDT）本质上和提升回归树类似，唯一不同的是使用损失函数的负梯度在当前模型的值取近似替代回归提升树中的残差去拟合回归树。这里算法解析部分可以参考上文回归提升树。</p><h3 id="2-梯度回归树（GBDT）构建步骤"><a href="#2-梯度回归树（GBDT）构建步骤" class="headerlink" title="2. 梯度回归树（GBDT）构建步骤"></a>2. 梯度回归树（GBDT）构建步骤</h3><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：梯度提升树$\hat{f}(x) $;(因为这里用损失函数负梯度的值去近似残差，因此使用$\hat{f}(x) $更严谨些)<br>（1）初始化：这里初始化与回归提升树略有不同<br>$$f_0(x)=argmin_c∑^N_{i=1}L(y_i,c)$$</p><p>（2）对$m=1,2,…,M$<br>（a）对$i=1,2,…,M$计算损失函数在当前模型的值作为残差$\gamma_{mi}$的近似</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267354146996.jpg" alt=""></p><p>（b）对$\gamma_{mi}$拟合一个回归树，得到第$m$棵树的叶节点区域$R_{mj}$，$j=1,2,…,J$<br>（c）对$j=1,2,..,J$，计算<br>$$c_{mj}=argmin_{(c)}∑_{(x_i \in R_{mj})}$$<br>（d）更新$f_m(x)=f_{m-1}+∑^J_{j=1}c_{mj}I(x\in R_{mj})$</p><p>（3）得到回归树<br>$$\hat{f}(x)=f_M(x)=∑^M_{m=1}∑^J_{j=1}c_{mj}I(c\in R_{mj})$$</p></blockquote><p><strong>算法解释</strong>：</p><ol><li>第（1）步初始化，估计使用损失函数极小化的常数值，它是只有一个根结点的树。</li><li>第（2a）步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，就是通常所说的残差；对于一般损失函数，它就是残差的近似值。</li><li>第（2b）步估计回归树叶节点区域，以拟合残差的近似值。</li><li>第（2c）步<strong>利用线性搜索估计叶节点区域的值，是损失函数极小化</strong>。</li><li>第（2d）步更新回归树，然后输出最终模型$\hat{y}(x)$。</li></ol><h3 id="3-GBDT缺点"><a href="#3-GBDT缺点" class="headerlink" title="3. GBDT缺点"></a>3. GBDT缺点</h3><p> <strong>效率低</strong>：gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。所以就有了<strong>XGBoost</strong>。</p><h2 id="七、XGBoost"><a href="#七、XGBoost" class="headerlink" title="七、XGBoost"></a>七、XGBoost</h2><h3 id="1-XGBoost原理详解"><a href="#1-XGBoost原理详解" class="headerlink" title="1. XGBoost原理详解"></a>1. XGBoost原理详解</h3><p>前面介绍了提升树算法，其实 XGBoost 就是一种特殊的提升树算法，准确的说它是一种梯度提升决策树（GBDT ，Gradient Boosting Decision Trees）。GBDT 与前面介绍的提升树方法主要的区别就是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，来拟合一颗回归树，即：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267877818108.jpg" alt=""></p><p>XGBoost 就是对 GBDT 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。</p><p>而 XGBoost 的特点就是<strong>计算速度快</strong>，<strong>模型表现好</strong>，这两点也正是这个项目的目标。</p><h3 id="2-XGBoost的目标函数"><a href="#2-XGBoost的目标函数" class="headerlink" title="2. XGBoost的目标函数"></a>2. XGBoost的目标函数</h3><p>传统 GBDT 算法的目标函数只有损失函数这一项，而 XGBoost 在此基础上进行了改进，增加了正则项以防止模型过度复杂：</p><p>$$Obj =∑^N_{i=1}L(y_i,\hat{y}<em>i)+∑</em>{m=1}^M\Omega(f_m), f_m \in F$$</p><p>在这里我们不能够使用 SGD 算法进行优化，因为我们需要寻找的新的函数 f 是一棵树，而不仅仅是一个数值向量。解决方案也是和提升树一样，采用 Boosting 的思想，<strong>从一个常量（通常是0）进行预测，每次添加一个新的预测残差的函数</strong>：<br>$f_0(x)=0$<br>$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$<br>$f_M(x)=∑^M_{m=1}T(x;\Theta_m),m=1,2,…,M$</p><p>由以上公式我们可以知道，我们所要做的唯一的一件事就是寻找一个方法来确定第$m$轮的函数$f_m(x_i)$。但是怎么确定每一轮迭代的函数$f_m(x)$呢，答案是优化！</p><h3 id="3-XGBoost算法学习过程"><a href="#3-XGBoost算法学习过程" class="headerlink" title="3. XGBoost算法学习过程"></a>3. XGBoost算法学习过程</h3><p>第$m$轮迭代的预测结果为：<br>$$\hat{y}_m(x_i)=\hat{y}_{m-1}(x_i)+f_m(x_i)$$</p><p>目标函数为：</p><p>$Obj_m =∑^N_{i=1}L(y_i,f_m(x_i))+∑_{m=1}^M\Omega(f_m)$</p><p>$=∑^N_{i=1}L(y_i,f_{m-1}(x)+f_m(x_i)+\Omega(f_m))+const$</p><p>推导到这里，应该就清楚了，我们的目标就是最小化上式去除常量的部分。那么我们首先来考虑一下损失函数为平方(Square）损失的情况，即：</p><p>$Obj_m =∑^N_{i=1}(y_i,f_{m-1}(x_i)+f_m(x_i))^2+\Omega(f_m)$</p><p>$=∑^N_{i=1}[2(f_{m-1}-y_i)f_m(x_i)+f_m(x_i)^2]+\Omega(f_m)+const$</p><p>有上述分析，我们知道对于目标函数当我们将损失函数设定为平方损失的时候，目标函数最终可转化为一个关于 $f_m(x)$ 的二次函数，这时候我们很容易对其进行优化，甚至可以求出它的解析解。</p><p>但是，为了算法的通用性和可扩展性的考虑，XGBoost 并没有指定损失函数为平方损失函数，此时我们会发现其实目标函数的表达是还是相当复杂的： </p><p>$$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+\Omega(f_m)+const$$</p><p>这时候该怎么做呢，陈天奇大神想出了我们高数中学过的泰勒展开式，具体怎么做的呢？</p><h3 id="4-泰勒展开时近似损失函数"><a href="#4-泰勒展开时近似损失函数" class="headerlink" title="4. 泰勒展开时近似损失函数"></a>4. 泰勒展开时近似损失函数</h3><p>为了更好的介绍 XGBoost 中是如何使用泰勒展开式来近似损失函数的，首先让我们回顾一下泰勒展开式的二阶形式：</p><p>$$f(x+\Delta x)=f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2+R(\Delta x)$$</p><p>其中$R(\Delta x)$表示$\Delta x$的高阶无穷小。因此，有：<br>$$f(x+\Delta x)\approx f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2$$</p><p>有了泰勒公式，我们给出如下定义：</p><p>$$g_i=\partial _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$<br>$$h_i=\partial^2 _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$</p><p>这里我们把$L(y_i,f_{m-1}(x_i))$看成是$f_{m-1}(x_i)$为自变量的函数，因此$g_i$和$h_i$为其一阶导和二阶导数（其实是偏导），并且我们将目标函数中的$f_m(x_i)$看成上式中自变量的增量$\Delta x$，因此将目标函数按$f_{m-1}(x_i)$进行泰勒展开，得到：</p><p>$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+ \Omega(f_m)+const$<br>$\approx ∑^N_{i=1}[L(y_i,f_{m-1}(x_i))+g_if_m(x_i)+\frac 12h_if^2_m(x_i)] + \Omega(f_m)+const$</p><p>去除掉常量部分，我们可以得到新的目标函数：<br>$$Obj_m=∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if^2_m(x_i)]+\Omega(f_m) $$</p><p>这样做的好处是：</p><blockquote><p><strong>理论上的好处</strong>：使得我们更加清楚的知道我们在学习什么，以及更好的收敛性；<br><strong>工程上的好处</strong> ：</p><ol><li>$g_i$和$h_i$都来自于损失函数的定义</li><li>函数的学习过程仅仅通过$g_i$和$h_i$依赖于目标函数</li><li>可以利用不同的模块分开实现不同的损失函数，例如平方损失函数和 logistic 损失函数，这样损失函数不会受限制。</li></ol></blockquote><h3 id="5-正则化的处理"><a href="#5-正则化的处理" class="headerlink" title="5. 正则化的处理"></a>5. 正则化的处理</h3><p>目标函数中正则化项存在的原因是为了限制模型的复杂度，<strong>让模型在训练集上能够取得较好的结果的前提下尽可能地简单</strong>。而前面我们也提到了，在 XGBoost 中，对于采用前向分布方法一步步迭代的优化时，我们模型的复杂度就是当前要定义的决策树的复杂度。</p><h4 id="决策树函数的定义"><a href="#决策树函数的定义" class="headerlink" title="决策树函数的定义"></a>决策树函数的定义</h4><p>为此，我们首先重新定义树：我们将树定义为一个该树中所有叶子节点的值的向量。并且，每个叶子的索引映射函数映射实例到每个叶子节点上：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268011299685.jpg" alt=""></p><h4 id="定义决策树的复杂度"><a href="#定义决策树的复杂度" class="headerlink" title="定义决策树的复杂度"></a>定义决策树的复杂度</h4><p>我们将决策树的复杂度，也就是目标函数定义如下：<br>$$\Omega (f_m)=\gamma T+\frac 12\lambda∑^T_{j=1}w_j^2$$</p><p>其中，$T$树中叶子结点的个数计算如下：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268013497837.jpg" alt=""></p><h4 id="新目标函数的优化"><a href="#新目标函数的优化" class="headerlink" title="新目标函数的优化"></a>新目标函数的优化</h4><p>首先，我们对叶子节点$j$中的实例进行如下定义：<br>$$I_j={i|q(x_i)=j }$$<br>此时目标函数为：<br>$$Obj_m\approx ∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if_m^2(x_i)]+\Omega(f_m)+const$$<br>$$=∑^N_{i=1}[g_iw_{q(x_i)}+\frac 12h_iw^2_{q(x_i)}]+\gamma T+\frac 12 \lambda ∑^T_{j=1}w^2_j$$<br>$$=∑^T_{j=1}[(∑_{i\in I_j}g_i)w_i+\frac 12(∑_{i \in I_j}h_i+\lambda)w^2_j]+\gamma T $$</p><p>首先，我们进行如下定义：</p><p>$$G_j=∑_{i \in I_j}g_i,H_j=∑_{i \in I_j}h_i$$</p><p>进一步简化目标函数：<br>$$Obj_m=∑^T_{j=1}[G_jw_j+\frac 12(H_i+\lambda)w^2_j]+\gamma T$$</p><p>众所周知，对于一元二次函数，由如下两条结论：<br>$$arg \min _xG_x + \frac 12Hx^2=-\frac GH,H&gt;0$$<br>$$\min _xG_x+\frac 12Hx^2=- \frac {G^2}{2H}$$</p><p>因此对于目标函数进行最小化，当$w_j=-\frac {G_j}{H_j+\lambda }$时，我们得到：<br>$$\min _{Obj_m}=∑^T_{j=1}[G_jw_j+\frac 12(H_j+\lambda)w^2_j]+\gamma T$$<br>$$=-\frac 12 ∑^T_{j=1}\frac {G^2_j}{H_j+\lambda }+\gamma T$$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268027278380.jpg" alt=""></p><p>至此，对于第 t 轮的一颗已经分裂好的决策树，我们能够求出其对应的最小化的目标函数。但是到目前为止，到底如何进行分裂我们还不知道具体的做法，接下来让我们一起学习 XGBoost 是如何寻找分裂点的。</p><h3 id="6-决策树构建"><a href="#6-决策树构建" class="headerlink" title="6. 决策树构建"></a>6. 决策树构建</h3><h4 id="决策树的生成策略"><a href="#决策树的生成策略" class="headerlink" title="决策树的生成策略"></a>决策树的生成策略</h4><p>对于回归决策树来说，在目标函数已经确定的情况下，接下来我们的问题是如何寻找对于当前训练样本的最优的决策树结构。当然，我们最容易想到的是穷举法。如果按照穷举法，我们列出所有的可能的决策树的结构$q$，然后基于决策树结构$q$去计算它的目标函数值，在计算完所有可能的决策树结构后，选择目标函数值最小的决策树结构$\hat q$作为最终决策树。</p><p>虽然理论上来说，穷举法能够寻找到最优的决策树结构，但是在有限的时间内我们无法去寻找到最优的决策树结构，因为可能的树结构有无穷多种。因此，在应用中我们采用的是贪心的策略来一步步地增长树的结构。也就是从根节点开始，不断地进行递归地分裂，直至在给定的准则下无法进行分裂为止。所以，接下来我们需要知道递归时如何进行合适并有效地分裂当前节点。</p><h4 id="寻找最优分裂点"><a href="#寻找最优分裂点" class="headerlink" title="寻找最优分裂点"></a>寻找最优分裂点</h4><p>对于一个叶子节点，如何进行分裂我们需要遵循的原则是分裂后的两颗子树的最优目标函数值之和要小于未分裂之前的父节点，为了叙述方便我们定义如下的目标函数值的 “增益”：\<br>$$Gain=\frac 12[\frac {G^2_L}{H_L+\lambda } + \frac {G^2_R}{H_R+\lambda } - \frac {(G_L+G_R)^2}{H_L+H_R+\lambda } -\gamma ]$$<br>上式表示的是在某个节点分裂前的目标函数值与分裂后的目标函数值的差值，由于我们的目标是寻找到最优的决策树，也就是说只有当$Gain$的值为正时我们才会选择进行分裂。</p><blockquote><p><strong>分裂点寻找方法</strong>：</p></blockquote><blockquote><ul><li>对每一个待分裂节点，枚举出所有的特征；</li><li>对于每一个特征，根据该特征将所有的实例进行排序；</li><li>使用线性扫描的方法计算该特征的每个可能的值作为分裂点时对应的 $Gain$对所有特征，使用上述扫描过程中找到的 $Gain$ 值大时特征及其对应的取值作为分裂点，将当前节点一分为二。</li></ul></blockquote><h4 id="离散变量处理"><a href="#离散变量处理" class="headerlink" title="离散变量处理"></a>离散变量处理</h4><p>传统的 GBDT 算法对连续型变量和离散型变量是进行分开处理的。例如 Spark 中的 GBDT 就是这样的，当我们的实例特征中有离散型变量的时候，就需要通过参数指定该离散型变量的种类，这样使得算法的用户友好性变得十分的糟糕。</p><p>而 XGBoost 在设计时就考虑到了这一点。实际上，我们不需要将离散型特征变量进行分开处理，XGBoost 使用 one-hot 编码的方式对离散型变量进行处理。</p><h4 id="剪枝策略"><a href="#剪枝策略" class="headerlink" title="剪枝策略"></a>剪枝策略</h4><ul><li><strong>前剪枝</strong><ul><li>当最优分裂点对应的增益值为负时停止分裂</li><li>但是这也会存在问题，即将来的某个时刻能够获取更高的增益</li></ul></li><li><strong>后剪枝</strong><ul><li>将决策树增长到它的最大深度，递归的进行剪枝，剪去那些使得增益值为负值的叶子节点。</li></ul></li></ul><h4 id="前向分步的步长"><a href="#前向分步的步长" class="headerlink" title="前向分步的步长"></a>前向分步的步长</h4><p>在 XGBoost 提升过程中，每产生一颗对当前残差最优化的决策树 $f_m(x)$时，并不是直接将决策树$f_m(x)$加入到模型中，而是对它乘以一个固定的算法参数 $\eta$之后才加入到模型中：</p><p>$$f_m(x)=f_{m-1}(x)+\eta f_m(x)$$</p><p>这样做的好处是防止单步决策树过拟合，以减少每棵树对最终木星的影响。</p><h3 id="7-XGBoost构建步骤流程"><a href="#7-XGBoost构建步骤流程" class="headerlink" title="7. XGBoost构建步骤流程"></a>7. XGBoost构建步骤流程</h3><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：梯度提升树$\hat{f}(x) = \hat f_M(x)$;</p><p>（1）初始化$f_0(x)=0$</p><p>（2）对$m=1,2,…,M$，依次进行循环迭代：</p><p>（a）对每个样本点，分别计算：<br>$$g_i= \partial _{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>$$h_i= \partial ^2_{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>（b）使用贪心策略构建一棵树$f_m(x)$，以使得下列的目标函数最小化：<br>$$Obj=-\frac 12 ∑^T_{j=1}\frac {G_j^2}{H_j+\lambda }+\gamma T$$<br>（c）更新：$f_M(x)=f_{m-1}(x)+\eta f_m(x)$<br>（3）得到XGBoost提升树：<br>$$f(x)=f_M(x)=∑^M_{m=1}\eta f_m(x)$$</p></blockquote><h3 id="8-XGBoost优缺点"><a href="#8-XGBoost优缺点" class="headerlink" title="8. XGBoost优缺点"></a>8. XGBoost优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li><strong>正则化</strong></li></ol><p>XGBoost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variancetradeoff 角度来讲，正则项降低了模型的 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。</p><ol start="2"><li><strong>并行处理</strong></li></ol><p>XGBoost 工具支持并行。Boosting 不是一种串行的结构吗?怎么并行的？注意 XGBoost 的并行不是 tree 粒度的并行，XGBoost 也是一次迭代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代的预测值）。XGBoost 的并行是在特征粒度上的。</p><p>我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost 在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p><ol start="3"><li><strong>灵活性</strong></li></ol><p>XGBoost 支持用户自定义目标函数和评估函数，只要目标函数二阶可导就行。</p><ol start="4"><li><strong>缺失值的处理</strong></li></ol><p>对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。</p><ol start="5"><li><strong>剪枝</strong></li></ol><p>XGBoost 先从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝。比起 GBM，这样不容易陷入局部最优解。</p><ol start="6"><li><strong>内置交叉验证</strong></li></ol><p>XGBoost 允许在每一轮 Boosting 迭代中使用交叉验证。因此，可以方便地获得最优 Boosting 迭代次数。而 GBM 使用网格搜索，只能检测有限个值。</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>虽然说 XGBoost 在 Kaggle 比赛中获得了不错的成绩，但并不代表 XGBoost 是一个完美的算法，它当然也有自己的缺点和不足之处:</p><ol><li><strong>算法参数过多</strong></li></ol><p>调参复杂，需要对 XGBoost 原理十分清楚才能很好的使用 XGBoost。</p><ol start="2"><li><strong>只适合处理结构化数据</strong></li></ol><p>相对于深度学习算法来说，XGBoost 算法只适合处理结构化的特征数据，而对于类似图像中的目标检测等任务重的非结构化数据没有很好的处理能力。</p><ol start="3"><li><strong>不适合处理超高维特征数据</strong></li></ol><p>XGBoost 算法对于中低维数据具有很好的处理速度和精度，但是对于例如大规模图像物体识别，或者是推荐算法的某些场景中会出现的超高维特征的数据就无能为力了，这时候我们就需要借助于深度学习等算法。</p><h2 id="八、-总结"><a href="#八、-总结" class="headerlink" title="八、 总结"></a>八、 总结</h2><h3 id="1-Boosting家族"><a href="#1-Boosting家族" class="headerlink" title="1. Boosting家族"></a>1. Boosting家族</h3><p>提升(Boosting)是集成学习方法里的一个重要方法，其主要思想是将弱分类器组装成一个强分类器。在 PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p><p><strong>AdaBoost、GBDT、XGBoost</strong>都属于（加法模型、前向分步、指数损失函数）家族，都实现了通过将多个弱学习器组合成强学习器已达到提升预测准确度的目的。实现的过程略有不同且适用于不同场景。<br>Boosting并非是一个方法，而是一类方法。这里按照损失函数的不同，将其细分为若干类算法，下表给出了4种不同损失函数对应的Boosting方法：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267180767470.jpg" alt=""></p><h3 id="2-Adaboost"><a href="#2-Adaboost" class="headerlink" title="2. Adaboost"></a>2. Adaboost</h3><p><strong>AdaBoost</strong>是一种典型的分类回归树，适合解决二分类问题。多分类问题解决起来比较麻烦（参考上文）</p><p><strong>AdaBoost</strong>是一种采用加法模型、前向分步算法、指数损失函数的提升树。可以表示为 Boosting 的前向分布算法的一个特例。 <a href="http://ihoge.cn/2018/adaboost.html" target="_blank" rel="noopener">更多内容请参考</a></p><h3 id="3-回归提升树和-Adaboost"><a href="#3-回归提升树和-Adaboost" class="headerlink" title="3. 回归提升树和 Adaboost"></a>3. 回归提升树和 Adaboost</h3><p><strong>回归提升树</strong>与<strong>AdaBoot</strong>不同的地方在于它使用的是平方损失函数来解决回归的问题，通过计算残差发现模型的不足，并不断拟合更新<strong>残差</strong>来构建新树。（这里残差的实现的功能与Adaboost有些类似）</p><h3 id="4-GBDT-和回归提升树"><a href="#4-GBDT-和回归提升树" class="headerlink" title="4. GBDT 和回归提升树"></a>4. GBDT 和回归提升树</h3><ol><li><strong>梯度提升树</strong>可以看作是特殊的一种回归提升树。它通过计算损失函数的负梯度在当前模型的值来近似回归树中的“残差”发现模型的不足，并通过拟合该“残差”构建新树。</li><li>回归提升树初始化另$f_0(x)=0$；GBDT初始化时令$f_{ 0 }(x)=arg\min _{ c } \sum _{ i=1 }^{ N }{ L(y_i,c) } $估计是损失函数极小化的常数值。</li><li>回归提升树通过平方损失函数计算残差，GBDT通过计算负梯度作为伪残差。</li></ol><h3 id="5-XGBoost-和-GBDT"><a href="#5-XGBoost-和-GBDT" class="headerlink" title="5. XGBoost 和 GBDT"></a>5. XGBoost 和 GBDT</h3><ul><li><p>Xgboost 是 GB 算法的高效实现，xgboost 中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）</p></li><li><p>xgboost在目标函数中显示的加上了正则化项</p></li><li><p>GB 中使用 Loss Function 对 f(x) 的一阶导数计算出伪残差用于学习生成$f_m$，xgboost 不仅使用到了一阶导数，还使用二阶导数</p></li><li><p>CART 回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost 寻找分割点的标准是最大化，lamda，gama 与正则化项相关</p></li></ul><h3 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h3><p>[1] 《统计学习方法》  ——李航 2012 清华大学出版社<br>[2] 《机器学习》  ——周志华 2016 清华大学出版社<br>[3] <a href="http://ihoge.cn/2018/adaboost.html" target="_blank" rel="noopener">http://ihoge.cn/2018/adaboost.html</a><br>[4] <a href="https://www.jianshu.com/nb/7305482" target="_blank" rel="noopener">https://www.jianshu.com/nb/7305482</a><br>[5] <a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/#Gradient_Boosting" target="_blank" rel="noopener">http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/#Gradient_Boosting</a><br>[6] <a href="https://www.jianshu.com/p/d55f7aaac4a7" target="_blank" rel="noopener">https://www.jianshu.com/p/d55f7aaac4a7</a><br>[7] <a href="http://gitbook.cn/gitchat/column/5ac2f0509e924a1dc029dd84/topic/5ac9e0e5dbd50e7493d35d3f" target="_blank" rel="noopener">http://gitbook.cn/gitchat/column/5ac2f0509e924a1dc029dd84/topic/5ac9e0e5dbd50e7493d35d3f</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;本文是综合了之前的以往多个笔记汇总而成，内容包含：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;一、Boosting基本概念
二、前向分步加法模型
    1. 加法模型
    2. 前向分步算法
三、AdaBoost
    1. 算法解析
   
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost算法学习总结</title>
    <link href="http://www.ihoge.cn/2018/XGBoost.html"/>
    <id>http://www.ihoge.cn/2018/XGBoost.html</id>
    <published>2018-05-20T04:20:21.000Z</published>
    <updated>2018-08-05T10:16:49.409Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、XGBoost原理详解"><a href="#一、XGBoost原理详解" class="headerlink" title="一、XGBoost原理详解"></a>一、XGBoost原理详解</h2><p>前面介绍了提升树算法，其实 XGBoost 就是一种特殊的提升树算法，准确的说它是一种梯度提升决策树（GBDT ，Gradient Boosting Decision Trees）。GBDT 与前面介绍的提升树方法主要的区别就是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，来拟合一颗回归树，即：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267877818108.jpg" alt=""></p><p>XGBoost 就是对 GBDT 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。</p><p>而 XGBoost 的特点就是<strong>计算速度快</strong>，<strong>模型表现好</strong>，这两点也正是这个项目的目标。</p><h2 id="二、XGBoost的目标函数"><a href="#二、XGBoost的目标函数" class="headerlink" title="二、XGBoost的目标函数"></a>二、XGBoost的目标函数</h2><p>传统 GBDT 算法的目标函数只有损失函数这一项，而 XGBoost 在此基础上进行了改进，增加了正则项以防止模型过度复杂：</p><p>$$Obj =∑^N_{i=1}L(y_i,\hat{y}<em>i)+∑</em>{m=1}^M\Omega(f_m), f_m \in F$$</p><p>在这里我们不能够使用 SGD 算法进行优化，因为我们需要寻找的新的函数 f 是一棵树，而不仅仅是一个数值向量。解决方案也是和提升树一样，采用 Boosting 的思想，<strong>从一个常量（通常是0）进行预测，每次添加一个新的预测残差的函数</strong>：<br>$f_0(x)=0$<br>$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$<br>$f_M(x)=∑^M_{m=1}T(x;\Theta_m),m=1,2,…,M$</p><p>由以上公式我们可以知道，我们所要做的唯一的一件事就是寻找一个方法来确定第$m$轮的函数$f_m(x_i)$。但是怎么确定每一轮迭代的函数$f_m(x)$呢，答案是优化！</p><h2 id="三、XGBoost算法学习过程"><a href="#三、XGBoost算法学习过程" class="headerlink" title="三、XGBoost算法学习过程"></a>三、XGBoost算法学习过程</h2><p>第$m$轮迭代的预测结果为：<br>$$\hat{y}_m(x_i)=\hat{y}_{m-1}(x_i)+f_m(x_i)$$</p><p>目标函数为：</p><p>$Obj_m =∑^N_{i=1}L(y_i,f_m(x_i))+∑_{m=1}^M\Omega(f_m)$</p><p>$=∑^N_{i=1}L(y_i,f_{m-1}(x)+f_m(x_i)+\Omega(f_m))+const$</p><p>推导到这里，应该就清楚了，我们的目标就是最小化上式去除常量的部分。那么我们首先来考虑一下损失函数为平方(Square）损失的情况，即：</p><p>$Obj_m =∑^N_{i=1}(y_i,f_{m-1}(x_i)+f_m(x_i))^2+\Omega(f_m)$</p><p>$=∑^N_{i=1}[2(f_{m-1}-y_i)f_m(x_i)+f_m(x_i)^2]+\Omega(f_m)+const$</p><p>有上述分析，我们知道对于目标函数当我们将损失函数设定为平方损失的时候，目标函数最终可转化为一个关于 $f_m(x)$ 的二次函数，这时候我们很容易对其进行优化，甚至可以求出它的解析解。</p><p>但是，为了算法的通用性和可扩展性的考虑，XGBoost 并没有指定损失函数为平方损失函数，此时我们会发现其实目标函数的表达是还是相当复杂的： </p><p>$$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+\Omega(f_m)+const$$</p><p>这时候该怎么做呢，陈天奇大神想出了我们高数中学过的泰勒展开式，具体怎么做的呢？</p><h2 id="三、泰勒展开时近似损失函数"><a href="#三、泰勒展开时近似损失函数" class="headerlink" title="三、泰勒展开时近似损失函数"></a>三、泰勒展开时近似损失函数</h2><p>为了更好的介绍 XGBoost 中是如何使用泰勒展开式来近似损失函数的，首先让我们回顾一下泰勒展开式的二阶形式：</p><p>$$f(x+\Delta x)=f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2+R(\Delta x)$$</p><p>其中$R(\Delta x)$表示$\Delta x$的高阶无穷小。因此，有：<br>$$f(x+\Delta x)\approx f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2$$</p><p>有了泰勒公式，我们给出如下定义：</p><p>$$g_i=\partial _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$<br>$$h_i=\partial^2 _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$</p><p>这里我们把$L(y_i,f_{m-1}(x_i))$看成是$f_{m-1}(x_i)$为自变量的函数，因此$g_i$和$h_i$为其一阶导和二阶导数（其实是偏导），并且我们将目标函数中的$f_m(x_i)$看成上式中自变量的增量$\Delta x$，因此将目标函数按$f_{m-1}(x_i)$进行泰勒展开，得到：</p><p>$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+ \Omega(f_m)+const$<br>$\approx ∑^N_{i=1}[L(y_i,f_{m-1}(x_i))+g_if_m(x_i)+\frac 12h_if^2_m(x_i)] + \Omega(f_m)+const$</p><p>去除掉常量部分，我们可以得到新的目标函数：<br>$$Obj_m=∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if^2_m(x_i)]+\Omega(f_m) $$</p><p>这样做的好处是：</p><blockquote><p><strong>理论上的好处</strong>：使得我们更加清楚的知道我们在学习什么，以及更好的收敛性；<br><strong>工程上的好处</strong> ：</p><ol><li>$g_i$和$h_i$都来自于损失函数的定义</li><li>函数的学习过程仅仅通过$g_i$和$h_i$依赖于目标函数</li><li>可以利用不同的模块分开实现不同的损失函数，例如平方损失函数和 logistic 损失函数，这样损失函数不会受限制。</li></ol></blockquote><h2 id="五、正则化的处理"><a href="#五、正则化的处理" class="headerlink" title="五、正则化的处理"></a>五、正则化的处理</h2><p>目标函数中正则化项存在的原因是为了限制模型的复杂度，<strong>让模型在训练集上能够取得较好的结果的前提下尽可能地简单</strong>。而前面我们也提到了，在 XGBoost 中，对于采用前向分布方法一步步迭代的优化时，我们模型的复杂度就是当前要定义的决策树的复杂度。</p><h3 id="1-决策树函数的定义"><a href="#1-决策树函数的定义" class="headerlink" title="1. 决策树函数的定义"></a>1. 决策树函数的定义</h3><p>为此，我们首先重新定义树：我们将树定义为一个该树中所有叶子节点的值的向量。并且，每个叶子的索引映射函数映射实例到每个叶子节点上：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268011299685.jpg" alt=""></p><h3 id="2-定义决策树的复杂度"><a href="#2-定义决策树的复杂度" class="headerlink" title="2. 定义决策树的复杂度"></a>2. 定义决策树的复杂度</h3><p>我们将决策树的复杂度，也就是目标函数定义如下：<br>$$\Omega (f_m)=\gamma T+\frac 12\lambda∑^T_{j=1}w_j^2$$</p><p>其中，$T$树中叶子结点的个数计算如下：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268013497837.jpg" alt=""></p><h3 id="3-新目标函数的优化"><a href="#3-新目标函数的优化" class="headerlink" title="3. 新目标函数的优化"></a>3. 新目标函数的优化</h3><p>首先，我们对叶子节点$j$中的实例进行如下定义：<br>$$I_j={i|q(x_i)=j }$$<br>此时目标函数为：<br>$$Obj_m\approx ∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if_m^2(x_i)]+\Omega(f_m)+const$$<br>$=∑^N_{i=1}[g_iw_{q(x_i)}+\frac 12h_iw^2_{q(x_i)}]+\gamma T+\frac 12 \lambda ∑^T_{j=1}w^2_j$<br>$=∑^T_{j=1}[(∑_{i\in I_j}g_i)w_i+\frac 12(∑_{i \in I_j}h_i+\lambda)w^2_j]+\gamma T $</p><p>首先，我们进行如下定义：</p><p>$$G_j=∑_{i \in I_j}g_i,H_j=∑_{i \in I_j}h_i$$</p><p>进一步简化目标函数：<br>$$Obj_m=∑^T_{j=1}[G_jw_j+\frac 12(H_i+\lambda)w^2_j]+\gamma T$$</p><p>众所周知，对于一元二次函数，由如下两条结论：<br>$$arg \min _xG_x + \frac 12Hx^2=-\frac GH,H&gt;0$$<br>$$\min _xG_x+\frac 12Hx^2=- \frac {G^2}{2H}$$</p><p>因此对于目标函数进行最小化，当$w_j=-\frac {G_j}{H_j+\lambda }$时，我们得到：<br>$$\min _{Obj_m}=∑^T_{j=1}[G_jw_j+\frac 12(H_j+\lambda)w^2_j]+\gamma T$$<br>$=-\frac 12 ∑^T_{j=1}\frac {G^2_j}{H_j+\lambda }+\gamma T$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268027278380.jpg" alt=""></p><p>至此，对于第 t 轮的一颗已经分裂好的决策树，我们能够求出其对应的最小化的目标函数。但是到目前为止，到底如何进行分裂我们还不知道具体的做法，接下来让我们一起学习 XGBoost 是如何寻找分裂点的。</p><h2 id="六、构建决策树"><a href="#六、构建决策树" class="headerlink" title="六、构建决策树"></a>六、构建决策树</h2><h3 id="1-决策树的生成策略"><a href="#1-决策树的生成策略" class="headerlink" title="1. 决策树的生成策略"></a>1. 决策树的生成策略</h3><p>对于回归决策树来说，在目标函数已经确定的情况下，接下来我们的问题是如何寻找对于当前训练样本的最优的决策树结构。当然，我们最容易想到的是穷举法。如果按照穷举法，我们列出所有的可能的决策树的结构$q$，然后基于决策树结构$q$去计算它的目标函数值，在计算完所有可能的决策树结构后，选择目标函数值最小的决策树结构$\hat q$作为最终决策树。</p><p>虽然理论上来说，穷举法能够寻找到最优的决策树结构，但是在有限的时间内我们无法去寻找到最优的决策树结构，因为可能的树结构有无穷多种。因此，在应用中我们采用的是贪心的策略来一步步地增长树的结构。也就是从根节点开始，不断地进行递归地分裂，直至在给定的准则下无法进行分裂为止。所以，接下来我们需要知道递归时如何进行合适并有效地分裂当前节点。</p><h3 id="2-寻找最优分裂点"><a href="#2-寻找最优分裂点" class="headerlink" title="2. 寻找最优分裂点"></a>2. 寻找最优分裂点</h3><p>对于一个叶子节点，如何进行分裂我们需要遵循的原则是分裂后的两颗子树的最优目标函数值之和要小于未分裂之前的父节点，为了叙述方便我们定义如下的目标函数值的 “增益”：\<br>$$Gain=\frac 12[\frac {G^2_L}{H_L+\lambda } + \frac {G^2_R}{H_R+\lambda } - \frac {(G_L+G_R)^2}{H_L+H_R+\lambda } -\gamma ]$$<br>上式表示的是在某个节点分裂前的目标函数值与分裂后的目标函数值的差值，由于我们的目标是寻找到最优的决策树，也就是说只有当$Gain$的值为正时我们才会选择进行分裂。</p><blockquote><p><strong>分裂点寻找方法</strong>：</p></blockquote><blockquote><ul><li>对每一个待分裂节点，枚举出所有的特征；</li><li>对于每一个特征，根据该特征将所有的实例进行排序；</li><li>使用线性扫描的方法计算该特征的每个可能的值作为分裂点时对应的 $Gain$对所有特征，使用上述扫描过程中找到的 $Gain$ 值大时特征及其对应的取值作为分裂点，将当前节点一分为二。</li></ul></blockquote><h3 id="3-离散变量处理"><a href="#3-离散变量处理" class="headerlink" title="3. 离散变量处理"></a>3. 离散变量处理</h3><p>传统的 GBDT 算法对连续型变量和离散型变量是进行分开处理的。例如 Spark 中的 GBDT 就是这样的，当我们的实例特征中有离散型变量的时候，就需要通过参数指定该离散型变量的种类，这样使得算法的用户友好性变得十分的糟糕。</p><p>而 XGBoost 在设计时就考虑到了这一点。实际上，我们不需要将离散型特征变量进行分开处理，XGBoost 使用 one-hot 编码的方式对离散型变量进行处理。</p><h3 id="4-剪枝策略"><a href="#4-剪枝策略" class="headerlink" title="4. 剪枝策略"></a>4. 剪枝策略</h3><ul><li><strong>前剪枝</strong><ul><li>当最优分裂点对应的增益值为负时停止分裂</li><li>但是这也会存在问题，即将来的某个时刻能够获取更高的增益</li></ul></li><li><strong>后剪枝</strong><ul><li>将决策树增长到它的最大深度，递归的进行剪枝，剪去那些使得增益值为负值的叶子节点。</li></ul></li></ul><h3 id="5-前向分步的步长"><a href="#5-前向分步的步长" class="headerlink" title="5. 前向分步的步长"></a>5. 前向分步的步长</h3><p>在 XGBoost 提升过程中，每产生一颗对当前残差最优化的决策树 $f_m(x)$时，并不是直接将决策树$f_m(x)$加入到模型中，而是对它乘以一个固定的算法参数 $\eta$之后才加入到模型中：</p><p>$$f_m(x)=f_{m-1}(x)+\eta f_m(x)$$</p><p>这样做的好处是防止单步决策树过拟合，以减少每棵树对最终木星的影响。</p><h2 id="七、XGBoost构建步骤"><a href="#七、XGBoost构建步骤" class="headerlink" title="七、XGBoost构建步骤"></a>七、XGBoost构建步骤</h2><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：梯度提升树$\hat{f}(x) = \hat f_M(x)$;</p><p>（1）初始化$f_0(x)=0$</p><p>（2）对$m=1,2,…,M$，依次进行循环迭代：</p><p>（a）对每个样本点，分别计算：<br>$$g_i= \partial _{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>$$h_i= \partial ^2_{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>（b）使用贪心策略构建一棵树$f_m(x)$，以使得下列的目标函数最小化：<br>$$Obj=-\frac 12 ∑^T_{j=1}\frac {G_j^2}{H_j+\lambda }+\gamma T$$<br>（c）更新：$f_M(x)=f_{m-1}(x)+\eta f_m(x)$<br>（3）得到XGBoost提升树：<br>$$f(x)=f_M(x)=∑^M_{m=1}\eta f_m(x)$$</p></blockquote><h2 id="八、XGBoost优缺点"><a href="#八、XGBoost优缺点" class="headerlink" title="八、XGBoost优缺点"></a>八、XGBoost优缺点</h2><h3 id="1-优点"><a href="#1-优点" class="headerlink" title="1. 优点"></a>1. 优点</h3><ol><li><strong>正则化</strong></li></ol><p>XGBoost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variancetradeoff 角度来讲，正则项降低了模型的 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。</p><ol start="2"><li><strong>并行处理</strong></li></ol><p>XGBoost 工具支持并行。Boosting 不是一种串行的结构吗?怎么并行的？注意 XGBoost 的并行不是 tree 粒度的并行，XGBoost 也是一次迭代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代的预测值）。XGBoost 的并行是在特征粒度上的。</p><p>我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost 在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p><ol start="3"><li><strong>灵活性</strong></li></ol><p>XGBoost 支持用户自定义目标函数和评估函数，只要目标函数二阶可导就行。</p><ol start="4"><li><strong>缺失值的处理</strong></li></ol><p>对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。</p><ol start="5"><li><strong>剪枝</strong></li></ol><p>XGBoost 先从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝。比起 GBM，这样不容易陷入局部最优解。</p><ol start="6"><li><strong>内置交叉验证</strong></li></ol><p>XGBoost 允许在每一轮 Boosting 迭代中使用交叉验证。因此，可以方便地获得最优 Boosting 迭代次数。而 GBM 使用网格搜索，只能检测有限个值。</p><h3 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2. 缺点"></a>2. 缺点</h3><p>虽然说 XGBoost 在 Kaggle 比赛中获得了不错的成绩，但并不代表 XGBoost 是一个完美的算法，它当然也有自己的缺点和不足之处:</p><ol><li><strong>算法参数过多</strong></li></ol><p>调参复杂，需要对 XGBoost 原理十分清楚才能很好的使用 XGBoost。</p><ol start="2"><li><strong>只适合处理结构化数据</strong></li></ol><p>相对于深度学习算法来说，XGBoost 算法只适合处理结构化的特征数据，而对于类似图像中的目标检测等任务重的非结构化数据没有很好的处理能力。</p><ol start="3"><li><strong>不适合处理超高维特征数据</strong></li></ol><p>XGBoost 算法对于中低维数据具有很好的处理速度和精度，但是对于例如大规模图像物体识别，或者是推荐算法的某些场景中会出现的超高维特征的数据就无能为力了，这时候我们就需要借助于深度学习等算法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、XGBoost原理详解&quot;&gt;&lt;a href=&quot;#一、XGBoost原理详解&quot; class=&quot;headerlink&quot; title=&quot;一、XGBoost原理详解&quot;&gt;&lt;/a&gt;一、XGBoost原理详解&lt;/h2&gt;&lt;p&gt;前面介绍了提升树
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost简单案例实现</title>
    <link href="http://www.ihoge.cn/2018/XGBoost1.html"/>
    <id>http://www.ihoge.cn/2018/XGBoost1.html</id>
    <published>2018-05-18T12:10:59.000Z</published>
    <updated>2018-08-05T10:17:07.236Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XGBoost案例"><a href="#XGBoost案例" class="headerlink" title="XGBoost案例"></a>XGBoost案例</h1><p>本文中，我们用到印第安人发病的糖尿病数据集。<a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv" target="_blank" rel="noopener">右键点击下载</a></p><p>该数据集由8个描述患者医疗细节的输入变量和一个输出变量组成，以指示患者是否会在5年内发生糖尿病。</p><p>对于第一个XGBoost模型来说，这是一个很好的数据集，因为所有的输入变量都是数值型的，是一个简单的二元分类问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install xgboost</span><br></pre></td></tr></table></figure><h2 id="一、基础应用"><a href="#一、基础应用" class="headerlink" title="一、基础应用"></a>一、基础应用</h2><p><strong>引入 XGBoost 包</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> loadtxt</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><p><strong>加载数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">Y = data.iloc[:,<span class="number">8</span>]</span><br></pre></td></tr></table></figure><p><strong>分训练集测试集</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seed = <span class="number">5</span></span><br><span class="line">test_size = <span class="number">0.2</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)</span><br></pre></td></tr></table></figure><p><strong>训练 XBGoost 模型</strong></p><p>xgboost 有封装好的分类器和回归器，可以直接用 XGBClassifier 建立模型</p><p><a href="http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" target="_blank" rel="noopener">XGBClassifier文档</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = XGBClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><pre><code>XGBClassifier(base_score=0.5, booster=&apos;gbtree&apos;, colsample_bylevel=1,       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,       n_jobs=1, nthread=None, objective=&apos;binary:logistic&apos;, random_state=0,       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,       silent=True, subsample=1)</code></pre><p><a href="http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" target="_blank" rel="noopener">了解更多XGBoost参数信息</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">print(<span class="string">"Accuracy: %.2f%%"</span> % (accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 80.52%</code></pre><h2 id="二、查看模型效果"><a href="#二、查看模型效果" class="headerlink" title="二、查看模型效果"></a>二、查看模型效果</h2><p>XGBoost 可以在模型训练时，评价模型在测试集上的表现，也可以输出每一步的得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = XGBClassifier()</span><br><span class="line">eval_set = [(X_test, y_test)]</span><br><span class="line">model.fit(X_train, y_train, early_stopping_rounds=<span class="number">10</span>, eval_metric=<span class="string">"logloss"</span>, eval_set=eval_set, verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>[0]    validation_0-logloss:0.660999Will train until validation_0-logloss hasn&apos;t improved in 10 rounds.[1]    validation_0-logloss:0.630583[2]    validation_0-logloss:0.605378......[74]    validation_0-logloss:0.421685Stopping. Best iteration:[64]    validation_0-logloss:0.417813</code></pre><p><strong>打印出Early Stopping 的点：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Stopping. Best iteration:</span><br><span class="line">[64]validation_0-logloss:0.417813</span><br></pre></td></tr></table></figure></p><h2 id="三、输出重要特征"><a href="#三、输出重要特征" class="headerlink" title="三、输出重要特征"></a>三、输出重要特征</h2><p>gradient boosting 还有一个优点是可以给出训练好的模型的特征重要性，<br>这样就可以知道哪些变量需要被保留，哪些可以舍弃</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line">plot_importance(model)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/output_15_0-1.png" alt="output_15_0"></p><h2 id="四、模型调参"><a href="#四、模型调参" class="headerlink" title="四、模型调参"></a>四、模型调参</h2><p>如何调参呢，下面是三个超参数的一般实践最佳值，可以先将它们设定为这个范围，然后画出 learning curves，再调解参数找到最佳模型：</p><ul><li>learning_rate ＝ 0.1 或更小，越小就需要加入更多弱学习器；</li><li>max_depth ＝ 2～8；</li><li>subsample ＝ 训练集的 30%～80%；</li></ul><p>更多<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15266532194941.jpg" alt=""></p><p>接下来我们用 GridSearchCV 来进行调参会更方便一些：</p><p>可以调的超参数组合有：</p><ol><li>树的个数和大小 (n_estimators and max_depth).</li><li>学习率和树的个数 (learning_rate and n_estimators).</li><li>行列的 subsampling rates (subsample, colsample_bytree and colsample_bylevel).</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = XGBClassifier()</span><br><span class="line">learning_rate = [<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]</span><br><span class="line">max_depth = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">param_grid = dict(learning_rate=learning_rate, max_depth=max_depth)</span><br><span class="line">kfold = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle=<span class="keyword">True</span>, random_state=<span class="number">7</span>)</span><br><span class="line">grid_search = GridSearchCV(model, param_grid, scoring=<span class="string">"neg_log_loss"</span>, n_jobs=<span class="number">-1</span>, cv=kfold)</span><br><span class="line">grid_result = grid_search.fit(X, Y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best: %f using %s"</span> % (grid_result.best_score_, grid_result.best_params_))</span><br></pre></td></tr></table></figure><pre><code>Best: -0.474370 using {&apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 2}</code></pre><p><strong>显示最佳参数</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Best: -0.474370 using &#123;&apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 2&#125;</span><br></pre></td></tr></table></figure><p>我们还可以用下面的代码打印出每一个参数组合对应的得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">means = grid_result.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">stds = grid_result.cv_results_[<span class="string">'std_test_score'</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">'params'</span>]</span><br><span class="line"><span class="keyword">for</span> mean, stdev, param <span class="keyword">in</span> zip(means, stds, params):</span><br><span class="line">    print(<span class="string">"平均得分：%f (得分标准差：%f) %r"</span> % (mean, stdev, param))</span><br></pre></td></tr></table></figure><pre><code>平均得分：-0.690191 (得分标准差：0.000436) {&apos;learning_rate&apos;: 0.0001, &apos;max_depth&apos;: 2}平均得分：-0.689811 (得分标准差：0.000475) {&apos;learning_rate&apos;: 0.0001, &apos;max_depth&apos;: 3}......平均得分：-0.607988 (得分标准差：0.098076) {&apos;learning_rate&apos;: 0.2, &apos;max_depth&apos;: 5}平均得分：-0.647131 (得分标准差：0.098951) {&apos;learning_rate&apos;: 0.2, &apos;max_depth&apos;: 6}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;XGBoost案例&quot;&gt;&lt;a href=&quot;#XGBoost案例&quot; class=&quot;headerlink&quot; title=&quot;XGBoost案例&quot;&gt;&lt;/a&gt;XGBoost案例&lt;/h1&gt;&lt;p&gt;本文中，我们用到印第安人发病的糖尿病数据集。&lt;a href=&quot;https://raw
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="案例" scheme="http://www.ihoge.cn/tags/%E6%A1%88%E4%BE%8B/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost提升树学习笔记</title>
    <link href="http://www.ihoge.cn/2018/adaboost.html"/>
    <id>http://www.ihoge.cn/2018/adaboost.html</id>
    <published>2018-05-18T04:20:21.000Z</published>
    <updated>2018-08-05T10:17:10.896Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="提升方法-AdaBoost提升树学习笔记"><a href="#提升方法-AdaBoost提升树学习笔记" class="headerlink" title="提升方法-AdaBoost提升树学习笔记"></a>提升方法-AdaBoost提升树学习笔记</h1><p>作为非数学专业出身看到密密麻麻的数学公式刚开始真的是非常头疼。算法的物理逻辑的时候尚能理解，但是涉及到具体的数学公式实现就开始懵逼了：为什么要用这个公式，这个公式是怎么推到的，这个公式达到什么样的效果？<br>这里结合自己的理解和画图，用最直白的语言对每个公式作用进行解剖。</p><hr><h2 id="一、AdaBoost核心概念总结"><a href="#一、AdaBoost核心概念总结" class="headerlink" title="一、AdaBoost核心概念总结"></a>一、AdaBoost核心概念总结</h2><ol><li><p>提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些基本分类器线性组合，构成一个强分类器。代表性的提升方法是AdaBoost算法。（重点是：更新<strong>分类器的参数</strong>和<strong>训练集的权重见下2</strong>）</p><p> <strong>AdaBoost模型是弱分类器的线性组合：</strong><br> $$f(x)=∑^M_{m=1}\alpha _mG_m(x)$$</p><ul><li>$M$表示该提升树共有$M$个弱分类器组成</li><li>$G_m(x)$表示第$m$个弱分类器</li><li>$\alpha_m$为第$m$个弱分类器的参数（反应该分类器的重要性）</li></ul></li><li><p>AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，<strong>核心思想是：<code>提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值</code></strong>。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中<strong><code>给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</code></strong>。</p></li><li>AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，这说明了它作为提升方法的有效性。（<strong>每次迭代误差递减且误差$0≤\epsilon &lt;0.5$</strong>）</li><li>AdaBoost算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里，<strong><code>模型是加法模型，损失函数是指数损失，算法是前向分步算法时的二分类学习方法</code></strong>。每一步中极小化损失函数。</li><li>提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中最有效的方法之一。</li></ol><blockquote><p>AdaBoost是一种典型的提升树算法。</p></blockquote><p>通过上面的总结我们看到，AdaBoost是一个神奇的算法，以精妙的方式通过更新数据集的权重以及各个弱分类器的参数组合成一个强分类器。那么它具体是怎么做到的呢？</p><h2 id="二、AdaBoost算法学习过程"><a href="#二、AdaBoost算法学习过程" class="headerlink" title="二、AdaBoost算法学习过程"></a>二、AdaBoost算法学习过程</h2><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in X ⊆R^n,y_i\in Y={-1,1}$，弱学习算法$G_m(x)$;</p><p><strong>输出：</strong>最终强化算法分类器$G(x)$<br>（1）初始化训练数据总和为1的权值分布：（初始权重为归一化后的均值既$\frac 1N$）<br>$$D_1=(w_{11},…,w_{1i},…w_{1N}),w_{1i}=\frac 1N, i=1,2,…N$$<br>（2）对$m=1,2,…M$：（弱分类器的个数）</p><p>（a）使用具有权值分布的$D_m$的训练数据集学习，得到基本分类器：(数据集$X$到{-1,1}的映射)<br>$$G_m(x):X-&gt;{-1,1}$$<br>（b）计算$Gm(x)$在训练数据集上的分类误差率：（公式不够简洁明了，其实总结下来非常好理解：误差率$e_m$=误分类样本的权值之和）<br>$$e_m=∑^N_{i=1}P(G_m(x_i)≠y_i)=∑^N_{i=1}w_{mi}I(G_m(x_i)≠y_i)$$</p><ul><li>我们来考虑下误差$e_m$的取值空间：由于训练集权制之和为1，因此误差$0≤e_m≤1$。但是这样还不够。因为我们在选择分裂阈值的时候会选择一个最优或局部最优的取值来分裂，且当$e_m=0.5$是表明该分裂阈值对预测无贡献。因此最终得到的$e_m$的实际取值应小于$e_m≤0.5$。</li><li>所以最终：$0≤e_m≤0.5$，且每次迭代误差$e_m$递减。这点对下面的参数理解很重要。</li></ul><p>（c）计算$G_m(x)$的系数:(这里对数为自然对数)<br>$$\alpha_m=\frac 12log\frac{1-e_m}{e_m} $$</p><ul><li>那么问题来了，为什么要用这个公式来计算更新每个基分类器的参数？我们先画个图出来观察下这个函数。（其中y轴为$\alpha _m$，x轴为误差$e_m$）</li></ul><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266077461675.jpg" alt=""></p><ul><li>由（2-b）我们得到误差$e_m$的取值范围为$0≤e_m&lt;0.5$，结合该图可以可知$0&lt;\alpha_m&lt;1$。</li><li>另外可以发现，通过该函数的转换，弱分类器$G_m(x)$的误差的越小，参数$\alpha_m$越大。即实现了<strong><code>给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</code></strong></li></ul><p>（d）更新训练数据集的权值分布：（该权值决定数据集的重要性，并让误差的计算变得简单）<br>$$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…w_{m+1,N})$$<br>$$w_{m+1,i}=\frac {w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x-i)),i=1,2,…N$$</p><ul><li>这里$y_i={-1,1} $为真实值，$G_m(x_i)={-1,1}$为预测值。当预测正确时$y_iG_m(x_i)$为1，反之为-1。</li><li>令$\delta_{m_i}=\alpha_my_iG_m(x_i)$，$\theta_{mi}=\frac {w_{mi}}{Z_m}$(把它看作一个用于归一化权值的加权平均常数)。权重$w_{m+1,i}$的更新函数可以简化为$$w_{m+1,i}=\theta_{mi}exp(\delta <em>{mi}),i=1,2,…N$$画出$y=w</em>{m+1,i}=exp(\delta_{mi})$的图形来看一下：<img src="http://p6rvh6ej2.bkt.clouddn.com/15266159564936.jpg" alt="">由于$0&lt;\alpha_m&lt;1$，所以$-1&lt;\delta_{m,i }&lt;1$。且<strong>使得预测错误的数据集样本点更新后的权重变大，预测正确的权值变小，然后对所有权值进行归一化</strong>。这就是该函数实现的作用。(图中y=1是当$\alpha$无限接近于0时的情况：解释为，当$\alpha_m$权值越大，权重$w_{m+1,i}$更新改变的效果越明显。)</li><li>这里，$Z_m$是规范化因子，目的是使各数据集的权重进行归一化。理解为$Z_m$=更新后的各数据集权重之和。<br>$$Z_m=∑^N_{i=1}w_{mi}exp(-\alpha_my_iG_m(x_i))$$</li></ul><p>（3）构建基本分类器的新型组合$f(x)=∑^M_{m=1}\alpha_mG_m(x)$，即：<br>$$G(x)=sign(f(x))=sign(∑^M_{m=1}\alpha_mG_m(x))$$</p><ul><li>函数$sign()$的意义是将正数判别为1，负数判别为-1，最终达到分类的目的。如图：<img src="http://p6rvh6ej2.bkt.clouddn.com/15266164023757.jpg" alt=""></li></ul><blockquote><p><strong>参数$\alpha_m$公式及权重$w_{m+1,i} $</strong>其实是通过前向分步算法分别得到的$\alpha_m$和$G_m(x)$并使得$f_m(x)$再训练数据集$T$上的指数损失最小。具体的推导过程可参考《统计学习方法》–李航  第145～146页</p></blockquote><h2 id="三、AdaBoost算法实现步骤"><a href="#三、AdaBoost算法实现步骤" class="headerlink" title="三、AdaBoost算法实现步骤"></a>三、AdaBoost算法实现步骤</h2><p>上面解释了AdaBoost算法的具体内容。这里写出它的分布实现步骤再对上文算法加深下理解：</p><p>（1）假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$。</p><p>（2）AdaBoost反复学习基本分类器，在每一轮$m＝1,2,…,M$顺次地执行下列操作：</p><p>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$</p><p>（b）计算基本分类器$G_m(x)$再加权训练数据集上的分类误差率（即误分类样本的权值之和。这里要注意$w_{mi}$表示第$m$轮中第$i$个实例的权值，且权值之和为1，即$∑^N_{i=1}w_{mi}=1$）：<br>$$e_m=P(G_m(x_i)≠y_i)=∑_{G_m(x_i)≠y_i}w_{mi}$$</p><p>（c）计算基本分类器$G_m (x)$的系数$\alpha_m$。$alpha_m$表示$G_m(x)$在最终分类器中的重要性。由上面（2-c）可知，<strong>当$e_m≤1/2$时，$alpha_m≥0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的分类器在最终分类器中的作用越大。</strong></p><p>（d）更新训练数据的权值分布为下一轮作准备。式（2-d）的权重更新函数可以写成：</p><ul><li><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266248077288.jpg" alt=""></p></li><li><p>由此可知，被基本分类器$G_m (x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大$e^{(2\alpha_m)}=\frac{e_m}{1-e_m} $倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。</p></li></ul><p>（3）线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$ 表示了基本分类器$G_m (x)$的重要性，这里，所有$\alpha_m$ 之和并不为1。$f(x)$的符号决定实例x的类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p><blockquote><p>提升方法是即采用加法模型（即基数函数的线性组合）与前向分步算法，以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。</p></blockquote><blockquote><p>喜欢本文的伙伴请关注我的博客<a href="http://ihoge.cn" target="_blank" rel="noopener">http://ihoge.cn</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;提升方法-AdaBoost提升树学习笔记&quot;&gt;&lt;a href=&quot;#提升方法-AdaBoost提升树学习笔记&quot; class=&quot;headerlink&quot; title=&quot;提升方法-AdaBoost提升树学习笔记&quot;&gt;&lt;/a&gt;提升方法-Ada
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>牛顿法、拟牛顿法、高斯-牛顿法、共轭梯度法推导总结</title>
    <link href="http://www.ihoge.cn/2018/newton1.html"/>
    <id>http://www.ihoge.cn/2018/newton1.html</id>
    <published>2018-05-16T04:20:21.000Z</published>
    <updated>2018-08-05T10:17:14.456Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><p>线性最小二乘问题，我们可以通过理论推导可以得到其解析解，但是对于非线性最小二乘问题，则需要依赖迭代优化的方法，牛顿算法是解决非线性最优的常见算法之一。<br>最近整理了几篇关于牛顿法及其优化算法都不甚满意，网上大多相关技术博客很多是拼凑起来的，多数不全面（某些推导中间过程被省略），或者数学公式的符号表达方式不统一，造成看起来非常凌乱。因此本文旨在对基于牛顿法的非线性最小二乘优化问题的基本概念和算法推导做个系统的梳理。</p><ul><li>基本数学名词及概念</li><li>海赛矩阵和雅可比矩阵</li><li>牛顿算法推导</li><li>拟牛顿算法（DFP和BFGS）</li><li>高斯牛顿算法</li><li>共轭梯度法</li><li>补充优化算法</li></ul><hr><h2 id="一、基本概念定义"><a href="#一、基本概念定义" class="headerlink" title="一、基本概念定义"></a>一、基本概念定义</h2><h3 id="1-非线性方程定义及最优化方法简述"><a href="#1-非线性方程定义及最优化方法简述" class="headerlink" title="1.非线性方程定义及最优化方法简述"></a>1.非线性方程定义及最优化方法简述</h3><p>指因变量与自变量之间的关系不是线性的关系，比如平方关系、对数关系、指数关系、三角函数关系等等。对于此类方程，求解n元实函数f在整个n维向量空间Rn上的最优值点往往很难得到精确解，经常需要求近似解问题。</p><p>求解该最优化问题的方法大多是逐次一维搜索的迭代算法，基本思想是在一个近似点处选定一个有利于搜索方向，沿这个方向进行一维搜索，得到新的近似点。如此反复迭代，知道满足预定的精度要求为止。根据搜索方向的取法不同，这类迭代算法可分为两类：</p><ul><li><strong>解析法</strong> 需要用目标函数的倒函数</li><li><strong>梯度法</strong> 又称最速下降法，是早期的解析法，收敛速度较慢</li><li><strong>牛顿法</strong> 收敛速度快，但不稳定，计算也较困难。高斯牛顿法基于其改进，但目标作用不同</li><li><strong>共轭梯度法</strong> 介于最速下降法与牛顿法之间。收敛较快，效果好</li><li><strong>变尺度法</strong> 效率较高，常用DFP法(Davidon Fletcher Powell)</li><li><strong>直接法</strong> 不涉及导数,只用到函数值。有交替方向法(又称坐标轮换法)、模式搜索法、旋转方向法、鲍威尔共轭方向法和单纯形加速法等。</li></ul><h3 id="2-非线性最小二乘问题"><a href="#2-非线性最小二乘问题" class="headerlink" title="2.非线性最小二乘问题"></a>2.非线性最小二乘问题</h3><p>非线性最小二乘问题来自于非线性回归，即通过观察自变量和因变量数据，求非线性目标函数的系数参数，使得函数模型与观测量尽量相似。</p><p><strong>高斯牛顿法</strong>是解决非线性最小二乘问题的最基本方法，并且它<strong>只能处理二次函数。(使用时必须将目标函数转化为二次的)</strong></p><h3 id="3-基本数学表达"><a href="#3-基本数学表达" class="headerlink" title="3.基本数学表达"></a>3.基本数学表达</h3><ol><li><p><strong>梯度(gradient)</strong></p><p> 常用 $\nabla $ 表示，由多元函数的哥哥偏导数组成的问题。以二元函数为例，其梯度为：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263936905896.jpg" alt=""></p></li><li><p><strong>海赛矩阵(Hessian matrix)</strong></p><p> 由多元函数的二阶偏导数组成的方阵，描述函数的局部曲率，以二元函数为例：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263938618855.jpg" alt=""></p></li><li><p><strong>雅可比矩阵(Jacobian matrix)</strong></p><p> 是多元函数一阶偏导数以一定方式排列成的矩阵，体现了一个可微方程与给出点的最优线性逼近。以二元函数为例：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263939236262.jpg" alt=""><br> 如果扩展多维的话$F: R_n -&gt; R_m$，则雅可比矩阵是一个$m$行$n$列的矩阵，表示为：$J_F(x_1,x_2,…,x_n)$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263940046795.jpg" alt=""><br> 雅可比矩阵作用，如果$P$是$R_n$中的一点，$F$在$P$点可微分，那么在这一点的导数由$J_F(P)$给出，在此情况下，由$F(P)$描述的线性算子即接近点$P$的$F$的最优线性逼近，$x$逼近于$P$：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263940899460.jpg" alt=""></p></li></ol><p>黑森和雅可比矩阵参考：<a href="http://jacoxu.com/jacobian矩阵和hessian矩阵/" target="_blank" rel="noopener">http://jacoxu.com/jacobian矩阵和hessian矩阵/</a></p><ol start="4"><li><p><strong>残差(residual)</strong></p><p> 表示实际观测值与估计值(拟合值)之间的差。</p></li></ol><hr><h2 id="二、牛顿法"><a href="#二、牛顿法" class="headerlink" title="二、牛顿法"></a>二、牛顿法</h2><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法更快。比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。</p><p>从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。如下图是一个最小化一个目标方程的例子，红色曲线是利用牛顿法迭代求解，绿色曲线是利用梯度下降法求解。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15263961508272.jpg" alt=""></p><h3 id="1-求解方程"><a href="#1-求解方程" class="headerlink" title="1.求解方程"></a>1.求解方程</h3><p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p><p>原理是利用泰勒公式，在$x^{(0)}$处展开，且展开到一阶，即$f(x)=f(x^{(0)})+(x-x^{(0)})f’(x^{(0)}) $</p><p>求解方程$f(x)=0$，即$f(x^{(0)})+(x-x^{(0)})f’(x^{(0)})=0 $</p><p>求解$x=x^{(1)}=x^{(0)}-\frac {f(x^{(0)})}{f’(x^{(0)})}$</p><p>因为这是利用泰勒公式的一阶展开，$f(x)=f(x^{(0)})+(x-x^{(0)})f’(x^{(0)}) $此处并非完全相等，而是近似相等。这里求得的$x^{(1)}$并不能让$f(x)=0$，只能说$f(x^{(1)})$比$f(x^{(0)})$更接近$f(x)=0$，这样就可以通过不断迭代去逼近$f(x)$。</p><p>进而推出：$x^{(k+1)}=x^{(k)}-\frac {f(x^{(k)})}{f’(x^{(k)})}$</p><p>通过迭代，这恶搞狮子必然在$f(x^*)=0 $的时候收敛，如下图：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/1022856-20170916202719078-1588446775.gif" alt="1022856-20170916202719078-1588446775"></p><p>在最优化的问题中， 线性最优化至少可以使用单纯形法(或称不动点算法)求解， 但对于非线性优化问题， 牛顿法提供了一种求解的办法。 假设任务是优化一个目标函数$f$, 求函数$f$的极大极小问题, 可以转化为求解函数$f$的导数$f′=0$的问题, 这样求可以把优化问题看成方程求解问题。剩下的就和上面的牛顿法求解很相似了。</p><h3 id="2-一维无约束极值最优化"><a href="#2-一维无约束极值最优化" class="headerlink" title="2.一维无约束极值最优化"></a>2.一维无约束极值最优化</h3><p>$$min f(x),x\in R^1 $$</p><p>其中$x^*$为目标函数的极小点即$f’(x)$的根</p><p>首先把$f(x)$在探索点$x^{(k)}$处泰勒展开到2阶形式进行近似：<br>$$f (x)=f(x^{(k)})+f’(x^{(k)})(x-x^{(k)})+\frac 12f’’(x^{(k)})(x-x^{(k)})^2 $$<br>然后用$f(x)$的最小点作为新的探索点$x^{(k+1)}$</p><p>据此令：<br>$$f’(x)=f’(x^{(k)})+f’’(x^{(k)})(x-x^{(k)})=0 $$</p><p>求出迭代公式：<br>$$x^{(k+1)}=x^{(k)}-\frac {f’(x^{(k)})}{f’’(x^{(k)})}, k=0,1,…$$</p><p>因此，一维牛顿法最优化问题的<strong>计算步骤</strong>如下：<br>（1）给定初始点$x^{(0)}$，允许误差$\epsilon &gt;0$，置$k=0$<br>（2）如果$f’(x^{(k)})&lt; \epsilon $，则停止迭代，得到$x^{(k)}$；否则继续<br>（3）计算点$x^{(k+1)}$，$x^{(k+1)}=x^{(k)}-\frac {f(x^{(k)})}{f’’(x^{(k)})}$，置$k=k+1$，转（2）</p><p>需要注意的是，牛顿法在求极值的时候，如果初始点选取不好，则可能不收敛于极小点</p><h3 id="3-多维无约束极值最优化"><a href="#3-多维无约束极值最优化" class="headerlink" title="3.多维无约束极值最优化"></a>3.多维无约束极值最优化</h3><p>$$minf(x), x\in R^n$$<br>其中$x^*$为目标函数的极小点。</p><p>假设$f(x)$具有二阶连续偏导数，若第$k$次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行二阶泰勒展开：<br>$$f(x)=f(x^{(k)})+\nabla f(x^{(k)})^T(x-x^{(k)})+\frac 12(x-x^{(k)})^TH(x^{(k)})(x-x^{(k)})     (式一) $$<br>这里$\nabla f(x^{(k)})$是$f(x)$的梯度向量在点$x^{(k)}$的值；</p><p>$H(x^{(k)})$是$f(x)$的海赛矩阵（Hesse matrix）$H(x)=[\frac {ϑ^2f}{ϑ_{x_i}ϑ_{x_j}}]_{n*n} $在点$x^{(k)}$的值；</p><p>函数$f(x)$有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别是当$H(x^{(k)})$是正定矩阵时，函数$f(x)$的极值为极小值。</p><p>牛顿利用极小点的必要条件：$\nabla f(x)=0$</p><p>每次迭代中从点$x^{(k)} $开始，求目标函数的极小点，作为第$k+1$次迭代值$x^{(k+1)}$</p><p>假设：$x^{(k+1)}$满足$\nabla f(x^{(k+1)})=0 $ </p><p>由式二得：$$\nabla f(x)=\nabla f(x^{(k)})+H(x^{(k)})(x-x^{(k)})$$</p><p>由于：$$\nabla f(x^{(k+1)})=0 $$</p><p>即：$$\nabla f(x^{k)})+H(x^{(k)})(x^{(k+1)}-x^{(k)})=0 $$</p><p>得：$$x^{(k+1)}=x^{(k)}-H(x^{(k)})^{-1}\nabla f(x^{(k)})$$<br>简写为：$$x^{(k+1)}=x^{(k)}-H_k^{-1}g_k $$<br><strong>用该公式作为迭代公式的算法就是牛顿法。</strong>其中，$H_kp_k=-g_k$</p><p>下面给出<strong>计算步骤</strong>：</p><p>输入：目标函数$f(x)$，梯度为$g(x)=\nabla f(x)$，海赛矩阵$H(x)$，精度要求$\epsilon $;</p><p>输出：$f(x)$的极小点$x^*$</p><p>（1）取初始点$x^{(0)}$，置$k=0$<br>（2）计算梯度$g_k=\nabla f(x^{(k)})$<br>（3）若$||g_k||&lt;\epsilon $，停止计算，得近似解$x^*=x^{(k)}$；否则转（3）<br>（4）计算$H_k=H(x^{(k)})$，并根据迭代公式求出：$p_k=H(x^{(k)})^{-1}\nabla f(x^{(k)})$<br>（5）置$x^{(k+1)}=x^{(k)}+p_k$<br>（6）置$k=k+1$，转（2）</p><p>步骤（4）求$p_k$，需要求$H_K^{-1}$，计算比较复杂，所以有其他改进方法。</p><hr><h2 id="三、拟牛顿法"><a href="#三、拟牛顿法" class="headerlink" title="三、拟牛顿法"></a>三、拟牛顿法</h2><p>在牛顿法的迭代中，需要计算海赛矩阵的逆矩阵$H^{-1}$,这一计算比较复杂，考虑用一个正定矩阵$G_k=G(x^{(k)})$来近似代替$H_K^{-1}=H^{-1}(x^{(k)}) $。这就是拟牛顿法的基本想法。</p><h3 id="1-拟牛顿法推导"><a href="#1-拟牛顿法推导" class="headerlink" title="1.拟牛顿法推导"></a>1.拟牛顿法推导</h3><p>先看牛顿迭代中海赛矩阵$H_k$满足的条件。</p><p>首先，$H_k $满足以下关系:</p><p>在$$\nabla f(x)=\nabla f(x^{(k)})+H(x^{(k)})(x-x^{(k)})$$中取$$x=x^{(k+1)}$$</p><p>即得：$$g_{k+1}-g_k=H(x^{(k)})(x^{(k+1)}-x^{(k)}) $$</p><p>记：$g_k=\nabla f(x^{(k)})$；$y_k=g_{k+1}-g_k$；$\delta _k=x^{(k+1)}-x^{(k)} $；$H_k=H(x^{(k)})$；$p_k=-H_k^{-1}g_k$</p><p>则：$$y_k=H_k\delta _k$$<br>或 $$H_k^{-1}y_k=\delta _k$$</p><p>该式称为<strong>拟牛顿条件</strong></p><p>如果$H_k$是正定的（$H_k^{-1}$也是正定的），那么可以保证牛顿法搜索方向$p_k$是下降方向。这是因为搜索方向是$p_k=-H_k^{-1}g_k$</p><p>由式：$$x^{(k+1)}=x^{(k)}-H_k^{-1}g_k $$</p><p>有：$$x=x^{(k)}+ \lambda p_k =x^{(k)} - \lambda H_k^{(-1)}g_k$$</p><p>所以$f(x)$在$x^{(k)}$得泰勒展开式（见上文）可以近似写成：$$f(x)=f(x^{(k)})-\lambda g^T_kH^{-1}_kg_k $$</p><p>因$H^{-1}_k$正定，故有$g_k^TH^{-1}_kg_k&gt;0 $，当$\lambda $为一个充分小得正数时，总有$f(x)&lt;f(x^{(k)})$，也就是说$p_k$是下降方向。</p><p>拟牛顿法将$G_k$作为$H_k^{-1}$的近似，要求矩阵$G_k$满足同样的条件。首先，每次迭代矩阵$G_k$是正定的。同时，$G_k$满足拟牛顿条件：$G_{k+1}y_k=\delta _k$</p><p>按照拟牛顿条件选择$G_k$作为$H_k^{-1}$的近似或选择$B_k$作为$H_k$的近似的算法成为拟牛顿法。</p><p><strong>按照拟牛顿条件选择$G_k$作为$H_k^{-1}$的近似；或者选择$B_k$作为$H_k$的近似的算法称为拟牛顿法。</strong></p><p>按照拟牛顿条件，在每次迭代中可以选择更新矩阵$G_{k+1}$：<br>$$G_{k+1}=G_k+\nabla G_k$$</p><p>这种选择有一定的灵活性，因此有多种具体实现方法。下面介绍Broyden类拟牛顿法。</p><h3 id="2-DFP（Davidon-Fletcher-Powell）算法（DFP-algorithm）"><a href="#2-DFP（Davidon-Fletcher-Powell）算法（DFP-algorithm）" class="headerlink" title="2.DFP（Davidon-Fletcher-Powell）算法（DFP algorithm）"></a>2.DFP（Davidon-Fletcher-Powell）算法（DFP algorithm）</h3><p>DFP算法选择$G_{k+1}$的方法是，假设每一步迭代中矩阵$G_{k+1}$是由$G_k$加上两个附加项构成的，即：<br>$$G_{k+1}=G_k+P_k+Q_k $$<br>其中$P_k，Q_k$是待定矩阵。这时：<br>$$G_{k+1}y_k=G_ky_k+P_ky_k+Q_ky_k $$</p><p>为使$G_{k+1}$满足拟牛顿条件$y_k=H_k\delta _k$，可使$P_k$和$Q_k$满足：<br>$$P_ky_k=\delta _k$$<br>$$Q_ky_k=-G_ky_k$$</p><p>事实上，不难找出这样的$P_k$和$Q_k$，例如取：<br>$$P_k=\frac {\delta _k\delta _k^T}{\delta _k^Ty_k}$$<br>$$Q_k=- \frac {G_ky_ky_k^TG_k}{y^T_kG_ky_k}$$</p><p>这样就可以得到矩阵$G_{k+1}$的迭代公式：<br>$$G_{k+1}=G_k+\frac {\delta _k\delta _k^T}{\delta _k^Ty_k}-\frac {G_ky_ky_k^TG_k}{y^T_kG_ky_k}$$<br>称为<strong>DFP算法</strong>。</p><pre><code>可以证明，如果初始矩阵$G_0$是正定的，则迭代过程中的每个矩阵$G_k$都是正定的。</code></pre><p><strong>DFP算法步骤迭代如下</strong></p><p>输入：目标函数$f(x)$，梯度$g(x)=\nabla f(x)$，精度要求为$\epsilon $<br>输出：$f(x)$的极小点$x^*$。</p><p>（1）选定初始点$x^{(0)}$，取$G_0$为正定对称矩阵，置$k=0$<br>（2）计算$g_k=g(x^{(k)})$。若$||g_k||&lt;\epsilon $，则停止计算，得近似解$x^<em>=x^{(k)}$，否则转（3）<br>（3）置$p_k=-G_kg_k$<code>（这里与BFGS不同）</code><br>（4）一维搜索：求$\lambda _k$使得：$$f(x^{(k)}+\lambda _kp_k)=Min(f(x^{(k)}+\lambda p_k)),{(\lambda ≥0)}$$<br>（5）置$x^{(k+1)}=x^{(k)}+\lambda _kp_k$<br>（6）计算$g_{k+1}=g(x^{(k+1)})$，若$||g_{k+1}||&lt;\epsilon $，则停止计算，得近似解$x^</em>=x^{(k+1)}$；否则，按$G_{k+1}$的迭代公式（上文）计算出$G_{k+1}$<code>（这里与BFGS不同）</code><br>（7）置$k=k+1$，转（3）</p><h3 id="3-BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）"><a href="#3-BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）" class="headerlink" title="3.BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）"></a>3.BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）</h3><p><strong>BFGS算法是最流行的拟牛顿算法。</strong></p><p>考虑用$G_k$逼近海赛矩阵的逆矩阵$H^{-1} $，也可以考虑用$B_k$逼近海赛矩阵。</p><p>这时，相应的拟牛顿条件是：$$B_{k+1}\delta _k=y_k$$</p><p>可以用同样的方法得到另一迭代公式.首先，令：<br>$$B_{k+1}=B_k+P_k+Q_k$$<br>$$B_{k+1}\delta _k=B_k\delta _k+P_k\delta _k+Q_k\delta_k $$</p><p>考虑使$P_k和Q_k$满足：<br>$$P_k\delta_k=y_k$$<br>$$Q_k\delta_k=-B_k\delta_k$$<br>找出适合条件的$P_k和Q_k$，得到<strong>BFGS算法矩阵$B_{k+1}$的迭代公式：</strong><br>$$B_{k+1}=B_k+\frac {y_ky_k^T}{y^T_k\delta_k}-\frac {B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k} $$</p><p>可以证明，如果初始矩阵$B_0$是正定的，则迭代过程中的每个矩阵$B_k$都是正定的。</p><p><strong>BFGS拟牛顿算法的迭代步骤</strong>：</p><p>输入：目标函数$f(x)$，$g(x)=\nabla f(x)$，精度要求$\epsilon$；<br>输出：$f(x)$的极小点$x^*$</p><p>（1）选定初始点$x^{(0)}$，取$B_0$为正定矩阵的对称矩阵，置$k=0$<br>（2）计算$g_k=g(x^{(k)})$。若$||g_k||&lt;\epsilon $，则停止计算，得近似解$x^<em>=x^{(k)}$；否则转（3）<br>（3）由$B_kp_k=-g_k$求出$p_k$ <code>(这里与DFP不同)</code><br>（4）一维搜索：求$\lambda _k$使得：$$f(x^{(k)}+\lambda _kp_k)=Min(f(x^{(k)}+\lambda p_k)),{(\lambda ≥0)}$$<br>（5）置$x^{(k+1)}=x^{(k)}+\lambda _kp_k$<br>（6）计算$g_{k+1}=g(x^{(k+1)})$，若$||g_{k+1}||&lt;\epsilon $，则停止计算，得近似解$x^</em>=x^{(k+1)}$；否则，按$B_{k+1}$的迭代公式（上文）计算出$B_{k+1}$<code>（这里与DFP不同）</code><br>（7）置$k=k+1$，转（3）</p><h3 id="4-Broyden类算法（Broyden’s-algorithm）"><a href="#4-Broyden类算法（Broyden’s-algorithm）" class="headerlink" title="4.Broyden类算法（Broyden’s algorithm）"></a>4.Broyden类算法（Broyden’s algorithm）</h3><p><strong>该算法是由DFP算法和BFGS算法相结合派生出的一类拟牛顿法。</strong></p><p>我们可以从BFDS算法矩阵$B_k$的迭代式（$B_{k+1}=B_k+\frac {y_ky_k^T}{y^T_k\delta_k}-\frac {B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k} $）得到BFGS算法关于$G_k$的迭代公式。</p><p>事实上，若记$G_k=B^{-1}<em>k,G</em>{k+1}=B^{-1}_{k+1} $</p><p>那么对以上BFDS算法矩阵$B_k$的迭代式两次应用Sherman-Morrison公式得：<br>$$G_{k+1}=(I-\frac {\delta _ky_k^T}{\delta_k^Ty_k })G_k(I-\frac {\delta _ky_k^T}{\delta_k^Ty_k })^T+\frac {\delta_k\delta_k^T}{\delta _k^Ty_k} $$<br>称为<strong>BFGS算法关于G_k的迭代公式</strong>。</p><p>将DFP算法的迭代公式：$$G_{k+1}=G_k+\frac {\delta _k\delta _k^T}{\delta _k^Ty_k}-\frac {G_ky_ky_k^TG_k}{y^T_kG_ky_k}$$<br><strong>记作</strong>$G^{DFP}$</p><p>将BFGS算法迭代公式：<br>$$B_{k+1}=B_k+\frac {y_ky_k^T}{y^T_k\delta_k}-\frac {B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k} $$<br><strong>记作</strong>$G^{BFGS}$</p><p>他们都满足方程拟牛顿条件式，所以他们的线性组合<br>$$G_{k+1}=\alpha G^{DFP}+(1-\alpha)G^{BFGS}$$也满足拟牛顿条件式，而且是正定的。</p><p>其中$0≤\alpha≤1$，这样就得到了一类拟牛顿法，称为Broyden类算法。其步骤与上文类似，唯（3）和（6）步有所不同。</p><p>⚠️ Sherman-Morrison公式：假设$A$是$n$阶可逆矩阵，$u,v$是$n$维向量，且$A+uv^T$也是可逆矩阵，则有：$$(A+uv^T)^{-1}=A^{-1}-\frac {A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} $$</p><h2 id="四、高斯牛顿法"><a href="#四、高斯牛顿法" class="headerlink" title="四、高斯牛顿法"></a>四、高斯牛顿法</h2><p>以后再补充。</p><h2 id="五、共轭梯度法（Conjugate-Gradient）"><a href="#五、共轭梯度法（Conjugate-Gradient）" class="headerlink" title="五、共轭梯度法（Conjugate Gradient）"></a>五、共轭梯度法（Conjugate Gradient）</h2><p>共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p><p>具体的实现步骤请参加<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB" target="_blank" rel="noopener">wiki百科共轭梯度法</a>。</p><p>下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：(绿色为梯度下降法，红色代表共轭梯度法)<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262668307656.jpg" alt=""></p><h2 id="六、其他优化方法"><a href="#六、其他优化方法" class="headerlink" title="六、其他优化方法"></a>六、其他优化方法</h2><h3 id="1、启发式优化方法"><a href="#1、启发式优化方法" class="headerlink" title="1、启发式优化方法"></a>1、启发式优化方法</h3><p>启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。</p><p>还有一种特殊的优化算法被称之多目标优化算法，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有NSGAII算法、MOEA/D算法以及人工免疫算法等。</p><h3 id="2、解决约束优化问题——拉格朗日乘数法"><a href="#2、解决约束优化问题——拉格朗日乘数法" class="headerlink" title="2、解决约束优化问题——拉格朗日乘数法"></a>2、解决约束优化问题——拉格朗日乘数法</h3><p>有关拉格朗日乘数法的介绍请见另一篇博客：<a href="http://www.cnblogs.com/maybe2030/p/4946256.html" target="_blank" rel="noopener">《拉格朗日乘数法》</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;p&gt;线性最小二乘问题，我们可以通过理论推导可以得到其解析解，但是对于非线性最小二乘问题，则需要依赖迭
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降、随机梯度下降、批量梯度下降</title>
    <link href="http://www.ihoge.cn/2018/GradientDescent.html"/>
    <id>http://www.ihoge.cn/2018/GradientDescent.html</id>
    <published>2018-05-14T03:20:21.000Z</published>
    <updated>2018-08-05T10:17:17.903Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>李航老师在《统计学习方法》中将机器学习的三要素总结为：模型、策略和算法。其大致含义如下：</p><p><strong>模型</strong>：其实就是机器学习训练的过程中所要学习的条件概率分布或者决策函数。</p><p><strong>策略</strong>：就是使用一种什么样的评价，度量模型训练过程中的学习好坏的方法，同时根据这个方法去实施的调整模型的参数，以期望训练的模型将来对未知的数据具有最好的预测准确度。</p><p><strong>算法</strong>：算法是指模型的具体计算方法。它基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法去求解这个最优模型。</p><p>很多时候机器学习工程师又戏称调参工程师, 由此可见参数调优时作为机器学习工程师必须掌握的一项核心技能。</p><p>这篇文章的目的旨在对常用的参数调优算法进行一次梳理便于随时翻阅。</p><h1 id="1-梯度下降法（Gradient-Descent）"><a href="#1-梯度下降法（Gradient-Descent）" class="headerlink" title="1. 梯度下降法（Gradient Descent）"></a>1. 梯度下降法（Gradient Descent）</h1><h2 id="1-1-一般解释"><a href="#1-1-一般解释" class="headerlink" title="1.1 一般解释"></a>1.1 一般解释</h2><p>$f(x)$在$x_0$的梯度：就是$f(x)$变化最快的方向。梯度下降法是一个最优化算法，通常也称为<em>最速下降法</em>。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262278257042.jpg" alt=""><br>假设$f(x)$是一座山，站在半山腰，往x方向走1米，高度上升0.4米，也就是说x方向上的偏导是 0.4；往y方向走1米，高度上升0.3米，也就是说y方向上的偏导是 0.3；这样梯度方向就是 (0.4 , 0.3)，也就是往这个方向走1米，所上升的高度最高。梯度不仅仅是$f(x)$在某一点变化最快的方向，而且是上升最快的方向；如果想下山，下降最快的方向就是逆着梯度的方向，这就是梯度下降法，又叫最速下降法。</p><h2 id="1-2-梯度下降算法用途"><a href="#1-2-梯度下降算法用途" class="headerlink" title="1.2 梯度下降算法用途"></a>1.2 梯度下降算法用途</h2><p>最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现在已经不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。 </p><p>在梯度下降算法中，都是围绕以下这个式子展开：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262224519674.jpg" alt=""><br>其中在上面的式子中$h_θ(x)$代表，输入为x的时候的其当时θ参数下的输出值，与y相减则是一个相对误差，之后再平方乘以1/2，并且其中:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262225198545.jpg" alt=""><br>这里我列举了一个简单的例子，当然实际的x可以有n多个维度。我们知道曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。这个θ的更新过程可以描述为:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262225480705.jpg" alt=""><br>这里就是根据每一个 x 的分量以及当时的偏差值进行 θ 的更新，其中 α 为步长，这个参数如果设置的太大，那么很容易就在最优值附加徘徊；相反，如果设置的太小，则会导致收敛速度过慢。</p><p>关于步长和学习速率的关系,这里提一下其实这两个是一个概念，叫法不一样，最优化问题中叫步长，但一般在神经网络中也叫学习速率。</p><h2 id="1-3-梯度下降、随机梯度下降、批量梯度下降"><a href="#1-3-梯度下降、随机梯度下降、批量梯度下降" class="headerlink" title="1.3 梯度下降、随机梯度下降、批量梯度下降"></a>1.3 梯度下降、随机梯度下降、批量梯度下降</h2><ul><li><p><strong>梯度下降</strong>：梯度下降就是上面的推导，要留意，在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整θ.其计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦~</p></li><li><p><strong>随机梯度下降</strong>：可以看到多了随机两个字，随机也就是说用样本中的一个例子来近似所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中。随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。</p></li><li><p><strong>批量梯度下降</strong>：其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是随机指定一个例子替代样本不太准，而且批量的话还是非常可以反映样本的一个分布情况的。批量梯度下降最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p></li><li><p><strong>概括</strong>：</p><p>  随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><p>  随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p></li><li><p><strong>对批量梯度下降法和随机梯度下降法的总结：</strong></p><p>  批量梯度下降—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p><p>  随机梯度下降—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p></li></ul><h3 id="梯度下降代码"><a href="#梯度下降代码" class="headerlink" title="梯度下降代码:"></a>梯度下降代码:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="comment">#这里更新权重的时候所有的样本点都用上了</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>output:iters_count 219iters_count 220iters_count 221iters_count 222iters_count 223iters_count 224iters_count 225theta: [3.0027765778748003, 3.997918297015663]final loss: 9.68238055213e-05iters: 225[Finished in 0.2s]</code></pre><h3 id="随机梯度下降代码"><a href="#随机梯度下降代码" class="headerlink" title="随机梯度下降代码:"></a>随机梯度下降代码:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每次选取一个值,随机一个点更新 θ</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="comment">#每一次选取随机的一个点进行权重的更新</span></span><br><span class="line">    i = random.randint(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>iters_count 1226iters_count 1227iters_count 1228iters_count 1229iters_count 1230iters_count 1231iters_count 1232theta: [3.002441488688225, 3.9975844154600226]final loss: 9.989420302e-05iters: 1232[Finished in 0.3s]</code></pre><h3 id="批量随机梯度下降代码："><a href="#批量随机梯度下降代码：" class="headerlink" title="批量随机梯度下降代码："></a>批量随机梯度下降代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里用2个样本点</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    i = random.randint(<span class="number">0</span>,<span class="number">3</span>) <span class="comment">#注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点</span></span><br><span class="line">    j = (i+<span class="number">1</span>)%<span class="number">4</span></span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[j][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[j][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[j]) * input_x[j][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[j]) * input_x[j][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>.....iters_count 543iters_count 544iters_count 545iters_count 546iters_count 547iters_count 548iters_count 549theta: [3.0023012574840764, 3.997553282857357]final loss: 9.81717138358e-05iters: 549</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;李航老师在《统计学习方法》中将机器学习的三要素总结为：模型、策略和算法。其大致含义如下：&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>KNN——不需要学习的算法</title>
    <link href="http://www.ihoge.cn/2018/KNN1.html"/>
    <id>http://www.ihoge.cn/2018/KNN1.html</id>
    <published>2018-05-11T15:38:21.000Z</published>
    <updated>2018-08-05T10:17:29.003Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="KNN——不需要学习的算法"><a href="#KNN——不需要学习的算法" class="headerlink" title="KNN——不需要学习的算法"></a>KNN——不需要学习的算法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>KNN（K-Nearest Neighbor）算法是机器学习中算法中最基础和简单的算法之一。它既能用于分类，也能用于回归。本文将在不同的小节详细地介绍 KNN 算法在分类和回归两种任务下的运用原理。</p><p>KNN 算法的思想非常简单：对于任意的 n 维输入向量，其对应于特征空间一个点，输出为该特征向量所对应的类别标签或者预测值。KNN 算法在机器学习算法中有一个十分特别的地方，那就是它没有一个显示的学习过程。它实际上的工作原理是利用训练数据对特征向量空间进行划分，并将其划分的结果作为其最终的算法模型。</p><h2 id="1-KNN分类算法"><a href="#1-KNN分类算法" class="headerlink" title="1. KNN分类算法"></a>1. KNN分类算法</h2><p>KNN 分类算法的分类预测过程十分的简单和容易理解：对于一个需要预测的输入向量 x，我们只需要在训练数据集中寻找 k 个与向量 x 最近的向量的集合，然后把 x 的类标预测为这 k 个样本中类标数最多的那一类。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15260483350913.jpg" alt=""><br>如图所示，$w_1,w_2,w_3$分别代表的是训练机中的三个类别。图中$x_u$最相近的5（k=5）个点如图中箭头所指，很明显与$x_u$最相近的5个点中最多的泪标为$w_1$，因此KNN算法将$x_u$的类别预测为$w_1$。</p><p>基于上述思想给出KNN算法公式原理：</p><h3 id="1-1-KNN算法原理"><a href="#1-1-KNN算法原理" class="headerlink" title="1.1 KNN算法原理"></a>1.1 KNN算法原理</h3><p><strong>输入</strong>：训练数据集</p><p>$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_N,y_N)}$</p><p>其中：<br>$x_i\in X⊆R^n$ 为 n 维的实例特征向量。<br>$y_i\in Y={c_1,c_2,…,c_K}$为实例的类别，$i=1,2,3,…,N$，预测实例$x$。</p><p><strong>输出</strong>：预测实例$x$所属类别$y$。</p><p><strong>算法执行步骤</strong>：</p><ol><li>根据给定的距离度量方法（一般情况下使用欧氏距离）在训练集$T$中寻找出与$x$最相近的$k$个样本点，并将这$k$个样本点所表示的集合记为$N_k(x)$；</li><li>根据多数投票原则确定实例$x$所属的类别$y$。</li></ol><p>从上述的 KNN 原理中，我们发现有两个因素必须确定才能是 KNN 分类算法真正能勾运行：（1）算法超参数K；（2）模型向量空间的距离度量。</p><h3 id="1-2-K值的确定"><a href="#1-2-K值的确定" class="headerlink" title="1.2 K值的确定"></a>1.2 K值的确定</h3><p>KNN 算法中只有唯一的一个超参数 K，很明显 K 值的选择对最终算法的预测结果会产生至关重要的影响。接下来就简单的讨论一下 K 值的大小对算法结果的影响以及一般情况下如何选择 K 值。</p><p>如果 K 值选择的比较小，这时候我们就相当于使用较小的领域中的训练样本对实例进行预测。这时候，算法的近似误差（Approximate Error）会减小，因为只有与输入实例相近的训练样本才能才会对预测结果起作用。但是它也会有明显的缺点：算法的估计误差会偏大，预测的结果会对近邻点十分敏感，也就是说如果近邻点是噪声点的话，那么预测就会出错。也就是说，K 值太小会使得 KNN 算法容易过拟合。</p><p>同理，如果 K 值选的比较大的话，这时候距离较远的训练样本都能够对实例的预测结果产生影响。这时候，而模型相对比较鲁棒，不会因个别噪声点对最终的预测产生影响。但是缺点也是十分明显的：算法的近似误差会偏大，距离较远的点（与预测实例不相似）也会同样对预测结果产生作用，使得预测产生较大偏差。此时相当于模型发生欠拟合。</p><p>因此，在实际的工程实践过程中，我们一般<strong>采用交叉验证的方式选取 K 值</strong>。从上面的分析也可以知道，一般 K 值取得比较小。<em>我们会选取 K 值在较小的范围，同时在测试集上准确率最高的那一个确定为最终的算法超参数 K。</em></p><h3 id="1-3-距离的度量"><a href="#1-3-距离的度量" class="headerlink" title="1.3 距离的度量"></a>1.3 距离的度量</h3><p>样本空间中两个点之间的距离度量表示的是两个样本点之间的相似程度：距离越短，表示相似程度越高；相反，距离越大，表示两个样本的相似程度低。</p><p>常用的距离度量方式有：</p><ol><li>闵可夫斯基距离；</li><li>欧氏距离；</li><li>曼哈顿距离；</li><li>切比雪夫距离；</li><li>余弦距离。</li></ol><h4 id="1-3-1-闵可夫斯基距离"><a href="#1-3-1-闵可夫斯基距离" class="headerlink" title="1.3.1 闵可夫斯基距离"></a>1.3.1 闵可夫斯基距离</h4><p>闵可夫斯基距离不是一种距离，而是一类距离的定义。对于 n 维空间中的两个点 $x(x_1,x_2,x_3,…,x_n)$和$y(y_1,y_2,y_3,…,y_n)$，那么$x$和$y$亮点之间的闵可夫斯基距离为：$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{p}}}$$<br>其中p是一个可变参数：</p><ul><li>当p=1时，被称为曼哈顿距离；</li><li>当p=2时，被称为欧式距离；</li><li>当p=$\infty $时，被称为切比雪夫距离。</li></ul><h4 id="1-3-2-欧式距离"><a href="#1-3-2-欧式距离" class="headerlink" title="1.3.2 欧式距离"></a>1.3.2 欧式距离</h4><p>由以上说明可知，欧式距离的计算公式为：<br>$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{2}}}$$</p><p>欧式距离（L2 范数）是最易于理解的一种距离计算方法，源自欧式空间中两点间的距离公式，也是最常用的距离度量方式。</p><h4 id="1-3-3-曼哈顿距离"><a href="#1-3-3-曼哈顿距离" class="headerlink" title="1.3.3 曼哈顿距离"></a>1.3.3 曼哈顿距离</h4><p>由闵可夫斯基距离定义可知，曼哈顿距离的计算公式为：<br>$$d_{xy}=\sum_{i=1}^{n}{\left| x_{i}-y_{i} \right|}$$</p><h2 id="KNN-算法核心：KDTree"><a href="#KNN-算法核心：KDTree" class="headerlink" title="KNN 算法核心：KDTree"></a>KNN 算法核心：KDTree</h2><p>通过以上的分析，我们知道 KNN 分类算法的思想非常简单，它采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？</p><p>也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。</p><p>因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。</p><h3 id="KD-树简介"><a href="#KD-树简介" class="headerlink" title="KD 树简介"></a>KD 树简介</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15260517984690.jpg" alt=""><br>KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><h3 id="KD-树的构造"><a href="#KD-树的构造" class="headerlink" title="KD 树的构造"></a>KD 树的构造</h3><p>KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。</p><ul><li><p><strong>构造跟节点</strong></p><p>  首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。</p></li><li><p><strong>递归构造子节点</strong></p><p>  递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。</p><p>  通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。</p></li></ul><h2 id="KNN-回归算法"><a href="#KNN-回归算法" class="headerlink" title="KNN 回归算法"></a>KNN 回归算法</h2><p>上面我们讲的 KNN 算法主要是用于分类的情况，实际上，KNN 算法也能够用于回归预测。下面介绍一下 KNN 算法如何用于回归的情况。</p><p>众所周知，KNN 算法用于分类的方法如下：首先，对于一个新来的预测实例，我们在训练集上寻找它的最相近的 K 个近邻；然后，采用投票法将它分到这 K 个邻居中的最多的那个类。</p><p>但是，怎么将 KNN 算法用于回归呢？其实大致的步骤是一样的，也是对新来的预测实例寻找 K 近邻，然后对这 K 个样本的目标值去均值即可作为新样本的预测值：</p><p>$$f\left( x \right)=\frac{1}{K}\sum_{i=1}^{K}{x_{i}}$$</p><h2 id="KNN-预测代码演示"><a href="#KNN-预测代码演示" class="headerlink" title="KNN 预测代码演示"></a>KNN 预测代码演示</h2><p>接下来，我们使用 scikit-learn 库中的 KNN 对 iris 数据集分类效果进行预测实战。众所周知，iris 数据集有四个维度的特征，但是为了方便展示效果，我们只使用其中的两个维度。完整的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets</span><br><span class="line"></span><br><span class="line">n_neighbors = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import some data to play with</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we only take the first two features. We could avoid this ugly</span></span><br><span class="line"><span class="comment"># slicing by using a two-dim dataset</span></span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create color maps</span></span><br><span class="line">cmap_light = ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">'uniform'</span>, <span class="string">'distance'</span>]:</span><br><span class="line">    <span class="comment"># we create an instance of Neighbours Classifier and fit the data.</span></span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    print(<span class="string">"train_score"</span>, clf.score(X,y))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">"3-Class classification (k = %i, weights = '%s')"</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15260528682435.jpg" alt=""></p><ul><li><p><strong>代码解释</strong></p><p>  代码中邻居数使用的是 n_neighbors = 15，只使用 iris 的前两维特征作为分类特征。权重度量采用了两种方式：<strong>均值（uniform）和距离（distance）</strong>。均值代表的是所有的 K 个近邻在分类时重要性选取的是一样的，该参数是默认参数；距离也就是说，分类时 K 个邻居中每个邻居所占的权重与它与预测实例的距离成反比。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;KNN——不需要学习的算法&quot;&gt;&lt;a href=&quot;#KNN——不需要学习的算法&quot; class=&quot;headerlink&quot; title=&quot;KNN——不需要学习的算法&quot;&gt;&lt;/a&gt;KNN——不需要学习的算法&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.ihoge.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Spark SQL 基础框架和核心组件</title>
    <link href="http://www.ihoge.cn/2018/SparkSQL1.html"/>
    <id>http://www.ihoge.cn/2018/SparkSQL1.html</id>
    <published>2018-05-09T17:00:21.000Z</published>
    <updated>2018-05-28T07:51:54.349Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark-SQL-基础框架和核心组件"><a href="#Spark-SQL-基础框架和核心组件" class="headerlink" title="Spark SQL 基础框架和核心组件"></a>Spark SQL 基础框架和核心组件</h1><p>今天被问到<code>Spark Sql</code>和<code>Dataframe</code>的关系，当时没有描述好。事后脑海中关于它们的一些细节才一一涌现出来。还是理解的不够深刻，所以这里重新做下相关概念知识的整理也算是一种复习。</p><h2 id="1-Spark-SQL发展史"><a href="#1-Spark-SQL发展史" class="headerlink" title="1. Spark SQL发展史"></a>1. Spark SQL发展史</h2><p>在<code>Spark</code>的早期版本，为什么解决<code>Hive</code>查询在性能上的问题，在<code>Spark</code>生态圈引入了一个名为<code>Shark</code>的项目。（使用<code>Spark</code>的计算引擎而不是<code>MapReduce</code>来执行<code>Hive</code>查询）。<code>Shark</code>是在<code>Hive</code>代码库上构建的，使用<code>Hive</code>查询编译器来解析<code>Hive</code>查询并生成一个抽象语法树，它会转化为一个具有某些基本优化的逻辑计划。<code>Shark</code>应用了额外的优化手段并创建了一个<code>RDD</code>操作额度物理计划，然后在<code>Spark</code>上执行。基于这种对<code>Hive</code>的过分依赖，也让<code>Shark</code>本身具有一些问题：</p><ol><li>只适用于查询<code>Hive</code>表。无法在RDD上进行关系查询。</li><li>在<code>Spark</code>程序中将<code>Hive-QL</code>作为字符串运行容易出错。</li><li>它的<code>Hive</code>优化器是为<code>MapReduce</code>附案例创建的，很难讲<code>Spark</code>扩展到新的数据源和新的处理模型。<br>基于以上原因，<code>Spark</code>团队终止了<code>Shark</code>，原班人马合并全力开发<code>Spark SQL</code>。下面是<code>Spark SQL</code>的发展史。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258813010356.jpg" alt=""><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258818229539.jpg" alt=""><h2 id="2-Spark-SQL的构架"><a href="#2-Spark-SQL的构架" class="headerlink" title="2. Spark SQL的构架"></a>2. Spark SQL的构架</h2><code>Spark SQL</code>是在<code>Spark</code>核心执行引擎上的一个库，它借助JDBC/ODBC公开了SQL接口，用于数据仓库应用程序，或通过命令行进行交互式查询。因此<strong>任何商务智能（BI）工具都可以连接到Spark SQL以内存速度执行分析</strong>。<br>它还提供了<code>Java、Scala、Python、R</code>支持的<code>Dataset API</code>和<code>Dataframe API</code>。（注意：Python和R不支持Dataset）<br><code>Spark SQL</code>用户可以使用<code>Data Sourse API</code>从各种数据源读取和写入数据，从而创建<code>Dataframe</code> 或者 <code>Dataset</code>。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258850302556.jpg" alt=""><br><code>Spark SQL</code>还扩展了用于所有其他<code>Spark</code>库（如 <code>SparkR、SparkStreaming、Structred Streaming、ML、MLlib、GraphX）的DataSet API、Dataframe API和Data Sources API</code>。如下图所示：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258851101623.jpg" alt=""></li></ol><p><code>Spark SQL</code>引入了<code>Catalyst</code>的可扩展优化器，一直吃大多数常见的数据源和算法。<code>Catalyst</code>支持添加新的数据源、优化规则和某些领域（如机器学习）使用的数据类型。</p><h2 id="3-Spark-SQL的四个组件"><a href="#3-Spark-SQL的四个组件" class="headerlink" title="3. Spark SQL的四个组件"></a>3. Spark SQL的四个组件</h2><p>Spark SQL 中的四个组件分别是：<strong>SQL、 Data Sources API、 DataFrame API、 DataSet API</strong><br>下面一一展开：</p><ul><li><p><strong>Spark SQL</strong>可以使用SQL语言向Hive表写入读取数据。</p><p>  SQL通过ODBC/JDBC或命令行选项在Java、Scala、Python、R语言中使用。在编程语言中使用SQL时，结果会转换成DataFrame。<strong>SQL</strong>的优点是可以轻松的处理Hive表，并且可以利用Thrift服务器将BI工具连接到分布式SQL引擎，并利用JDBC/ODBC接口提交SQL或HQL查询。</p></li><li><p><strong>Data Sources API</strong>为使用Spark SQL 读取和写入数据提供了统一的接口。</p><p>  Data Sources API的优点有：</p><pre><code>1. 容易加载/保存Dataframe2. 通过到数据源的*谓词下推*方式进行高效的数据访问，采用这种方式能从数据源读取更少的数据。3. 为任何新的数据源构建库4. 无需包含在Spark代码中5. 很容易与Spark组件包共享新的数据源。--谓词下推的思路是让筛选数据的条件表达式尽可能靠近数据源执行，从而减少数据额移动和传输。谓词就是筛选数据的条件，下推就是让这些筛选动作尽量在靠近数据源的位置执行。</code></pre></li><li><p><strong>DataFrame API</strong>旨在让大数据分析工作更为简单易行。</p><p>  这个API收到了R和Python（Pandas库）中的DataFrame的启发，但他被实际用于大规模数据集的分布式处理一支持大数据分析。<em>DataFrame可以看作是对现有RDD API的扩展，也是RDD纸上的一个抽象</em><br>  DataFrame的优点有：</p><pre><code>1. 易于使用域特定语言开发应用程序2. 在传统RDD基础上的高性能，并且在Scala、Java、Python、R中都具备近似性能3. 在数据源中自动发现模式类型（Schema）和分区4. 支持多种数据源5. 通过Catalyst优化器进行优化和代码生成6. 可以与RDD、DatSet、Pandas和外部数据源（RDBMS关系型数据库、HBase、Cassandra等）互操作</code></pre></li><li><p><strong>DataSet API</strong></p><p>  Spark1.6 版本引入的 DataSet API 结合了 RDD 和 DataFrame的最大优点。 DatSet 会使用编码器将JVM对象转换为用 Spark 的 Tungsten 二进制格式存储的 DatSet 表形式。<br>  它的优点有：</p><pre><code>1. 和 RDD 一样，类型安全性2. 和 DataFrame 一样， 比 RDD 更快3. 与 DataFrame 和 RDD 之间具有互操性4. 缓存的 DatSet 专用的空间比 RDD 更少5. 使用编码器进行序列话比 Java 或 Kyro 序列化更快 </code></pre><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15258847849628.jpg" alt=""></p></li></ul><h2 id="4-Dataframe-和-DataSet-的演变与优势"><a href="#4-Dataframe-和-DataSet-的演变与优势" class="headerlink" title="4. Dataframe 和 DataSet 的演变与优势"></a>4. Dataframe 和 DataSet 的演变与优势</h2><p>Spark SQL 的 DataFrame 相比 R 语言或 Python 的 DataFrame，有更丰富的隐藏后台优化。 条吗可以从文件、Pandas DataFrame、 Hive 中的表、像 MySQL 这样的外部数据库或者 RDD 中创建。 Scala、Java、Python、R 都支持 DataFrame API。 虽然 DataFrame 提供了关系操作和更高的性能，但缺乏类型安全性，这会导致运行时错误。</p><p>在 1.6 版本， DataSet 和 DataFrame 还是单独的类。在 2.0 版本中， DataSet 和 DataFrame API 被统一起来，为开发人员提供单一的 API。 DataFrame 时一个特定的 DataSet[T]，其中 T = Row 的类型， 因此 DataFrame 和 DataSet 共享相同的方法。</p><p>DataSet API 支持 Scala 和 Java，还不支持 Python 和 R。 但是， DataSet API 的许多优点已经天然在 Python 和 R 语言中存在了。 DataFrame 则支持所有这四种语言。</p><h3 id="4-1-为什么使用-DataFrame-和-DataSet"><a href="#4-1-为什么使用-DataFrame-和-DataSet" class="headerlink" title="4.1 为什么使用 DataFrame 和 DataSet"></a>4.1 为什么使用 DataFrame 和 DataSet</h3><p>Spark 要对闭包进行计算、将其序列话，并将他们发送到执行进程。这意味着代码是以原始形式发送的，没有经过优化。 在 RDD 中无法表示结构化数据，对 RDD 进行查询也不可行。使用 RDD 很容易但有时候处理远足会把代码弄乱。与 Hadoop 的世界相比，纯手写的 MapReduce 作业可能比 Hive 和 Pig 作业慢，因为自爱 Hive 和 Pig 下会进行隐含的优化。 DataFrame 也可以用类似的方式看待。</p><p>因此，为什么要使用 DataFrame 和 DataSet，简单的答案是：<strong>速度和易用</strong>！</p><p>DataFrame 提供了优化、速度、自动模式发现、使用多中数据源和多语言的支持。</p><h3 id="4-2-优化"><a href="#4-2-优化" class="headerlink" title="4.2 优化"></a>4.2 优化</h3><p>Catalyst 为 DataFrame 提供了两种优化方式：</p><ol><li>谓词下推到数据源，只读取需要的数据</li><li>创建用于执行的物理计划，并生成比手写代码更优化的 JVM 字节码。</li></ol><p>和 RDD 不同， DataFrame 并不定义 DAG 图。 它会创建抽象语法书， Catalyst 引擎会使用基于规则和机遇开销的优化方法对其进行解析、检查和改进。<br><img src="media/15261969377501.jpg" alt=""></p><h3 id="4-3-速度"><a href="#4-3-速度" class="headerlink" title="4.3 速度"></a>4.3 速度</h3><p><img src="media/15261970734054.jpg" alt=""><br>上图展示了在单个机器上对1000万个证书对的 groupby 聚合操作。 Scala 和 Python 的 DataFrame 操作具有类似的执行时间，因为条吗都被便衣为相同的 JVM 字节码用来执行。</p><p>DataSet 使用优化的编译器把对象进行序列话和反序列化，以便进行处理并通过网络传输。 这些编码器比 Java 或 Kryo 序列话具有更高的性能。</p><p><img src="media/15261972316614.jpg" alt=""></p><h3 id="4-4-自动模式发现"><a href="#4-4-自动模式发现" class="headerlink" title="4.4 自动模式发现"></a>4.4 自动模式发现</h3><p>要从 RDD 创建 DataFrame，必须提供一个模式。而从 JSON、Parquet 和 ORC 文件创建 DataFrame 时，会自动发现一个模式，包括分区的发现。 Data Sources API 框架让这种情况成为可能。</p><h3 id="4-5-多数据源，多编程语言"><a href="#4-5-多数据源，多编程语言" class="headerlink" title="4.5 多数据源，多编程语言"></a>4.5 多数据源，多编程语言</h3><p>DataFrame API 支持按照最常用的格式（JSON、Parquet、ORC 和 Hive 表）进行读写</p><p>DataFrame API 支持本地文件系统、HDFS、S3和采用 JDBC 协议的外部 RDBMS 数据库读取数据</p><p>DataFrame API 支持第三方扩展：Avro、CSV、XML、HBase、ElasticSearch、Cassandra等。（<a href="http://spark-packages.org" target="_blank" rel="noopener">http://spark-packages.org</a>) 提供了第三方组件包的完整列表</p><p>Spark SQL 可以在 Java、Scala、Python、R中实现。利用 Spark SQL 的分布式 SQL 引擎，我们也可以编写纯 SQL 语句。</p><h3 id="4-6-RDD-和其他-API-的互操作性"><a href="#4-6-RDD-和其他-API-的互操作性" class="headerlink" title="4.6 RDD 和其他 API 的互操作性"></a>4.6 RDD 和其他 API 的互操作性</h3><p>Dataframe 可以使用 <strong>.rdd .toDF .toPandas .toDS</strong> 方法转换为 RDD 和 Pandas Dataframe。此外 DataFrame 可以与 Spark Streaming 和机器学习组件库配合使用。</p><h3 id="4-7-仅选择和读取必要的数据"><a href="#4-7-仅选择和读取必要的数据" class="headerlink" title="4.7 仅选择和读取必要的数据"></a>4.7 仅选择和读取必要的数据</h3><p>DataFrame API 、 DataSet API 和 DataSources API 的一个优点是通过将谓词下推到数据源系统，从而提供更丰富的优化手段。 <strong>列修剪、谓词下推、分区修剪</strong>会有这些框架自动完成。 这样，只有需要用到的数据才会读取和处理。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark-SQL-基础框架和核心组件&quot;&gt;&lt;a href=&quot;#Spark-SQL-基础框架和核心组件&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL 基础框架和核心组件&quot;&gt;&lt;/a&gt;Spark SQL 基
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark ML - 聚类算法</title>
    <link href="http://www.ihoge.cn/2018/ML2.html"/>
    <id>http://www.ihoge.cn/2018/ML2.html</id>
    <published>2018-05-05T18:00:21.000Z</published>
    <updated>2018-08-05T10:17:53.984Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="Spark-ML-聚类算法"><a href="#Spark-ML-聚类算法" class="headerlink" title="Spark ML - 聚类算法"></a>Spark ML - 聚类算法</h2><h3 id="1-KMeans快速聚类"><a href="#1-KMeans快速聚类" class="headerlink" title="1.KMeans快速聚类"></a>1.KMeans快速聚类</h3><p>首先到UR需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">KMeans</span>,<span class="type">KMeansModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>​    在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(</span><br><span class="line">    line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib版的教程类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    在得到数据后，我们即可通过ML包的固有流程：创建<code>Estimator</code>并调用其<code>fit()</code>方法来生成相应的<code>Transformer</code>对象，很显然，在这里<code>KMeans</code>类是<code>Estimator</code>，而用于保存训练后模型的<code>KMeansModel</code>类则属于<code>Transformer</code>：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kmeansmodel = <span class="keyword">new</span> <span class="type">KMeans</span>().</span><br><span class="line">      setK(<span class="number">3</span>).</span><br><span class="line">      setFeaturesCol(<span class="string">"features"</span>).</span><br><span class="line">      setPredictionCol(<span class="string">"prediction"</span>).</span><br><span class="line">      fit(df)</span><br></pre></td></tr></table></figure><p>​    与MLlib版本类似，ML包下的KMeans方法也有<code>Seed</code>（随机数种子）、<code>Tol</code>（收敛阈值）、<code>K</code>（簇个数）、<code>MaxIter</code>（最大迭代次数）、<code>initMode</code>（初始化方式）、<code>initStep</code>（KMeans||方法的步数）等参数可供设置，和其他的ML框架算法一样，用户可以通过相应的<code>setXXX()</code>方法来进行设置，或以<code>ParamMap</code>的形式传入参数，这里为了简介期间，使用<code>setXXX()</code>方法设置了参数K，其余参数均采用默认值。</p><p>​    与MLlib中的实现不同，<code>KMeansModel</code>作为一个<code>Transformer</code>，不再提供<code>predict()</code>样式的方法，而是提供了一致性的<code>transform()</code>方法，用于将存储在<code>DataFrame</code>中的给定数据集进行整体处理，生成带有预测簇标签的数据集：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> results = kmeansmodel.transform(df)</span><br></pre></td></tr></table></figure><p>​    为了方便观察，我们可以使用<code>collect()</code>方法，该方法将<code>DataFrame</code>中所有的数据组织成一个<code>Array</code>对象进行返回：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results.collect().foreach(</span><br><span class="line">      row =&gt; &#123;</span><br><span class="line">        println( row(<span class="number">0</span>) + <span class="string">" is predicted as cluster "</span> + row(<span class="number">1</span>))</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>也可以通过<code>KMeansModel</code>类自带的<code>clusterCenters</code>属性获取到模型的所有聚类中心情况：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.clusterCenters.foreach(</span><br><span class="line">      center =&gt; &#123;</span><br><span class="line">        println(<span class="string">"Clustering Center:"</span>+center)</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>​    与MLlib下的实现相同，<code>KMeansModel</code>类也提供了计算 <strong>集合内误差平方和（Within Set Sum of Squared Error, WSSSE)</strong> 的方法来度量聚类的有效性，在真实K值未知的情况下，该值的变化可以作为选取合适K值的一个重要参考：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.computeCost(df)</span><br></pre></td></tr></table></figure><h3 id="2-高斯混合模型-GMM-聚类算法"><a href="#2-高斯混合模型-GMM-聚类算法" class="headerlink" title="2.高斯混合模型(GMM)聚类算法"></a>2.高斯混合模型(GMM)聚类算法</h3><h4 id="2-1-基本原理"><a href="#2-1-基本原理" class="headerlink" title="2.1 基本原理"></a>2.1 基本原理</h4><p>​    <strong>高斯混合模型（Gaussian Mixture Model, GMM）</strong> 是一种概率式的聚类方法，属于生成式模型，它假设所有的数据样本都是由某一个给定参数的 <strong>多元高斯分布</strong> 所生成的。具体地，给定类个数<code>K</code>，对于给定样本空间中的样本 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-137bed5220372f1cad4f3cdf4529593e_l3.svg" alt="img"></p><p>，一个高斯混合模型的概率密度函数可以由K个多元高斯分布组合成的混合分布表示：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1d75e089a9a823703aa88d08ad53936e_l3.svg" alt="img"></p><p>其中，</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-a0f35b7e777b0ecf33b511cfb3174001_l3.svg" alt="img"></p><p>是以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>为均值向量， </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>为协方差矩阵的多元高斯分布的概率密度函数，可以看出，高斯混合模型由K个不同的多元高斯分布共同组成，每一个分布被称为高斯混合模型中的一个 <strong>成分(Component)</strong>， 而</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1a47f13ac9a37fcb6911a1b8e17cbb35_l3.svg" alt="img"></p><p>为第<code>i</code>个多元高斯分布在混合模型中的 <strong>权重</strong> ，且有 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-9312fca34e9b3afde8787a29c58fe192_l3.svg" alt="img"></p><p>。</p><p>假设已有一个存在的高斯混合模型，那么，样本空间中的样本的生成过程即是：以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-af3d8faef7e634b7b15f83cb1606b714_l3.svg" alt="img"></p><p>作为概率（实际上，权重可以直观理解成相应成分产生的样本占总样本的比例），选择出一个混合成分，根据该混合成分的概率密度函数，采样产生出相应的样本。</p><p>那么，利用GMM进行聚类的过程是利用GMM生成数据样本的“逆过程”：给定聚类簇数<code>K</code>，通过给定的数据集，以某一种 <strong>参数估计</strong> 的方法，推导出每一个混合成分的参数（即均值向量 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>、协方差矩阵 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>和权重 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-356e473b3185b432024c4643855f1b9d_l3.svg" alt="img"></p><p>），每一个多元高斯分布成分即对应于聚类后的一个簇。高斯混合模型在训练时使用了极大似然估计法，最大化以下对数似然函数：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-c006bbb984e97b258cf6bcc1d62ee2d7_l3.svg" alt="img"></p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-f57e2abf78c1ba038d4969a8fc513e7a_l3.svg" alt="img"></p><p>显然，该优化式无法直接通过解析方式求得解，故可采用 <strong>期望-最大化(Expectation-Maximization, EM)</strong> 方法求解，具体过程如下（为了简洁，这里省去了具体的数学表达式，详细可见<a href="https://en.wikipedia.org/wiki/Mixture_model" target="_blank" rel="noopener">wikipedia</a>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.根据给定的K值，初始化K个多元高斯分布以及其权重；</span><br><span class="line">2.根据贝叶斯定理，估计每个样本由每个成分生成的后验概率；(EM方法中的E步)</span><br><span class="line">3.根据均值，协方差的定义以及2步求出的后验概率，更新均值向量、协方差矩阵和权重；（EM方法的M步）</span><br><span class="line">重复2~3步，直到似然函数增加值已小于收敛阈值，或达到最大迭代次数</span><br></pre></td></tr></table></figure><p>​    当参数估计过程完成后，对于每一个样本点，根据贝叶斯定理计算出其属于每一个簇的后验概率，并将样本划分到后验概率最大的簇上去。相对于KMeans等直接给出样本点的簇划分的聚类方法，GMM这种给出样本点属于每个簇的概率的聚类方法，被称为 <strong>软聚类(Soft Clustering / Soft Assignment)</strong> 。</p><h4 id="2-2-模型的训练与分析"><a href="#2-2-模型的训练与分析" class="headerlink" title="2.2 模型的训练与分析"></a>2.2 模型的训练与分析</h4><p>​    Spark的ML库提供的高斯混合模型都在<code>org.apache.spark.ml.clustering</code>包下，和其他的聚类方法类似，其具体实现分为两个类：用于抽象GMM的超参数并进行训练的<code>GaussianMixture</code>类（<code>Estimator</code>）和训练后的模型<code>GaussianMixtureModel</code>类（<code>Transformer</code>），在使用前，引入需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">GaussianMixture</span>,<span class="type">GaussianMixtureModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    我们仍采用Iris数据集进行实验。为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib的操作类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    可以通过创建一个<code>GaussianMixture</code>类，设置相应的超参数，并调用<code>fit(..)</code>方法来训练一个GMM模型<code>GaussianMixtureModel</code>，在该方法调用前需要设置一系列超参数，如下表所示：</p><ul><li>K:聚类数目，默认为2 </li><li>maxIter : 最大迭代次数，默认为100 </li><li>seed : 随机数种子，默认为随机Long值 </li><li>Tol : 对数似然函数收敛阈值，默认为0.01 </li></ul><p>其中，每一个超参数均可通过名为<code>setXXX(...)</code>（如maxIterations即为<code>setMaxIterations()</code>）的方法进行设置。这里，我们建立一个简单的<code>GaussianMixture</code>对象，设定其聚类数目为3，其他参数取默认值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> gm = <span class="keyword">new</span> <span class="type">GaussianMixture</span>().setK(<span class="number">3</span>)</span><br><span class="line">               .setPredictionCol(<span class="string">"Prediction"</span>)</span><br><span class="line">               .setProbabilityCol(<span class="string">"Probability"</span>)</span><br><span class="line"><span class="keyword">val</span> gmm = gm.fit(df)</span><br></pre></td></tr></table></figure><p>和<code>KMeans</code>等硬聚类方法不同的是，除了可以得到对样本的聚簇归属预测外，还可以得到样本属于各个聚簇的概率（这里我们存在”Probability”列中）。</p><p>​    调用<code>transform()</code>方法处理数据集之后，打印数据集，可以看到每一个样本的预测簇以及其概率分布向量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = gmm.transform(df)</span><br><span class="line">result.show(<span class="number">150</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>​    得到模型后，即可查看模型的相关参数，与KMeans方法不同，GMM不直接给出聚类中心，而是给出各个混合成分（多元高斯分布）的参数。在ML的实现中，GMM的每一个混合成分都使用一个<code>MultivariateGaussian</code>类（位于<code>org.apache.spark.ml.stat.distribution</code>包）来存储，我们可以使用<code>GaussianMixtureModel</code>类的<code>weights</code>成员获取到各个混合成分的权重，使用<code>gaussians</code>成员来获取到各个混合成分的参数（均值向量和协方差矩阵）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until gmm.getK) &#123;</span><br><span class="line">      println(<span class="string">"Component %d : weight is %f \n mu vector is %s \n sigma matrix is %s"</span> format</span><br><span class="line">      (i, gmm.weights(i), gmm.gaussians(i).mean, gmm.gaussians(i).cov))</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;Spark-ML-聚类算法&quot;&gt;&lt;a href=&quot;#Spark-ML-聚类算法&quot; class=&quot;headerlink&quot; title=&quot;Spark ML - 聚类算法&quot;&gt;&lt;/a&gt;Spark ML - 聚类算法&lt;/h2&gt;&lt;h3 id=
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML - 协同过滤</title>
    <link href="http://www.ihoge.cn/2018/ML1.html"/>
    <id>http://www.ihoge.cn/2018/ML1.html</id>
    <published>2018-05-05T17:59:21.000Z</published>
    <updated>2018-05-28T07:53:22.257Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>​    获取spark自带的MovieLens数据集，其中每行包含一个用户、一个电影、一个该用户对该电影的评分以及时间戳。我们使用默认的<code>ALS.train()</code> 方法，即显性反馈（默认<code>implicitPrefs</code> 为false）来构建推荐模型并根据模型对评分预测的均方根误差来对模型进行评估。</p><h3 id="导入需要的包："><a href="#导入需要的包：" class="headerlink" title="导入需要的包："></a>导入需要的包：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">RegressionEvaluator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.recommendation.<span class="type">ALS</span></span><br></pre></td></tr></table></figure><h3 id="根据数据结构创建读取规范"><a href="#根据数据结构创建读取规范" class="headerlink" title="根据数据结构创建读取规范"></a>根据数据结构创建读取规范</h3><p> 创建一个Rating类型，即[Int, Int, Float, Long];然后建造一个把数据中每一行转化成Rating类的函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Rating</span>(<span class="params">userId: <span class="type">Int</span>, movieId: <span class="type">Int</span>, rating: <span class="type">Float</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">parseRating</span>(<span class="params">str: <span class="type">String</span></span>)</span>: <span class="type">Rating</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> fields = str.split(<span class="string">"::"</span>)</span><br><span class="line">            assert(fields.size == <span class="number">4</span>)</span><br><span class="line">            <span class="type">Rating</span>(fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>).toInt, fields(<span class="number">2</span>).toFloat, fields(<span class="number">3</span>).toLong)</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure><h3 id="读取数据："><a href="#读取数据：" class="headerlink" title="读取数据："></a>读取数据：</h3><p> 导入implicits，读取MovieLens数据集，把数据转化成Rating类型；</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> ratings = spark.sparkContext.textFile(<span class="string">"file:///home/hduser/spark/data/mllib/als/sample_movielens_ratings.txt"</span>).map(parseRating).toDF()</span><br></pre></td></tr></table></figure><p>然后打印数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ratings.show()</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p> 把MovieLens数据集划分训练集和测试集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(training, test) = ratings.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>))</span><br></pre></td></tr></table></figure><p> 使用ALS来建立推荐模型，这里我们构建了两个模型，一个是显性反馈，一个是隐性反馈</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alsExplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setUserCol(<span class="string">"userId"</span>). setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br><span class="line"><span class="keyword">val</span> alsImplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setImplicitPrefs(<span class="literal">true</span>). setUserCol(<span class="string">"userId"</span>).setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br></pre></td></tr></table></figure><p> 在 ML 中的实现有如下的参数:</p><ul><li><code>numBlocks</code> 是用于并行化计算的用户和商品的分块个数 (默认为10)。</li><li><code>rank</code> 是模型中隐语义因子的个数（默认为10）。</li><li><code>maxIter</code> 是迭代的次数（默认为10）。</li><li><code>regParam</code> 是ALS的正则化参数（默认为1.0）。</li><li><code>implicitPrefs</code> 决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本（默认是false，即用显性反馈）。</li><li><code>alpha</code> 是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准（默认为1.0）。</li><li><p><code>nonnegative</code> 决定是否对最小二乘法使用非负的限制（默认为false）。</p><p>可以调整这些参数，不断优化结果，使均方差变小。比如：imaxIter越大，regParam越 小，均方差会越小，推荐结果较优。</p></li></ul><p>接下来，把推荐模型放在训练数据上训练：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> modelExplicit = alsExplicit.fit(training)</span><br><span class="line"><span class="keyword">val</span> modelImplicit = alsImplicit.fit(training)</span><br></pre></td></tr></table></figure><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p> 使用训练好的推荐模型对测试集中的用户商品进行预测评分，得到预测评分的数据集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> predictionsExplicit = modelExplicit.transform(test)</span><br><span class="line"><span class="keyword">val</span> predictionsImplicit = modelImplicit.transform(test)</span><br></pre></td></tr></table></figure><p> 我们把结果输出，对比一下真实结果与预测结果：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictionsExplicit.show()</span><br><span class="line">predictionsImplicit.show()</span><br></pre></td></tr></table></figure><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p> 通过计算模型的均方根误差来对模型进行评估，均方根误差越小，模型越准确：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">RegressionEvaluator</span>().setMetricName(<span class="string">"rmse"</span>).setLabelCol(<span class="string">"rating"</span>). setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line"><span class="keyword">val</span> rmseExplicit = evaluator.evaluate(predictionsExplicit)</span><br><span class="line"><span class="keyword">val</span> rmseImplicit = evaluator.evaluate(predictionsImplicit)</span><br></pre></td></tr></table></figure><p> 打印出两个模型的均方根误差 ：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">s"Explicit:Root-mean-square error = <span class="subst">$rmseExplicit</span>"</span>)</span><br><span class="line">println(<span class="string">s"Implicit:Root-mean-square error = <span class="subst">$rmseImplicit</span>"</span>)</span><br></pre></td></tr></table></figure><p> 可以看到打分的均方差值为1.69和1.80左右。由于本例的数据量很少，预测的结果和实际相比有一定的差距。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;协同过滤算法&quot;&gt;&lt;a href=&quot;#协同过滤算法&quot; class=&quot;headerlink&quot; title=&quot;协同过滤算法&quot;&gt;&lt;/a&gt;协同过滤算法&lt;/h2&gt;&lt;p&gt;​    获取spark自带的MovieLens数据集，其中每行包含一个
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
</feed>
