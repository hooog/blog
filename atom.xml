<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hoooge&#39;s Blog</title>
  
  <subtitle>Life is short</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.ihoge.cn/"/>
  <updated>2018-04-13T16:44:12.099Z</updated>
  <id>http://www.ihoge.cn/</id>
  
  <author>
    <name>刘知行</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用aconda3-5.1.0（Python3.6.4） 搭建pyspark远程部署</title>
    <link href="http://www.ihoge.cn/2018/anacondaPyspark.html"/>
    <id>http://www.ihoge.cn/2018/anacondaPyspark.html</id>
    <published>2018-04-13T17:59:21.000Z</published>
    <updated>2018-04-13T16:44:12.099Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>首次安装的环境搭配是这样的：<br> jdk8<br> hadoop2.6.5<br> spark2.1<br> scala2.12.4<br> Anaconda3-5.1.0<br>一连串的报错让人惊喜无限，尽管反复调整配置始终无法解决。<br><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqavez8o4aj310q094jsn.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqavfnihv5j316g0katdf.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqavgahjl2j315y0e00vm.jpg" alt=""></p><p>坑了一整天后最后最终发现是<strong>版本不兼容！！</strong>再次提醒自己一定要重视各组件版本的问题。这里最主要的是spark和Anaconda版本的兼容问题，为了兼容python3尽量用新版的spark。最终解决方案的版本搭配如下：<br> jdk8<br> hadoop2.7.5<br> spark2.3.0<br> scala2.11.12<br> Anaconda3-5.1.0</p><h3 id="一、VM安装Ubuntu16-04虚拟机"><a href="#一、VM安装Ubuntu16-04虚拟机" class="headerlink" title="一、VM安装Ubuntu16.04虚拟机"></a>一、VM安装Ubuntu16.04虚拟机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vim</span><br><span class="line">sudo apt-get install openssh-server</span><br><span class="line"></span><br><span class="line"># 配置ssh免密登陆</span><br><span class="line">ssh localhost</span><br><span class="line">ssh-keygen -t rsa //一路回车</span><br><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line"></span><br><span class="line">sudo vi /etc/hosts //添加各个节点ip</span><br><span class="line">192.168.221.132 master</span><br><span class="line">192.168.221.133 slave1</span><br><span class="line">192.168.221.134 slave2</span><br><span class="line"></span><br><span class="line"># sudo vi /etc/hostname</span><br><span class="line">master</span><br></pre></td></tr></table></figure><h3 id="二、配置profile环境变量"><a href="#二、配置profile环境变量" class="headerlink" title="二、配置profile环境变量"></a>二、配置profile环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#Java</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_161</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jar</span><br><span class="line">#Hadoop</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">#Scala</span><br><span class="line">export SCALA_HOME=/home/hadoop/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br><span class="line">#Anaconda</span><br><span class="line">export PATH=/home/hadoop/anaconda3/bin:$PATH</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=jupyter</span><br><span class="line">export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot;</span><br><span class="line">#Spark</span><br><span class="line">export SPARK_HOME=/home/hadoop/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="三、hadoop-六个配置文件"><a href="#三、hadoop-六个配置文件" class="headerlink" title="三、hadoop 六个配置文件"></a>三、hadoop 六个配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/home/hadoop/hadoop/jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"># core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/home/hadoop/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:50090&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># slaves</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="三、spark两个配置文件"><a href="#三、spark两个配置文件" class="headerlink" title="三、spark两个配置文件"></a>三、spark两个配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># spark-env.sh</span><br><span class="line">#java</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_161</span><br><span class="line">#scala</span><br><span class="line">export SCALA_HOME=/home/hadoop/scala</span><br><span class="line">#hadoop</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=/home/hadoop/hadoop/etc/hadoop</span><br><span class="line">#spark</span><br><span class="line">export SPARK_HOME=/home/hadoop/spark</span><br><span class="line">export SPARK_LOCAL_DIRS=/home/hadoop/spark</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/home/hadoop/hadoop/bin/hadoop classpath)</span><br><span class="line">export SPARK_WORKER_CORES=1</span><br><span class="line">export SPARK_WORKER_INSTANCES=1</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line">export SPARK_MASTER_IP=master</span><br><span class="line">export SPARK_LIBRARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native</span><br><span class="line"></span><br><span class="line"># slaves</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="四、解压缩文件"><a href="#四、解压缩文件" class="headerlink" title="四、解压缩文件"></a>四、解压缩文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scp jdk-8u161-linux-x64.tar hadoop@master:~</span><br><span class="line">scp Anaconda3-5.1.0-Linux-x86_64.sh hadoop@master:~</span><br><span class="line">scp -r hadoop/ hadoop@master:~</span><br><span class="line">scp -r scala/ hadoop@master:~</span><br><span class="line">scp -r spark/ hadoop@master:~</span><br><span class="line"></span><br><span class="line">tar -xvf jdk-8u161-linux-x64.tar -C ./</span><br><span class="line"></span><br><span class="line">source ~/.profile</span><br><span class="line">分别查看jdk版本、hadoop版本、scala版本</span><br><span class="line"></span><br><span class="line"># 集群模式启动spark查看jps</span><br><span class="line">spark-shell --master spark://master:7077 --executor-memory 512m --total-executor-cores 2</span><br></pre></td></tr></table></figure><h3 id="五、安装Anaconda"><a href="#五、安装Anaconda" class="headerlink" title="五、安装Anaconda"></a>五、安装Anaconda</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-5.1.0-Linux-x86_64.sh -b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建配置jupyter_notebook_config.py</span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br><span class="line"></span><br><span class="line">c = get_config()</span><br><span class="line">c.IPKernelApp.pylab = &apos;inline&apos;</span><br><span class="line">c.NotebookApp.ip = &apos;*&apos; </span><br><span class="line">c.NotebookApp.open.browser = False</span><br><span class="line">c.NotebookApp.password = u&apos;&apos;</span><br><span class="line">c.NotebookApp.port = 8888</span><br></pre></td></tr></table></figure><h3 id="六、关机后克隆出两个新节点并配置相关内容"><a href="#六、关机后克隆出两个新节点并配置相关内容" class="headerlink" title="六、关机后克隆出两个新节点并配置相关内容"></a>六、关机后克隆出两个新节点并配置相关内容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">sudo vi /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="七、远程测试pyspark集群"><a href="#七、远程测试pyspark集群" class="headerlink" title="七、远程测试pyspark集群"></a>七、远程测试pyspark集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 服务器端启动集群</span><br><span class="line">start-all.sh</span><br><span class="line">spark/sbin/start-all.sh</span><br><span class="line"></span><br><span class="line"># hadoop和spark的进程都显示正常后开始启动pyspark</span><br><span class="line">1、local模式运行</span><br><span class="line">pyspark</span><br><span class="line"></span><br><span class="line">2、Stand Alone运行模式</span><br><span class="line">PYSPARK_DRIVER_PYTHON=ipython PYSPARK_DRIVER_PYTHON_OPTS=&apos;notebook&apos; MASTER=spark://master:7077 pyspark --num-executors 1 --total-executor-cores 3 --executor-memory 512m</span><br></pre></td></tr></table></figure><p>然后在远程Web端输入192.168.221.132:8888<br>页面打开后需要输入验证信息（第一次验证即可）：<br>输入上图<code>token</code>后面的字符串和用户密码<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqbhup77mqj31fu0j0wu2.jpg" alt=""><br>输入<code>sc</code>测试<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqbhwgb3wdj310m0dowfj.jpg" alt=""></p><p>至此，aconda3-5.1.0（Python3.6.4） 搭建pyspark远程服务器部署成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;首次安装的环境搭配是这样的：&lt;br&gt; jdk8&lt;br&gt; hadoop2.6.5&lt;br&gt; spark2.1
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="Environment" scheme="http://www.ihoge.cn/tags/Environment/"/>
    
  </entry>
  
  <entry>
    <title>Hive集成Spark+Sql</title>
    <link href="http://www.ihoge.cn/2018/HiveSpark.html"/>
    <id>http://www.ihoge.cn/2018/HiveSpark.html</id>
    <published>2018-04-13T03:59:21.000Z</published>
    <updated>2018-04-13T08:18:16.776Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、Hive安装"><a href="#一、Hive安装" class="headerlink" title="一、Hive安装"></a>一、Hive安装</h2><h3 id="1-Hive简介"><a href="#1-Hive简介" class="headerlink" title="1.Hive简介"></a>1.Hive简介</h3><p>​    Hive是Facebook开发的构建于Hadoop集群之上的数据仓库应用，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。</p><p>​    Hive是一个可以提供有效的、合理的且直观的组织和使用数据的模型，即使对于经验丰富的Java开发工程师来说，将这些常见的数据运算对应到底层的MapReduce Java API也是令人敬畏的。Hive可以帮用户做这些工作，用户就可以集中精力关注查询本身了。Hive可以将大多数的查询转换为MapReduce任务。Hive最适合于数据仓库应用程序，使用该应用程序进行相关的静态数据分析，不需要快速响应给出结果，而且数据本身也不会频繁变化。</p><p>​    Hive不是一个完整的数据库。Hadoop以及HDFS的设计本身约束和局限性限制了Hive所能胜任的工作。最大的限制就是Hive不支持记录级别的更新、插入或者删除。用户可以通过查询生成新表或将查询结果导入到文件中去。因为，Hadoop是一个面向批处理的系统，而MapReduce启动任务启动过程需要消耗很长时间，所以Hive延时也比较长。Hive还不支持事务。因此，Hive不支持联机事务处理（OLTP），更接近于一个联机分析技术（OLAP）工具，但是，目前还没有满足“联机”部分。</p><p>​    Hive提供了一系列的工具，可以用来进行数据提取转化加载(ETL)，其中，ETL是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。因此，Hive是最适合数据仓库应用程序的，它可以维护海量数据，而且可以对数据进行挖掘，然后形成意见和报告等。</p><p>​    因为大多数的数据仓库应用程序是基于SQL的关系数据库现实的，所以，Hive降低了将这些应用程序移植到Hadoop上的障碍。如果用户懂得SQL，那么学习使用Hive会很容易。因为Hive定义了简单的类SQL 查询语言——HiveQL，这里值得一提的是，与SQLServer、Oracle相比，HiveQL和MySQL提供的SQL语言更接近。同样的，相对于其他的Hadoop语言和工具来说，Hive也使得开发者将基于SQL的应用程序移植到Hadoop变得更加容易。</p><h3 id="2-Hive安装"><a href="#2-Hive安装" class="headerlink" title="2.Hive安装"></a>2.Hive安装</h3><p>​    接下来，开始Hive的安装，安装Hive之前，首先需要装好Hadoop和Spark。在<strong><a href="https://hive.apache.org/" target="_blank" rel="noopener">Hive官网</a></strong>可下载最新版本Hive，并且能够查阅版本改动说明，本次课程采用1.2.2版本进行安装。可以采用WinSCP传输apache-hive-1.2.2-bin.tar至虚拟机“下载”文件夹中，再进行后续安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载                                              # 进入下载文件夹</span><br><span class="line">sudo tar -zxf apache-hive-1.2.2-bin.tar.gz -C /usr/local    # 安装至/usr/local文件夹内</span><br><span class="line">cd /usr/local                                         # 进入/usr/local文件夹</span><br><span class="line">sudo mv ./apache-hive-1.2.2-bin/ ./hive               # 更名为hive</span><br><span class="line">sudo chown -R hadoop ./hive                           # 修改hive权限</span><br><span class="line">mkdir -p /usr/local/hive/warehouse                    # 创建元数据存储文件夹</span><br><span class="line">sudo chmod a+rwx /usr/local/hive/warehouse            # 修改文件权限</span><br></pre></td></tr></table></figure><p>然后添加Hive安装路径至系统环境变量</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure><p>添加下述路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Hive</span></span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>并使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure><p>修改hive读取spark的jar包地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/bin</span><br><span class="line">vim hive</span><br></pre></td></tr></table></figure><p>修改为</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add Spark assembly jar to the classpath</span></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$SPARK_HOME</span>"</span> ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  sparkAssemblyPath=`ls <span class="variable">$&#123;SPARK_HOME&#125;</span>/jars/*.jar`</span><br><span class="line">  CLASSPATH=<span class="string">"<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;sparkAssemblyPath&#125;</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>然后采用hive默认配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/conf</span><br><span class="line">cp hive-default.xml.template hive-default.xml</span><br></pre></td></tr></table></figure><p>尝试启动Hive，此时启动是以本地模式进行启动，能正常启动则说明安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>​    若出现jline等jar包错误，则需要进入到hadoop安装目录下的share/hadoop/yarn/lib下删除jline-0.9.94.jar文件，再启动hive即可（因为高版本的Hadoop对Hive有捆绑）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/share/hadoop/yarn/lib</span><br><span class="line">rm -rf jline-0.9.94.jar</span><br></pre></td></tr></table></figure><p>###3. Hive的基本配置</p><p>​    在安装Hive时，默认情况下，元数据存储在Derby数据库中。Derby是一个完全用Java编写的数据库，所以可以跨平台，但需要在JVM中运行 。因为多用户和系统可能需要并发访问元数据存储，所以默认的内置数据库并不适用于生产环境。任何一个适用于JDBC进行连接的数据库都可用作元数据库存储，这里我们把MySQL作为存储元数据的数据库。接下来，我们分别对这两种方式进行介绍，即使用Derby数据库的方式和使用MySQL数据库的方式。</p><h4 id="3-1-使用Derby作为元数据库"><a href="#3-1-使用Derby作为元数据库" class="headerlink" title="3.1 使用Derby作为元数据库"></a>3.1 使用Derby作为元数据库</h4><p>​    本地模式中，用户的“表”等元数据信息，都默认存储在file://user/hive/warehouse，对于其他模式默认存储路径是hdfs://namenode_server/user/hive/warehouse。使用如下命令编辑hive-site.xml文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>在hive-site.xml文件添加以下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:derby:;databaseName=/usr/local/hive/metastore_db;create=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                           </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​    若要以伪分布式模式和分布式模式配置Hive，只需根据Hadoop配置文件core-site.xml中fs.defaultFS的值对hive.metastore.warehouse.dir 进行相应修改即可。配置完成之后即可启动Hive，然后尝试使用HiveQL命令创建表。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> derby;</span><br><span class="line"><span class="keyword">use</span> derby;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> x(a <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> x;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> x;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure><h4 id="3-2-使用MySQL作为元数据库"><a href="#3-2-使用MySQL作为元数据库" class="headerlink" title="3.2 使用MySQL作为元数据库"></a>3.2 使用MySQL作为元数据库</h4><h5 id="3-2-1-安装MySQL"><a href="#3-2-1-安装MySQL" class="headerlink" title="3.2.1 安装MySQL"></a>3.2.1 安装MySQL</h5><p>首先，查看并卸载系统自带的MySQL相关安装包（或之前安装过MySQL），命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install rpm</span><br><span class="line">rpm -qa | grep mysql</span><br></pre></td></tr></table></figure><p>若没有安装rpm工具，系统会有提示，按照提示安装即可。接下来查看是否有系统自带的MySQL相关安装包，若有，按下面命令删除：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -e --nodeps mysql-libs-xxxxxx</span><br></pre></td></tr></table></figure><p>注：xxxxx是已经安装的mysql的版本号，然后进行MySQL的安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install mysql-server</span><br></pre></td></tr></table></figure><p>安装完成后，启动设置MySQL服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><p>当然，还可使用下列命令进行额外设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chkconfig mysql on                              # 设置开机自动启动</span><br><span class="line">sudo /usr/bin/mysqladmin -u root password '123'      # 设置root用户密码</span><br></pre></td></tr></table></figure><p>接下来，创建hive用户及其数据库等，用于存放Hive的元数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/mysql/my.cnf</span><br><span class="line">注释掉：bind-address            = 127.0.0.1</span><br><span class="line"></span><br><span class="line">create database hive;</span><br><span class="line">grant all on *.* to hive@localhost identified by &apos;hive&apos;; </span><br><span class="line">flush privileges;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure><p>切换hive用户登陆</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u hive -p hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure><p>若能看到hive数据库存在，则说明创建成功。</p><h5 id="3-2-2-修改Hive配置"><a href="#3-2-2-修改Hive配置" class="headerlink" title="3.2.2 修改Hive配置"></a>3.2.2 修改Hive配置</h5><p>接下来，修改hive-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>输入下列信息</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>或者指定元数据文件夹</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive;createDatebaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                           </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword <span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后将JDBC文件放到hive的lib文件夹内，JDBC包的下载参考前述部分</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载</span><br><span class="line">cp mysql-connector-java-5.1.26-bin.jar /usr/local/hive/lib</span><br><span class="line">mkdir -p /usr/local/hive/tmp</span><br><span class="line">sudo chmod a+rwx /usr/local/hive/tmp</span><br></pre></td></tr></table></figure><p>也可从官网直接下载最新版jdbc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz</span><br></pre></td></tr></table></figure><p>然后进行解压安装。当然，如果之前删除了jline-0.9.94.jar，此时需要把hive对应的jar包放进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/hive/lib/jline-2.12.jar  /usr/local/hadoop/share/hadoop/yarn/lib</span><br></pre></td></tr></table></figure><p>然后尝试启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line">start-all.sh</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>成启动后，即可输入hive –help查看hive常用命令。</p><h2 id="二、Hive使用"><a href="#二、Hive使用" class="headerlink" title="二、Hive使用"></a>二、Hive使用</h2><h3 id="1-Hive基本数据类型"><a href="#1-Hive基本数据类型" class="headerlink" title="1.Hive基本数据类型"></a>1.Hive基本数据类型</h3><p>首先，我们简单叙述一下HiveQL的基本数据类型。</p><p>Hive支持基本数据类型和复杂类型, 基本数据类型主要有数值类型(INT、FLOAT、DOUBLE ) 、布尔型和字符串, 复杂类型有三种:ARRAY、MAP 和 STRUCT。</p><h4 id="1-1-基本数据类型"><a href="#1-1-基本数据类型" class="headerlink" title="1.1 基本数据类型"></a>1.1 基本数据类型</h4><ul><li>TINYINT: 1个字节</li><li>SMALLINT: 2个字节</li><li>INT: 4个字节</li><li>BIGINT: 8个字节</li><li>BOOLEAN: TRUE/FALSE </li><li>FLOAT: 4个字节，单精度浮点型</li><li>DOUBLE: 8个字节，双精度浮点型STRING 字符串</li></ul><h4 id="1-2-复杂数据类型"><a href="#1-2-复杂数据类型" class="headerlink" title="1. 2 复杂数据类型"></a>1. 2 复杂数据类型</h4><ul><li>ARRAY: 有序字段</li><li>MAP: 无序字段</li><li>STRUCT: 一组命名的字段</li></ul><h3 id="2-常用的HiveQL操作命令"><a href="#2-常用的HiveQL操作命令" class="headerlink" title="2.常用的HiveQL操作命令"></a>2.常用的HiveQL操作命令</h3><p>​    Hive常用的HiveQL操作命令主要包括：数据定义、数据操作。接下来详细介绍一下这些命令即用法（想要了解更多请参照《Hive编程指南》一书）。</p><h4 id="2-1-数据定义"><a href="#2-1-数据定义" class="headerlink" title="2.1 数据定义"></a>2.1 数据定义</h4><p>主要用于创建修改和删除数据库、表、视图、函数和索引。</p><ul><li><p>创建、修改和删除数据库</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists hive;       #创建数据库</span><br><span class="line">show databases;                           #查看Hive中包含数据库</span><br><span class="line">show databases like 'h.*';                #查看Hive中以h开头数据库</span><br><span class="line">describe databases;                       #查看hive数据库位置等信息</span><br><span class="line">alter database hive set dbproperties;     #为hive设置键值对属性</span><br><span class="line">use hive;                                 #切换到hive数据库下</span><br><span class="line">drop database if exists hive;             #删除不含表的数据库</span><br><span class="line">drop database if exists hive cascade;     #删除数据库和它中的表</span><br></pre></td></tr></table></figure><p>注意，除 dbproperties属性外，数据库的元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置，没有办法删除或重置数据库属性。</p></li><li><p>创建、修改和删除表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#创建内部表（管理表）</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'username'</span>,</span><br><span class="line">      pwd <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'password'</span>,</span><br><span class="line">      address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      <span class="keyword">comment</span>  <span class="string">'home address'</span>,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt; <span class="keyword">comment</span> <span class="string">'number,sex'</span>) </span><br><span class="line">      <span class="keyword">comment</span> <span class="string">'description of the table'</span>  </span><br><span class="line">     tblproperties(<span class="string">'creator'</span>=<span class="string">'me'</span>,<span class="string">'time'</span>=<span class="string">'2016.1.1'</span>); </span><br><span class="line">#创建外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> usr2(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">      pwd <span class="keyword">string</span>,</span><br><span class="line">  address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt;) </span><br><span class="line">      <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">     location <span class="string">'/usr/local/hive/warehouse/hive.db/usr'</span>; </span><br><span class="line">#创建分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> usr3(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">      pwd <span class="keyword">string</span>,</span><br><span class="line">      address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt;) </span><br><span class="line">      partitioned <span class="keyword">by</span>(city <span class="keyword">string</span>,state <span class="keyword">string</span>);    </span><br><span class="line">#复制usr表的表模式  </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr1 <span class="keyword">like</span> hive.usr;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">in</span> hive;  </span><br><span class="line">show tables 'u.*';        #查看hive中以u开头的表</span><br><span class="line">describe hive.usr;        #查看usr表相关信息</span><br><span class="line">alter table usr rename to custom;      #重命名表 </span><br><span class="line">#为表增加一个分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> </span><br><span class="line">     <span class="keyword">partition</span>(city=”beijing”,state=”China”) </span><br><span class="line">     location <span class="string">'/usr/local/hive/warehouse/usr2/China/beijing'</span>; </span><br><span class="line">#修改分区路径</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">partition</span>(city=”beijing”,state=”China”)</span><br><span class="line">     <span class="keyword">set</span> location <span class="string">'/usr/local/hive/warehouse/usr2/CH/beijing'</span>;</span><br><span class="line">#删除分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">drop</span> <span class="keyword">if</span> <span class="keyword">exists</span>  <span class="keyword">partition</span>(city=”beijing”,state=”China”)</span><br><span class="line">#修改列信息</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr <span class="keyword">change</span> <span class="keyword">column</span> pwd <span class="keyword">password</span> <span class="keyword">string</span> <span class="keyword">after</span> address;</span><br><span class="line">alter table usr add columns(hobby string);                  #增加列</span><br><span class="line">alter table usr replace columns(uname string);              #删除替换列</span><br><span class="line">alter table usr set tblproperties('creator'='liming');      #修改表属性</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">partition</span>(city=”beijing”,state=”China”)    #修改存储属性</span><br><span class="line"><span class="keyword">set</span> fileformat sequencefile;             </span><br><span class="line">use hive;                                                   #切换到hive数据库下</span><br><span class="line">drop table if exists usr1;                                  #删除表</span><br><span class="line">drop database if exists hive cascade;                       #删除数据库和它中的表</span><br></pre></td></tr></table></figure></li><li><p>视图和索引的创建、修改和删除</p><p>基本语法格式</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create view view_name as....;                #创建视图</span><br><span class="line">alter view view_name set tblproperties(…);   #修改视图</span><br></pre></td></tr></table></figure><p>因为视图是只读的，所以 对于视图只允许改变元数据中的 tblproperties属性。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#删除视图</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> <span class="keyword">if</span> <span class="keyword">exists</span> view_name;</span><br><span class="line">#创建索引</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> index_name <span class="keyword">on</span> <span class="keyword">table</span> table_name(partition_name/column_name)  </span><br><span class="line"><span class="keyword">as</span> <span class="string">'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'</span> <span class="keyword">with</span> <span class="keyword">deferred</span> rebuild....;</span><br></pre></td></tr></table></figure><p>这里’org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’是一个索引处理器，即一个实现了索引接口的Java类，另外Hive还有其他的索引实现。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter index index_name on table table_name partition(...)  rebulid;   #重建索引</span><br></pre></td></tr></table></figure><p>如果使用 deferred rebuild，那么新索引成空白状态，任何时候可以进行第一次索引创建或重建。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show formatted index on table_name;                       #显示索引</span><br><span class="line">drop index if exists index_name on table table_name;      #删除索引</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2-数据操作"><a href="#2-2-数据操作" class="headerlink" title="2.2 数据操作"></a>2.2 数据操作</h4><p>主要实现的是将数据装载到表中（或是从表中导出），并进行相应查询操作</p><ul><li><p>向表中装载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.course(cid <span class="built_in">int</span>,<span class="keyword">sid</span> <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p>向表中装载数据有两种方法：从文件中导入和通过查询语句插入。</p><ul><li><p>从文件中导入</p><p>假如这个表中的记录存储于文件stu.txt中，该文件的存储路径为usr/local/hadoop/examples/stu.txt，内容如下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 Hello</span><br><span class="line">2 World</span><br><span class="line">3 CDA</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/usr/local/hadoop/examples/stu.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure></li><li><p>通过查询语句插入</p><p>使用如下命令，创建stu1表，它和stu表属性相同，我们要把从stu表中查询得到的数据插入到stu1中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu1 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>上面是创建表，并直接向新表插入数据；若表已经存在，向表中插入数据需执行以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu1 <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu <span class="keyword">where</span>（条件）;</span><br></pre></td></tr></table></figure><p>这里关键字overwrite的作用是替换掉表（或分区）中原有数据，换成into关键字，直接追加到原有内容后。</p></li></ul></li><li><p>写入临时文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/usr/local/hadoop/tmp/stu'</span>  <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure></li><li><p>查询操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,</span><br><span class="line">  <span class="keyword">case</span> </span><br><span class="line">  <span class="keyword">when</span> <span class="keyword">id</span>=<span class="number">1</span> <span class="keyword">then</span> <span class="string">'first'</span> </span><br><span class="line">  <span class="keyword">when</span> <span class="keyword">id</span>=<span class="number">2</span> <span class="keyword">then</span> <span class="string">'second'</span></span><br><span class="line">  <span class="keyword">else</span> <span class="string">'third'</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-3-连接"><a href="#2-3-连接" class="headerlink" title="2.3 连接"></a>2.3 连接</h4><p>​    连接（join）是将两个表中在共同数据项上相互匹配的那些行合并起来, HiveQL 的连接分为内连接、左向外连接、右向外连接、全外连接和半连接 5 种。</p><ul><li><p>内连接(等值连接)</p><p>内连接使用比较运算符根据每个表共有的列的值匹配两个表中的行。</p><p>首先，我们先把以下内容插入到course表中（自行完成）。</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 3</span><br><span class="line">2 1</span><br><span class="line">3 1</span><br></pre></td></tr></table></figure><p>​    下面, 查询stu和course表中学号相同的所有行，命令如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>左连接</p><p>​    左连接的结果集包括“LEFT OUTER”子句中指定的左表的所有行, 而不仅仅是连接列所匹配的行。如果左表的某行在右表中没有匹配行, 则在相关联的结果集中右表的所有选择列均为空值，命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>右连接</p><p>​    右连接是左向外连接的反向连接,将返回右表的所有行。如果右表的某行在左表中没有匹配行,则将为左表返回空值。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">right</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>全连接</p><p>​    全连接返回左表和右表中的所有行。当某行在另一表中没有匹配行时,则另一个表的选择列表包含空值。如果表之间有匹配行,则整个结果集包含基表的数据值。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>半连接</p><p>​    半连接是 Hive 所特有的, Hive 不支持 in 操作,但是拥有替代的方案; left semi join, 称为半连接, 需要注意的是连接的表不能在查询的列中,只能出现在 on 子句中。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.* <span class="keyword">from</span> stu <span class="keyword">left</span> semi <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><h2 id="三、Spark与Hive集成"><a href="#三、Spark与Hive集成" class="headerlink" title="三、Spark与Hive集成"></a>三、Spark与Hive集成</h2><h3 id="1-安装Spark"><a href="#1-安装Spark" class="headerlink" title="1.安装Spark"></a>1.安装Spark</h3><p>​    为了让Spark能够访问Hive，必须为Spark添加Hive支持。Spark官方提供的预编译版本，通常是不包含Hive支持的，需要采用源码编译，编译得到一个包含Hive支持的Spark版本。首先测试一下电脑上已经安装的Spark版本是否支持Hive</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>这样就启动进入了spark-shell，然后输入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br></pre></td></tr></table></figure><p>如果报错，则说明spark无法识别org.apache.spark.sql.hive.HiveContext，这时我们就需要采用源码编译方法得到支持hive的spark版本。</p><ul><li><p>下载源码文件</p><p>​    进入官网后，可以按照下图配置选择“2.1.0(Dec 28, 2016)”和“SourceCode”，然后，在图中红色方框内，有个“Download Spark: spark-2.1.0.tgz”的下载链接，点击该链接就可以下载Spark源码文件了。</p></li></ul><ul><li><p>编译过程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/spark-2.0.2</span><br><span class="line">./dev/make-distribution.sh —tgz —name h27hive -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.1 -Phive -Phive-thriftserver -DskipTests</span><br></pre></td></tr></table></figure><p>或可选择直接安装已编译好的版本，把下好的<code>spark-2.0.2-bin-h27hive.tgz</code>放到下载文件夹内</p></li><li><p>Spark解压安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载                                              # 进入下载文件夹</span><br><span class="line">sudo tar -zxf spark-2.0.2-bin-h27hive.tgz -C /usr/local   # 安装至/usr/local文件夹内</span><br><span class="line">cd /usr/local                                         # 进入/usr/local文件夹</span><br><span class="line">sudo mv ./spark-1.4.0-bin-hadoop2.4/ ./spark          # 更名为spark</span><br><span class="line">sudo chown -R hadoop ./spark                          # 修改sqoop权限</span><br></pre></td></tr></table></figure></li><li><p>添加环境变量</p><p>注，如果电脑上已经装了另一个spark，此处可不增设环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure><p>添加spark安装路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">spark</span></span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>并保存修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure></li><li><p>修改Spark配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf                              # 进入spark配置文件夹</span><br><span class="line">sudo cp spark-env.sh.template spark-env.sh            # 复制spark-env临时文件为配置文件</span><br><span class="line">vim spark-env.sh                                      # 编辑spark配置文件</span><br></pre></td></tr></table></figure><p>添加下述配置信息</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></table></figure><p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。在伪分布式模式下仅测试是否安装成功时，其他配置暂时可不做修改。</p></li><li><p>运行样例程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">bin/run-example SparkPi 2&gt;&amp;1 | grep "Pi is"</span><br></pre></td></tr></table></figure></li><li><p>放置Hive配置文件</p><p>为了让Spark能够访问Hive，需要把Hive的配置文件hive-site.xml拷贝到Spark的conf目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf</span><br><span class="line">cp /usr/local/hive/conf/hive-site.xml .</span><br><span class="line">ll</span><br></pre></td></tr></table></figure></li><li><p>测试是否集成成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>然后输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-在Hive中创建数据库和表"><a href="#2-在Hive中创建数据库和表" class="headerlink" title="2.在Hive中创建数据库和表"></a>2.在Hive中创建数据库和表</h3><p>首先启动MySQL数据库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure><p>​    由于Hive是基于Hadoop的数据仓库，使用HiveQL语言撰写的查询语句，最终都会被Hive自动解析成MapReduce任务由Hadoop去具体执行，因此，需要启动Hadoop，然后再启动Hive。<br>然后执行以下命令启动Hadoop：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>Hadoop启动成功以后，可以再启动Hive：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>然后在hive命令提示符内进行操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> sparktest;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> sparktest.student(</span><br><span class="line">&gt; <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">&gt; <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">&gt; gender <span class="keyword">string</span>,</span><br><span class="line">&gt; age <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">use</span> sparktest;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'Xueqian'</span>,<span class="string">'F'</span>,<span class="number">23</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'Weiliang'</span>,<span class="string">'M'</span>,<span class="number">24</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>通过上面操作，我们就在Hive中创建了sparktest.student表，这个表有两条数据。</p><h3 id="3-连接Hive读写数据"><a href="#3-连接Hive读写数据" class="headerlink" title="3.连接Hive读写数据"></a>3.连接Hive读写数据</h3><p>​    现在我们看如何使用Spark读写Hive中的数据。注意，操作到这里之前，你一定已经按照前面的各个操作步骤，启动了Hadoop、Hive、MySQL和spark-shell（包含Hive支持）。在进行编程之前，我们需要做一些准备工作，我们需要修改“/usr/local/sparkwithhive/conf/spark-env.sh”这个配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf/</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><p>这样就使用vim编辑器打开了spark-env.sh这个文件，输入下面内容：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$CLASSPATH</span>:/usr/<span class="built_in">local</span>/hive/lib</span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/usr/<span class="built_in">local</span>/hive/conf</span><br><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/usr/<span class="built_in">local</span>/hive/lib/mysql-connector-java-5.1.26-bin.jar</span><br></pre></td></tr></table></figure><p>保存并推出，然后启动spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>然后在shell界面中输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="string">"spark-warehouse"</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark Hive Example"</span>).config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line">sql(<span class="string">"SELECT * FROM sparktest.student"</span>).show()</span><br></pre></td></tr></table></figure><p>然后再开一个命令行界面，启动hive界面，查看spark-shell中对hive表插入数据的结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>然后输入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> sparktest;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>然后在spark-shell中进行数据插入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="comment">//下面我们设置两条数据表示两个学生信息</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="string">"3 Rongcheng M 26"</span>,<span class="string">"4 Guanhua M 27"</span>)).map(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment">//下面要设置模式信息</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"gender"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)))</span><br><span class="line"><span class="comment">//下面创建Row对象，每个Row对象都是rowRDD中的一行</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt))</span><br><span class="line"><span class="comment">//建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来</span></span><br><span class="line"><span class="keyword">val</span> studentDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"><span class="comment">//查看studentDF</span></span><br><span class="line">studentDF.show()</span><br><span class="line"><span class="comment">//下面注册临时表</span></span><br><span class="line">studentDF.registerTempTable(<span class="string">"tempTable"</span>)</span><br><span class="line">sql(<span class="string">"insert into sparktest.student select * from tempTable"</span>)</span><br></pre></td></tr></table></figure><p>然后切换到hive窗口，查看数据库内容变化</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>能够查询到新增数据结果，则说明操作成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、Hive安装&quot;&gt;&lt;a href=&quot;#一、Hive安装&quot; class=&quot;headerlink&quot; title=&quot;一、Hive安装&quot;&gt;&lt;/a&gt;一、Hive安装&lt;/h2&gt;&lt;h3 id=&quot;1-Hive简介&quot;&gt;&lt;a href=&quot;#1-H
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="Environment" scheme="http://www.ihoge.cn/tags/Environment/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘的9大成熟技术和商业应用</title>
    <link href="http://www.ihoge.cn/2018/DataMining.html"/>
    <id>http://www.ihoge.cn/2018/DataMining.html</id>
    <published>2018-04-10T17:05:59.000Z</published>
    <updated>2018-04-10T17:39:15.681Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>基于数据挖掘的9大主要成熟技术以及在数据化运营中的主要应用：<br>1、决策树<br>2、神经网络<br>3、回归<br>4、关联规则<br>5、聚类<br>6、贝叶斯分类<br>7、支持向量机<br>8、主成分分析<br>9、假设检验</p><h3 id="1-决策树"><a href="#1-决策树" class="headerlink" title="1　决策树"></a>1　决策树</h3><p>决策树（Decision Tree）是一种非常成熟的、普遍采用的数据挖掘技术。之所以称为树，是因为其建模过程类似一棵树的成长过程，即从根部开始，到树干，到分枝，再到细枝末节的分叉，最终生长出一片片的树叶。在决策树里，所分析的数据样本先是集成为一个树根，然后经过层层分枝，最终形成若干个结点，每个结点代表一个结论。</p><p>决策树算法之所以在数据分析挖掘应用中如此流行，主要原因在于决策树的构造不需要任何领域的知识，很适合探索式的知识发掘，并且可以处理高维度的数据。在众多的数据挖掘、统计分析算法中，决策树最大的优点在于它所产生的一系列从树根到树枝（或树叶）的规则，可以很容易地被分析师和业务人员理解，而且这些典型的规则甚至不用整理（或稍加整理），就是现成的可以应用的业务优化策略和业务优化路径。另外，决策树技术对数据的分布甚至缺失非常宽容，不容易受到极值的影响。</p><p>目前，最常用的3种决策树算法分别是<code>CHAID、CART</code>和<code>ID3（包括后来的C4.5，乃至C5.0）</code>。</p><p>CHAID(Chi-square Automatic Interaction Detector)算法的历史较长，中文简称为卡方自动相互关系检测。CHAID依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量，CHAID应用的前提是因变量为类别型变量（Category）。</p><p>CART(Classification and Regression Tree)算法产生于20世纪80年代中期，中文简称为分类与回归树，CART的分割逻辑与CHAID相同，每一层的划分都是基于对所有自变量的检验和选择上的。但是，CART采用的检验标准不是卡方检验，而是基尼系数（Gini）等不纯度的指标。两者最大的区别在于CHAID采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而CART则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART所生产的决策树是二分的，每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复使用多次（分割），这些都是不同于CHAID的特点。另外，如果是自变量存在数据缺失（Missing）的情况，CART的处理方式将会是寻找一个替代数据来代替（填充）缺失值，而CHAID则是把缺失数值作为单独的一类数值。</p><p><code>ID3</code>（Iterative Dichotomiser）算法与CART是同一时期产生的，中文简称为迭代的二分器，其最大的特点在于自变量的挑选标准是：基于信息增益的度量选择具有最高信息增益的属性作为结点的分裂（分割）属性，其结果就是对分割后的结点进行分类所需的信息量最小，这也是一种划分纯度的思想。至于之后发展起来的<code>C4.5</code>可以理解为ID3的发展版（后继版），两者的主要区别在于C4.5采用信息<code>增益率</code>（Gain Ratio）代替了ID3中的信息增益度量，如此替换的主要原因是信息增益度量有个缺点，就是倾向于选择具有大量值的属性。这里给个极端的例子，对于Member_Id的划分，每个Id都是一个最纯的组，但是这样的划分没有任何实际意义。而C4.5所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（SplitInformation）对其进行规范化约束。</p><p>决策树技术在数据化运营中的主要用途体现在：作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景，决策树甚至可以作为其他建模技术前期进行变量筛选的一种方法，即通过决策树的分割来筛选有效地输入自变量。</p><h3 id="2-神经网络"><a href="#2-神经网络" class="headerlink" title="2　神经网络"></a>2　神经网络</h3><p>神经网络（Neural Network）是通过数学算法来模仿人脑思维的，它是数据挖掘中机器学习的典型代表。神经网络是人脑的抽象计算模型，我们知道人脑中有数以百亿个神经元（人脑处理信息的微单元），这些神经元之间相互连接，使得人的大脑产生精密的逻辑思维。而数据挖掘中的“神经网络”也是由大量并行分布的人工神经元（微处理单元）组成的，它有通过调整连接强度从经验知识中进行学习的能力，并可以将这些知识进行应用。</p><p>简单来讲，“神经网络”就是通过输入多个非线性模型以及不同模型之间的加权互联（加权的过程在隐蔽层完成），最终得到一个输出模型。其中，隐蔽层所包含的就是非线性函数。</p><p>目前最主流的“神经网络”算法是<code>反馈传播（Backpropagation）</code>，该算法在多层前向型（Multilayer Feed-Forward）神经网络上进行学习，而多层前向型神经网络又是由一个输入层、一个或多个隐蔽层以及一个输出层组成的，“神经网络”的典型结构如图所示。<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fq81j3ffllj30s20dawfw.jpg" alt=""></p><p>由于“神经网络”拥有特有的大规模并行结构和信息的并行处理等特点，因此它具有良好的<code>自适应性</code>、<code>自组织性</code>和<code>高容错性</code>，并且具有较强的学习、记忆和识别功能。目前神经网络已经在信号处理、模式识别、专家系统、预测系统等众多领域中得到广泛的应用。</p><p>“神经网络”的主要缺点就是其知识和<code>结果的不可解释性</code>，没有人知道隐蔽层里的非线性函数到底是如何处理自变量的，“神经网络”应用中的产出物在很多时候让人看不清其中的逻辑关系。但是，它的这个缺点并没有影响该技术在数据化运营中的广泛应用，甚至可以这样认为，正是因为其结果具有不可解释性，反而更有可能促使我们发现新的没有认识到的规律和关系。</p><p>在利用“神经网络”技术建模的过程中，有以下5个因素对模型结果有着重大影响：</p><p>❑层数。</p><p>❑每层中输入变量的数量。</p><p>❑联系的种类。</p><p>❑联系的程度。</p><p>❑转换函数，又称激活函数或挤压函数。</p><p>“神经网络”技术在数据化运营中的主要用途体现在：作为<code>分类</code>、<code>预测</code>问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。</p><h3 id="3-回归"><a href="#3-回归" class="headerlink" title="3　回归"></a>3　回归</h3><p>回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。其中，<strong>在数据化运营中更多使用的是逻辑斯蒂回归</strong>，它又包括响应预测、分类划分等内容。</p><p>多元线性回归主要描述一个因变量如何随着一批自变量的变化而变化，其回归公式（回归方程）就是因变量与自变量关系的数据反映。因变量的变化包括两部分：系统性变化与随机变化，其中，系统性变化是由自变量引起的（自变量可以解释的），随机变化是不能由自变量解释的，通常也称作残值。</p><p>在用来估算多元线性回归方程中自变量系数的方法中，最常用的是最小二乘法，即找出一组对应自变量的相应参数，以使因变量的实际观测值与回归方程的预测值之间的总方差减到最小。</p><p>对多元线性回归方程的参数估计，是基于下列假设的：</p><p>❑输入变量是确定的变量，不是随机变量，而且输入的变量间无线性相关，即无共线性。</p><p>❑随机误差的期望值总和为零，即随机误差与自变量不相关。</p><p>❑随机误差呈现正态分布 [1]。</p><p>如果不满足上述假设，就不能用最小二乘法进行回归系数的估算了。</p><p><strong>逻辑斯蒂回归（Logistic Regression）相比于线性回归来说，在数据化运营中有更主流更频繁的应用</strong>，主要是因为该分析技术可以很好地回答诸如预测、分类等数据化运营常见的分析项目主题。简单来讲，凡是预测“两选一”事件的可能性（比如，“响应”还是“不响应”；“买”还是“不买”；“流失”还是“不流失”），都可以采用逻辑斯蒂回归方程。</p><p>逻辑斯蒂回归预测的因变量是介于0和1之间的概率，如果对这个概率进行换算，就可以用线性公式描述因变量与自变量的关系了，具体公式如下：<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq81p6q5gnj30al021t8k.jpg" alt=""></p><p>与多元线性回归所采用的最小二乘法的参数估计方法相对应，最大似然法是逻辑斯蒂回归所采用的参数估计方法，其原理是找到这样一个参数，可以让样本数据所包含的观察值被观察到的可能性最大。这种寻找最大可能性的方法需要反复计算，对计算能力有很高的要求。最大似然法的优点是在大样本数据中参数的估值稳定、偏差小，估值方差小。</p><h3 id="4-关联规则"><a href="#4-关联规则" class="headerlink" title="4　关联规则"></a>4　关联规则</h3><p>关联规则（Association Rule）是在数据库和数据挖掘领域中被发明并被广泛研究的一种重要模型，关联规则数据挖掘的主要目的是<code>找出数据集中的频繁模式（Frequent Pattern）</code>，即多次重复出现的模式和并发关系（Cooccurrence Relationships），<code>即同时出现的关系，频繁和并发关系也称作关联（Association）</code>。</p><p>应用关联规则最经典的案例就是购物篮分析（Basket Analysis），通过分析顾客购物篮中商品之间的关联，可以挖掘顾客的购物习惯，从而帮助零售商更好地制定有针对性的营销策略。</p><p>以下列举一个简单的关联规则的例子：</p><p>婴儿尿不湿→啤酒[支持度=10%，置信度=70%]</p><p>这个规则表明，在所有顾客中，有10%的顾客同时购买了婴儿尿不湿和啤酒，而在所有购买了婴儿尿不湿的顾客中，占70%的人同时还购买了啤酒。发现这个关联规则后，超市零售商决定把婴儿尿不湿和啤酒摆放在一起进行促销，结果明显提升了销售额，这就是发生在沃尔玛超市中“啤酒和尿不湿”的经典营销案例。</p><p>上面的案例是否让你对支持度和置信度有了一定的了解？事实上，支持度（Support）和置信度（Confidence）是衡量关联规则强度的两个重要指标，它们分别反映着所发现规则的有用性和确定性。其中<code>支持度：规则X→Y的支持度是指事物全集中包含X∪Y的事物百分比</code>。支持度主要衡量规则的有用性，如果支持度太小，则说明相应规则只是偶发事件。在商业实战中，偶发事件很可能没有商业价值；<code>置信度：规则X→Y的置信度是指既包含了X又包含了Y的事物数量占所有包含了X的事物数量的百分比</code>。置信度主要衡量规则的确定性（可预测性），如果置信度太低，那么从X就很难可靠地推断出Y来，置信度太低的规则在实践应用中也没有太大用处。</p><p>在众多的关联规则数据挖掘算法中，最著名的就是<code>Apriori</code>算法，该算法具体分为以下两步进行：</p><p>（1）生成所有的频繁项目集。一个频繁项目集（Frequent Itemset）是一个支持度高于最小支持度阀值（min-sup）的项目集。</p><p>（2）从频繁项目集中生成所有的可信关联规则。这里可信关联规则是指置信度大于最小置信度阀值（min-conf）的规则。</p><p>关联规则算法不但在数值型数据集的分析中有很大用途，而且在纯文本文档和网页文件中，也有着重要用途。比如发现单词间的并发关系以及Web的使用模式等，这些都是Web数据挖掘、搜索及推荐的基础。</p><h3 id="5-聚类"><a href="#5-聚类" class="headerlink" title="5　聚类"></a>5　聚类</h3><p>聚类（Clustering）分析有一个通俗的解释和比喻，那就是“物以类聚，人以群分”。针对几个特定的业务指标，可以将观察对象的群体按照相似性和相异性进行不同群组的划分。经过划分后，每个群组内部各对象间的相似度会很高，而在不同群组之间的对象彼此间将具有很高的相异度。</p><p>聚类分析的算法可以分为<code>划分的方法</code>（Partitioning Method）、<code>层次的方法</code>（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。</p><p>对于划分的方法（Partitioning Method），<code>当给定m个对象的数据集，以及希望生成的细分群体数量K后，即可采用这种方法将这些对象分成K组</code>（K≤m），使得每个组内对象是相似的，而组间的对象是相异的。最常用的划分方法是<code>K-Means</code>方法，其具体原理是：首先，随机选择K个对象，并且所选择的每个对象都代表一个组的初始均值或初始的组中心值；对剩余的每个对象，根据其与各个组初始均值的距离，将它们分配给最近的（最相似）小组；然后，重新计算每个小组新的均值；这个过程不断重复，直到所有的对象在K组分布中都找到离自己最近的组。</p><p><code>层次的方法（Hierarchical Method）则是指依次让最相似的数据对象两两合并，这样不断地合并，最后就形成了一棵聚类树</code>。</p><p>聚类技术在数据分析和数据化运营中的主要用途表现在：既可以直接作为模型对观察对象进行群体划分，为业务方的精细化运营提供具体的细分依据和相应的运营方案建议，又可在数据处理阶段用作数据探索的工具，包括发现离群点、孤立点，数据降维的手段和方法，通过聚类发现数据间的深层次的关系等。</p><h3 id="6-贝叶斯分类方法"><a href="#6-贝叶斯分类方法" class="headerlink" title="6　贝叶斯分类方法"></a>6　贝叶斯分类方法</h3><p>贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。比如通过一个给定观察值的相关属性来判断其属于一个特定类别的概率。贝叶斯分类方法是基于贝叶斯定理的，已经有研究表明，朴素贝叶斯分类方法作为一种简单贝叶斯分类算法甚至可以跟决策树和神经网络算法相媲美。</p><p>贝叶斯定理的公式如下：<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq820l60g6j307201smx0.jpg" alt=""></p><p>其中，X表示n个属性的测量描述；H为某种假设，比如假设某观察值X属于某个特定的类别C；对于分类问题，希望确定P(H|X)，即能通过给定的X的测量描述，来得到H成立的概率，也就是给出X的属性值，计算出该观察值属于类别C的概率。因为P(H|X)是后验概率（Posterior Probability），所以又称其为在条件X下，H的后验概率。</p><p>举例来说，假设数据属性仅限于用教育背景和收入来描述顾客，而X是一位硕士学历，收入10万元的顾客。假定H表示假设我们的顾客将购买苹果手机，则P(H|X)表示当我们知道顾客的教育背景和收入情况后，该顾客将购买苹果手机的概率；相反，P(X|H)则表示如果已知顾客购买苹果手机，则该顾客是硕士学历并且收入10万元的概率；而P(X)则是X的先验概率，表示顾客中的某个人属于硕士学历且收入10万元的概率；P(H)也是先验概率，只不过是任意给定顾客将购买苹果手机的概率，而不会去管他们的教育背景和收入情况。</p><p>从上面的介绍可见，相比于先验概率P(H)，后验概率P(H|X)基于了更多的信息（比如顾客的信息属性），而P(H)是独立于X的。</p><p>贝叶斯定理是朴素贝叶斯分类法（Naive Bayesian Classifier）的基础，如果给定数据集里有M个分类类别，通过朴素贝叶斯分类法，可以预测给定观察值是否属于具有最高后验概率的特定类别，也就是说，朴素贝叶斯分类方法预测X属于类别$C_i$时，表示当且仅当<br>$$P(C_i|X)＞P(C_j|X)1≤j≤m，j≠i$$<br>此时如果最大化$P(C_i|X)$，其$P(C_i|X)$最大的类$C_i$被称为最大后验假设，根据贝叶斯定理可知，由于$P(X)$对于所有的类别是均等的，因此只需要$P(X|C_i)P(C_i)$取最大即可。</p><p>为了预测一个未知样本$X$的类别，可对每个类别$C_i$估算相应的$P(X|C_i)P(C_i)$。样本$X$归属于类别$C_i$，当且仅当<br>$$P(C_i|X)＞P(C_j|X)1≤j≤m，j≠i$$</p><p>贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。</p><h3 id="7-支持向量机"><a href="#7-支持向量机" class="headerlink" title="7　支持向量机"></a>7　支持向量机</h3><p>支持向量机（Support Vector Machine）是Vapnik等人于1995年率先提出的，是近年来机器学习研究的一个重大成果。与传统的神经网络技术相比，支持向量机不仅结构简单，而且各项技术的性能也明显提升，因此它成为当今机器学习领域的热点之一。</p><p>作为一种新的分类方法，支持向量机以结构风险最小为原则。在线性的情况下，就在原空间寻找两类样本的最优分类超平面。在非线性的情况下，它<code>使用一种非线性的映射，将原训练集数据映射到较高的维上</code>。在新的维上，它搜索线性最佳分离超平面。使用一个适当的对足够高维的非线性映射，两类数据总可以被超平面分开。</p><p>支持向量机的基本概念如下：</p><p>设给定的训练样本集为${(x_1,y_1),(x_2,y_2),…,(x_n,y_n)}$，其中$x_i∈R_n,y∈[-1,1]$。</p><p>再假设该训练集可被一个超平面线性划分，设该超平面记为$(w,x)+b=0$。</p><p>支持向量机的基本思想可用下图的两维情况举例说明。（线性可分情况下的最优分类线）<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fq82c0jkiaj30m50g3dgg.jpg" alt=""></p><p>图中圆形和方形代表两类样本，H为分类线，H1、H2，分别为过各类样本中离分类线最近的样本并且平行于分类线的直线，它们之间的距离叫做<code>分类间隔（Margin）</code>。所谓的最优分类线就是要求分类线<code>不但能将两类正确分开（训练错误为0），而且能使分类间隔最大</code>。推广到高维空间，最优分类线就成了最优分类面。</p><p>其中，距离超平面最近的一类向量被称为<code>支持向量（Support Vector）</code>，<code>一组支持向量可以唯一地确定一个超平面</code>。通过学习算法，SVM可以自动寻找出那些对分类有较好区分能力的支持向量，由此构造出的分类器则可以最大化类与类的间隔，因而有较好的适应能力和较高的分类准确率。</p><p>支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的——对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合 。</p><p>支持向量机主要用在预测、分类这样的实际分析需求场景中。</p><h3 id="8-主成分分析"><a href="#8-主成分分析" class="headerlink" title="8　主成分分析"></a>8　主成分分析</h3><p>严格意义上讲，主成分分析（Principal Components Analysis）属于传统的统计分析技术范畴，但是正如本章前面所阐述的，统计分析与数据挖掘并没有严格的分割，因此在数据挖掘实战应用中也常常会用到这种方式，从这个角度讲，主成分分析也是数据挖掘商业实战中常用的一种分析技术和数据处理技术。</p><p>主成分分析会通过线性组合将多个原始变量合并成若干个主成分，这样<code>每个主成分都变成了原始变量的线性组合</code>。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。</p><p>主成分分析的主要步骤如下：</p><p>1）通常要先进行各变量的标准化工作，标准化的目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，从而让不同的变量经过标准化处理后可以有平等的分析和比较基础。</p><p>2）选择<code>协方差阵</code>或者<code>相关阵计算特征根</code>及对应的<code>特征向量</code>。</p><p>3）<code>计算方差贡献率</code>，并根据方差贡献率的阀值选取合适的主成分个数。</p><p>4）根据主成分载荷的大小对选择的主成分进行命名。</p><p>5）根据主成分载荷计算各个主成分的得分。</p><p>将主成分进行推广和延伸即成为<code>因子分析（Factor Analysis）</code>，因子分析在<code>综合原始变量信息的基础上将会力图构筑若干个意义较为明确的公因子</code>；也就是说，采用少数几个因子描述多个指标之间的联系，将比较密切的变量归为同一类中，每类变量即是一个因子。之所以称其为因子，是因为它们实际上是<code>不可测量的，只能解释</code>。</p><p><code>主成分分析是因子分析的一个特例</code>，两者的区别和联系主要表现在以下方面：</p><p>❑主成分分析会把主成分表示成各个原始变量的线性组合，而因子分析则把原始变量表示成各个因子的线性组合。这个区别最直观也最容易记住。</p><p>❑主成分分析的重点在于解释原始变量的总方差，而因子分析的重点在于解释原始变量的协方差。</p><p>❑在主成分分析中，有几个原始变量就有几个主成分，而在因子分析中，因子个数可以根据业务场景的需要人为指定，并且指定的因子数量不同，则分析结果也会有差异。</p><p>❑在主成分分析中，给定的协方差矩阵或者相关矩阵的特征值是唯一时，主成分也是唯一的，但是在因子分析中，因子不是唯一的，并且通过旋转可以得到不同的因子。</p><p>主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。</p><h3 id="9-假设检验"><a href="#9-假设检验" class="headerlink" title="9　假设检验"></a>9　假设检验</h3><p>假设检验（Hypothesis Test）是现代统计学的基础和核心之一，其主要研究在一定的条件下，总体是否具备某些特定特征。</p><p>假设检验的基本原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否发生。如果在一次试验中，小概率事件发生了，那么说明假设在一定的显著性水平下不可靠或者不成立；如果在一次试验中，小概率事件没有发生，那么也只能说明没有足够理由相信假设是错误的，但是也并不能说明假设是正确的，因为无法收集到所有的证据来证明假设是正确的。</p><p>假设检验的结论是在一定的显著性水平下得出的。因此，当采用此方法观测事件并下结论时，有可能会犯错，这些错误主要有两大类：</p><p>❑第Ⅰ类错误：当原假设为真时，却否定它而犯的错误，即拒绝正确假设的错误，也叫弃真错误。犯第Ⅰ类错误的概率记为α，通常也叫α错误，α=1-置信度。</p><p>❑第Ⅱ类错误：当原假设为假时，却肯定它而犯的错误，即接受错误假设的错误，也叫纳伪错误。犯第Ⅱ类错误的概率记为β，通常也叫β错误。</p><p>上述这两类错误在其他条件不变的情况下是相反的，即α增大时，β就减小；α减小时，β就增大。α错误容易受数据分析人员的控制，因此在假设检验中，通常会先控制第Ⅰ类错误发生的概率α，具体表现为：在做假设检验之前先指定一个α的具体数值，通常取0.05，也可以取0.1或0.001。</p><p>在数据化运营的商业实践中，假设检验最常用的场景就是用于“运营效果的评估”上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;基于数据挖掘的9大主要成熟技术以及在数据化运营中的主要应用：&lt;br&gt;1、决策树&lt;br&gt;2、神经网络&lt;br&gt;3、回归&lt;br&gt;4、关联规则&lt;br&gt;5、聚类&lt;br&gt;6、贝叶斯分类&lt;br&gt;7、支持向量机&lt;br&gt;8、主成分分析&lt;br&gt;9、假设检验&lt;
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="数据挖掘" scheme="http://www.ihoge.cn/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>PCA主成分分析+SVM人脸识别准确率97%+</title>
    <link href="http://www.ihoge.cn/2018/PCA+SVM%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB.html"/>
    <id>http://www.ihoge.cn/2018/PCA+SVM人脸识别.html</id>
    <published>2018-04-09T17:05:59.000Z</published>
    <updated>2018-04-10T10:01:34.819Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>这里使用的测试数据共包含40位人员照片，每个人10张照片。也可登陆<a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facesataglance.html" target="_blank" rel="noopener">http://www.cl.cam.ac.uk/research/dtg/attarchive/facesataglance.html</a> 查看400张照片的缩略图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_olivetti_faces</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level = logging.INFO, format=<span class="string">"%(asctime)s %(message)s"</span>) <span class="comment"># 这里INFO必须大写</span></span><br><span class="line"></span><br><span class="line">data_home = <span class="string">'code/datasets/'</span></span><br><span class="line">logging.info(<span class="string">"开始加载数据"</span>)</span><br><span class="line">faces = fetch_olivetti_faces(data_home=data_home)</span><br><span class="line">logging.info(<span class="string">"加载完成"</span>)</span><br></pre></td></tr></table></figure><p>这里做下简单的解释：</p><p>加载的图片保存在faces变量里，sklaern已经把每张照片处理成剪切掉头发部分并且64x64大小且人脸居中显示。在真实生产环境中这一步很重要，否则模型将被大量的噪声干扰（即照片背景，变化的发型等，这些特征都应该排除在输入特征之外）。最后要成功下载数据集还需要安装Python图片图里工具Pillow否则无法对图片解码。下面输出下数据的概要信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = faces.data</span><br><span class="line">y = faces.target</span><br><span class="line"></span><br><span class="line">targets = np.unique(faces.target)</span><br><span class="line">target_names = np.array([<span class="string">"p%d"</span> % t <span class="keyword">for</span> t <span class="keyword">in</span> targets]) <span class="comment">#给每个人做标签</span></span><br><span class="line">n_targets = target_name.shape[<span class="number">0</span>]</span><br><span class="line">n_samples, h, w = faces.images.shape</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Samples count:&#123;&#125;\nTarget count:&#123;&#125;'</span>.format(n_samples, n_targets))</span><br><span class="line">print(<span class="string">'Image size:&#123;&#125;x&#123;&#125;\nData shape:&#123;&#125;'</span>.format(w, h, X.shape))</span><br></pre></td></tr></table></figure><pre><code>Samples count:400Target count:40Image size:64x64Data shape:(400, 4096)</code></pre><p>由输出可知，共有40人，照片总量400，输入特征(64x64=4096)个。</p><p>为了直观观察数据，从每个人物的照片里随机选择一张显示，定义下画图工具：</p><p>其中输入参数images是一个二维数据，每一行都是一个图片数据。在加载数据时，fech_ollivetti_faces()函数已经自动做了预处理，图片的每个像素的RBG值都转换成了[0,1]浮点数。因此，画出来的照片也是黑白的。子图片识别领域一般用黑白照片就可以了，减少计算量的同时也更加准确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gallery</span><span class="params">(images, titles, h, w, n_row=<span class="number">2</span>, n_col=<span class="number">5</span>)</span>:</span></span><br><span class="line"><span class="comment">#     显示图片阵列：</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">2</span>*n_col, <span class="number">2.2</span>*n_row),dpi=<span class="number">140</span>)</span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0</span>, left=<span class="number">.01</span>, right=<span class="number">.99</span>, top=<span class="number">.90</span>, hspace=<span class="number">.01</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_row * n_col):</span><br><span class="line">        plt.subplot(n_row, n_col, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i].reshape((h,w)), cmap=plt.cm.gray)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">n_row = <span class="number">2</span></span><br><span class="line">n_col = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">sample_images = <span class="keyword">None</span></span><br><span class="line">sample_titles = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_targets):</span><br><span class="line">    people_images = X[y==i]  <span class="comment"># 注意这里传入i</span></span><br><span class="line">    people_sample_index = np.random.randint(<span class="number">0</span>, people_images.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    people_sample_image = people_images[people_sample_index, :]</span><br><span class="line">    <span class="keyword">if</span> sample_images <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        sample_images = np.concatenate((sample_images, people_sample_image), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sample_images =people_sample_image</span><br><span class="line">    sample_titles.append(target_names[i])   <span class="comment"># 这里target_names是在前面生成的标签</span></span><br><span class="line">    </span><br><span class="line">plot_gallery(sample_images, sample_titles, h, w, n_row, n_col)</span><br><span class="line"></span><br><span class="line"><span class="comment">#代码中X[y=i]可以选择除特定的所有照片，随机选出来的照片放在sample.images数组对象里，最后调用之前定义的函数把照片画出来。</span></span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq6vy8axmgj31bo0gs0xb.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h2 id="支持向量机第一次尝试"><a href="#支持向量机第一次尝试" class="headerlink" title="支持向量机第一次尝试"></a>支持向量机第一次尝试</h2><p>直接食用支持向量机来实现人脸识别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">clf = SVC(class_weight=<span class="string">'balanced'</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">"耗时：&#123;&#125;秒"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>耗时：1.0220119953155518秒</code></pre><h3 id="1、接着对人脸数据进行预测：使用confusion-matrix查看准确性："><a href="#1、接着对人脸数据进行预测：使用confusion-matrix查看准确性：" class="headerlink" title="1、接着对人脸数据进行预测：使用confusion_matrix查看准确性："></a><strong>1、接着对人脸数据进行预测：使用<code>confusion_matrix</code>查看准确性：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">cm = confusion_matrix(y_test, y_pred, labels=range(n_targets))</span><br><span class="line">print(<span class="string">"confusion_matrix:\n"</span>)</span><br><span class="line"><span class="comment"># np.set_printoptions(threshold=np.nan)</span></span><br><span class="line">print(cm[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>confusion_matrix:[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</code></pre><p>上面<code>np.set_printoptions()</code>是为了确保cm完整输出，这是因为这个数组是40x40的，默认情况下不会全部输出。??</p><h3 id="2、使用classification-report查看准确性"><a href="#2、使用classification-report查看准确性" class="headerlink" title="2、使用classification_report查看准确性"></a><strong>2、使用<code>classification_report</code>查看准确性</strong></h3><p>但是很明显输出结果效果很差。 因为<code>confusion_matrix</code>理想的输出是矩阵的对角线上有数组，其他地方都为0，而且这里很多图片都被预测成索引为12的类别了。我买再来看下<code>classification_report</code>的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">print(classification_report(y_test, y_pred, target_names = target_names)) <span class="comment">#这里y_test和y_pred不要颠倒。</span></span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support         p0       0.00      0.00      0.00         1         p1       0.00      0.00      0.00         3         p2       0.00      0.00      0.00         2         p3       0.00      0.00      0.00         1         p4       0.00      0.00      0.00         1         p5       0.00      0.00      0.00         1         p6       0.00      0.00      0.00         4         p7       0.00      0.00      0.00         2         p8       0.00      0.00      0.00         4         p9       0.00      0.00      0.00         2        p10       0.00      0.00      0.00         1        p11       0.00      0.00      0.00         0        p12       0.00      0.00      0.00         4        p13       0.00      0.00      0.00         4        p14       0.00      0.00      0.00         1        p15       0.00      0.00      0.00         1        p16       0.00      0.00      0.00         3        p17       0.00      0.00      0.00         2        p18       0.00      0.00      0.00         2        p19       0.00      0.00      0.00         2        p20       0.00      0.00      0.00         1        p21       0.00      0.00      0.00         2        p22       0.00      0.00      0.00         3        p23       0.00      0.00      0.00         2        p24       0.00      0.00      0.00         3        p25       0.00      0.00      0.00         3        p26       0.00      0.00      0.00         2        p27       0.00      0.00      0.00         2        p28       0.00      0.00      0.00         0        p29       0.00      0.00      0.00         2        p30       0.00      0.00      0.00         2        p31       0.00      0.00      0.00         3        p32       0.00      0.00      0.00         2        p33       0.00      0.00      0.00         2        p34       0.00      0.00      0.00         0        p35       0.00      0.00      0.00         2        p36       0.00      0.00      0.00         3        p37       0.00      0.00      0.00         1        p38       0.00      0.00      0.00         2        p39       0.00      0.00      0.00         2avg / total       0.00      0.00      0.00        80/Users/hadoop/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.  &apos;precision&apos;, &apos;predicted&apos;, average, warn_for)/Users/hadoop/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.  &apos;recall&apos;, &apos;true&apos;, average, warn_for)</code></pre><p>结果不出预料，果然很差。</p><p>这是因为：<strong>这里把每个像素都作为一个输入特征来处理，这样那个数据噪声太严重了，模型根本没有办法对训练数据集进行拟合。这里有4096个特征向量，可是数据集大小才400个，比特征个数少了太多，而且分出20%作为测试集。这种情况下根本无法进行准确的训练和预测。</strong></p><h2 id="使用PCA来处理数据集"><a href="#使用PCA来处理数据集" class="headerlink" title="使用PCA来处理数据集"></a>使用PCA来处理数据集</h2><h3 id="选择-k-值"><a href="#选择-k-值" class="headerlink" title="选择$k$值"></a>选择$k$值</h3><p>解决上述问题的办法有两种，一个是加大数据样本量（在这里这个不太现实），或者使用PCA给数据降维，值选择前k个最重要的特征。</p><p>这里我们根据PCA算法来计算失真程度来确定k值。</p><p>在sklearn里，可以从PCA模型的<code>explained_variance_ratio_</code>变量里获取经PCA处理后的数据还原率。这是一个数组，所有元素求和即可知道选择的$k$值的数据还原率。随着$k$的增大，数值会无限接近于1。</p><p>利用这一特征，可以让$k$取值10～300之间，每个30取一次样。针对这里的情况选择失真度小于5%即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Exploring explained variance ratio for dataset ..."</span>)</span><br><span class="line">candidate_components = range(<span class="number">10</span>, <span class="number">300</span>, <span class="number">30</span>)</span><br><span class="line">explained_ratios = []</span><br><span class="line">t = time()</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> candidate_components:</span><br><span class="line">    pca = PCA(n_components=c)</span><br><span class="line">    X_pca = pca.fit_transform(X)</span><br><span class="line">    explained_ratios.append(np.sum(pca.explained_variance_ratio_))</span><br><span class="line">print(<span class="string">'Done in &#123;0:.2f&#125;s'</span>.format(time()-t))</span><br></pre></td></tr></table></figure><pre><code>Exploring explained variance ratio for dataset ...Done in 2.17s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(candidate_components, explained_ratios)</span><br><span class="line">plt.xlabel(<span class="string">'Number of PCA Components'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Explained Variance Ratio'</span>)</span><br><span class="line">plt.title(<span class="string">'Explained variance ratio for PCA'</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0.5</span>, <span class="number">1.05</span>, <span class="number">.05</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">300</span>, <span class="number">20</span>));</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fq6vyorh25j30jj0ctdh6.jpg" alt=""></p><p>由上图可知，若要保留95%的数据还原率，$k$值选择120即可。为了更直观的看不同$k$值的区别，这里画出来体验下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">title_prefix</span><span class="params">(prefix, title)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"&#123;&#125;: &#123;&#125;"</span>.format(prefix, title)</span><br><span class="line"></span><br><span class="line">n_row = <span class="number">1</span></span><br><span class="line">n_col = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">sample_images = sample_images[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">sample_titles = sample_titles[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">plotting_images = sample_images</span><br><span class="line">plotting_titles = [title_prefix(<span class="string">'orig'</span>, t) <span class="keyword">for</span> t <span class="keyword">in</span> sample_titles]</span><br><span class="line">candidate_components = [<span class="number">120</span>, <span class="number">75</span>, <span class="number">37</span>, <span class="number">19</span>, <span class="number">8</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> candidate_components:</span><br><span class="line">    print(<span class="string">"Fitting and projecting on PCA(n_components=&#123;&#125;) ..."</span>.format(c))</span><br><span class="line">    t = time()</span><br><span class="line">    pca = PCA(n_components=c)</span><br><span class="line">    pca.fit(X)</span><br><span class="line">    X_sample_pca = pca.transform(sample_images)</span><br><span class="line">    X_sample_inv = pca.inverse_transform(X_sample_pca)</span><br><span class="line">    plotting_images = np.concatenate((plotting_images, X_sample_inv), axis=<span class="number">0</span>)</span><br><span class="line">    sample_title_pca = [title_prefix(<span class="string">'&#123;&#125;'</span>.format(c), t) <span class="keyword">for</span> t <span class="keyword">in</span> sample_titles]</span><br><span class="line">    plotting_titles = np.concatenate((plotting_titles, sample_title_pca), axis=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">"Done in &#123;0:.2f&#125;s"</span>.format(time() - t))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Plotting sample image with different number of PCA conpoments ..."</span>)</span><br><span class="line">plot_gallery(plotting_images, plotting_titles, h, w,</span><br><span class="line">    n_row * (len(candidate_components) + <span class="number">1</span>), n_col)</span><br></pre></td></tr></table></figure><pre><code>Fitting and projecting on PCA(n_components=120) ...Done in 0.18sFitting and projecting on PCA(n_components=75) ...Done in 0.14sFitting and projecting on PCA(n_components=37) ...Done in 0.11sFitting and projecting on PCA(n_components=19) ...Done in 0.07sFitting and projecting on PCA(n_components=8) ...Done in 0.06sPlotting sample image with different number of PCA conpoments ...</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq6vzviawhj31411bntk5.jpg" alt=""></p><h3 id="利用GridSearchCV选出最优参数"><a href="#利用GridSearchCV选出最优参数" class="headerlink" title="利用GridSearchCV选出最优参数"></a>利用GridSearchCV选出最优参数</h3><p>接下来选择$k=120$作为<code>PCA</code>的参数对数据集和测试集进行特征提取，然后调用<code>GridSearchCV</code>选出最优参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_components = <span class="number">120</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Fitting PCA by using training data ..."</span>)</span><br><span class="line">t = time()</span><br><span class="line">pca = PCA(n_components=n_components, svd_solver=<span class="string">'randomized'</span>, whiten=<span class="keyword">True</span>).fit(X_train)</span><br><span class="line">print(<span class="string">"Done in &#123;0:.2f&#125;s"</span>.format(time() - t))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Projecting input data for PCA ..."</span>)</span><br><span class="line">t = time()</span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br><span class="line">print(<span class="string">"Done in &#123;0:.2f&#125;s"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>Fitting PCA by using training data ...Done in 0.16sProjecting input data for PCA ...Done in 0.01s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Searching the best parameters for SVC ..."</span>)</span><br><span class="line">param_grid = &#123;<span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>],</span><br><span class="line">              <span class="string">'gamma'</span>: [<span class="number">0.0001</span>, <span class="number">0.0005</span>, <span class="number">0.001</span>, <span class="number">0.005</span>, <span class="number">0.01</span>]&#125;</span><br><span class="line">clf = GridSearchCV(SVC(kernel=<span class="string">'rbf'</span>, class_weight=<span class="string">'balanced'</span>), param_grid, verbose=<span class="number">2</span>, n_jobs=<span class="number">4</span>)<span class="comment"># 参数n_jobs=4表示启动4个进程</span></span><br><span class="line">clf = clf.fit(X_train_pca, y_train)</span><br><span class="line">print(<span class="string">"Best parameters found by grid search:"</span>)</span><br><span class="line">print(clf.best_params_)</span><br></pre></td></tr></table></figure><pre><code>Searching the best parameters for SVC ...Fitting 3 folds for each of 20 candidates, totalling 60 fits[CV] C=1, gamma=0.0001 ...............................................[CV] C=1, gamma=0.0001 ...............................................[CV] C=1, gamma=0.0001 ...............................................[CV] C=1, gamma=0.0005 ...............................................[CV] ................................ C=1, gamma=0.0001, total=   0.1s[CV] C=1, gamma=0.0005 ...............................................[CV] ................................ C=1, gamma=0.0001, total=   0.1s[CV] C=1, gamma=0.0005 ...............................................[CV] ................................ C=1, gamma=0.0001, total=   0.1s[CV] C=1, gamma=0.001 ................................................[CV] ................................ C=1, gamma=0.0005, total=   0.1s[CV] C=1, gamma=0.001 ................................................[CV] ................................ C=1, gamma=0.0005, total=   0.1s[CV] C=1, gamma=0.001 ................................................[CV] ................................ C=1, gamma=0.0005, total=   0.1s[CV] C=1, gamma=0.01 .................................................[CV] ................................. C=1, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.0001 ...............................................[CV] ................................. C=1, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.0005 ...............................................[CV] ................................. C=1, gamma=0.001, total=   0.1s[CV] C=1, gamma=0.005 ................................................[CV] .................................. C=1, gamma=0.01, total=   0.1s[CV] C=1, gamma=0.01 .................................................[CV] ................................ C=5, gamma=0.0001, total=   0.1s[CV] C=5, gamma=0.0001 ...............................................[CV] ................................ C=5, gamma=0.0005, total=   0.1s[CV] C=5, gamma=0.001 ................................................[CV] ................................. C=1, gamma=0.005, total=   0.1s[CV] C=1, gamma=0.005 ................................................[CV] .................................. C=1, gamma=0.01, total=   0.1s[CV] C=1, gamma=0.01 .................................................[CV] ................................ C=5, gamma=0.0001, total=   0.1s[CV] C=5, gamma=0.0005 ...............................................[CV] ................................. C=5, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.001 ................................................[CV] ................................. C=1, gamma=0.005, total=   0.1s[CV] C=1, gamma=0.005 ................................................[CV] ................................ C=5, gamma=0.0005, total=   0.1s[CV] C=5, gamma=0.0005 ...............................................[CV] .................................. C=1, gamma=0.01, total=   0.1s[CV] C=5, gamma=0.0001 ...............................................[CV] ................................. C=5, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.001 ................................................[CV] ................................. C=1, gamma=0.005, total=   0.1s[CV] ................................ C=5, gamma=0.0005, total=   0.1s[CV] C=5, gamma=0.005 ................................................[CV] C=5, gamma=0.01 .................................................[CV] ................................ C=5, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.0001 ..............................................[CV] ................................. C=5, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.001 ...............................................[CV] ................................. C=5, gamma=0.005, total=   0.1s[CV] C=5, gamma=0.005 ................................................[CV] .................................. C=5, gamma=0.01, total=   0.1s[CV] C=5, gamma=0.01 .................................................[CV] ............................... C=10, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.0005 ..............................................[CV] ................................ C=10, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.001 ...............................................[CV] ................................. C=5, gamma=0.005, total=   0.1s[CV] C=5, gamma=0.005 ................................................[CV] ............................... C=10, gamma=0.0005, total=   0.1s[CV] C=10, gamma=0.0005 ..............................................[CV] .................................. C=5, gamma=0.01, total=   0.1s[CV] C=10, gamma=0.0001 ..............................................[CV] ................................ C=10, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.001 ...............................................[CV] ................................. C=5, gamma=0.005, total=   0.1s[CV] C=5, gamma=0.01 .................................................[CV] ............................... C=10, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.0001 ..............................................[CV] ............................... C=10, gamma=0.0005, total=   0.1s[CV] C=10, gamma=0.0005 ..............................................[CV] ................................ C=10, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.005 ...............................................[CV] .................................. C=5, gamma=0.01, total=   0.1s[CV] C=10, gamma=0.005 ...............................................[CV] ............................... C=10, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.01 ................................................[CV] ............................... C=10, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.0005 ..............................................[CV] ................................ C=10, gamma=0.005, total=   0.1s[CV] C=50, gamma=0.001 ...............................................[CV] ................................ C=10, gamma=0.005, total=   0.1s[CV] C=10, gamma=0.005 ...............................................[CV] ................................. C=10, gamma=0.01, total=   0.1s[CV] C=50, gamma=0.0001 ..............................................[CV] ............................... C=50, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.0005 ..............................................[CV] ................................ C=50, gamma=0.001, total=   0.1s[CV] C=50, gamma=0.001 ...............................................[CV] ............................... C=50, gamma=0.0001, total=   0.1s[CV] ................................ C=10, gamma=0.005, total=   0.1s[CV] C=10, gamma=0.01 ................................................[CV] ............................... C=50, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.0001 ..............................................[CV] C=50, gamma=0.0005 ..............................................[CV] ................................ C=50, gamma=0.001, total=   0.1s[CV] ................................. C=10, gamma=0.01, total=   0.1s[CV] C=10, gamma=0.01 ................................................[CV] C=50, gamma=0.005 ...............................................[CV] ............................... C=50, gamma=0.0001, total=   0.1s[CV] C=50, gamma=0.0001 ..............................................[CV] ............................... C=50, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.001 ...............................................[CV] ................................. C=10, gamma=0.01, total=   0.1s[CV] ................................ C=50, gamma=0.005, total=   0.1s[CV] C=50, gamma=0.005 ...............................................[CV] C=50, gamma=0.005 ...............................................[CV] ............................... C=50, gamma=0.0001, total=   0.1s[CV] ................................ C=50, gamma=0.001, total=   0.1s[CV] ................................ C=50, gamma=0.005, total=   0.1s[CV] ................................ C=50, gamma=0.005, total=   0.1s[CV] C=50, gamma=0.01 ................................................[CV] ................................. C=50, gamma=0.01, total=   0.0s[CV] C=50, gamma=0.01 ................................................[CV] ................................. C=50, gamma=0.01, total=   0.0s[CV] C=50, gamma=0.01 ................................................[CV] ................................. C=50, gamma=0.01, total=   0.0sBest parameters found by grid search:{&apos;C&apos;: 10, &apos;gamma&apos;: 0.0005}[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    1.9s finished</code></pre><h3 id="测试模型准确性"><a href="#测试模型准确性" class="headerlink" title="测试模型准确性"></a>测试模型准确性</h3><p>接着使用这一模型对测试集进行预测，并分别使用<code>confusion_matrix</code>和<code>classification_report</code>查看其效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t = time()</span><br><span class="line">y_pred = clf.best_estimator_.predict(X_test_pca)</span><br><span class="line">cm = confusion_matrix(y_test, y_pred, labels=range(n_targets))</span><br><span class="line">print(<span class="string">"Done in &#123;0:.2f&#125;.\n"</span>.format(time()-t))</span><br><span class="line">print(<span class="string">"confusion matrix:"</span>)</span><br><span class="line">np.set_printoptions(threshold=np.nan)</span><br><span class="line">print(cm[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>Done in 0.01.confusion matrix:[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(y_test, y_pred, target_names=target_names)) <span class="comment">#这里注意y_test和y_pred位置不要颠倒</span></span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support         p0       1.00      1.00      1.00         1         p1       1.00      1.00      1.00         3         p2       1.00      0.50      0.67         2         p3       1.00      1.00      1.00         1         p4       1.00      1.00      1.00         1         p5       1.00      1.00      1.00         1         p6       1.00      0.75      0.86         4         p7       1.00      1.00      1.00         2         p8       1.00      1.00      1.00         4         p9       1.00      1.00      1.00         2        p10       1.00      1.00      1.00         1        p11       1.00      1.00      1.00         4        p12       1.00      1.00      1.00         4        p13       1.00      1.00      1.00         1        p14       1.00      1.00      1.00         1        p15       0.75      1.00      0.86         3        p16       1.00      1.00      1.00         2        p17       1.00      1.00      1.00         2        p18       1.00      1.00      1.00         2        p19       1.00      1.00      1.00         1        p20       1.00      1.00      1.00         2        p21       1.00      1.00      1.00         3        p22       1.00      1.00      1.00         2        p23       1.00      1.00      1.00         3        p24       0.75      1.00      0.86         3        p25       1.00      1.00      1.00         2        p26       1.00      1.00      1.00         2        p27       1.00      1.00      1.00         2        p28       1.00      1.00      1.00         2        p29       1.00      1.00      1.00         3        p30       1.00      1.00      1.00         2        p31       1.00      1.00      1.00         2        p32       1.00      1.00      1.00         2        p33       1.00      1.00      1.00         3        p34       1.00      1.00      1.00         1        p35       1.00      1.00      1.00         2        p36       1.00      1.00      1.00         2avg / total       0.98      0.97      0.97        80/Users/hadoop/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 37, does not match size of target_names, 40  .format(len(labels), len(target_names))</code></pre><h3 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h3><p>效果非常乐观，但是仍有个问题：<strong><code>怎么确定p0～p37分别对应的是哪个一个人？</code></strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;加载数据&quot;&gt;&lt;a href=&quot;#加载数据&quot; class=&quot;headerlink&quot; title=&quot;加载数据&quot;&gt;&lt;/a&gt;加载数据&lt;/h2&gt;&lt;p&gt;这里使用的测试数据共包含40位人员照片，每个人10张照片。也可登陆&lt;a href=&quot;ht
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯--文档分类</title>
    <link href="http://www.ihoge.cn/2018/MultinomialNB.html"/>
    <id>http://www.ihoge.cn/2018/MultinomialNB.html</id>
    <published>2018-04-09T12:10:59.000Z</published>
    <updated>2018-04-10T10:01:37.805Z</updated>
    
    <content type="html"><![CDATA[<h3 id="把文档转换成向量"><a href="#把文档转换成向量" class="headerlink" title="把文档转换成向量"></a>把文档转换成向量</h3><p>TF-IDF是一种统计方法，用以评估一个词语对于一份文档的重要程度。</p><ul><li>TF表示词频， 即：词语在一片文档中出现的次数 ÷ 词语总数</li><li>IDF表示一个词的<strong>逆向文档频率指数</strong>， 即：对（总文档数目÷包含该词语的文档的数目）的商取对数  $log(m / m_{i-in-m})$</li></ul><p>基础原理：词语的重要性随着它在文档中出现的次数成正比例增加，但同时会随着它在语料库中出现的频率呈反比下降。</p><p>sklearn中有包实现了把文档转换成向量的过程，首先把训练用额语料库读入内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_files</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">news_train = load_files(<span class="string">'code/datasets/mlcomp/379/train'</span>)</span><br><span class="line">print(len(news_train.data), <span class="string">"\n"</span>,len(news_train.target_names))</span><br><span class="line">print(<span class="string">"done in &#123;&#125; seconds"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>13180  20done in 6.034918308258057 seconds</code></pre><p>news_train.data是一个数组，包含了所有文档的文本信息。<br>news_train.target_names也是一个数组，包含了所有文档的属性类别，对应的是读取train文件夹时，train文件夹下所有的子文件夹名称。</p><p>该语料库总共有13180个文档，其中分成20个类别，接着需要转换成由TF-IDF表达的权重信息构成向量。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq6g2bkj9sj30hf09lta4.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">vectorizer  = TfidfVectorizer(encoding = <span class="string">'latin-1'</span>)</span><br><span class="line">X_train = vectorizer.fit_transform((d <span class="keyword">for</span>  d <span class="keyword">in</span> news_train.data))</span><br><span class="line">print(<span class="string">"文档 [&#123;0&#125;]特征值的非零个数:&#123;1&#125;"</span>.format(news_train.filenames[<span class="number">0</span>] , X_train[<span class="number">0</span>].getnnz()))</span><br><span class="line">print(<span class="string">"训练集："</span>,X_train.shape)</span><br><span class="line">print(<span class="string">"耗时： &#123;0&#125; s."</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>文档 [code/datasets/mlcomp/379/train/talk.politics.misc/17860-178992]特征值的非零个数:108训练集： (13180, 130274)耗时： 3.740567207336426 s.</code></pre><p><strong>TfidfVectorizer</strong>类是用来把所有的文档转换成矩阵，该矩阵每一行都代表一个文档，一行中的每个元素代表一个对应的词语的重要性，词语的重要性由TF-IDF来表示。其<code>fit_transform()</code>方法是<code>fit()</code>和<code>transform()</code>的结合,<code>fit()</code>先完成语料库分析，提取词典等操作<code>transform()</code>把每篇文档转换为向量，最终构成一个矩阵，保存在<code>X_train</code>里。</p><p>程序输出可以看到该词典总共有130274个词语，即每篇文档都可以转换成一个13274维的向量组。第一篇文档中只有108个非零元素，即这篇文档由108个不重复的单词组成，在这篇文档中出现的这108个单词次的<strong>TF-IDF</strong>会被计算出来，保存在向量的指定位置。这里的到X_train是一个纬度为12180 x 130274的系数矩阵。</p><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">y_train = news_train.target</span><br><span class="line">clf = MultinomialNB(alpha=<span class="number">0.001</span>)  <span class="comment">#alpga表示平滑参数，越小越容易造成过拟合；越大越容易欠拟合。</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train_score:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"耗时：&#123;0&#125;s"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>train_score: 0.9974203338391502耗时：0.23757004737854004s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载测试集检验结果</span></span><br><span class="line">news_test = load_files(<span class="string">'code/datasets/mlcomp/379/test'</span>)</span><br><span class="line">print(len(news_test.data))</span><br><span class="line">print(len(news_test.target_names))</span><br></pre></td></tr></table></figure><pre><code>564820</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把测试集文档数学向量化</span></span><br><span class="line">t = time()</span><br><span class="line"><span class="comment"># vectorizer  = TfidfVectorizer(encoding = 'latin-1')  # 这里注意vectorizer这条语句上文已经生成执行，这里不可重复执行</span></span><br><span class="line">X_test = vectorizer.transform((d <span class="keyword">for</span>  d <span class="keyword">in</span> news_test.data))</span><br><span class="line">y_test = news_test.target</span><br><span class="line"></span><br><span class="line">print(<span class="string">"测试集："</span>,X_test.shape)</span><br><span class="line">print(<span class="string">"耗时： &#123;0&#125; s."</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>测试集： (5648, 130274)耗时： 1.64164400100708 s.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(<span class="string">"Train_score:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"Test_score:"</span>, clf.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    r = np.random.randint(X_test.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> clf.predict(X_test[r]) == y_test[r]:</span><br><span class="line">        print(<span class="string">"√：&#123;0&#125;"</span>.format(r))</span><br><span class="line">    <span class="keyword">else</span>:print(<span class="string">"X：&#123;0&#125;"</span>.format(r))</span><br></pre></td></tr></table></figure><pre><code>Train_score: 0.9974203338391502Test_score: 0.9123583569405099√：1874√：2214√：2579√：1247√：375√：5384√：5029√：1951√：4885√：1980</code></pre><h3 id="评价模型："><a href="#评价模型：" class="headerlink" title="评价模型："></a>评价模型：</h3><h4 id="classification-report-查看查准率、召回率、F1"><a href="#classification-report-查看查准率、召回率、F1" class="headerlink" title="classification_report()查看查准率、召回率、F1"></a><code>classification_report()</code>查看查准率、召回率、F1</h4><p>使用<code>classification_report()</code>函数查看针对每个类别的预测准确性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">print(clf)</span><br><span class="line">print(<span class="string">"查看针对每个类别的预测准确性："</span>)</span><br><span class="line">print(classification_report(y_test, y_pred, </span><br><span class="line">                            target_names = news_test.target_names))</span><br></pre></td></tr></table></figure><pre><code>MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)查看针对每个类别的预测准确性：                          precision    recall  f1-score   support             alt.atheism       0.90      0.92      0.91       245           comp.graphics       0.80      0.90      0.84       298 comp.os.ms-windows.misc       0.85      0.80      0.82       292comp.sys.ibm.pc.hardware       0.81      0.82      0.81       301   comp.sys.mac.hardware       0.90      0.92      0.91       256          comp.windows.x       0.89      0.88      0.88       297            misc.forsale       0.88      0.82      0.85       290               rec.autos       0.93      0.93      0.93       324         rec.motorcycles       0.97      0.97      0.97       294      rec.sport.baseball       0.97      0.96      0.97       315        rec.sport.hockey       0.97      0.99      0.98       302               sci.crypt       0.96      0.95      0.96       297         sci.electronics       0.91      0.85      0.88       313                 sci.med       0.96      0.96      0.96       277               sci.space       0.95      0.97      0.96       305  soc.religion.christian       0.93      0.96      0.94       293      talk.politics.guns       0.90      0.96      0.93       246   talk.politics.mideast       0.95      0.98      0.97       296      talk.politics.misc       0.91      0.89      0.90       236      talk.religion.misc       0.89      0.77      0.82       171             avg / total       0.91      0.91      0.91      5648</code></pre><h4 id="confusion-matrix混淆矩阵"><a href="#confusion-matrix混淆矩阵" class="headerlink" title="confusion_matrix混淆矩阵"></a><code>confusion_matrix</code>混淆矩阵</h4><p>通过<code>confusion_matrix</code>函数生成混淆矩阵，观察每种类别别错误分类的情况。例如，这些被错误分类的文档是被错误分类到哪些类别里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">print(cm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一行表示类别0的文档被正确分类的由255个，其中有2、5、13个错误分类被分到了14、15、19类中了。</span></span><br></pre></td></tr></table></figure><pre><code>[[225   0   0   0   0   0   0   0   0   0   0   0   0   0   2   5   0   0   0  13] [  1 267   6   4   2   8   1   1   0   0   0   2   3   2   1   0   0   0   0   0] [  1  12 233  26   4   9   3   0   0   0   0   0   2   1   0   0   0   0   1   0] [  0   9  16 246   7   3  10   1   0   0   1   0   8   0   0   0   0   0   0   0] [  0   2   3   5 236   2   2   1   0   0   0   3   1   0   1   0   0   0   0   0] [  0  22   6   3   0 260   0   0   0   2   0   1   0   0   1   0   2   0   0   0] [  0   2   5  11   3   1 238   9   2   3   1   0   7   0   1   0   2   2   3   0] [  0   1   0   0   1   0   7 302   4   1   0   0   1   2   3   0   2   0   0   0] [  0   0   0   0   0   2   2   3 285   0   0   0   1   0   0   0   0   0   0   1] [  0   1   0   0   1   1   1   2   0 302   6   0   0   1   0   0   0   0   0   0] [  0   0   0   0   0   0   0   0   2   1 299   0   0   0   0   0   0   0   0   0] [  0   1   2   1   1   1   2   0   0   0   0 283   1   0   0   0   2   1   2   0] [  0  11   2   6   5   2   4   5   1   1   1   3 267   1   3   0   0   0   1   0] [  1   1   0   1   1   1   0   0   0   0   0   1   1 265   2   1   0   0   2   0] [  0   3   0   0   1   0   0   0   0   0   0   1   1   1 296   0   1   0   1   0] [  3   1   0   1   0   0   0   0   0   0   1   0   0   2   0 281   0   1   2   1] [  1   0   1   0   0   0   0   0   1   0   0   0   0   0   0   0 237   1   4   1] [  1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   3   0 290   1   0] [  1   1   0   0   1   1   0   1   0   0   0   0   0   0   0   1  12   7 210   1] [ 16   1   0   0   0   0   0   0   0   0   0   0   0   0   0  12   5   2   4 131]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>), dpi=<span class="number">120</span>)</span><br><span class="line">plt.title(<span class="string">'Confusion matrix of the classifier'</span>)</span><br><span class="line">ax = plt.gca()                                  </span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            </span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'none'</span>)</span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'none'</span>)</span><br><span class="line">ax.set_xticklabels([])</span><br><span class="line">ax.set_yticklabels([])</span><br><span class="line">plt.matshow(cm, fignum=<span class="number">1</span>, cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.colorbar();</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除对角线外，颜色越浅说明错误越多</span></span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq6kdjano8j30gq0hegm8.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上图不直观，重新画图</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> HeatMap</span><br><span class="line"></span><br><span class="line">x_axis = np.arange(<span class="number">20</span>)</span><br><span class="line">y_axis = np.arange(<span class="number">20</span>)</span><br><span class="line">data = [[i, j, cm[i][j]] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">20</span>)]</span><br><span class="line">heatmap = HeatMap()</span><br><span class="line">heatmap.add(<span class="string">"混淆矩阵"</span>, x_axis, y_axis, data, is_visualmap=<span class="keyword">True</span>,</span><br><span class="line">            visual_text_color=<span class="string">"#fff"</span>, visual_orient=<span class="string">'horizontal'</span>)</span><br><span class="line"><span class="comment"># heatmap.render()</span></span><br><span class="line"><span class="comment"># heatmap</span></span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq6kc4ryv9j30h00gu0tm.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;把文档转换成向量&quot;&gt;&lt;a href=&quot;#把文档转换成向量&quot; class=&quot;headerlink&quot; title=&quot;把文档转换成向量&quot;&gt;&lt;/a&gt;把文档转换成向量&lt;/h3&gt;&lt;p&gt;TF-IDF是一种统计方法，用以评估一个词语对于一份文档的重要程度。&lt;/p&gt;
&lt;ul&gt;
&lt;l
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>SVM绘图</title>
    <link href="http://www.ihoge.cn/2018/SVM%E7%BB%98%E5%9B%BE.html"/>
    <id>http://www.ihoge.cn/2018/SVM绘图.html</id>
    <published>2018-04-08T17:10:59.000Z</published>
    <updated>2018-04-10T10:01:40.957Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">class2 = np.array([[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Decision Boundary'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">8</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">1</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.arrow(<span class="number">4</span>, <span class="number">4</span>, <span class="number">-1</span>, <span class="number">-1</span>, shape=<span class="string">'full'</span>, color=<span class="string">'r'</span>)</span><br><span class="line">plt.plot([<span class="number">3</span>, <span class="number">3</span>], [<span class="number">0.5</span>, <span class="number">6</span>], <span class="string">'--b'</span>)</span><br><span class="line">plt.arrow(<span class="number">4</span>, <span class="number">4</span>, <span class="number">-1</span>, <span class="number">0</span>, shape=<span class="string">'full'</span>, color=<span class="string">'b'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'margin 1'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">4</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">3.1</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'margin 2'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">3.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">4</span>, <span class="number">3.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'support vector'</span>,</span><br><span class="line">             xy=(<span class="number">4</span>, <span class="number">4</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">5</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'support vector'</span>,</span><br><span class="line">             xy=(<span class="number">2</span>, <span class="number">2</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">0.5</span>, <span class="number">1.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq43bc7pj3j30gx0c6gm7.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Support Vector Machine'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">8</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">1</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>], <span class="string">'--b'</span>, [<span class="number">2</span>, <span class="number">6</span>], [<span class="number">6</span>, <span class="number">2</span>], <span class="string">'--b'</span>)</span><br><span class="line">plt.arrow(<span class="number">4</span>, <span class="number">4</span>, <span class="number">-1</span>, <span class="number">-1</span>, shape=<span class="string">'full'</span>, color=<span class="string">'b'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'$w^T x + b = 0$'</span>,</span><br><span class="line">             xy=(<span class="number">5</span>, <span class="number">1</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">6</span>, <span class="number">1</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$w^T x + b = 1$'</span>,</span><br><span class="line">             xy=(<span class="number">6</span>, <span class="number">2</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">7</span>, <span class="number">2</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$w^T x + b = -1$'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">0.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">4.5</span>, <span class="number">0.2</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'd'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">3.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">2</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'A'</span>,</span><br><span class="line">             xy=(<span class="number">4</span>, <span class="number">4</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">5</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq43bz1cb3j30hg0c63za.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>), dpi=<span class="number">140</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 1</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">100</span>, </span><br><span class="line">                  n_features=<span class="number">2</span>, </span><br><span class="line">                  centers=[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>)], </span><br><span class="line">                  random_state=<span class="number">4</span>, </span><br><span class="line">                  shuffle=<span class="keyword">False</span>,</span><br><span class="line">                  cluster_std=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Non-linear Separatable'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>][:, <span class="number">0</span>], X[y==<span class="number">0</span>][:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>][:, <span class="number">0</span>], X[y==<span class="number">1</span>][:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">0.5</span>, <span class="number">2.5</span>], [<span class="number">2.5</span>, <span class="number">0.5</span>], <span class="string">'-r'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 2</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">1.5</span>, <span class="number">1.5</span>], [<span class="number">1.2</span>, <span class="number">1.7</span>]])</span><br><span class="line">class2 = np.array([[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">5.5</span>, <span class="number">3.5</span>], [<span class="number">4.5</span>, <span class="number">4.5</span>], [<span class="number">2</span>, <span class="number">1.5</span>]])</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Slack Variable'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">1</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>], <span class="string">'--b'</span>, [<span class="number">2</span>, <span class="number">6</span>], [<span class="number">6</span>, <span class="number">2</span>], <span class="string">'--b'</span>)</span><br><span class="line">plt.arrow(<span class="number">2</span>, <span class="number">1.5</span>, <span class="number">2.25</span>, <span class="number">2.25</span>, shape=<span class="string">'full'</span>, color=<span class="string">'b'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'violate margin rule.'</span>,</span><br><span class="line">             xy=(<span class="number">2</span>, <span class="number">1.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">0.2</span>, <span class="number">0.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'normal sample. $\epsilon = 0$'</span>,</span><br><span class="line">             xy=(<span class="number">4</span>, <span class="number">5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">4.5</span>, <span class="number">5.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$\epsilon &gt; 0$'</span>,</span><br><span class="line">             xy=(<span class="number">3</span>, <span class="number">2.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">3</span>, <span class="number">1.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq43cif8rwj30wy0e9mz8.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Cost'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1.5</span>, <span class="number">0</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0.015</span>, <span class="number">0.015</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$'</span>,</span><br><span class="line">             xy=(<span class="number">0.7</span>, <span class="number">0.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">1</span>, <span class="number">1</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$'</span>,</span><br><span class="line">             xy=(<span class="number">1.5</span>, <span class="number">0</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">1.8</span>, <span class="number">0.2</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq43cufjjfj30ih0d1dgg.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>), dpi=<span class="number">144</span>)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">1</span>], [<span class="number">5</span>, <span class="number">1</span>]])</span><br><span class="line">class2 = np.array([[<span class="number">2.2</span>, <span class="number">4</span>], [<span class="number">1.5</span>, <span class="number">5</span>], [<span class="number">1.8</span>, <span class="number">4.6</span>], [<span class="number">2.4</span>, <span class="number">5</span>], [<span class="number">3.2</span>, <span class="number">5</span>], [<span class="number">3.7</span>, <span class="number">4</span>], [<span class="number">4.5</span>, <span class="number">4.5</span>], [<span class="number">5.4</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 1</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Non-linear Separatable in Low Dimension'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.yticks(())</span><br><span class="line">plt.xlabel(<span class="string">'X1'</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], np.zeros(class1[:, <span class="number">0</span>].shape[<span class="number">0</span>]) + <span class="number">0.05</span>, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], np.zeros(class2[:, <span class="number">0</span>].shape[<span class="number">0</span>]) + <span class="number">0.05</span>, marker=<span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 2</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Linear Separatable in High Dimension'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.xlabel(<span class="string">'X1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'X2'</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">3.8</span>, <span class="number">2</span>], <span class="string">'-r'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq43d6r1acj30w50feab6.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(x, mean, sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(- (x - mean)**<span class="number">2</span> / (<span class="number">2</span> * sigma**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">6</span>, <span class="number">500</span>)</span><br><span class="line">mean = <span class="number">1</span></span><br><span class="line">sigma1 = <span class="number">0.1</span></span><br><span class="line">sigma2 = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">3</span>), dpi=<span class="number">144</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 1</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Gaussian for $\sigma=&#123;0&#125;$'</span>.format(sigma1))</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.1</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma1), <span class="string">'r-'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 2</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Gaussian for $\sigma=&#123;0&#125;$'</span>.format(sigma2))</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.1</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma2), <span class="string">'r-'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fq43dh16lcj30xp0bmq48.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>SVM支持向量机SVC实现</title>
    <link href="http://www.ihoge.cn/2018/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVC.html"/>
    <id>http://www.ihoge.cn/2018/支持向量机SVC.html</id>
    <published>2018-04-08T17:05:59.000Z</published>
    <updated>2018-04-10T10:01:23.892Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>支持向量机(support vector machine)是一种分类算法，但是也可以做回归，根据输入的数据不同可做不同的模型（若输入标签为连续值则做回归，若输入标签为分类值则用SVC()做分类）。通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。<br><code>sklearn</code>里对SVM的算法实现在包<code>sklearn.svm</code>里。</p><h3 id="svm-SVC分类器简介："><a href="#svm-SVC分类器简介：" class="headerlink" title="svm.SVC分类器简介："></a><code>svm.SVC</code>分类器简介：</h3><ul><li><p><strong>C</strong>：C-SVC的惩罚参数C?默认值是1.0</p><p>  C越大，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。</p></li><li><p><strong>kernel</strong> ：核函数，默认是rbf，可以是‘linear’,‘poly’, ‘rbf’</p><pre><code>liner – 线性核函数：u&apos;vpoly – 多项式核函数：(gamma*u&apos;*v + coef0)^degreerbf – RBF高斯核函数：exp(-gamma|u-v|^2)</code></pre></li></ul><ul><li><p><strong>degree</strong> ：多项式poly函数的维度，默认是3，选择其他核函数时会被忽略。</p></li><li><p><strong>gamma</strong> ： ‘rbf’,‘poly’ 和‘sigmoid’的核函数参数。默认是’auto’，则会选择1/n_features</p></li><li><p><strong>coef0</strong> ：核函数的常数项。对于‘poly’和 ‘sigmoid’有用。</p></li><li><p><strong>probability</strong> ：是否采用概率估计？.默认为False</p></li><li><p><strong>shrinking</strong> ：是否采用shrinking heuristic方法，默认为true</p></li><li><p><strong>tol</strong> ：停止训练的误差值大小，默认为1e-3</p></li><li><p><strong>cache_size</strong> ：核函数cache缓存大小，默认为200</p></li><li><p><strong>class_weight</strong> ：类别的权重，字典形式传递。设置第几类的参数C为weight * C(C-SVC中的C)</p></li><li><p><strong>verbose</strong> ：允许冗余输出？</p></li><li><p><strong>max_iter</strong> ：最大迭代次数。-1为无限制。</p></li><li><p><strong>decision_function_shape</strong> ：‘ovo’, ‘ovr’ or None, default=None3</p></li><li><p><strong>random_state</strong> ：数据洗牌时的种子值，int值</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用np.meshgrid()生成一个坐标矩阵，然预测坐标矩阵中每个点所属的类别，最后用contourf()函数</span></span><br><span class="line"><span class="comment"># 为最表矩阵中不同类别填充不同颜色</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_hyperplane</span><span class="params">(clf, X, y, </span></span></span><br><span class="line"><span class="function"><span class="params">                    h=<span class="number">0.02</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">                    draw_sv=True, </span></span></span><br><span class="line"><span class="function"><span class="params">                    title=<span class="string">'hyperplan'</span>)</span>:</span></span><br><span class="line">    <span class="comment"># create a mesh to plot in</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line"></span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.xticks(())</span><br><span class="line">    plt.yticks(())</span><br><span class="line">    </span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=<span class="string">'hot'</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    markers = [<span class="string">'o'</span>, <span class="string">'s'</span>, <span class="string">'^'</span>]</span><br><span class="line">    colors = [<span class="string">'b'</span>, <span class="string">'r'</span>, <span class="string">'c'</span>]</span><br><span class="line">    labels = np.unique(y)</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        plt.scatter(X[y==label][:, <span class="number">0</span>], </span><br><span class="line">                    X[y==label][:, <span class="number">1</span>], </span><br><span class="line">                    c=colors[label], </span><br><span class="line">                    marker=markers[label], s=<span class="number">20</span>)</span><br><span class="line">    <span class="keyword">if</span> draw_sv:</span><br><span class="line">        sv = clf.support_vectors_</span><br><span class="line">        plt.scatter(sv[:, <span class="number">0</span>], sv[:, <span class="number">1</span>], c=<span class="string">'black'</span>, marker=<span class="string">'x'</span>, s=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><h3 id="1、第一个例子："><a href="#1、第一个例子：" class="headerlink" title="1、第一个例子："></a><strong>1、第一个例子：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">100</span>, centers=<span class="number">2</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">clf = svm.SVC(C = <span class="number">1.0</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line">print(clf.score(X,y))</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">3</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plot_hyperplane(clf, X, y, h=<span class="number">0.01</span>, title=<span class="string">'Maximiin Margin Hyperplan'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq5iil2vlrj30ma07iwex.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上图带有X标记点的是支持向量，它保存在模型的support_vectors_里。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.support_vectors_</span><br></pre></td></tr></table></figure><pre><code>array([[0.70993435, 3.70954839],       [1.65719647, 3.86747763],       [1.7033305 , 1.48075002]])</code></pre><h3 id="2、第二个例子"><a href="#2、第二个例子" class="headerlink" title="2、第二个例子"></a><strong>2、第二个例子</strong></h3><p>生成一个有两个特征、三纵类别的数据集，然后分别构造4个SVM算法来拟合数据集，分别是线性和函数、三姐多项式核函数、gamma=0.5的高斯核RBF核函数和gamma=1的高斯核函数。最后把四个算法拟合出来的分割超平面画出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">100</span>, centers=<span class="number">3</span>, random_state=<span class="number">0</span>, cluster_std=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">clf_linear = svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">clf_poly = svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">'poly'</span>, degree=<span class="number">3</span>)</span><br><span class="line">clf_rbf = svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">0.5</span>)</span><br><span class="line">clf_rbf1 = svm.SVC(C=<span class="number">1.0</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">10</span>),dpi=<span class="number">140</span>)</span><br><span class="line"></span><br><span class="line">clfs = [clf_linear, clf_poly, clf_rbf, clf_rbf1]</span><br><span class="line">titles = [<span class="string">'linearSVC'</span>,<span class="string">'polySVC_d3'</span>,<span class="string">'rbfSVC_0.5'</span>,<span class="string">'rbfSVC_1'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, i <span class="keyword">in</span> zip(clfs, range(len(clfs))):</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plot_hyperplane(clf, X, y, title=titles[i])</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq5iizji9fj31140v2q7c.jpg" alt=""></p><p>这里需要注意的是坐下和右下高斯函数的图。既然支持向量是离超平面最近的点，那为什么高斯函数的图中离分割超平面很远的点也是支持向量呢？</p><p>原因是高斯核函数把输入特征向量映射到了无限维的向量空间里，在高维空间里这些点其实就是最近的点。</p><h3 id="3、例三：乳腺癌检测"><a href="#3、例三：乳腺癌检测" class="headerlink" title="3、例三：乳腺癌检测"></a><strong>3、例三：乳腺癌检测</strong></h3><ul><li><strong>使用RBF高斯核函数</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line">print(<span class="string">'data shape:&#123;0&#125;; no. positive:&#123;1&#125;; no. negative:&#123;2&#125;'</span>.format(X.shape, y[y==<span class="number">1</span>].shape[<span class="number">0</span>], y[y==<span class="number">0</span>].shape[<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.2</span>)</span><br></pre></td></tr></table></figure><pre><code>data shape:(569, 30); no. positive:357; no. negative:212</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">clf = SVC(C=<span class="number">1</span>, kernel=<span class="string">'rbf'</span>, gamma=<span class="number">.1</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">"Train_score:&#123;0&#125;\nTest_score:&#123;1&#125;"</span>.format(clf.score(X_train, y_train), clf.score(X_test, y_test)))</span><br></pre></td></tr></table></figure><pre><code>Train_score:1.0Test_score:0.6754385964912281</code></pre><p>上例典型过拟合。代码gamma参数为0.1，这个参数说明作用呢？</p><p>回忆下RBF 核函数$$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\exp \left( -\frac{\left( x^{\left( i \right)}-x^{\left( j \right)} \right)^{2}}{2\sigma ^{2}} \right)$$<br>$γ$（指本式中的$\sigma$）主要定义了单个样本对整个分类超平面的影响，当$γ$<br>比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量，反之，当$γ$比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多。scikit-learn中默认值是1/n_features（1/样本量）。</p><p>因此判断造成过拟合的原因是$γ$太大。下面用<code>GridSearchCV</code>来自动选择$γ$的最优参数以及对应的交叉验证评分及召回率和F1得分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics </span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.001</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment"># Set the parameters by cross-validation</span></span><br><span class="line">param_grid = &#123;<span class="string">'gamma'</span>: thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(SVC(kernel=<span class="string">'rbf'</span>), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">"best param: &#123;0&#125;\nbest score: &#123;1&#125;"</span>.format(clf.best_params_, </span><br><span class="line">                                                clf.best_score_))</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"查准率："</span>,metrics.precision_score(y_pred, y_test))</span><br><span class="line">print(<span class="string">"召回率："</span>,metrics.recall_score(y_pred, y_test))</span><br><span class="line">print(<span class="string">"F1："</span>,metrics.f1_score(y_pred, y_test))</span><br></pre></td></tr></table></figure><pre><code>best param: {&apos;gamma&apos;: 9.090909090909092e-05}best score: 0.945054945054945查准率： 0.961038961038961召回率： 0.925F1： 0.9426751592356688</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learn_curve</span><span class="params">(estimator, title, X, y, ylim = None, cv=None, n_jobs=<span class="number">1</span>, train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.</span>, <span class="number">5</span>)</span>)</span>:</span></span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(<span class="string">"train exs"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    </span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, </span><br><span class="line">                     train_score_mean + train_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'r'</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, </span><br><span class="line">                     test_score_mean + test_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>, label=<span class="string">'train score训练得分'</span>)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, <span class="string">'o-'</span>, color=<span class="string">'g'</span>, label=<span class="string">'cross-validation score交叉验证得分'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">title = <span class="string">'Learing Curve for SVC-RBF'</span></span><br><span class="line"></span><br><span class="line">gammas=[<span class="number">0.001</span>, <span class="number">0.0005</span>, <span class="number">0.0003</span>, <span class="number">0.0001</span>]</span><br><span class="line">start = time.clock()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">10</span>), dpi=<span class="number">140</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(gammas)):</span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plot_learn_curve(SVC(C=<span class="number">1.</span>, gamma=gammas[i]), gammas[i], X, y, cv=cv)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.figure(figsize=(8,6), dpi=120)</span></span><br><span class="line"><span class="comment"># plot_learn_curve(SVC(C=1., gamma=gammas[0]), title, X, y, cv=cv)</span></span><br><span class="line">print(<span class="string">"耗时："</span>, time.clock() - start)</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq5kp9xbugj319e0wmn3e.jpg" alt=""></p><ul><li><strong>使用多项式核函数</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clf = SVC(C=<span class="number">1.</span>, kernel=<span class="string">'poly'</span>, degree=<span class="number">2</span>)</span><br><span class="line">start = time.clock()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"耗时："</span>,time.clock()-start)</span><br><span class="line">print(<span class="string">"Train_score:&#123;0&#125;\nTest_score:&#123;1&#125;"</span>.format(clf.score(X_train, y_train), clf.score(X_test, y_test)))</span><br><span class="line">print(<span class="string">"查准率："</span>,metrics.precision_score(y_pred, y_test))</span><br><span class="line">print(<span class="string">"召回率："</span>,metrics.recall_score(y_pred, y_test))</span><br><span class="line">print(<span class="string">"F1："</span>,metrics.f1_score(y_pred, y_test))</span><br></pre></td></tr></table></figure><pre><code>耗时： 13.866157000000015Train_score:0.978021978021978Test_score:0.9736842105263158查准率： 0.9866666666666667召回率： 0.9736842105263158F1： 0.9801324503311258</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">titles = [<span class="string">'poly_SVC_D=1'</span>,<span class="string">'ploy_SVC_D=2'</span>]</span><br><span class="line">degrees = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>), dpi=<span class="number">130</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(degrees)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, len(degrees), i+<span class="number">1</span>)</span><br><span class="line">    plot_learn_curve(SVC(kernel=<span class="string">'poly'</span>, degree=degrees[i]), titles[i], X, y, cv=cv)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"耗时："</span>, time.clock()-start)</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq5njno5cbj310k0dxacq.jpg" alt=""></p><p>可以看出，一阶多项式核函数的拟合效果更好。平均交叉验证数据集评分0.94，最高时可达0.975。当然消耗的时间也更久，一阶多项式计算代价比高斯核函数的SVC运算事件多出了好多倍。 之前笔者使用逻辑回归算法检测乳腺癌检测问题时，使用二项多项式逻辑回归增加特征同时使用L1范数作为正则项的拟合效果比这里的支持向量机效果好，更重要的是逻辑回归算法的运算效率远远高于二阶多项式核函数的支持向量机算法。当然，这里的支持向量机算法的效果还是比使用L2范数作为正则项的逻辑回归准确率高，由此可见木星选择和参数调优对机器学习的重要性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;支持向量机(support vector machine)是一种分类算法，但是也可以做回归，根据输入的数据不同可做不同的模型（若输入标签为连续值则做回归，若输入标签为分类值则用SVC()做分类）。通过寻求结构化风险最小来提高学习机泛化能力
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>sklearn聚合算法整理</title>
    <link href="http://www.ihoge.cn/2018/sklearn-ensemble.html"/>
    <id>http://www.ihoge.cn/2018/sklearn-ensemble.html</id>
    <published>2018-04-08T16:19:59.000Z</published>
    <updated>2018-04-10T10:01:44.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="随机森林分类预测泰坦尼尼克号幸存者"><a href="#随机森林分类预测泰坦尼尼克号幸存者" class="headerlink" title="随机森林分类预测泰坦尼尼克号幸存者"></a>随机森林分类预测泰坦尼尼克号幸存者</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_dataset</span><span class="params">(fname)</span>:</span></span><br><span class="line">    data = pd.read_csv(fname, index_col=<span class="number">0</span>)</span><br><span class="line">    data.drop([<span class="string">'Name'</span>, <span class="string">'Ticket'</span>, <span class="string">'Cabin'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">    lables = data[<span class="string">'Sex'</span>].unique().tolist()</span><br><span class="line">    data[<span class="string">'Sex'</span>] = [*map(<span class="keyword">lambda</span> x: lables.index(x) , data[<span class="string">'Sex'</span>])]</span><br><span class="line">    lables = data[<span class="string">'Embarked'</span>].unique().tolist()</span><br><span class="line">    data[<span class="string">'Embarked'</span>] = data[<span class="string">'Embarked'</span>].apply(<span class="keyword">lambda</span> n: lables.index(n))</span><br><span class="line">    data = data.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">train = read_dataset(<span class="string">'code/datasets/titanic/train.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">y = train[<span class="string">'Survived'</span>].values</span><br><span class="line">X = train.drop([<span class="string">'Survived'</span>], axis=<span class="number">1</span>).values</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">print(<span class="string">"X_train_shape:"</span>, X_train.shape, <span class="string">" y_train_shape:"</span>, y_train.shape)</span><br><span class="line">print(<span class="string">"X_test_shape:"</span>, X_test.shape,<span class="string">"  y_test_shape:"</span>, y_test.shape)</span><br></pre></td></tr></table></figure><pre><code>X_train_shape: (712, 7)  y_train_shape: (712,)X_test_shape: (179, 7)   y_test_shape: (179,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">****</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">entropy_thresholds = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">gini_thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">50</span>)</span><br><span class="line"><span class="comment">#设置参数矩阵：</span></span><br><span class="line">param_grid = [&#123;<span class="string">'criterion'</span>: [<span class="string">'entropy'</span>], <span class="string">'min_impurity_decrease'</span>: entropy_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'criterion'</span>: [<span class="string">'gini'</span>], <span class="string">'min_impurity_decrease'</span>: gini_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'max_depth'</span>: np.arange(<span class="number">2</span>,<span class="number">10</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'min_samples_split'</span>: np.arange(<span class="number">2</span>,<span class="number">20</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'n_estimators'</span>:np.arange(<span class="number">2</span>,<span class="number">20</span>)&#125;]</span><br><span class="line">clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"耗时："</span>,time.clock() - start)</span><br><span class="line">print(<span class="string">"best param:&#123;0&#125;\nbest score:&#123;1&#125;"</span>.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><pre><code>耗时： 13.397480000000002best param:{&apos;min_samples_split&apos;: 10}best score:0.8406285072951739</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">clf = RandomForestClassifier(min_samples_split=<span class="number">10</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集得分:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"测试集得分:"</span>, clf.score(X_test, y_test))</span><br><span class="line">print(<span class="string">"查准率:"</span>, metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"召回率:"</span>, metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"F1_score:"</span>, metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><pre><code>训练集得分: 0.8974719101123596测试集得分: 0.7988826815642458查准率: 0.8082191780821918召回率: 0.7283950617283951F1_score: 0.7662337662337663</code></pre><p>这次分别对模型的<code>criterion</code>,<code>max_depth</code>,<code>min_samples_split</code>,<code>n_estimators</code>四个参数进行了比较。</p><p>经过多次执行发现结果仍不是很稳定，最优参数集中在<code>min_samples_split</code>分别为8，10，12上</p><h2 id="自助聚合算法预测泰坦尼尼克号幸存者"><a href="#自助聚合算法预测泰坦尼尼克号幸存者" class="headerlink" title="自助聚合算法预测泰坦尼尼克号幸存者"></a>自助聚合算法预测泰坦尼尼克号幸存者</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"></span><br><span class="line">clf = BaggingClassifier(n_estimators=<span class="number">50</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集得分:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"测试集得分:"</span>, clf.score(X_test, y_test))</span><br><span class="line">print(<span class="string">"查准率:"</span>, metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"召回率:"</span>, metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"F1_score:"</span>, metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><pre><code>训练集得分: 0.9817415730337079测试集得分: 0.7877094972067039查准率: 0.7792207792207793召回率: 0.7407407407407407F1_score: 0.7594936708860759</code></pre><h2 id="Boosting正向激励算法预测泰坦尼尼克号幸存者"><a href="#Boosting正向激励算法预测泰坦尼尼克号幸存者" class="headerlink" title="Boosting正向激励算法预测泰坦尼尼克号幸存者"></a>Boosting正向激励算法预测泰坦尼尼克号幸存者</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">clf = AdaBoostClassifier()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集得分:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"测试集得分:"</span>, clf.score(X_test, y_test))</span><br><span class="line">print(<span class="string">"查准率:"</span>, metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"召回率:"</span>, metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"F1_score:"</span>, metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><pre><code>训练集得分: 0.8300561797752809测试集得分: 0.8156424581005587查准率: 0.8076923076923077召回率: 0.7777777777777778F1_score: 0.7924528301886792</code></pre><h2 id="Extra-Trees算法预测泰坦尼尼克号幸存者"><a href="#Extra-Trees算法预测泰坦尼尼克号幸存者" class="headerlink" title="Extra Trees算法预测泰坦尼尼克号幸存者"></a>Extra Trees算法预测泰坦尼尼克号幸存者</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">entropy_thresholds = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">gini_thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">50</span>)</span><br><span class="line"><span class="comment">#设置参数矩阵：</span></span><br><span class="line">param_grid = [&#123;<span class="string">'criterion'</span>: [<span class="string">'entropy'</span>], <span class="string">'min_impurity_decrease'</span>: entropy_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'criterion'</span>: [<span class="string">'gini'</span>], <span class="string">'min_impurity_decrease'</span>: gini_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'max_depth'</span>: np.arange(<span class="number">2</span>,<span class="number">10</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'min_samples_split'</span>: np.arange(<span class="number">2</span>,<span class="number">20</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'n_estimators'</span>:np.arange(<span class="number">2</span>,<span class="number">20</span>)&#125;]</span><br><span class="line">clf = GridSearchCV(ExtraTreesClassifier(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"耗时："</span>,time.clock() - start)</span><br><span class="line">print(<span class="string">"best param:&#123;0&#125;\nbest score:&#123;1&#125;"</span>.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><pre><code>耗时： 16.29516799999999best param:{&apos;min_samples_split&apos;: 12}best score:0.8226711560044894</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">clf = ExtraTreesClassifier(min_samples_split=<span class="number">12</span><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">entropy_thresholds = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">gini_thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">50</span>)</span><br><span class="line"><span class="comment">#设置参数矩阵：</span></span><br><span class="line">param_grid = [&#123;<span class="string">'criterion'</span>: [<span class="string">'entropy'</span>], <span class="string">'min_impurity_decrease'</span>: entropy_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'criterion'</span>: [<span class="string">'gini'</span>], <span class="string">'min_impurity_decrease'</span>: gini_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'max_depth'</span>: np.arange(<span class="number">2</span>,<span class="number">10</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'min_samples_split'</span>: np.arange(<span class="number">2</span>,<span class="number">20</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'n_estimators'</span>:np.arange(<span class="number">2</span>,<span class="number">20</span>)&#125;]</span><br><span class="line">clf = GridSearchCV(ExtraTreesClassifier(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"耗时："</span>,time.clock() - start)</span><br><span class="line">print(<span class="string">"best param:&#123;0&#125;\nbest score:&#123;1&#125;"</span>.format(clf.best_params_, clf.best_score_)))</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"训练集得分:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"测试集得分:"</span>, clf.score(X_test, y_test))</span><br><span class="line">print(<span class="string">"查准率:"</span>, metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"召回率:"</span>, metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"F1_score:"</span>, metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><pre><code>训练集得分: 0.8932584269662921测试集得分: 0.8100558659217877查准率: 0.8405797101449275召回率: 0.7160493827160493F1_score: 0.7733333333333333</code></pre><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><p>针对此数据集预测泰坦尼克号的结果对比中，Boosting正向激励算法性能最佳最稳定，其次是参数优化后的Extra Trees算法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;随机森林分类预测泰坦尼尼克号幸存者&quot;&gt;&lt;a href=&quot;#随机森林分类预测泰坦尼尼克号幸存者&quot; class=&quot;headerlink&quot; title=&quot;随机森林分类预测泰坦尼尼克号幸存者&quot;&gt;&lt;/a&gt;随机森林分类预测泰坦尼尼克号幸存者&lt;/h2&gt;&lt;figure class
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>SVM支持向量机原理及核函数</title>
    <link href="http://www.ihoge.cn/2018/SVM.html"/>
    <id>http://www.ihoge.cn/2018/SVM.html</id>
    <published>2018-04-07T12:05:59.000Z</published>
    <updated>2018-04-07T12:05:29.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="支持向量机原理"><a href="#支持向量机原理" class="headerlink" title="支持向量机原理"></a>支持向量机原理</h2><h3 id="大距离分类算法"><a href="#大距离分类算法" class="headerlink" title="大距离分类算法"></a>大距离分类算法</h3><p>1、名词解释：</p><ul><li><p><strong>分割超平面</strong>：如下图所示，构造一个分割线把圆形的点和方形的点分开，这个线称为<strong>分割超平面</strong>。</p></li><li><p><strong>支持向量</strong>：离分割超平面最近的点</p></li><li><p><strong>间距</strong>：支持向量到分割超平面距离的两倍</p></li></ul><p><strong>SVM算法的原理就是找到一个分割超平面，它能把数据正确的分类，并且间距最大！</strong></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq43bc7pj3j30gx0c6gm7.jpg" alt=""></p><p>2、计算间距</p><p>在而为空间里，可以使用方程$w_1x_1+w_2x_2+b=0$来表示分割超平面。针对高纬度空间，可以写成一般化的向量形式，即$w^Tx+b=0$。这里画出与分割线超平面平行的两条直线，分别穿过两个类别的支持向量。这两条直线的方程分别为$w^Tx+b=-1$和$w^Tx+b=1$。如下图所示：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq43bz1cb3j30hg0c63za.jpg" alt=""></p><p>根据点到直线的距离公式，可以算出支持向量A到分割超平面的距离为：$$d=\frac{\left| w^{T}A+b \right|}{\left| \left| w \right| \right|}$$</p><p>由于点$A$在直线$w^Tx+b=-1$和$w^Tx+b=1$在，代入可的支持向量$A$到分割超平面的距离为$d=\frac{1}{\left| \left| w \right| \right|}$。为了使间距最大，只需找到合适的参数$w$和$b$，使$\frac{1}{\left| \left| w \right| \right|}$最大即可。$||w||$使向量$w$的<strong>L2范数</strong>，计算公式为：<br>$$\left| \left| w \right| \right|=\sqrt{\sum_{i=1}^{n}{w_{i}^{2}}}$$<br>求$\frac{1}{\left| \left| w \right| \right|}$的最大值即使求$||w||^2$的最小值：<br>$$\left| \left| w \right| \right|^{2}=\sum_{i=1}^{n}{w_{i}^{2}}$$</p><p>其中$n$为限量$w$的纬度。除了间距最大，分割超平面更能用来解决分类问题！回到上图，针对方形点$x$，必须满足$w^Tx+b≥1$的约束条件。针对圆形的点必须满足$w^Tx+b&lt;=-1$的约束条件。<br>类别是离散的值，分别使用-1表示圆点类别，1表示方点类别，即$y\in \left(-1,1 \right)$。针对数据集中的所有样本$x^{(i)},y^{(i)}$，只要满足以下约束条件，则由以下参数$w$和参数$b$定义的分割超平面进行分类：$$y^{(i)}(w^Tx^{(i)}+b)≥1$$</p><p><strong>一句话概括：求解SVM算法，就是在满足约束条件$y^{(i)}(w^Tx^{(i)}+b)≥1$的前提下，求解$||w||^2$的最小值。</strong></p><h3 id="松弛系数"><a href="#松弛系数" class="headerlink" title="松弛系数"></a>松弛系数</h3><p>针对现行不可分的数据集，上面的方法就不能用了。解决这个问题的办法就是引入一个参数$\epsilon$，称为<strong>松弛系数</strong>。然后把优化的目标函数变为：<br>$$\mbox{argmin}\left| \left| w^{2} \right| \right|+R\sum_{i=1}^{m}{\epsilon_{i}}$$</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq43cif8rwj30wy0e9mz8.jpg" alt=""></p><p>其中$m$为数据集的个数，$R$为算法参数，其约束条件变为：$$y^{\left( i \right)}\left( w^T{x^{\left( i \right)}}+b \right)\geq 1-\epsilon_{i}$$</p><p>理解松弛系数：</p><p>可以把$ε_{i}$理解为样本$x^{(i)}$违反一大间距规则的程度。针对大多数满足约束条件的样本$ε=0$。而对部分违反最大间距规则的样本$ε&gt;0$。参数$R$则表示对违反约束的样本的”惩罚”。$R$越大对违反约束的点“惩罚力度”越大反之越小 。这样模型就会倾向于<strong>允许部分点违反最大间距规则</strong>。</p><p>把$y^{\left( i \right)}\left( w^{T}x^{\left( i \right)}+b \right)$作为横坐标，违反约束条件的代价$J_i$作为纵坐标画图：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq43cufjjfj30ih0d1dgg.jpg" alt=""></p><p>上图可以看出，针对哪些没有违反约束条件的样本，其成本为0。违反了约束条件的样本其成本与$ε$成正比，斜线的斜率为$R$。</p><p>因此，引入松弛系数类似于逻辑回归的成本函数引入正则项，目的是为了纠正过拟合问题，让SVM对噪声数据由更强的忍耐性。如上上图所示，当出现违反大间距规则的噪声样本出现时，仍能让分割超平面是原来的样子，这就是松弛系数的作用。</p><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>核函数是<strong>特征转换</strong>函数。</p><h3 id="最简单的核函数"><a href="#最简单的核函数" class="headerlink" title="最简单的核函数"></a>最简单的核函数</h3><p>回顾上面内容，我们的任务是找出合适的参数$w,b$，使得分割超平面间距最大，且能正确对数据进行分类。<strong>间距最大是我们的优化目标</strong>。真确地对数据分类是约束条件。即<strong>在满足约束条件$y^{(i)}(w^Tx^{(i)}+b)≥1$的前提下，求解$||w||^2$的最小值。</strong></p><p><strong>拉格朗日乘子法</strong>是解决约束条件下求函数极值的理想方法。其方法是引入非负系数$α$来作为约束条件的权重：$$L=\frac{1}{2}\left| \left| w \right| \right|^{2}-\sum_{i=1}^{m}{\alpha _{i}\left( y^{\left( i \right)}\left( w^{T}x^{\left( i \right)}+b \right)-1 \right)}$$</p><p>由于极值的偏导数为0，因此这需要让$L$对$w$求导使之为0得到$w$和$α$对关系：$$w=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}x^{\left( i \right)}}$$<br>接着继续求$L$对$b$对偏导数得出:$$\sum_{i=1}^{m}{y^{\left( i \right)}\alpha_{i}=0}$$<br>把这两个式子代入$L$通过数学运算得出：$$L=\sum_{i=1}^{m}{a_{i}-}\frac{1}{2}\sum_{i=1}^{m}{\sum_{j=1}^{m}{a_{i}}a_{j}y^{\left( i \right)}y^{\left( j \right)}x^{\left( i \right)T}x^{\left( j \right)}}$$<br>这个公式中$m$是数据集个数，$a$是拉格朗日乘子法引入的一个系数，针对数据集中的每个样本$x^{(i)}$,都有对应的$a_i$。$x^{(i)}$是数据集中地$i$个样本的输入，它是一个向量，$y^{(i)}$是对应的输出标签，值为$y\in \left( -1,1 \right)$。</p><p>这个公式的最小值求解这里就不说明了。最后求出的$a$有个明显的特点。即大部分$a_i=0$。因为只有那些支持向量所对应的样本直接决定了间隙的大小。实际上以上推导出这个公式就是为了引入支持向量机的另外一个核心概念：<strong>核函数</strong>:$$K({x^{({i})},x^{({j})}})=x^{({i})T}x^{({j})}$$</p><p>$L$里的$x^{(i)T}x^{(j)}$部分，其中$x^{(i)}$是一个特征向量，所以$x^{(i)T}x^{(j)}$是一个数值，就是两个输入特征向量的内积。预测函数为：$$w^{T}x+b=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}x^{\left( i \right)T}x+b}$$</p><p>当$w^{T}x+b&gt;0$，预测函数为类别1，当$w^{T}x+b&lt;0$，预测类别为-1。注意到预测函数里也包含式子$x^{({i})T}x$。我们把$K({x^{({i})},x^{({j})}})=x^{({i})T}x^{({j})}$称为核函数。 $x^{(i)T}x^{(j)}$是两个向量内积，它的物理含义是衡量两个向量的<strong>相似性</strong>。典型地，当两个向量相互垂直是，即完全线性无关，此时$x^{(i)T}x^{(j)}=0$。引入核函数后预测函数为：<br>$$w^{T}x+b=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}K\left( x^{\left( i \right)},x \right)+b}$$</p><h3 id="相似性函数"><a href="#相似性函数" class="headerlink" title="相似性函数"></a>相似性函数</h3><p>假设数据集已有一个数图特征，如下图，如何进行分类。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq43d6r1acj30w50feab6.jpg" alt=""></p><p>解决这个问题的方式是：<strong>用一定规则把这些无法进行线性分割的样本映射到更高纬度的空间里，然后找出超平面</strong>。</p><p>SVM的核函数就是为了实现这种相似性映射。最简单的核函数是$K({x^{({i})},x^{({j})}})=x^{({i})T}x^{({j})}$，它衡量的是两个输入特征向量的相似性。可以通过定义和函数$K({x^{({i})},x^{({j})}})$来重新定义相似性，从而得到想要的映射。例如在基因测试领域，我们需要根据DNA分子的特征来定义相似性函数，即和函数。在文本处理领域，也可以自己定义和函数来衡量两个词之间的相似性。</p><p>怎么把低维度的空间映射到高纬度的空间呢？</p><p>举个例子：联想下利用多项式解决线性回归欠拟合问题的方法。如果输入特征是一维的$[x_1]$变量，我们把它变成二维的一个方法是把输入特征转化为$[x_1,2x_1^2]$，定义这种特征映射的函数就称之为<strong>相似性函数$Φ({x})$</strong>。这样在原来低维度计算相似性的运算$x^{({i})T}x^{({j})}$，就可以转换为高纬度空间里进行相似性运算$Φ({x^{({i})}})^{T}Φ({x^{({i})}})$。</p><p><strong>核函数$K({x^{({i})},x^{({j})}})$和相似性函数$Φ({x})$的关系：</strong></p><p>相似性函数是特征的<strong>映射函数</strong>，起到转换的作用。而核函数是<strong>特征向量的内积</strong>。经过相似性函数转换后，核函数变成$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\Phi \left( x^{\left( i \right)} \right)^{T}\Phi \left( x^{\left( i \right)} \right)$。</p><h3 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h3><p>核函数一般和应用场景相关，在不同领域所应用的核函数可能也不相同。但是实际上也有一些通用核函数“万金油”，一般有两种：<strong>多项式核函数</strong>和<strong>高斯核函数</strong>。</p><p>1、多项式核函数：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq4aza9jwoj306q01p3yh.jpg" alt=""></p><p>2、高斯核函数：$$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\exp \left( -\frac{\left( x^{\left( i \right)}-x^{\left( j \right)} \right)^{2}}{2\sigma ^{2}} \right)$$</p><p>如果输入的特征是一维的标量，那么高斯核函数对应的形状就是一个反钟形的曲线，其参数$σ$控制反钟形的宽度。如下图所示：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fq43dh16lcj30xp0bmq48.jpg" alt=""></p><p>由于$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\Phi \left( x^{\left( i \right)} \right)^{T}\Phi \left( x^{\left( i \right)} \right)$，经过合适的数学变换，可得高斯核函数对应的特征转换函数为：<br>$$\Phi \left( x \right)=\sum_{i=0}^{\infty }{\exp \left( -x^{2} \right)\sqrt{\frac{2^{i}}{i!}}}x^{i}$$<br>前面无限多项的累加器，其物理意义就是把特征向量转换到无限多维向量空间里，即<strong>高斯函数可以吧输入特征扩展到无限多维空间里</strong>。公式的推导公式会用到<strong>泰勒公式</strong>。</p><p>$$高斯预测函数=\sum_{i=1}^{m}{a_{i}y^{\left( i \right)}K\left( x^{\left( i \right)},x \right)+b}$$<br>其中$K\left( x^{\left( i \right)},x^{\left( j \right)} \right)$是高斯核函数，$a_i$只在支持向量对应的样本出不为0.由此可知，<strong>预测函数时中心点在支持向量机处的高斯函数的线性组合</strong>，其线性组合的系数为$a_iy^{(i)}$。因此，高斯核函数也称为<strong>RBF核函数</strong>，即反钟形函数的线性组合。</p><h3 id="核函数的对比"><a href="#核函数的对比" class="headerlink" title="核函数的对比"></a>核函数的对比</h3><p>$$简单线性核函数K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=x^{\left( i \right)T}x^{\left( j \right)}$$</p><p>$$多项式核函数：$$<img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq4aza9jwoj306q01p3yh.jpg" alt=""><br>$$高斯核函数K\left( x^{\left( i \right)},x^{\left( j \right)} \right)=\exp \left( -\frac{\left( x^{\left( i \right)}-x^{\left( j \right)} \right)^{2}}{2\sigma ^{2}} \right)$$<br>1、<strong>线性核函数：</strong>这是最简单的核函数，它直接计算两个输入特征向量的内积。</p><ul><li>优点：简单高效，结果易解释，总能生成一个最简洁的线性分割超平面</li><li>缺点：只适用线性可分的数据集</li></ul><p>2、<strong>多项式核函数：</strong>通过多项式来作为特征映射函数</p><ul><li>优点：可以拟合出复杂的分割超平面。</li><li>缺点：参数太多。有$γ,c,n$三个参数要选择，选择起来比较困难；另外多项式的阶数不宜太高否则会给模型求解带来困难。</li></ul><p>3、<strong>高斯核函数：</strong></p><ul><li>优点：可以把特征映射到无限多维，并且没有多项式计算那么困难，参数也比较好选择。</li><li>缺点：不容易解释，计算速度比较慢，容易过拟合。</li></ul><h3 id="核函数的选择"><a href="#核函数的选择" class="headerlink" title="核函数的选择"></a>核函数的选择</h3><p>1、最一般的选择原则是针对数据量很大的时候，可以选择复杂一点的模型。虽然复杂模型容易过拟合，但由于数据量很大，可以有效弥补过拟合问题。如果数据集较小选择简单点的模型，否则很容易过拟合，此时特别要注意模型是否欠拟合，如果欠拟合可以增加多项式纠正欠拟合。</p><p>2、根据样本量$m$和特征量$n$进行选择：</p><ul><li>特征相比样本较大（如m=10～1000，n=10000）：选逻辑回归或者线性函数SVM</li><li>特征较少，样本量中（如m=10～10000，n=1～1000）：选择高斯SVM</li><li>特征量少，样本多（如m=50000+，n=1~1000)：选多项式或高斯SVM</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;支持向量机原理&quot;&gt;&lt;a href=&quot;#支持向量机原理&quot; class=&quot;headerlink&quot; title=&quot;支持向量机原理&quot;&gt;&lt;/a&gt;支持向量机原理&lt;/h2&gt;&lt;h3 id=&quot;大距离分类算法&quot;&gt;&lt;a href=&quot;#大距离分类算法&quot;
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>DecisionTree决策树大全</title>
    <link href="http://www.ihoge.cn/2018/DecisionTree.html"/>
    <id>http://www.ihoge.cn/2018/DecisionTree.html</id>
    <published>2018-04-06T17:05:59.000Z</published>
    <updated>2018-04-07T12:52:14.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="利用信息墒判定先对那个特征进行分裂"><a href="#利用信息墒判定先对那个特征进行分裂" class="headerlink" title="利用信息墒判定先对那个特征进行分裂"></a>利用信息墒判定先对那个特征进行分裂</h2><p>信息墒是衡量信息不确定性的指标，信息墒公式：<br>$$H\left( X \right)=-\sum_{x\in X}^{}{P\left( x \right)\log_{2}P\left( x \right)}$$<br>其中$P(x)$表示事件$x$出现的概率。回到决策树的构建问题上，我们遍历所有特征，分别计算，使用这个公式划分数据集前后信息墒的变化值，然后选择信息墒变化幅度最大的那个特征来作为数据集划分依据。即：选择<strong>信息增益</strong>最大的特征作为分裂节点。</p><p>这里以概率$P(X)$为横坐标，以信息墒$Entropy$为纵坐标把信息墒和概率的函数关系$\mbox{E}ntropy=-P\left( x \right)\log_{2}P\left( x \right)$在二维坐标上画出来，可以看出，当概率$P(X)$越接近0或越接近1时，信息墒的值越小。当概率值为1时信息墒为0，此时数据是最“纯净”的。我们在选择特征时，选择信息增益最大的特征，物理上即让数据尽量往更纯净的方向上变换。因此<strong>信息增益</strong>是用来衡量数据变得更有序更纯净的程度的指标。</p><p><em>延伸一下：如写博客或者看书的过程，是减熵的过程。通过阅读和写作减少了不确定的信息，从而实现减熵。人生价值的实现，在于消费资源（增熵过程）来获取能量，经过自己的劳动付出（减熵过程），让世界变得更加纯净有序，<code>信息增益 = 减熵量 - 增熵量</code>即是衡量人生价值的尺度</em>。</p><h2 id="决策树的创建"><a href="#决策树的创建" class="headerlink" title="决策树的创建"></a>决策树的创建</h2><p>决策树的构建过程，就是从训练集中归纳出一组分类规则，使它与熟练数据矛盾较小的同时具有较强的泛化能力。基本分为以下几步：</p><ul><li>计算数据集划分钱的信息墒</li><li>遍历所有为作为划分条件的特征，分别计算根据每个特征划分数据集后的信息墒。</li><li>选择信息增益最大的特征，并使用这个特镇作为数据划分节点来划分数据。</li><li>递归地处理被划分后的所子数据集，从未被选择的特征里继续重复以上步骤，选择出最优数据划分特征来划分子数据集…</li></ul><p>这里递归结束的条件一般有两个：<strong>一是所有特征都用完了，二是划分后的信息墒增益足够小了</strong>。针对这个停止条件，需要实现选择信息增益的门限值来作为递归结束条件。</p><p>使用信息增益作为特征选择指标的决策树构建算法，称为<strong>ID3算法</strong></p><p>1、离散化</p><p>当特征数据是连续值时，就需要先对数据进行离散化处理。（如考试得分0～100，1～59为不及格，60～80为达标，80～100为优秀）这样离散处理后的数据就可以用来构建决策树了。</p><p>2、<strong>正则项（重要）</strong></p><p>最大化信息增益来选择特征，在决策树的构建过程中，容易造成优先选择类别最多的特征进行分裂，因为这样划分后的子数据集最“纯净”，其信息增益最大。但这不是我们想看到的结果。解决办法如下：</p><ul><li><p><strong>计算划分后的子数据集的信息墒时，加上一个与类别个数成正比的正则项来作为最后的信息墒</strong>：这样当算法选择的某个类别较多的特征，使信息墒较小时，由于受到正则项的“惩罚”，导致最终的信息墒也较大。这样通过合适的参数可以使算法训练得到某种程度的平衡。</p></li><li><p><strong>使用信息增益比</strong>来作为特征选择的标准。</p></li></ul><p>3、基尼不纯度</p><p>信息墒是横量信息不确定性的指标，实际上也是衡量信息“纯度”的指标。</p><p>基尼不纯度<code>(Gini impurity)</code>也是衡量信息不纯度的指标，公式如下：</p><p>$$Gini\left( D \right)=\sum_{x\in X}^{_ {_ {\;}}}{P\left( x \right)\left( 1-P\left( x \right) \right)=1-}\sum_{x\in X}^{_ {_ {\; }}}{P\left( x \right)^{2}}$$<br>同样，这里以概率$P(X)$为横坐标，以信息墒$Gini(x)$为纵坐标把信息墒和概率的函数关系在二维坐标上画出来，可以看出其形状几乎和信息墒的形状一样。<code>CART</code>算法使用基尼不纯度来作为特征选择标准，<code>GART</code>也是一种决策树构建算法。</p><h2 id="剪枝算法"><a href="#剪枝算法" class="headerlink" title="剪枝算法"></a>剪枝算法</h2><p>使用决策树模型拟合数据时，容易产生过拟合。解决办法是对决策树进行剪枝处理。决策树剪枝有两种思路：</p><p>1、<strong>前剪枝</strong><code>（Pre-Pruning）</code></p><p>在构造决策树的同时进行剪枝。在决策树构建中，如果无法进一步降低信息墒的情况下就会停止创建分支。为了避免过拟合，可以设定一个阀值，信息墒见效的数量小于这个阀值，即是还可以继续降低熵也停止继续创建分支。这种方法就是前剪枝。</p><p>2、<strong>后剪枝</strong><code>（Post-Pruning）</code></p><p>后剪枝是指决策树构造完成后进行剪枝。剪枝的过程是对拥有同样符节点的一组节点进行检查，判断如果将其合并，信息墒的增加量是否小于某一阀值。如果小于阀值即可合并分支。</p><p>后剪枝是目前比较普遍的做法。后剪枝的过程就是删除一些子树，然后用子树的根节点代替作为新的叶子节点。这个新叶子所标示的类别通过大多数原则来确定。即把这个叶子节点里样本最多的类别，作为这个叶子节点的类别。</p><p>后剪枝的算法有很多种，其中常见的一种称为<strong>减低错误率剪枝法（Reduced-Errorpruning）</strong>。其思路是自底向上，从已经构建好的完全决策树中找出一个子树，然后用子树的根节点代替这颗子树，作为新的叶子节点。叶子节点所表示的类别通过大多数原则确定，这样就构建出一个简化版决策树。然后使用交叉验证数据集来测试简化版本的决策树，看看其错误率是不是降低了。如果错误率降低了，则可以用这个简化版的决策树来代替完全决策树，否则还采用原来的决策树。通过遍历所有的子树，直到针对交叉验证数据集无法进一步降低错误率为止。</p><h2 id="sklearn种决策树的算法参数"><a href="#sklearn种决策树的算法参数" class="headerlink" title="sklearn种决策树的算法参数"></a>sklearn种决策树的算法参数</h2><h3 id="1、模型参数"><a href="#1、模型参数" class="headerlink" title="1、模型参数"></a>1、模型参数</h3><p>sklern中使用<code>sklearn.tree.DecisionTreeClassifier</code>类来实现决策树分类算法。其实几个典型的参数解释如下：</p><table><thead><tr><th style="text-align:left">名称</th><th style="text-align:center">功能</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">criterion</td><td style="text-align:center">特征选择标准</td><td style="text-align:left">‘gini’ or ‘entropy’ (default=”gini”)，前者是基尼系数，后者是信息熵。两种算法差异不大对准确率无影响，信息墒云孙效率低一点，因为它有对数运算.一般说使用默认的基尼系数”gini”就可以了，即CART算法。除非你更喜欢类似ID3, C4.5的最优特征选择方法。</td></tr><tr><td style="text-align:left">splitter</td><td style="text-align:center">特征划分标准</td><td style="text-align:left">‘best’ or ‘random’ (default=”best”) 前者在特征的所有划分点中找出最优的划分点。后者是随机的在部分划分点中找局部最优的划分点。 默认的”best”适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐”random” 。</td></tr><tr><td style="text-align:left">max_depth</td><td style="text-align:center">决策树最大深度</td><td style="text-align:left">int or None, optional (default=None) 一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。常用来解决过拟合</td></tr><tr><td style="text-align:left">min_impurity_decrease</td><td style="text-align:center">节点划分最小不纯度</td><td style="text-align:left">float, optional (default=0.) 这个值限制了决策树的增长，如果某节点的不纯度(基尼系数，信息增益，均方差，绝对差)小于这个阈值，则该节点不再生成子节点。 sklearn 0.19.1版本之前叫 min_impurity_split</td></tr><tr><td style="text-align:left">min_samples_split</td><td style="text-align:center">内部节点再划分所需最小样本数</td><td style="text-align:left">int, float, optional (default=2) 如果是 int，则取传入值本身作为最小样本数； 如果是 float，则去 ceil(min_samples_split * 样本数量) 的值作为最小样本数，即向上取整。</td></tr><tr><td style="text-align:left">min_samples_leaf</td><td style="text-align:center">叶子节点最少样本数</td><td style="text-align:left">如果是 int，则取传入值本身作为最小样本数； 如果是 float，则去 ceil(min_samples_leaf * 样本数量) 的值作为最小样本数，即向上取整。 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。</td></tr><tr><td style="text-align:left">max_leaf_nodes</td><td style="text-align:center">最大叶子节点数</td><td style="text-align:left">int or None, optional (default=None) 通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</td></tr><tr><td style="text-align:left">min_impurity_split</td><td style="text-align:center">信息增益的阀值</td><td style="text-align:left">决策树在创建分支时，信息增益必须大于这个阀值，否则不分裂</td></tr><tr><td style="text-align:left">min_weight_fraction_leaf</td><td style="text-align:center">叶子节点最小的样本权重和</td><td style="text-align:left">float, optional (default=0.) 这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</td></tr><tr><td style="text-align:left">class_weight</td><td style="text-align:center">类别权重</td><td style="text-align:left">dict, list of dicts, “balanced” or None, default=None 指定样本各类别的的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重，或者用“balanced”，如果使用“balanced”，则算法会自己计算权重，样本量少的类别所对应的样本权重会高。当然，如果你的样本类别分布没有明显的偏倚，则可以不管这个参数，选择默认的”None” 不适用于回归树 sklearn.tree.DecisionTreeRegressor</td></tr></tbody></table><h3 id="模型调参注意事项："><a href="#模型调参注意事项：" class="headerlink" title="模型调参注意事项："></a>模型调参注意事项：</h3><ul><li>当样本少数量但是样本特征非常多的时候，决策树很容易过拟合，一般来说，样本数比特征数多一些会比较容易建立健壮的模型</li><li>如果样本数量少但是样本特征非常多，在拟合决策树模型前，推荐先做维度规约，比如主成分分析（PCA），特征选择（Losso）或者独立成分分析（ICA）。这样特征的维度会大大减小。再来拟合决策树模型效果会好。</li><li>推荐多用决策树的可视化，同时先限制决策树的深度（比如最多3层），这样可以先观察下生成的决策树里数据的初步拟合情况，然后再决定是否要增加深度。</li><li>在训练模型先，注意观察样本的类别情况（主要指分类树），如果类别分布非常不均匀，就要考虑用class_weight来限制模型过于偏向样本多的类别。</li><li>决策树的数组使用的是numpy的float32类型，如果训练数据不是这样的格式，算法会先做copy再运行。</li><li>如果输入的样本矩阵是稀疏的，推荐在拟合前调用csc_matrix稀疏化，在预测前调用csr_matrix稀疏化。</li></ul><h2 id="实例：预测泰坦尼克号幸存者"><a href="#实例：预测泰坦尼克号幸存者" class="headerlink" title="实例：预测泰坦尼克号幸存者"></a>实例：预测泰坦尼克号幸存者</h2><p>数据预处理前期工作：</p><ul><li>筛选特征值，丢掉不需要的特征数据</li><li>对性别进行二值化处理（转换为0和1）</li><li>港口转换成数值型数据</li><li>处理缺失值（如年龄，有很多缺失值）</li></ul><p>1、首先读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_dataset</span><span class="params">(fname)</span>:</span></span><br><span class="line"><span class="comment">#     指定第一列作为行索引</span></span><br><span class="line">    data = pd.read_csv(fname, index_col=<span class="number">0</span>)</span><br><span class="line"><span class="comment">#     丢弃无用数据</span></span><br><span class="line">    data.drop([<span class="string">'Name'</span>, <span class="string">'Ticket'</span>, <span class="string">'Cabin'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment">#     处理性别数据</span></span><br><span class="line">    lables = data[<span class="string">'Sex'</span>].unique().tolist()</span><br><span class="line">    data[<span class="string">'Sex'</span>] = [*map(<span class="keyword">lambda</span> x: lables.index(x) , data[<span class="string">'Sex'</span>])]</span><br><span class="line"><span class="comment">#     处理登船港口数据</span></span><br><span class="line">    lables = data[<span class="string">'Embarked'</span>].unique().tolist()</span><br><span class="line">    data[<span class="string">'Embarked'</span>] = data[<span class="string">'Embarked'</span>].apply(<span class="keyword">lambda</span> n: lables.index(n))</span><br><span class="line"><span class="comment">#     处理缺失数据填充0</span></span><br><span class="line">    data = data.fillna(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">train = read_dataset(<span class="string">'code/datasets/titanic/train.csv'</span>)</span><br></pre></td></tr></table></figure><p>2、拆分数据集</p><p>把<code>Survived</code>列提取出来作为标签，然后在元数据集中将其丢弃。同时拆分数据集和交叉验证数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">y = train[<span class="string">'Survived'</span>].values</span><br><span class="line">X = train.drop([<span class="string">'Survived'</span>], axis=<span class="number">1</span>).values</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">print(<span class="string">"X_train_shape:"</span>, X_train.shape, <span class="string">" y_train_shape:"</span>, y_train.shape)</span><br><span class="line">print(<span class="string">"X_test_shape:"</span>, X_test.shape,<span class="string">"  y_test_shape:"</span>, y_test.shape)</span><br></pre></td></tr></table></figure><pre><code>X_train_shape: (712, 7)  y_train_shape: (712,)X_test_shape: (179, 7)   y_test_shape: (179,)</code></pre><p>3、拟合数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">"train score:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"test score:"</span>, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><pre><code>train score: 0.9845505617977528test score: 0.7597765363128491</code></pre><h2 id="优化模型参数"><a href="#优化模型参数" class="headerlink" title="优化模型参数"></a>优化模型参数</h2><h3 id="1、通过max-depth参数来优化模型"><a href="#1、通过max-depth参数来优化模型" class="headerlink" title="1、通过max_depth参数来优化模型"></a>1、通过<code>max_depth</code>参数来优化模型</h3><p>从以上输出数据可以看出，针对训练样本评分很高，但针对测试数据集评分较低。很明显这是过拟合的特征。解决决策树过拟合的方法是剪枝，包括前剪枝和后剪枝。但是<code>sklearn</code>不支持后剪枝，这里通过<code>max_depth</code>参数限定决策树深度，在一定程度上避免过拟合。</p><p>这里先创建一个函数使用不同的模型深度训练模型，并计算评分数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv_score</span><span class="params">(d)</span>:</span></span><br><span class="line">    clf = DecisionTreeClassifier(max_depth=d)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    <span class="keyword">return</span>(clf.score(X_train, y_train), clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line">depths = np.arange(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">scores = [cv_score(d) <span class="keyword">for</span> d <span class="keyword">in</span> depths]</span><br><span class="line">tr_scores = [s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> scores]</span><br><span class="line">te_scores = [s[<span class="number">1</span>] <span class="keyword">for</span> s <span class="keyword">in</span> scores]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出交叉验证数据集评分最高的索引</span></span><br><span class="line">tr_best_index = np.argmax(tr_scores)</span><br><span class="line">te_best_index = np.argmax(te_scores)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"bestdepth:"</span>, te_best_index+<span class="number">1</span>, <span class="string">" bestdepth_score:"</span>, te_scores[te_best_index], <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><pre><code>bestdepth: 5  bestdepth_score: 0.8603351955307262 </code></pre><p><strong>这里由于以上<code>train_test_split</code>方法对数据切分是随机打散的，造成每次用不同的数据集训练模型总得到不同的最佳深度。</strong>这里写个循环反复测试，最终验证这里看到最佳的分支深度为5出现的频率最高，初步确定5为深度模型最佳。</p><p>把模型参数和对应的评分画出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">depths = np.arange(<span class="number">1</span>,<span class="number">10</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(<span class="string">'max depth of decison tree'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Scores'</span>)</span><br><span class="line">plt.plot(depths, te_scores, label=<span class="string">'test_scores'</span>)</span><br><span class="line">plt.plot(depths, tr_scores, label=<span class="string">'train_scores'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq3frlhc9vj30ii0c6jsa.jpg" alt=""></p><h3 id="2、通过min-impurity-decrease来优化模型"><a href="#2、通过min-impurity-decrease来优化模型" class="headerlink" title="2、通过min_impurity_decrease来优化模型"></a>2、通过<code>min_impurity_decrease</code>来优化模型</h3><p>这个参数用来指定信息墒或者基尼不纯度的阀值，当决策树分裂后，其信息增益低于这个阀值时则不再分裂。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minsplit_score</span><span class="params">(val)</span>:</span></span><br><span class="line">    clf = DecisionTreeClassifier(criterion=<span class="string">'gini'</span>, min_impurity_decrease=val)</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    <span class="keyword">return</span> (clf.score(X_train, y_train), clf.score(X_test, y_test), )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定参数范围，分别训练模型并计算得分</span></span><br><span class="line"></span><br><span class="line">vals = np.linspace(<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">100</span>)</span><br><span class="line">scores = [minsplit_score(v) <span class="keyword">for</span> v <span class="keyword">in</span> vals]</span><br><span class="line">tr_scores = [s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> scores]</span><br><span class="line">te_scores = [s[<span class="number">1</span>] <span class="keyword">for</span> s <span class="keyword">in</span> scores]</span><br><span class="line"></span><br><span class="line">bestmin_index = np.argmax(te_scores)</span><br><span class="line">bestscore = te_scores[bestmin_index]</span><br><span class="line">print(<span class="string">"bestmin:"</span>, vals[bestmin_index])</span><br><span class="line">print(<span class="string">"bestscore:"</span>, bestscore)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>,<span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.xlabel(<span class="string">"min_impurity_decrease"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Scores"</span>)</span><br><span class="line">plt.plot(vals, te_scores, label=<span class="string">'test_scores'</span>)</span><br><span class="line">plt.plot(vals, tr_scores, label=<span class="string">'train_scores'</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure><pre><code>bestmin: 0.00202020202020202bestscore: 0.7988826815642458</code></pre><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq3frwqn2fj30hx0c7aac.jpg" alt=""></p><p><strong>问题：每次使用不同随机切割的数据集得出最佳参数为0.002很接近0，该怎么解读？</strong></p><p>值此为我们找到了两个参数,最佳深度depth=5 和最佳min_impurity_decrease=0.002，下面我来用两个参数简历模型进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics </span><br><span class="line"></span><br><span class="line">model = DecisionTreeClassifier(max_depth=<span class="number">5</span>, min_impurity_decrease=<span class="number">0.002</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tees_score:"</span>, model.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"查准率:"</span>,metrics.precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"召回率:"</span>,metrics.recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"F1_score:"</span>,metrics.f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><pre><code>tees_score: 0.7821229050279329查准率: 0.8461538461538461召回率: 0.5866666666666667F1_score: 0.6929133858267718</code></pre><h2 id="模型参数选择工具包"><a href="#模型参数选择工具包" class="headerlink" title="模型参数选择工具包"></a>模型参数选择工具包</h2><p>至此发现以上两种模型优化方法有两问题：</p><ul><li><p>1、数据不稳定：–&gt; 每次重新分配训练集测试集，原参数就不是最优了。 解决办法是多次计算求平均值。</p></li><li><p>2、不能一次选择多个参数：–&gt; 想考察max_depth和min_impurity_decrease两者结合起来的最优参数就没法实现。</p></li></ul><p>所幸<code>scikit-learn</code>在<code>sklearn.model_selection</code>包提供了大量的模型选择和评估的工具供我们使用。针对该问题可以使用<code>GridSearchCV</code>类来解决。</p><h3 id="利用GridSearchCV求最优参数"><a href="#利用GridSearchCV求最优参数" class="headerlink" title="利用GridSearchCV求最优参数"></a>利用<code>GridSearchCV</code>求最优参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">50</span>)</span><br><span class="line">param_grid = &#123;<span class="string">'min_impurity_decrease'</span>:thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"best_parms:&#123;0&#125;\nbest_score:&#123;1&#125;"</span>.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><pre><code>best_parms:{&apos;min_impurity_decrease&apos;: 0.00816326530612245}best_score:0.8114478114478114</code></pre><p>模型解读：<br>1、关键字参数<code>param_grid</code>是一个字典，字典的关键字对应的值是一个列表。<code>GridSearchCV</code>会枚举列表里所有值来构建模型多次计算训练模型，并计算模型评分，最终得出指定参数值的平均评分及标准差。</p><p>2、关键参数<code>sv</code>，用来指定交叉验证数据集的生成规则。这里sv=5表示每次计算都把数据集分成5份，拿其中一份作为交叉验证数据集，其他作为训练集。最终得出最优参数及最优评分保存在<code>clf.best_params_</code>和<code>clf.best_score_</code>里。</p><p>3、此外<code>clf.cv_results_</code>里保存了计算过程的所有中间结果。</p><h3 id="画出学习曲线："><a href="#画出学习曲线：" class="headerlink" title="画出学习曲线："></a>画出学习曲线：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_curve</span><span class="params">(train_sizes, cv_results, xlabel)</span>:</span></span><br><span class="line">    train_scores_mean = cv_results[<span class="string">'mean_train_score'</span>]</span><br><span class="line">    train_scores_std = cv_results[<span class="string">'std_train_score'</span>]</span><br><span class="line">    test_scores_mean = cv_results[<span class="string">'mean_test_score'</span>]</span><br><span class="line">    test_scores_std = cv_results[<span class="string">'std_test_score'</span>]</span><br><span class="line">    plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line">    plt.title(<span class="string">'parameters turning'</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.ylabel(<span class="string">'score'</span>)</span><br><span class="line">    plt.fill_between(train_sizes, </span><br><span class="line">                     train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, </span><br><span class="line">                     alpha=<span class="number">0.1</span>, color=<span class="string">"r"</span>)</span><br><span class="line">    plt.fill_between(train_sizes, </span><br><span class="line">                     test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, </span><br><span class="line">                     alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'.--'</span>, color=<span class="string">"r"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">'.-'</span>, color=<span class="string">"g"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">"best"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">50</span>)</span><br><span class="line"><span class="comment"># Set the parameters by cross-validation</span></span><br><span class="line">param_grid = &#123;<span class="string">'min_impurity_decrease'</span>: thresholds&#125;</span><br><span class="line"></span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(<span class="string">"best param: &#123;0&#125;\nbest score: &#123;1&#125;"</span>.format(clf.best_params_, </span><br><span class="line">                                                clf.best_score_))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot_curve(thresholds, clf.cv_results_, xlabel='gini thresholds')</span></span><br></pre></td></tr></table></figure><pre><code>best param: {&apos;min_impurity_decrease&apos;: 0.00816326530612245}best score: 0.8114478114478114</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fq3fs68mvsj30i70cr3zb.jpg" alt=""></p><h3 id="多组参数之间选择最优参数："><a href="#多组参数之间选择最优参数：" class="headerlink" title="多组参数之间选择最优参数："></a>多组参数之间选择最优参数：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">entropy_thresholds = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">gini_thresholds = np.linspace(<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">100</span>)</span><br><span class="line"><span class="comment">#设置参数矩阵：</span></span><br><span class="line">param_grid = [&#123;<span class="string">'criterion'</span>: [<span class="string">'entropy'</span>], <span class="string">'min_impurity_decrease'</span>: entropy_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'criterion'</span>: [<span class="string">'gini'</span>], <span class="string">'min_impurity_decrease'</span>: gini_thresholds&#125;,</span><br><span class="line">              &#123;<span class="string">'max_depth'</span>: np.arange(<span class="number">2</span>,<span class="number">10</span>)&#125;,</span><br><span class="line">              &#123;<span class="string">'min_samples_split'</span>: np.arange(<span class="number">2</span>,<span class="number">30</span>,<span class="number">2</span>)&#125;]</span><br><span class="line">clf = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=<span class="number">5</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">print(<span class="string">"best param:&#123;0&#125;\nbest score:&#123;1&#125;"</span>.format(clf.best_params_, clf.best_score_))</span><br></pre></td></tr></table></figure><pre><code>best param:{&apos;min_impurity_decrease&apos;: 0.00816326530612245}best score:0.8114478114478114</code></pre><p><code>结果1、{&#39;criterion&#39;: &#39;gini&#39;, &#39;min_impurity_decrease&#39;: 0.00816326530612245} -&gt;6</code></p><p><code>结果2、{&#39;min_samples_split&#39;: 22} -&gt;10</code></p><p><code>结果3、{&#39;min_samples_split&#39;: 20} -&gt;4</code></p><p><strong>结果波动很大，这里做了20次测试，对应结果1出现6次，结果2出现10次，结果3出现4次。</strong></p><p><strong>代码解读</strong>：<br>关键部分还是<code>param_grid</code>参数，他是一个列表。很对列表的第一个字典，选择信息墒<code>（entropy）</code>作为判断标准，取值0～1范围50等分；</p><p>第二个字典选择基尼系数，<code>min_impurity_decrease</code>取值0～0.2范围50等分。</p><p><code>GridSearchCV</code>会针对列表中的每个字典进行迭代，最终比较列表中每个字典所对应的参数组合，选择出最优的参数。</p><h3 id="生成决策树图形"><a href="#生成决策树图形" class="headerlink" title="生成决策树图形"></a>生成决策树图形</h3><p>下面代码可以生成.dot文件，需要电脑上安装<code>graphviz</code>才能把文件转换成图片格式。</p><p><code>Mac</code>上可以使用<code>brew install graphviz</code>命令来安装，它会同时安装8个依赖包。这里一定注意<code>Mac</code>环境下的权限问题：由于<code>Homebrew</code>默认是安装在<code>/usr/local</code>下，而<code>Mac</code>有强制保护不支持<code>sudo chown -R uname local</code>对<code>local</code>文件夹进行权限修改。 </p><p>这里的解决方式是把<code>local</code>下<code>bin</code>,<code>lib</code>,<code>Cellar</code>等所需单个文件夹下进行赋权，即可成功安装。</p><ol><li>在电脑上安装 graphviz</li><li>运行 <code>dot -Tpng tree.dot -o filename.png</code></li><li>在当前目录查看生成的决策树 filename.png</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"></span><br><span class="line">clf = DecisionTreeClassifier(min_samples_split=<span class="number">22</span>)</span><br><span class="line">clf = clf.fit(X_train, y_train)</span><br><span class="line">train_score = clf.score(X_train, y_train)</span><br><span class="line">test_score = clf.score(X_test, y_test)</span><br><span class="line">print(<span class="string">'train score: &#123;0&#125;; test score: &#123;1&#125;'</span>.format(train_score, test_score))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出 titanic.dot 文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"tree.dot"</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f = tree.export_graphviz(clf, out_file=f)</span><br></pre></td></tr></table></figure><pre><code>train score: 0.8834269662921348; test score: 0.8268156424581006</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fq3fe05mm1j31kw0zfgqd.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;利用信息墒判定先对那个特征进行分裂&quot;&gt;&lt;a href=&quot;#利用信息墒判定先对那个特征进行分裂&quot; class=&quot;headerlink&quot; title=&quot;利用信息墒判定先对那个特征进行分裂&quot;&gt;&lt;/a&gt;利用信息墒判定先对那个特征进行分裂
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>scikit-learn逻辑回归</title>
    <link href="http://www.ihoge.cn/2018/Logistic-regression.html"/>
    <id>http://www.ihoge.cn/2018/Logistic-regression.html</id>
    <published>2018-04-05T13:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><h3 id="逻辑回归算法的预测函数"><a href="#逻辑回归算法的预测函数" class="headerlink" title="逻辑回归算法的预测函数"></a>逻辑回归算法的预测函数</h3><p>参考：<a href="http://blog.kamidox.com/logistic-regression.html" target="_blank" rel="noopener">逻辑回归梯度下降算法推导</a></p><p>这是一个非黑即白的世界，我们需要找出一个函数模型使其输出值在[0,1]之间，然后选择0.5位基准值大于0.5就认为预测值为1反之为0。</p><p>该函数就是<br><code>Sigmoid函数，也叫做Logistic函数：</code><br>$g\left( x \right)\; =\; \frac{1}{1+e^{-z}}$</p><p>它结合线性回归函数$h_{\theta }\left( x \right)\; =\; \theta ^{T}x$，生成逻辑回归算法的预测函数：<br>$h_{\theta }\left( x \right)\; =\; g\left( z \right)=g\left( \theta ^{T}x \right)=\frac{1}{1+e^{-\theta ^{^{T}x}}}$</p><h3 id="成本函数"><a href="#成本函数" class="headerlink" title="成本函数"></a>成本函数</h3><p>我们不能用线性回归模型的成本函数来推导逻辑回归的成本函数那样态复杂。为了方便求出成本函数我们分别分成y=1和y=0两种情况分别考虑其预测值与真实值的误差：<br>$$\;\mbox{C}ost\left(h_{\theta }\left(x\right),y\right)\; =\;\left[\begin{array}{c}-\log\left(h_{\theta}\left(x\right)\right),\;\;\;\;\; y\;=1 \ -\log\left(1-h_{\theta }\left( x \right) \right),\; \; \; y\; =0 \end{array} \right]$$</p><p>成本函数统一写法：<br>$$\; \mbox{C}ost\left( h_{\theta }\left( x \right),y \right)\; =\; -y\log \left( h_{\theta }\left( x \right) \right)\; -\; \left( 1-y \right)\log \left( 1-h_{\theta }\left( x \right) \right)$$</p><p>由于y是{0,1}区间内的离散值，当y=1时， 1-y=0，上半式的后半部分为0；反之亦然。因此上市与分表达的成本计算公式是等价的。至此根据一个样本的计算公式，很容易卸除所有样本的成本平均值，可以得出：<br>$$J\left( \theta  \right)\; =\; -\frac{1}{m}\left[ \sum_{i=1}^{m}{y^{\left( i \right)}\log \left( h_{\theta }\left( x^{\left( i \right)} \right) \right)\; +\; \left( 1-y^{\left( i \right)} \right)\log \left( 1-h_{\theta }\left( x^{\left( i \right)} \right) \right)} \right]$$</p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>根据梯度下降算法的定义可以得出：</p><p>$$\theta_{j\; =\; }\theta_{j\; }-\; \alpha \frac{\partial }{\partial \theta_{j}}J\left( \theta\right)$$<br>这里的关键是求解成本函数的偏导数，最终高推导出来的梯度下降算法公式为：</p><p>$$\theta_{j\;=\;}\theta_{j\;}-\;\alpha\frac{1}{m}\sum_{i=1}^{m}{\left(h_{\theta }\left(x^{\left(i\right)\;}\right)\;-\;y^{\left(i\right)}\right)x_{j}^{\left(i\right)}}$$</p><h2 id="多元分类"><a href="#多元分类" class="headerlink" title="多元分类"></a>多元分类</h2><p>逻辑回归解决多元分类问题的思路是：y={0,1,2,…n}，总共有n+1个类别，首先把问题转化为二元分类问题，分别把y=0作为一个类别同时y={1,2,3…n}作为另一个类别；接着把y=1作为一个类别，以此类推，在计算他们的概率。这里总共需要n+1个预测函数：<br>$$$$<br>预测出来的概率最高的类别，就是样本所属的类别。（因为概率越接近1成本函数值越低，越接近真实值）</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>过拟合是指墨芯很好的拟合了驯良样本，但对新数据预测的准确性很差，这是因为模型太复杂了。解决办法是减少输入特征的个数，或者获取更多的驯良样本。这里介绍正则化也是用来解决模型过拟合问题的一个方法。</p><ul><li>保留所有特征，减小特征的权重$\theta_j$的值，确保所有的特征对预测值都有很少量大贡献。</li><li>当每个特征x对预测值都有少量的贡献时，模型就可以很好的工作，也就是正则化的目的，可以用来解决特征过多时的过拟合问题</li></ul><h3 id="线性回归模型的正则化"><a href="#线性回归模型的正则化" class="headerlink" title="线性回归模型的正则化"></a>线性回归模型的正则化</h3><p>$$J\left(\theta\right)\;=\;\frac{1}{2m}\left[\sum_{i=1}^{m}{\left(h_{\theta}\left(x^{\left(i\right)}\right)\;-\;y^{\left(i\right)}\right)^{2}}\right]\;+\;\lambda\sum_{j=1}^{n}{\theta_{j}^{2}}$$</p><p>公式的前半部分局势线性回归的成本函数，后半部分加入<strong>正则项</strong>。其中$λ$的值有两个目的，既要维持对训练样本的拟合，又要避免对训练样本的过拟合。如果λ太大，则能确保不出现过拟合，但可能对现有训练样本出现欠拟合。<br>从数学的角度来看，成本函数增加了一个正则项 $\lambda\sum_{j=1}^{n}{\theta_{j}^{2}}$后，成本函数不再唯一的由预测值与真实值的误差所决定，还和参数$\theta$的大小有关。有了这个限制后要实现成本函数最小的目的，$\theta$就不能随便取值。比如某个较大的$\theta$值可能会让预测值与真实值的误差$\left( h_{\theta }\left( x^{\left( i \right)} \right)-y^{\left( i \right)} \right)^{2}$值很小，但是会导致$\theta_{j}^{2}$很大，最终的结果是成本函数太大。这样通过调节参数λ就可以控制正则项的权重，从而避免线性回归算法过拟合。<br>利用正则化的成本函数，可以推导出正则化后的参数迭代函数：</p><p>$$\theta_{j}\;=\;\theta_{j}\; -\; \alpha \frac{1}{m}\sum_{i=1}^{m}{\left[ \left( \left( h\left( x^{\left( i \right)} \right)\; -\; y^{\left( i \right)} \right)x_{j}^{\left( i \right)} \right)\; +\; \frac{\lambda }{m}\theta_{j} \right]}\; $$</p><p>$$\theta_{j}\;=\;\theta_{j}\; \left( 1\; -\; \alpha \frac{\lambda }{m} \right)\; -\; \alpha \frac{1}{m}\sum_{i=1}^{m}{}\left( \left( h\left( x^{\left( i \right)} \right)\; -\; y^{\left( i \right)} \right)x_{j}^{\left( i \right)} \right)\;$$</p><p>$\left( 1\; -\; \alpha \frac{\lambda }{m} \right)$因子在每次迭代时都将把$θ<em>{j}$ 收缩一点点。因为α和λ是正数，而 $m$ 是训练阳历的个数，是个比较大的正整数。为什么要对 $θ</em>{j }$ 进行收缩呢？因为加入正则项的成本函数和 $θ<em>{j}^{2}$ 成正比，所以迭代是需要不断试图减小 $θ</em>{j }$ 的值。</p><h3 id="逻辑回归模型正则化"><a href="#逻辑回归模型正则化" class="headerlink" title="逻辑回归模型正则化"></a>逻辑回归模型正则化</h3><p>使用相同的思路，也可以对逻辑回归模型的成本函数进行正则化，其方法也是在原来的成本函数上加上正则项：<br>$$J\left( \theta  \right)\; =\; -\frac{1}{m}\left[ \sum_{i=1}^{m}{y^{\left( i \right)}\log \left( h_{\theta }\left( x^{\left( i \right)} \right) \right)\; +\; \left( 1-y^{\left( i \right)} \right)\log \left( 1-h_{\theta }\left( x^{\left( i \right)} \right) \right)} \right]\; +\; \frac{\lambda }{2m}\sum_{j=1}^{n}{\theta_{j}^{2}\;}$$<br>相应的，正则化后参数迭代公式为：</p><p>$$\theta_{j}\; \; =\; \theta_{j}\; -\; \alpha \frac{\partial }{\partial \theta_{j}}J\left( \theta  \right)$$<br>$$\theta_{j}\; =\; \theta_{j}\; -\; \alpha \left[ \frac{1}{m}\sum_{i=1}^{m}{}\left( h_{\theta }\left( x^{\left( i \right)} \right)\; -\; y^{\left( i \right)} \right)x_{j}^{\left( i \right)}\; +\; \frac{\lambda }{m}\theta_{j} \right]\;$$<br>$$\theta_{j}\; =\; \theta_{j}\; \left( 1\; -\; \alpha \frac{\lambda }{m} \right)\; -\; \alpha \frac{1}{m}\sum_{i=1}^{m}{}\left( \left( h\left( x^{\left( i \right)} \right)\; -\; y^{\left( i \right)} \right)x_{j}^{\left( i \right)} \right)\;$$</p><p>需要注意的是，上式中$j&gt;=1$，因此$\theta_{0}$没有参与正则化。另外需要注意逻辑回归和线性回归的参数迭代算法看起来形式是一样的，但其实他们的算法不一样，因为两个式子的预测函数$h_{θ}({x})$不一样。针对线性回归：$h_{θ}({x}) = θ^{T}x$，而针对逻辑回归的是：$h_{\theta }\left( x \right)\; =\; g\left( z \right)=g\left( \theta ^{T}x \right)=\frac{1}{1+e^{-\theta ^{^{T}x}}}$</p><h2 id="算法参数"><a href="#算法参数" class="headerlink" title="算法参数"></a>算法参数</h2><h3 id="正则项权重"><a href="#正则项权重" class="headerlink" title="正则项权重"></a>正则项权重</h3><p>上面介绍的正则项权重 λ ，在<code>LogisiticRegression</code>里有个参数 C 与之对应但成反比：</p><ul><li>C 越大正则项权重越小，模型容易出现过拟合</li><li>C 越小正则项权重越大，模型容易出现欠拟合</li></ul><h3 id="L1-L2范数"><a href="#L1-L2范数" class="headerlink" title="L1\L2范数"></a>L1\L2范数</h3><p>创建逻辑回归模型时，有个参数 penalty ，其取值有‘l1’或‘l2’，实际上就是指定前面介绍的正则项的形式。 在成本函数里添加的正则项 $\sum_{j=1}^{n}{\theta_{j}^{2}}$， 这时实际上就是个L2正则项。<br>L1:</p><p>$\left|\left|\theta\right|\right|_ {1}\; =\; \left|\theta_{1}\right|\;+\; \left| \theta_{2} \right|$<br>L2:</p><p>$\left| \left| \theta  \right| \right|_ {2}\; =\; \sqrt{\theta_{1}^{2}\; +\; \theta_{2}^{2}}$</p><p>L1 L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。范数作为正则项，会让模型参数$\theta$稀疏化， 既让模型参数向量里为0的元素尽量多。在支持向量机（support vector machine）学习过程中，实际是一种对于成本函数(cost function)求解最优，得出稀疏解。<br>L2 范数作为正则项式让模型参数尽量小，但不会为0，尽量让每个特征对预测值都有一些小的贡献，得出稠密解。<br>在梯度下降算法的迭代过程中，实际上是在成本函数的等高线上跳跃，并最终收敛在误差最小的点上（此处为未加正则项之前的成本误差）。而正则项的本质就是<strong>惩罚</strong>。 模型在训练的过程中，如果没有遵守正则项所表达的规则，那么成本会变大，即受到了惩罚，从而往正则项所表达的规则处收敛。 成本函数哎这两项规则的综合作用下，正则化后的模型参数应该收敛在误差等值线与正则项等值线相切的点上。<br>作为推论， L1 范数作为正则项由以下几个用途：</p><ul><li>特征选择： 它会让模型参数向量里的元素为0的点尽量多。 因此可以排除掉那些对预测值没有什么影响的特征，从而简化问题。所以 L1 范数解决过拟合措施实际上是减少特征数量。</li><li>可解释性： 模型参数向量稀疏化后，只会留下那些对预测值有重要影响的特征。 这样我们就容易解释模型的因果关系。 比如针对某个癌症的筛查，如果有100个特征，那么我们无从解释到底哪些特征对阳性成关键作用。 稀疏化后，只留下几个关键特征，就更容易看到因果关系</li></ul><p>由此可见， L1 范数作为正则项，更多的是一个分析工具，而适合用来对模型求解。因为它会把不重要的特征直接去除。 大部分情况下，我们解决过拟合问题，还是选择 L2 单数作为正则项， 这也是 sklearn 里的默认值。</p><h2 id="实例：乳腺癌检测"><a href="#实例：乳腺癌检测" class="headerlink" title="实例：乳腺癌检测"></a>实例：乳腺癌检测</h2><p>使用逻辑回归算法解决乳腺癌检测问题。 我们需要先采集肿瘤病灶造影图片， 然后对图片进行分析， 从图片中提取特征， 在根据特征来训练模型。 最终使用模型来检测新采集到的肿瘤病灶造影， 判断是良性还是恶性。 这个是典型的二元分类问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">X = cancer.data</span><br><span class="line">y = cancer.target</span><br><span class="line"></span><br><span class="line">print(X.shape, y.shape,<span class="string">'\n'</span>, X[<span class="number">0</span>], <span class="string">'\n'</span>, y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>(569, 30) (569,)  [1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01 1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02 6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01 1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01 4.601e-01 1.189e-01]  0</code></pre><p>实际上它只关注了 10 个特征，然后又构造出来每个特征的标准差及最大值，这样每个特征又衍生出了两个特征，所以共有30个特征。</p><p>这里该案例使用了特征提取手段，这一方法在实际工程应用中是很常用的。</p><p>举个例子：<br>我们需要监控数据中心每个物理主机的运行情况，其中 CPU 占用率、内存占用率、网络吞吐量是几个重要的指标。 问：有台主机 CPU 占用率80%， 这个主机状态是否正常？ 要不要发警告？ 这个要看情况！仅从 CPU 占用率来看还不能判断主机是否正常，还要看内存占用情况和网络吞吐量情况。 如果此时内存占用也成比例上升， 且网络吞吐量也在合理的水平，那么造成这一状态的可能是用户访问量过大， 导致主机负责增加， 不需要警告。 但如果内存占用、 网络吞吐量和 CPU 占用不在同一量级那么主机就可以处于不正常的状态。 所以这里需要构建一个复合特征， 如 CPU 占用率和内存占用率的比值， 以及 CPU 占用率和网络吞吐量的值， 这样构造出来的特征更真实地体现了现实问题中的内在规则。 </p><p>所以： <strong>提取特征时，不妨从事物内在逻辑关系入手，分析已有特征之间的关系， 从二构造出新的特征</strong></p><p>疑问： 该方式是否直接会导致多重共线性的出现？</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">model = LogisticRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">train_score = model.score(X_train, y_train)</span><br><span class="line">test_score = model.score(X_test, y_test)</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">print(<span class="string">"train_score:"</span>, train_score)</span><br><span class="line">print(<span class="string">"test_score:"</span>, test_score)</span><br></pre></td></tr></table></figure><pre><code>train_score: 0.9626373626373627test_score: 0.9473684210526315</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score</span><br><span class="line">print(<span class="string">"查准率："</span>, precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"召回率："</span>, recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"F1Score："</span>, f1_score(y_test, y_pred))</span><br><span class="line"></span><br><span class="line">print(np.equal(y_pred, y_test).shape[<span class="number">0</span>], y_test.shape[<span class="number">0</span>]) <span class="comment"># 输出预测匹配成功数量和测试样本的数量</span></span><br></pre></td></tr></table></figure><pre><code>查准率： 0.9358974358974359召回率： 0.9733333333333334F1Score： 0.954248366013072114 114</code></pre><p>这里数量上显示全部都预测正确，而test_score却不是1，是因为sklearn不是使用这个数据来计算得分，因为这个数据不能完全反映误差情况，而是使用预测概率来计算模型得分。</p><p><em>那么查准率和召回率是否同理？</em></p><h3 id="查看预测自信度"><a href="#查看预测自信度" class="headerlink" title="查看预测自信度"></a>查看预测自信度</h3><p>二元分类模型会针对每个样本输出的两个概率，即0和1的概率，哪个概率高就预测器哪个类别。我们可以找出针对测试数据集，模型预测的“自信度”低于90%的样本。我们先计算出测试数据集里每个样本的预测概率数据，针对每个样本会有两个数据：一个预测为0，一个预测为1。结合找出预测为阴性和阳性的概率大于0.1的样本。我们可以看下概率数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算每个测试样本的预测概率：</span></span><br><span class="line">y_pred_proba = model.predict_proba(X_test)</span><br><span class="line">print(<span class="string">"自信度示例："</span>,y_pred_proba[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>自信度示例： [0.00452578 0.99547422]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_pred_proba_0 = y_pred_proba[:, <span class="number">0</span>] &gt; <span class="number">0.1</span></span><br><span class="line">result = y_pred_proba[y_pred_proba_0]</span><br><span class="line"></span><br><span class="line">y_pred_proba_1 = result[:, <span class="number">1</span>] &gt; <span class="number">0.1</span></span><br><span class="line">print(result[y_pred_proba_1])</span><br></pre></td></tr></table></figure><pre><code>[[0.11338788 0.88661212] [0.18245824 0.81754176] [0.13110396 0.86889604] [0.35245276 0.64754724] [0.30664405 0.69335595] [0.24931118 0.75068882] [0.8350464  0.1649536 ] [0.44807883 0.55192117] [0.74071324 0.25928676] [0.43085792 0.56914208] [0.13388416 0.86611584] [0.33507985 0.66492015] [0.53672412 0.46327588] [0.11422612 0.88577388] [0.42946531 0.57053469] [0.69759146 0.30240854] [0.25982004 0.74017996] [0.12179042 0.87820958] [0.88546887 0.11453113]]</code></pre><h3 id="模型优化"><a href="#模型优化" class="headerlink" title="模型优化"></a>模型优化</h3><p>这里使用Pipeline来增加多项式特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">poly_model</span><span class="params">(degree=<span class="number">2</span>, penalty=penalty)</span>:</span></span><br><span class="line">    poly_features = PolynomialFeatures(degree=degree, include_bias=<span class="keyword">False</span>)</span><br><span class="line">    log_regr = LogisticRegression(penalty=penalty) <span class="comment"># 注意这里是L1而不是11，指的是使用L1范式作为其正则项</span></span><br><span class="line">    pipeline = Pipeline([(<span class="string">"poly_features"</span>,poly_features),(<span class="string">"log_regr"</span>,log_regr)])</span><br><span class="line">    <span class="keyword">return</span> pipeline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接着增加二阶多项式特征，创建并训练模型</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">model = poly_model(degree=<span class="number">2</span>, penalty=<span class="string">'l1'</span>)</span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train_score:"</span>,model.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"test_score:"</span>,model.score(X_test, y_test))</span><br></pre></td></tr></table></figure><pre><code>train_score: 0.9934065934065934test_score: 0.9649122807017544</code></pre><p>这里要注意的是使用L1范式作为其正则项，参数为<code>penalty=l1</code>。L1范数作为其正则项，可以实现参数的稀疏化，即自动帮我买选择出哪些对模型有关联的特征。我买可以观察下有多少个特征没有被丢弃即对应的模型参数$\theta_j$非0：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_regr = model.named_steps[<span class="string">'log_regr'</span>]</span><br><span class="line">print(<span class="string">"特征总量："</span>,log_regr.coef_.shape[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"特征保留量："</span>, np.count_nonzero(log_regr.coef_))</span><br></pre></td></tr></table></figure><pre><code>特征总量： 495特征保留量： 114</code></pre><p>逻辑回归模型的<code>coef_</code>属性里保存的就是模型参数。 从输出结果看，增加二阶多项式特征后，输入特征由原来的30个增加到了595个，在L1范数的“惩罚”下最终只保留了92个有效特征</p><h3 id="实验：利用决策树画出原始数据对预测相关性非0对特征"><a href="#实验：利用决策树画出原始数据对预测相关性非0对特征" class="headerlink" title="实验：利用决策树画出原始数据对预测相关性非0对特征"></a>实验：利用决策树画出原始数据对预测相关性非0对特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">dtmodel = DecisionTreeRegressor(max_depth=<span class="number">5</span>)</span><br><span class="line">dtmodel.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">"train_score"</span>, dtmodel.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"test_score"</span>, dtmodel.score(X_test, y_test))</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Bar</span><br><span class="line">index = np.nonzero(dtmodel.feature_importances_)</span><br><span class="line">bar = Bar()</span><br><span class="line">bar.add(<span class="string">""</span>, cancer.feature_names[index],dtmodel.feature_importances_[index])</span><br><span class="line">bar</span><br></pre></td></tr></table></figure><pre><code>train_score 0.9910875596851206test_score 0.6296416546416548</code></pre><p><img src="/images/sklearn/10.png" alt="png"></p><h3 id="评估模型：画出学习曲线"><a href="#评估模型：画出学习曲线" class="headerlink" title="评估模型：画出学习曲线"></a>评估模型：画出学习曲线</h3><p>首先画出L1范数作为正则项所对应的一阶和二阶多项式的学习曲线：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learn_curve</span><span class="params">(estimator, title, X, y, ylim = None, cv=None, n_jobs=<span class="number">1</span>, train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.</span>, <span class="number">5</span>)</span>)</span>:</span></span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(<span class="string">"train exs"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    </span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, </span><br><span class="line">                     train_score_mean + train_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'r'</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, </span><br><span class="line">                     test_score_mean + test_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>, label=<span class="string">'train score训练得分'</span>)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, <span class="string">'o-'</span>, color=<span class="string">'g'</span>, label=<span class="string">'cross-validation score交叉验证得分'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">titles = [<span class="string">"degree:1 penalty=L1"</span>,<span class="string">"degree:2 penalty=L1"</span>]</span><br><span class="line">degrees = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">penalty = <span class="string">'l1'</span></span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(degrees)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, len(degrees), i + <span class="number">1</span>)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i], penalty=penalty), titles[i],</span><br><span class="line">                    X, y, ylim = (<span class="number">0.8</span>, <span class="number">1.01</span>), cv = cv)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'耗时：'</span>, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/11.png" alt="png"></p><p>L2范数作为正则项画出对应一阶和二阶多项式学习曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">titles = [<span class="string">"degree:1 penalty=L2"</span>,<span class="string">"degree:2 penalty=L2"</span>]</span><br><span class="line">degrees = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">penalty = <span class="string">'l2'</span></span><br><span class="line"></span><br><span class="line">start = time.clock()</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(degrees)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, len(degrees), i + <span class="number">1</span>)</span><br><span class="line">    plot_learn_curve(poly_model(degree=degrees[i],penalty=penalty), titles[i],</span><br><span class="line">                    X, y, ylim = (<span class="number">0.8</span>, <span class="number">1.01</span>), cv = cv)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'耗时：'</span>, time.clock() - start)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/12.png" alt="png"></p><p>从上面两个图可以看出，使用二阶多项式并使用L1范数作为正则项的模型最优，训练样本评分最高，交叉验证样本评分最高。<br>训练样本评分和交叉验证样本评分之间的间隙还比较大，这说明可以通过采集更多数据来训练模型，以便进一步优化模型.</p><p>通过时间消耗对比上可以看出利用L1范式作为正则项需要花费的时间更多，是因为<code>sklearn</code>的<code>learning_curve()</code>函数在画学习曲线的过程中要对模型进行多次训练，并计算交叉验证样本评分。同时为了让曲线更平滑，针对每个点还会进行多次计算球平均值。这个就是<code>ShufferSplit</code>类的作用。在这个实例里只有569个样本是很小的数据集。如果数据集增加100倍，拿出来画学习曲线将是场灾难。</p><p>问题是针对大数据集，怎么画学习曲线？</p><p>思路一：可以考虑从大数据集选取一小部分数据来画学习曲线，待选择好最优的模型之后，在使用全部的数据来训练模型。这时需要警惕的是，尽量保证选择出来的这部分数据的<strong>标签分布与大数据集的标签分布相同</strong>，如针对二元分类，<strong>阳性和阴性比例要一致！</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;h3 id=&quot;逻辑回归算法的预测函数&quot;&gt;&lt;a href=&quot;#逻辑回归算法的预测函数&quot; class=&quot;headerlink&quot; ti
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>github 创建分支位作为云端备份解决hexo博客多PC间同步的问题</title>
    <link href="http://www.ihoge.cn/2018/githubhexos.html"/>
    <id>http://www.ihoge.cn/2018/githubhexos.html</id>
    <published>2018-04-05T09:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="方式一"><a href="#方式一" class="headerlink" title="方式一"></a>方式一</h2><p>1、这里需要删除next目录及根目录下的<code>.git</code>,（或者修改添加你不需要上传到远程的部分）同时删除bolg根目录下的<code>.gitignore</code>。这里我选择在主目录做了备份。</p><p>2、现在回到bolg根目录分别执行：</p><ul><li><code>git init</code>  初始化本地仓库</li><li><code>git checkout -b hexo</code> 创建并切换hexo分支</li><li><code>git remote add origin https://github.com/hooog/hooog.github.io.git</code>给hexo分支关联远程映射</li><li><code>git add .</code>  添加blog目录下所有文件，注意有个<code>.</code>（<code>.gitignore</code>声明过的文件不包含在内)</li><li><code>git commit -m &#39;添加描述&#39;</code></li><li><code>git push origin hexo</code> 将hexo分支上传到远程仓库 </li><li>如果加错了的话执行<code>git rm -r --cached .</code><br>到这里，云端备份就完成了。<br>同理也可以把分支备份到<code>Coding</code>的私密仓库上。（如果源代码涉及保密信息的话）</li></ul><p>3、将远程仓库的内容拷贝到新<code>PC</code>端端本地：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add origin https://github.com/hooog/hooog.github.io.git</span><br><span class="line">git fetch --all</span><br><span class="line">git reset --hard origin/master</span><br></pre></td></tr></table></figure></p><p><code>fetch</code>是将云端所有内容拉取下来。<code>reset</code>则是不做任何合并处理，强制将本地内容指向刚刚同步下来的云端内容（正常pull的话需要考虑不少冲突的问题，比较麻烦。）</p><p>4、更新文章后的同步操作：<br>假设在B电脑写完了文章，也<code>hexo d -g</code>发布成功，这时候需要将新文章的md文件更新上去。</p><p><code>git add .</code></p><p>这时候可以使用<code>git status</code>查看下状态，一般只显示刚刚更改过的文件状态。</p><p>然后执行：<br><code>git commit -m &quot;更新信息&quot;</code><br><code>git push origin hexo</code></p><p>在回到A电脑上的时候，只需要<br><code>git pull</code><br>即可同步更新</p><p>5、日常改动<br>平时我们对源文件有修改的时候记得先pull一遍代码，再将代码push到Hexo分支，就和日常的使用git一样~<br>依次执行：<br><code>git add .</code><br><code>git commit -m “…”</code><br><code>git push origin hexo</code>指令将改动推送到GitHub（此时当前分支应为hexo）；然后执行：<br><code>hexo g -d</code>发布网站到master分支上。</p><h2 id="方式二"><a href="#方式二" class="headerlink" title="方式二"></a>方式二</h2><p>把Hexo的源码备份到Github分支里面，思路就是上传到分支里存储，修改本地的时候先上传存储，再发布。更换电脑的时候再下载下来源文件</p><p>打开git-bash<br><code>git init</code><br><code>git remote add origin git@github.com:username/username.github.io.git</code><br><code>git add .</code><br><code>git commit -m &quot;blog&quot;</code><br><code>git push origin master:hexo</code></p><p>现在你会发现github你的博客仓库已经有了一个新分支<code>hexo</code>，我们的备份工作完成。后续以后，本地写好博文之后，可以先执行<br><code>git add .</code><br><code>git commit -m &quot;blog&quot;</code><br><code>git push origin master:hexo</code>进行备份，然后<br><code>hexo d -g</code>进行更新静态文件。这里推荐先进行备份，因为万一更新网站之后不小心丢失了源文件，就又得重新再来了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;方式一&quot;&gt;&lt;a href=&quot;#方式一&quot; class=&quot;headerlink&quot; title=&quot;方式一&quot;&gt;&lt;/a&gt;方式一&lt;/h2&gt;&lt;p&gt;1、这里需要删除next目录及根目录下的&lt;code&gt;.git&lt;/code&gt;,（或者修改添加你不需
      
    
    </summary>
    
      <category term="Hexo" scheme="http://www.ihoge.cn/categories/Hexo/"/>
    
    
      <category term="Github" scheme="http://www.ihoge.cn/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>使用Github和Coding共同托管hexo博客解决无法被百度收录的问题</title>
    <link href="http://www.ihoge.cn/2018/githubhexocodingbaidu.html"/>
    <id>http://www.ihoge.cn/2018/githubhexocodingbaidu.html</id>
    <published>2018-04-05T05:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>1、之前在阿里云注册的<code>.top</code>域名一直不受百度待见，遂重新申请了<code>.cn</code>域名。更换域名大致流程如下：</p><ul><li>在新申请的域名下添加域名解析：主机记录添加<code>@</code>和<code>www</code>两种<br><img src="/images/hexo/1.png" alt="png"></li><li>修改<code>CNAME</code>文件写入新域名<code>www.ihoge.cn</code></li><li>在<code>Github Pages</code>中更新新域名并<code>save</code></li></ul><p>2、更换域名发现仍不能被收录，原因是<code>Github</code>嫌弃百度蜘蛛爬取太频繁，屏蔽了百度。作为国内的小伙伴无非是一种很麻烦的事。这里我选择用<code>Coding</code>和<code>Github</code>双线托管，默认解析到<code>Github</code>,并给百度设置专线解析到<code>Coding</code></p><p>3、使用<code>Coding</code>作为<code>hexo</code>博客的第二托管平台。</p><ul><li>登陆<a href="https://coding.net并注册账户，这里需要简单的操作免费升级下账户才能使用`Coding`的`Pages`服务。" target="_blank" rel="noopener">https://coding.net并注册账户，这里需要简单的操作免费升级下账户才能使用`Coding`的`Pages`服务。</a></li><li>在<code>Coding</code>上新建项目<code>bolg</code></li><li>将<code>hexo</code>博客同步到新创建的仓库中<ul><li>第一次使用<code>Coding</code>需要使用<code>ssh</code>，方法和<code>Github</code>上一样</li><li>按照官方格式修改本地<code>hexo</code>根目录下的配置文件<br><img src="/images/hexo/2.png" alt="png"></li></ul></li><li>完成之后在<code>hexo</code>的根目录输入：<br><code>ssh -T git@git.coding.net</code></li><li>上一步执行没有报错就说明成功了。然后重新部署就可以将代码同时上传到<code>Github</code>和<code>Coding</code>上了。</li></ul><p>4、设置<code>Coding</code>的<code>Pages</code>服务</p><ul><li>部署来源选择master分支</li><li>绑定购买的域名</li><li><p>放置<code>Hosted by Coding Pages</code>文字版到首页提交审核（方便后面的seo优化）方法是找到<code>themes\next\layout\_partials\footer.swig</code>添加代码如下：<br><img src="/images/hexo/5.png" alt="png"></p></li><li><p>这里要注意上传的<code>CNAME</code>文件，上面填入购买的域名</p></li></ul><p>5、绑定域名并指定百度专线：<br><img src="/images/hexo/3.png" alt="png"></p><p>6、由于之前用<code>Github</code>托管造成百度站长抓取网站一直失败，显示服务器被拒绝。后来利用给百度指定<code>Coding</code>专线的方式才解决该问题。<br><img src="/images/hexo/4.png" alt="png"></p><p>7、将网站链接提交到百度<br>百度提供三种验证方式，以Html标签为例，在themes\next\layout_partials\head.swing中添加验证代码：<br><figure class="highlight plain"><figcaption><span>name</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">同理将链接提交到Google和搜狗</span><br><span class="line"></span><br><span class="line">8、给站点添加sitemap</span><br><span class="line"></span><br><span class="line">- hexo安装sitemap</span><br></pre></td></tr></table></figure></p><p>npm install hexo-generator-sitemap –save #sitemap.xml适合提交给谷歌搜素引擎<br>npm install hexo-generator-baidu-sitemap –save #baidusitemap.xml适合提交百度搜索引擎<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 在主题配置文件`_config.yml`中找到`sitemap`添加以下代码</span><br></pre></td></tr></table></figure></p><h1 id="自动生成sitemap"><a href="#自动生成sitemap" class="headerlink" title="自动生成sitemap"></a>自动生成sitemap</h1><p>sitemap:<br>    path: sitemap.xml<br>baidusitemap:<br>    path: baidusitemap.xml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 修改站点`_config`</span><br></pre></td></tr></table></figure></p><h1 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h1><h2 id="If-your-site-is-put-in-a-subdirectory-set-url-as-‘http-yoursite-com-child-39-and-root-as-‘-child-‘"><a href="#If-your-site-is-put-in-a-subdirectory-set-url-as-‘http-yoursite-com-child-39-and-root-as-‘-child-‘" class="headerlink" title="If your site is put in a subdirectory, set url as ‘http://yoursite.com/child&#39; and root as ‘/child/‘"></a>If your site is put in a subdirectory, set url as ‘<a href="http://yoursite.com/child&#39;" target="_blank" rel="noopener">http://yoursite.com/child&#39;</a> and root as ‘/child/‘</h2><p>url: <a href="http://www.ihoge.cn">http://www.ihoge.cn</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">9、添加蜘蛛协议robots.txt</span><br><span class="line">新建robots.txt文件，添加以下文件内容，把robots.txt放在hexo站点的source文件下。</span><br></pre></td></tr></table></figure></p><p>User-agent: *<br>Allow: /</p><p>Sitemap: <a href="http://www.ihoge.cn/sitemap.xml">http://www.ihoge.cn/sitemap.xml</a><br>Sitemap: <a href="http://www.ihoge.cn/baidusitemap.xml">http://www.ihoge.cn/baidusitemap.xml</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hexo d -g 提交后到百度站长平台找到Robots检测并更新查看是否生效。</span><br><span class="line"></span><br><span class="line">10、keywords 和 description</span><br><span class="line">在\scaffolds\post.md中添加如下代码，用于生成的文章中添加关键字和描述。</span><br></pre></td></tr></table></figure></p><p>keywords:<br>description:<br><code>`</code><br>（这里未操作）</p><p>11、修改文章链接<br>HEXO默认的文章链接形式为<code>domain/year/month/day/postname</code>，默认就是一个四级url，并且可能造成url过长，对搜索引擎是十分不友好的，我们可以改成 domain/postname 的形式。编辑站点<code>_config.yml</code>文件，修改其中的<code>permalink</code>字段改为<code>permalink: :year/:title/</code>即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;1、之前在阿里云注册的&lt;code&gt;.top&lt;/code&gt;域名一直不受百度待见，遂重新申请了&lt;code&gt;.cn&lt;/code&gt;域名。更换域名大致流程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在新申请的域名下添加域名解析：主机记录添加&lt;code&gt;@&lt;/
      
    
    </summary>
    
      <category term="Hexo" scheme="http://www.ihoge.cn/categories/Hexo/"/>
    
    
      <category term="Coding" scheme="http://www.ihoge.cn/tags/Coding/"/>
    
      <category term="Github" scheme="http://www.ihoge.cn/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>递归、尾递归、循环的比较及Python尾递归的优化</title>
    <link href="http://www.ihoge.cn/2018/Recursive.html"/>
    <id>http://www.ihoge.cn/2018/Recursive.html</id>
    <published>2018-04-02T19:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>最近在读魏老师推荐的算法图解这本书。该书内容很有趣，把枯燥无味的算法用一个个日常生活的小案例诠释的清晰易懂，很适合作为业余读物。<br>在读到关于递归算法的时候，本着打破砂锅问到底的精神大致研究了下递归算法和递归进阶 —&gt; <strong>尾递归</strong>。<br>后来发现Python的解释器不支持尾递归算法优化，而且<strong>Python之父曾经明确表示Python将不会支持尾递归优化！T T…</strong><br>不过看我华夏泱泱大国，人才济济，还是有哥们写出了Python上对尾递归爆栈问题的解决办法。</p><h2 id="什么是递归"><a href="#什么是递归" class="headerlink" title="什么是递归"></a>什么是递归</h2><p>一言概之： <strong>递归是一种优雅的问题解决办法。</strong> 它将程序员分为了三个截然不同的阵营：恨它的、爱它的、爱了几年以及恨了几年后又爱上它的。初接触时我对这句话是怀疑的，为什么我感觉递归让解决方案变得更绕了呢？ 直到认真去看了几个用递归思想解决问题的例子之后方才发现此话所言非虚。<br>粗略的说，递归就是让函数调用自己。它让解决方案更清晰，但并没有性能上的优势。正如Leigh Caldwell在Stack Overflow上说过一句话：“如果使用循环，程序的性能可能更高，如果使用递归，程序可能更容易理解。如何选择要看什么对你来说更重要”（这一点下面会详细说明）。由于递归函数调用自己，因此编写递归函数时，必须告诉它何时停止递归否则会陷入无限循环。所以每个递归函数都必须有两部分：<strong>基准条件和递归条件</strong></p><p>使用递归必须要理解 <strong>调用栈</strong> 这个概念。栈是一种简单的数据结构，遵循先进后出的原则。计算机在内部使用被称为 调用栈 的栈。</p><p>在递归的过程中，当计算机调用一个递归函数 A 的时候首先为该函数分配一块内存，并将函数调用涉及到的方法和所有变量的值存储到内存 a 中。递归函数 A 调用自己产生函数 B ，同样计算机会给函数 B 分配一块新的内存 b 用于存储 B 所涉及到的所有变量及方法，并压在 a 上。然后递归函数 B 继续调用自身生成函数 C 并分配内存块 c 压在 内存块 b上。只要递归函数未触发基准条件函数将一直循环调用并生成一个个新的内存块压在上一个内存块上面。这个操作叫 <strong>push（压栈）</strong>。</p><p>计算机使用一个 <strong>（栈）</strong> 来表示这些内存块，其中内存块 b 位于 a 内存块上面 c 在  b 上面（栈顶的内存块就是函数的当前活跃部分）。假设函数 C 触发了递归函数的基准条件，在执行完函数 C 后，栈顶的内存块 c 被弹出，这个操作被称作 <strong>pop（弹栈）</strong></p><p>这里在执行函数 B 和 C 时，函数 A 只执行了一部分。这是一个很重要的概念： <strong>调用另一个函数时，当前函数暂停并处于未完成状态</strong>。 该变量的所有变量值都还在内存中。 执行完函数 C 后，会又回到函数 B ，B 执行完毕又返回 A 并从离开的地方接着往下执行。 这个 <strong>栈</strong> 用于存放对多个函数的变量， 被称为 <strong>调用栈</strong>。</p><p>使用 <strong>栈</strong> 很方便，但是也要付出代价：存储先进的信息会占用大量的内存。每个函数调用都要占用一定的内存，如果栈很高，就意味着计算机存储了大量函数调用的信息。 而且 <strong>调用栈</strong> 的高度是有限制的（如Python中默认的最高层数时1000）。若递归层数超过一定阀值则会造成 <strong>栈溢出</strong> 也叫 <strong>爆栈</strong> 在这种情况下有三种选择：</p><ul><li>增大阀值</li><li>重新编写代码</li><li>使用 <strong>尾递归</strong></li></ul><h2 id="什么是尾递归"><a href="#什么是尾递归" class="headerlink" title="什么是尾递归"></a>什么是尾递归</h2><p>关于尾递归，百度百科解释的算是很清晰明了了：</p><p>当编译器检测到一个函数调用是尾递归的时候，它就覆盖当前的活动记录而不是在栈中去创建一个新的。编译器可以做到这点，因为递归调用是当前活跃期内最后一条待执行的语句，于是当这个调用返回时栈帧中并没有其他事情可做，因此也就没有保存栈帧的必要了。通过覆盖当前的栈帧而不是在其之上重新添加一个，这样所使用的栈空间就大大缩减了，这使得实际的运行效率会变得更高。</p><ul><li><p><strong>尾递归就是把当前的运算结果（或路径）放在参数里传给下层函数</strong></p></li><li><p><strong>如果在递归函数中，递归调用返回的结果总被直接返回，则称为尾部递归。</strong> </p></li></ul><h2 id="递归、尾递归、循环的比较"><a href="#递归、尾递归、循环的比较" class="headerlink" title="递归、尾递归、循环的比较"></a>递归、尾递归、循环的比较</h2><p>下面是我分别用普通递归、尾递归、循环实现阶乘的小程序，这个是我能想到最简洁直观的例子了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置允许递归的深度</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.setrecursionlimit(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 普通递归阶乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fact</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> n * fun(n<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尾递归阶乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fact_iter</span><span class="params">(n, num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line">    <span class="keyword">return</span> fact_iter(n - <span class="number">1</span>, n * num)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环阶乘</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fact_loop</span><span class="params">(n)</span>:</span></span><br><span class="line">    num = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n):</span><br><span class="line">        num = num * i</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure><h2 id="尾递归的Python优化"><a href="#尾递归的Python优化" class="headerlink" title="尾递归的Python优化"></a>尾递归的Python优化</h2><p>Python的爸爸已经明确表示Python将不会支持尾递归优化了。但是我先是在网上查相关资料的时候发现了两种方法对尾递归的优化：</p><h3 id="方式一：实现一个-tail-call-optimized-装饰器"><a href="#方式一：实现一个-tail-call-optimized-装饰器" class="headerlink" title="方式一：实现一个 tail_call_optimized 装饰器"></a>方式一：实现一个 tail_call_optimized 装饰器</h3><p><a href="http://code.activestate.com/recipes/474088/" target="_blank" rel="noopener">源码：点这里</a></p><p>为了更清晰的展示开启尾递归优化前、后调用栈的变化和tail_call_optimized装饰器抛异常退出递归调用栈的作用, 一个牛人利用pudb调试工具做了动图。<br><a href="http://python.jobbole.com/86937/" target="_blank" rel="noopener">动图：点这里</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TailRecurseException</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args, kwargs)</span>:</span></span><br><span class="line">        self.args = args</span><br><span class="line">        self.kwargs = kwargs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tail_call_optimized</span><span class="params">(g)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function decorates a function with tail call</span></span><br><span class="line"><span class="string">    optimization. It does this by throwing an exception</span></span><br><span class="line"><span class="string">    if it is it's own grandparent, and catching such</span></span><br><span class="line"><span class="string">    exceptions to fake the tail call optimization.</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    This function fails if the decorated</span></span><br><span class="line"><span class="string">    function recurses in a non-tail context.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        f = sys._getframe()</span><br><span class="line">        <span class="comment"># 为什么是grandparent, 函数默认的第一层递归是父调用,</span></span><br><span class="line">        <span class="comment"># 对于尾递归, 不希望产生新的函数调用(即:祖父调用),</span></span><br><span class="line">        <span class="comment"># 所以这里抛出异常, 拿到参数, 退出被修饰函数的递归调用栈!</span></span><br><span class="line">        <span class="keyword">if</span> f.f_back <span class="keyword">and</span> f.f_back.f_back \</span><br><span class="line">            <span class="keyword">and</span> f.f_back.f_back.f_code == f.f_code:</span><br><span class="line">            <span class="comment"># 抛出异常</span></span><br><span class="line">            <span class="keyword">raise</span> TailRecurseException(args, kwargs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">return</span> g(*args, **kwargs)</span><br><span class="line">                <span class="keyword">except</span> (TailRecurseException, e):</span><br><span class="line">                    <span class="comment"># 捕获异常, 拿到参数, 退出被修饰函数的递归调用栈</span></span><br><span class="line">                    args = e.args</span><br><span class="line">                    kwargs = e.kwargs</span><br><span class="line">    func.__doc__ = g.__doc__</span><br><span class="line">    <span class="keyword">return</span> func</span><br><span class="line"> </span><br><span class="line"><span class="meta">@tail_call_optimized</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span><span class="params">(n, acc=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"calculate a factorial"</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> acc</span><br><span class="line">    <span class="keyword">return</span> factorial(n<span class="number">-1</span>, n*acc)</span><br><span class="line"> </span><br><span class="line">print(factorial(<span class="number">100</span>))</span><br></pre></td></tr></table></figure><p>因为尾递归没有调用栈的嵌套, 所以Python也不会报RuntimeError: maximum recursion depth exceeded错误了!</p><p>但是对于这种发发是否真正意义上实现了完美的优化呢？ 针对这个方法有个知乎大神发话了：</p><p>TCO，tail-call optimization，其实有多种解读方式</p><p>最常见的解读方式是：对于尾调用的函数调用，不要浪费栈空间，而要复用调用者的栈空间。这样的结果就是一长串尾调用不会爆栈，而没有TCO的话同样的调用就会爆栈。从这个意义上说，如上方法的那个recipe确实达到了TCO的部分目的：</p><ul><li>通过stack introspection查看调用链上的调用者之中有没有自己</li><li>有的话，通过抛异常来迫使栈回退（stack unwind）到之前的一个自己的frame</li><li>在回退到的frame接住异常，拿出后来调用的参数，用新参数再次调用自己</li></ul><p>这样就可以让尾递归不爆栈。但这样做性能是没保证的…而且对于完全没递归过的一般尾调用也不起作用。一种对TCO的常见误解是：由编译器或运行时系统把尾调用/尾递归实现得很快。这不是TCO真正要强调的事情——不爆栈才是最重要的。也就是说其实重点不在“优化”，而在于“尾调用不爆栈”这个语义保证。“proper tail-call”的叫法远比“tail-call optimization”来得合适。因而像这种种做法，可以算部分TCO，但算不上“性能优化”意义上的优化。</p><h3 id="方式二：pattern-matching"><a href="#方式二：pattern-matching" class="headerlink" title="方式二：pattern-matching"></a>方式二：pattern-matching</h3><p>关于尾递归实现：</p><p> 很简单，永远不要在函数内部去调用自己，甚至不要调用任何东西。每一次调用，都让外层一个agent来做。保证函数栈深永远不超过2或3即可。</p><p>这哥们更牛逼了，直接做了个Python的模式匹配库，包含了尾递归的优化，实现了coroutine。</p><p>把需要优化的函数的return改成yield，外面套个装饰器，就叫tail_call_opm。装饰器最内层的逻辑是<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        ret=next(ret) </span><br><span class="line">    except:</span><br><span class="line">        return ret</span><br></pre></td></tr></table></figure></p><p>这个应该没有复用释放的空间…但刷题时换了这个就不爆栈了。返回闭包的话情况应该会更复杂一些。<br><a href="https://github.com/Xython/pattern-matching" target="_blank" rel="noopener">pattern-matching参考GitHub地址</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;最近在读魏老师推荐的算法图解这本书。该书内容很有趣，把枯燥无味的算法用一个个日常生活的小案例诠释的清晰易懂，很适合作为业余读物。&lt;br&gt;在读到关于递归算法的时候，本着打破砂锅问到底的精神大致研究了下递归算法和递归进阶 —&amp;gt; &lt;str
      
    
    </summary>
    
      <category term="Python" scheme="http://www.ihoge.cn/categories/Python/"/>
    
    
      <category term="递归" scheme="http://www.ihoge.cn/tags/%E9%80%92%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>一个自编译线性回归的案例</title>
    <link href="http://www.ihoge.cn/2018/sklearnLiner.html"/>
    <id>http://www.ihoge.cn/2018/sklearnLiner.html</id>
    <published>2018-04-02T09:05:59.000Z</published>
    <updated>2018-04-07T13:02:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>在线性回归中，我们想要建立一个模型，来拟合一个因变量 y 与一个或多个独立自变量(预测变量) x 之间的关系。</p><p>给定：</p><p>数据集<br>$$\left{ \left( x^{\left( 1 \right)},y^{\left( 1 \right)} \right),\; …,\; \left(x^{\left( m \right)},y^{\left( m \right)} \right) \right}$$</p><p>$x_{i}$是d-维向量$X^{i}\; =\; \left( x_{1}^{\left( i \right)},\; …,\; x_{d}^{\left( i \right)} \right)$</p><p>$y^{(i)}$是一个目标变量，它是一个标量</p><p>线性回归模型可以理解为一个非常简单的神经网络：</p><p>它有一个实值加权向量$w\; =\; \left( w^{\left( i \right)},\; …,\; w^{\left( d \right)} \right)$<br>它有一个实值偏置量 b<br>它使用恒等函数作为其激活函数</p><p>线性回归模型可以使用以下方法进行训练</p><p>a) <strong>梯度下降法</strong></p><p>b) <strong>正态方程(封闭形式解)</strong>： $w\; =\; \left( X^{T}X \right)^{-1}X^{T}y$</p><p>其中 X 是一个矩阵，其形式为$\left( m,\; n_{featu\mbox{re}s} \right)$，包含所有训练样本的维度信息。</p><p>而正态方程需要计算$\left( X^{T}X \right)$的转置。这个操作的计算复杂度介于$O\left( n_{featu\mbox{re}s}^{2.4} \right)$和$O\left( n_{featu\mbox{re}s}^{3} \right)$之间，而这取决于所选择的实现方法。因此，如果训练集中数据的特征数量很大，那么使用正态方程训练的过程将变得非常缓慢。</p><p>线性回归模型的训练过程有不同的步骤。首先(在步骤 0 中)，模型的参数将被初始化。在达到指定训练次数或参数收敛前，重复以下其他步骤。</p><h3 id="第-0-步："><a href="#第-0-步：" class="headerlink" title="第 0 步："></a>第 0 步：</h3><p>用0 (或小的随机值)来初始化权重向量和偏置量，或者直接使用正态方程计算模型参数</p><h3 id="第-1-步-只有在使用梯度下降法训练时需要-："><a href="#第-1-步-只有在使用梯度下降法训练时需要-：" class="headerlink" title="第 1 步(只有在使用梯度下降法训练时需要)："></a>第 1 步(只有在使用梯度下降法训练时需要)：</h3><p>计算输入的特征与权重值的线性组合，这可以通过矢量化和矢量传播来对所有训练样本进行处理：<br>$\dot{y}\; =\; X\; \cdot \; w\; +b$</p><p>其中 X 是所有训练样本的维度矩阵，其形式为$\left( m,\; n_{featu\mbox{re}s} \right)$；这里我用· 表示$\wedge$ 。</p><h3 id="第-2-步-只有在使用梯度下降法训练时需要-："><a href="#第-2-步-只有在使用梯度下降法训练时需要-：" class="headerlink" title="第 2 步(只有在使用梯度下降法训练时需要)："></a>第 2 步(只有在使用梯度下降法训练时需要)：</h3><p>用均方误差计算训练集上的损失：$J\left( w,b \right)\; =\; \frac{1}{m}\sum_{i=1}^{m}{\left( \dot{y}^{\left( i \right)}\; -\; y^{\left( i \right)} \right)^{2}}$</p><h3 id="第-3-步-只有在使用梯度下降法训练时需要"><a href="#第-3-步-只有在使用梯度下降法训练时需要" class="headerlink" title="第 3 步(只有在使用梯度下降法训练时需要):"></a>第 3 步(只有在使用梯度下降法训练时需要):</h3><p>对每个参数，计算其对损失函数的偏导数：</p><p>$\frac{\partial J}{\partial w_{j}}\; =\; \frac{2}{m}\sum_{i=1}^{m}{\left( \dot{y}^{\left( i \right)}\; -\; y^{\left( i \right)} \right)}x_{j}^{\left( i \right)}$</p><p>$\frac{\partial J}{\partial b}\; =\; \frac{2}{m}\sum_{i=1}^{m}{\left( \dot{y}^{\left( i \right)}\; -\; y^{\left( i \right)} \right)}$</p><p>所有偏导数的梯度计算如下：</p><p>$\Delta _{w}J\; =\; \frac{2}{m}X^{T}\; \left( \dot{y}\; -\; y \right)$</p><p>$\Delta _{b}J\; =\; \frac{2}{m}\left( \dot{y}\; -\; y \right)$</p><h3 id="第-4-步-只有在使用梯度下降法训练时需要）"><a href="#第-4-步-只有在使用梯度下降法训练时需要）" class="headerlink" title="第 4 步(只有在使用梯度下降法训练时需要）:"></a>第 4 步(只有在使用梯度下降法训练时需要）:</h3><p>更新权重向量和偏置量：</p><p>$w\; =\; w\; -\; \eta \Delta _{w}J$</p><p>$\Delta _{b}J\; =\; \frac{2}{m}\left( \dot{y}\; -\; y \right)$</p><p>其中η表示学习率</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">np.random.seed(<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">500</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">5</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">500</span>, <span class="number">1</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(X, y)</span><br><span class="line">plt.title(<span class="string">"Dataset"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"First feature"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Second feature"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/7.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line">print(<span class="string">f'Shape X_train: <span class="subst">&#123;X_train.shape&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Shape y_train: <span class="subst">&#123;y_train.shape&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Shape X_test: <span class="subst">&#123;X_test.shape&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Shape y_test: <span class="subst">&#123;y_test.shape&#125;</span>'</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape X_train: (375, 1)Shape y_train: (375, 1)Shape X_test: (125, 1)Shape y_test: (125, 1)</code></pre><h4 id="线性回归分类-源码编译"><a href="#线性回归分类-源码编译" class="headerlink" title="线性回归分类 源码编译"></a>线性回归分类 源码编译</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>:</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">train_gradient_descent</span><span class="params">(self, X, y, learning_rate=<span class="number">0.01</span>, n_iters=<span class="number">100</span>)</span>:</span></span><br><span class="line">       <span class="string">"""</span></span><br><span class="line"><span class="string">       Trains a linear regression model using gradient descent</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       <span class="comment"># Step 0: Initialize the parameters</span></span><br><span class="line">       n_samples, n_features = X.shape</span><br><span class="line">       self.weights = np.zeros(shape=(n_features,<span class="number">1</span>))</span><br><span class="line">       self.bias = <span class="number">0</span></span><br><span class="line">       costs = []</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(n_iters):</span><br><span class="line">           <span class="comment"># Step 1: Compute a linear combination of the input features and weights</span></span><br><span class="line">           y_predict = np.dot(X, self.weights) + self.bias</span><br><span class="line"></span><br><span class="line">           <span class="comment"># Step 2: Compute cost over training set</span></span><br><span class="line">           cost = (<span class="number">1</span> / n_samples) * np.sum((y_predict - y)**<span class="number">2</span>)</span><br><span class="line">           costs.append(cost)</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">               print(<span class="string">f"Cost at iteration <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;cost&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">           <span class="comment"># Step 3: Compute the gradients</span></span><br><span class="line">           dJ_dw = (<span class="number">2</span> / n_samples) * np.dot(X.T, (y_predict - y))</span><br><span class="line">           dJ_db = (<span class="number">2</span> / n_samples) * np.sum((y_predict - y)) </span><br><span class="line"></span><br><span class="line">           <span class="comment"># Step 4: Update the parameters</span></span><br><span class="line">           self.weights = self.weights - learning_rate * dJ_dw</span><br><span class="line">           self.bias = self.bias - learning_rate * dJ_db</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> self.weights, self.bias, costs</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">train_normal_equation</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">       <span class="string">"""</span></span><br><span class="line"><span class="string">       Trains a linear regression model using the normal equation</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">       self.weights = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)</span><br><span class="line">       self.bias = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> self.weights, self.bias</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">       <span class="keyword">return</span> np.dot(X, self.weights) + self.bias</span><br></pre></td></tr></table></figure><h4 id="使用梯度下降进行训练"><a href="#使用梯度下降进行训练" class="headerlink" title="使用梯度下降进行训练"></a>使用梯度下降进行训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">regressor = LinearRegression()</span><br><span class="line">w_trained, b_trained, costs = regressor.train_gradient_descent(X_train, y_train, learning_rate=<span class="number">0.005</span>, n_iters=<span class="number">600</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(<span class="number">600</span>), costs)</span><br><span class="line">plt.title(<span class="string">"Development of cost during training"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Number of iterations"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Cost"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Cost at iteration 0: 66.45256981003433Cost at iteration 100: 2.208434614609594Cost at iteration 200: 1.2797812854182806Cost at iteration 300: 1.2042189195356685Cost at iteration 400: 1.1564867816573Cost at iteration 500: 1.121391041394467Text(0,0.5,&apos;Cost&apos;)</code></pre><p><img src="/images/sklearn/8.png" alt="png"></p><h4 id="测试（梯度下降模型）"><a href="#测试（梯度下降模型）" class="headerlink" title="测试（梯度下降模型）"></a>测试（梯度下降模型）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">n_samples, _ = X_train.shape</span><br><span class="line">n_samples_test, _ = X_test.shape</span><br><span class="line"></span><br><span class="line">y_p_train = regressor.predict(X_train)</span><br><span class="line">y_p_test = regressor.predict(X_test)</span><br><span class="line"></span><br><span class="line">error_train =  (<span class="number">1</span> / n_samples) * np.sum((y_p_train - y_train) ** <span class="number">2</span>)</span><br><span class="line">error_test =  (<span class="number">1</span> / n_samples_test) * np.sum((y_p_test - y_test) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Error on training set: <span class="subst">&#123;np.round(error_train, <span class="number">4</span>)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Error on test set: <span class="subst">&#123;np.round(error_test)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Error on training set: 1.0955Error on test set: 1.0</code></pre><h4 id="使用正规方程（normal-equation）训练"><a href="#使用正规方程（normal-equation）训练" class="headerlink" title="使用正规方程（normal equation）训练"></a>使用正规方程（normal equation）训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_b_train = np.c_[np.ones((n_samples)), X_train]</span><br><span class="line">X_b_test = np.c_[np.ones((n_samples_test)), X_test]</span><br><span class="line"></span><br><span class="line">reg_normal = LinearRegression()</span><br><span class="line">w_trained = reg_normal.train_normal_equation(X_b_train, y_train)</span><br></pre></td></tr></table></figure><h4 id="测试（正规方程模型）"><a href="#测试（正规方程模型）" class="headerlink" title="测试（正规方程模型）"></a>测试（正规方程模型）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y_p_train = reg_normal.predict(X_b_train)</span><br><span class="line">y_p_test = reg_normal.predict(X_b_test)</span><br><span class="line"></span><br><span class="line">error_train =  (<span class="number">1</span> / n_samples) * np.sum((y_p_train - y_train) ** <span class="number">2</span>)</span><br><span class="line">error_test =  (<span class="number">1</span> / n_samples_test) * np.sum((y_p_test - y_test) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f"Error on training set: <span class="subst">&#123;np.round(error_train, <span class="number">4</span>)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Error on test set: <span class="subst">&#123;np.round(error_test, <span class="number">4</span>)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure><pre><code>Error on training set: 1.0228Error on test set: 1.0432</code></pre><h4 id="可视化测试预测"><a href="#可视化测试预测" class="headerlink" title="可视化测试预测"></a>可视化测试预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.scatter(X_train, y_train)</span><br><span class="line">plt.scatter(X_test, y_p_test)</span><br><span class="line">plt.xlabel(<span class="string">"First feature"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Second feature"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>Text(0,0.5,&apos;Second feature&apos;)</code></pre><p><img src="/images/sklearn/9.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;在线性回归中，我们想要建立一个模型，来拟合一个因变量 y 与一个或多个独立自变量(预测变量) x 之间的关系。&lt;/p&gt;
&lt;p&gt;给定：&lt;/p&gt;
&lt;p&gt;数据集&lt;br&gt;$$\left{ \left( x^{\left( 1 \right)},y
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>Scala数据类型及基础语法</title>
    <link href="http://www.ihoge.cn/2018/scala1.html"/>
    <id>http://www.ihoge.cn/2018/scala1.html</id>
    <published>2018-03-31T16:28:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="scala简介"><a href="#scala简介" class="headerlink" title="scala简介"></a>scala简介</h2><pre><code>2004年，martin ordersky发明，javac的编译器，后来spark,kafka应用广泛，twitter应用推广。它具备面向对象和函数式编程的特点。[官网：www.scala-lang.org](www.scala-lang.org)</code></pre><h2 id="Windows环境安装"><a href="#Windows环境安装" class="headerlink" title="Windows环境安装"></a>Windows环境安装</h2><pre><code>a) 安装jdk-7u55-windows-x64.exeb) 安装scala-2.10.4.msi安装完以上两步，不用做任何修改。测试：在控制台下c:/&gt;scalac) 安装eclipse-java-juno-SR2-win32-x86_64.zip 解压缩即可d) 安装eclipse的scala插件update-site.zip    解压会有两个目录，features和plugins，分别把内容放到eclispe对应的目录下。    e) 重启eclipse    提示&quot;Upgrade of scala...&quot;，点yes    提示框&quot;setup Diagnostic&quot;，把Enable JDT weaving...选上    根据提示重启</code></pre><h2 id="第一个程序"><a href="#第一个程序" class="headerlink" title="第一个程序"></a>第一个程序</h2><h3 id="交互式编程"><a href="#交互式编程" class="headerlink" title="交互式编程"></a>交互式编程</h3><pre><code>C:\Users\Administrator&gt;scalaWelcome to Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_5Type in expressions to have them evaluated.Type :help for more information.scala&gt; 1+1res0: Int = 2</code></pre><h3 id="脚本形式-通过创建文件来在eclipse-idea中执行代码"><a href="#脚本形式-通过创建文件来在eclipse-idea中执行代码" class="headerlink" title="脚本形式:通过创建文件来在eclipse/idea中执行代码"></a>脚本形式:通过创建文件来在eclipse/idea中执行代码</h3><pre><code>object Test {    def main(args: Array[String] ) {        println(&quot;Hello world&quot;)    }}</code></pre><ul><li>注1：上面这个是单例对象，这个里面只能存放静态的东西</li><li>注2：自动导入3个包<pre><code>java.lang._scala._Predef._    </code></pre></li><li>注3：语句最后一行的分号不推荐写</li></ul><h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><h3 id="变量和常量"><a href="#变量和常量" class="headerlink" title="变量和常量"></a>变量和常量</h3><ul><li>a) 变量var<pre><code>格式：    var 变量名 [:数据类型] = 值例：var b :Int = 1    var c = 2    //类型自动推断</code></pre></li><li><p>b) 常量val</p><pre><code>格式：    val 常量名 [:数据类型] = 值例：    val b :Int = 1    val b = 1    b = 2    //报错，常量不能修改</code></pre></li><li><p>c) 常量可以用lazy修饰(了解)</p><pre><code>lazy val b :Int = 1    //b用到的时候再赋值</code></pre></li></ul><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><h4 id="基本类型"><a href="#基本类型" class="headerlink" title="基本类型"></a>基本类型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Byte8位有符号值，范围从-128至127</span><br><span class="line">Short16位有符号值，范围从-32768至32767</span><br><span class="line">Int32位有符号值，范围从-2147483648至2147483647</span><br><span class="line">Long64位有符号值，范围从-9223372036854775808至9223372036854775807</span><br><span class="line">Float32位IEEE 754单精度浮点值</span><br><span class="line">Double64位IEEE 754双精度浮点值</span><br><span class="line">Char16位无符号Unicode字符。范围从U+0000到U+FFFF</span><br><span class="line">String一个Char类型序列</span><br><span class="line">Boolean文字值true或文字值false</span><br><span class="line">Unit对应于无值，等价于void类型，只有一个对象叫()</span><br><span class="line">Null只有一个对象叫null</span><br><span class="line">Nothing在Scala中处于最底层，比如创建数组时不指定类型，就是Noting。抽象概念</span><br><span class="line">Any任何类型的超类型; 任何对象的类型为Any</span><br><span class="line">AnyRef任何引用类型的超类型</span><br></pre></td></tr></table></figure><h4 id="层次结构"><a href="#层次结构" class="headerlink" title="层次结构"></a>层次结构</h4><p>Scala.Any 任何类型的超类型</p><ul><li>AnyVal(值) 任何数值类型的超类型<br>  Int,Double等，Unit。</li><li>AnyRef（引用）任何引用类型的超类型<br>  List,Set,Map,Seq,Iterable<br>  java.lang.String<br>  Null</li></ul><h4 id="重点类型：元组"><a href="#重点类型：元组" class="headerlink" title="重点类型：元组"></a>重点类型：元组</h4><pre><code>格式：(元素1, 元素2, ....)访问：变量._N 其中N是元组元素的索引，从1开始    例：var t = (&quot;a&quot;, false, 1)        //t的类型是scala.Tuple3        var value = t._1    //&quot;a&quot;        var m,n,(x,y,z) =  (&quot;a&quot;, false, 1)         m    :(&quot;a&quot;, false, 1)         n    :(&quot;a&quot;, false, 1)         x    : &quot;a&quot;        y    : false        z    : 1</code></pre><h4 id="重点类型：字符串"><a href="#重点类型：字符串" class="headerlink" title="重点类型：字符串"></a>重点类型：字符串</h4><pre><code>i) 用的是java.lang.String，但是有时候根据需要，会隐式转换到其它类型，比如调用reverse/sorted/sortWith/drop/slice等方法，这些方法定义在IndexedSeqOptimized中ii)多行字符串表示，开始和结束用&quot;&quot;&quot;</code></pre><h4 id="了解：符号类型"><a href="#了解：符号类型" class="headerlink" title="了解：符号类型"></a>了解：符号类型</h4><pre><code>符号字面量： &apos;标识符,是scala.Symbol的实例,像模式匹配，类型判断会比较常用。var flag = &apos;startif (flag == &apos;start) println(1) else println(2)</code></pre><h3 id="运算符-scala没有运算符，它运算符全部封装成了方法。"><a href="#运算符-scala没有运算符，它运算符全部封装成了方法。" class="headerlink" title="运算符:scala没有运算符，它运算符全部封装成了方法。"></a>运算符:scala没有运算符，它运算符全部封装成了方法。</h3><pre><code>算术：+ - * / %比较: == != &gt; &lt; &gt;= &lt;= 逻辑：&amp;&amp; || !赋值：= += -= *= /* %=位：&amp; | ~ ^ &gt;&gt; &lt;&lt; &gt;&gt;&gt; 注：上面都是方法。例 1+2相当于1.+(2)，其中+是方法，2是参数</code></pre><h3 id="控制语句"><a href="#控制语句" class="headerlink" title="控制语句"></a>控制语句</h3><pre><code>(1) if，if...else...，if...else if...else...(2) scala中的if可以作为表达式用    var x = if(&quot;hello&quot;==&quot;hell&quot;) 1 else 0(3) switch被模式匹配替换</code></pre><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><pre><code>(1)while,do..while和for都有，while和do..while很像，但是for差别很大。(2)for循环格式不同    for(变量 &lt;- 集合 if 条件判断1;if 条件判断2...) {        所有条件判断都满足才执行    }(3)没有break和continue，用两种方法可以代替    i) 用for循环的条件判断    ii)方法2:非写break，要做以下两步     //1) 引入一个包scala.util.control.Breaks._     //2) 代码块用breakable修饰    for (value &lt;- 1 to 5) {        println(value)    }</code></pre><h3 id="集合框架-Array，List-Set-Map"><a href="#集合框架-Array，List-Set-Map" class="headerlink" title="集合框架(Array，List,Set,Map)"></a>集合框架(Array，List,Set,Map)</h3><pre><code>在scala中，数组Array归到集合的范畴包scala.collection，下面有两个分支immutable(不可改变的,默认)和mutable(可变的)</code></pre><h4 id="层次结构-1"><a href="#层次结构-1" class="headerlink" title="层次结构"></a>层次结构</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"> Traversable</span><br><span class="line">-Iterable</span><br><span class="line">**(immutable不可改变的)**</span><br><span class="line">-Set</span><br><span class="line">HashSet,TreeSt</span><br><span class="line">-Map</span><br><span class="line">HashMap,TreeMap</span><br><span class="line">-Seq</span><br><span class="line">-IndexedSeq</span><br><span class="line">Vector,Array,String,Range</span><br><span class="line">-LinearSeq</span><br><span class="line">List,Queue,Stack</span><br><span class="line"></span><br><span class="line">**(mutable可变的)**</span><br><span class="line">-Set</span><br><span class="line">HashSet</span><br><span class="line">-Map</span><br><span class="line">HashMap</span><br><span class="line">-Seq</span><br><span class="line">-IndexedSeq</span><br><span class="line">ArraySeq,StringBuilder</span><br><span class="line">-Buffer</span><br><span class="line">ArrayBuffer,ListBuffer</span><br><span class="line">-LinearSeq</span><br><span class="line">LinkedList,Queue</span><br><span class="line">Stack</span><br><span class="line">重点记忆几个</span><br><span class="line">不可变可变</span><br><span class="line">ArrayArrayBuffer</span><br><span class="line">ListListBuffer</span><br><span class="line">immutable.Mapmutable.Map</span><br><span class="line">immutable.Setmutable.Set</span><br></pre></td></tr></table></figure><h4 id="数组-非常重要-：数据类型相同的元素，按照一定顺序排序的集合。"><a href="#数组-非常重要-：数据类型相同的元素，按照一定顺序排序的集合。" class="headerlink" title="数组(非常重要)：数据类型相同的元素，按照一定顺序排序的集合。"></a>数组(非常重要)：数据类型相同的元素，按照一定顺序排序的集合。</h4><p>不可变数组 scala.Array<br>可变长数组 scala.collection.mutable.ArrayBuffer</p><ul><li><p>1.Array和ArrayBuffer<br>  Array创建:<br>  var 变量名 = new Array<a href="长度">类型</a>    //var arr = new Array<a href="10">Int</a><br>  var 变量名 = Array(元素1,元素2,…) //var arr = Array(1,3,5,7)，实际上是调用Array的方法apply</p><p>  取值：变量名(下标)</p></li><li>2.ArrayBuffer创建<br>  var 变量名 = new ArrayBuffer<a href="">类型</a>    //var arr = new ArrayBuffer<a href="">Int</a><br>  var 变量名 = ArrayBuffer(元素1,元素2,…) //var arr = ArrayBuffer(1,3,5,7)</li><li><p>3.共同方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sum 求和</span><br><span class="line">max 最大值</span><br><span class="line">min 最小值</span><br><span class="line">mkString(分隔符)例：Array(1,3,5,7).mkString(&quot;|&quot;)//结果是1|3|5|7|9</span><br><span class="line">sorted 排序（从小到大）</span><br><span class="line">sortBy</span><br><span class="line">sortWith 自定义排序</span><br><span class="line">reverse 翻转</span><br><span class="line">toArray ArrayBuffer转成Array</span><br><span class="line">toBuffer Array转成ArrayBuffer</span><br><span class="line">toMap：如果Array内部元素是对偶元组，可以转成map</span><br></pre></td></tr></table></figure></li><li><p>4.ArrayBuffer的独有方法<br>  += 追加元素<br>  ++= 追加集合<br>  trimEnd(n):删除末尾n个元素<br>  insert(index,value1,value2,..valueN):第index的位置，插入value1,value2,…valueN)<br>  remove(index,n):第index个位置删除n个元素<br>  clear():清空</p></li><li>5.遍历<br>  i) 通过下标<pre><code>for (i &lt;- 0 to 数组.length-1) {    println(i)) //i表示数组的下标}</code></pre>  ii)直接取值<pre><code>for (e &lt;- 数组) {    println(e)}</code></pre></li><li>6.使用yield创建新数组<br>  var arr = for (i &lt;- 数组) yield i * 2</li><li>7.多维数组<br>  var arr = Array(Array(元素1…),Array(元素,…)..)<br>  遍历<br>  for (i &lt;- arr) {<pre><code>for (j &lt;- i) {    println(j)}</code></pre>  }</li></ul><h4 id="List"><a href="#List" class="headerlink" title="List"></a>List</h4><h4 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h4><h4 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h4><h4 id="Array和List和Tuple都是不可变元素，有什么区别？"><a href="#Array和List和Tuple都是不可变元素，有什么区别？" class="headerlink" title="Array和List和Tuple都是不可变元素，有什么区别？"></a>Array和List和Tuple都是不可变元素，有什么区别？</h4><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;scala简介&quot;&gt;&lt;a href=&quot;#scala简介&quot; class=&quot;headerlink&quot; title=&quot;scala简介&quot;&gt;&lt;/a&gt;scala简介&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;2004年，martin ordersky发明，javac的编译器，后来spark,
      
    
    </summary>
    
      <category term="Scala" scheme="http://www.ihoge.cn/categories/Scala/"/>
    
    
  </entry>
  
  <entry>
    <title>基于计量经济学研究线性回归建模的一般步骤及模型优化</title>
    <link href="http://www.ihoge.cn/2018/%E5%9F%BA%E4%BA%8E%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%BB%BA%E6%A8%A1%E7%9A%84%E4%B8%80%E8%88%AC%E6%AD%A5%E9%AA%A4%E5%8F%8A%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96.html"/>
    <id>http://www.ihoge.cn/2018/基于计量经济学研究线性回归建模的一般步骤及模型优化.html</id>
    <published>2018-03-31T09:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>本文参考:</p><ul><li><strong>CDA徐杨老师</strong>授课内容梳理</li><li>scikit-learn 机器学习常用算法原理及编程实战 ——黄永昌</li><li>Python机器学习经典实例 ——[美]Prateek Joshi<br><img src="/images/01/math.png" alt="png"><h2 id="明确需求"><a href="#明确需求" class="headerlink" title="明确需求"></a>明确需求</h2>这一步骤虽然与建模无关，但它是让数据与业务紧密结合的枢纽。因此，首先要明确需求并作出可行性判断。</li></ul><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><ul><li>CC完整资料分析法</li><li>加权填充法</li><li>单一值插补法</li><li>基于函数插值法</li></ul><h3 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h3><ul><li>切比雪夫不等式是马尔可夫不等式的特殊情况。一般商业的实际应用场景中，数据多为非正态分布。切尔雪夫不等式规定的是数据的下界。也就是在3倍标准差的范围内至少包含90%的数据。这里可以考虑在3倍标准差以外的数据作为异常值剔除。具体根据实际业务场景进行判断。</li><li>对马尔可夫和切尔雪夫不等式感兴趣的同学可以参考<a href="https://www.zhihu.com/question/27821324" target="_blank" rel="noopener">马尔可夫不等式与年薪百万</a>。</li></ul><h3 id="其他："><a href="#其他：" class="headerlink" title="其他："></a>其他：</h3><ul><li>均值移除<br>  通常移除均值是为了保证特征均值为0（即标准化），这样做可以消除特征彼此间的偏差（bias）。</li><li>范围缩放（也叫特征缩放）<br>数据点中每个特征的数值范围可能变化很大，因此有时将特征数值范围缩放到合理的大小很重要。如X1的取值范围在[1,4]，X2的取值范围在[1,2000]。此时就可以让X1/4和X2/2000分别作为新的X1、X2。这样做的目的是为了让算法收敛更快提高效率，但对预测准确性没有影响。</li><li>归一化<br>数据归一化用于需要对特征向量的值进行调整时，以保证每个特征向量的值都缩放在相同的数值范围。机器学习中最常用的归一化形式就是将特征向量调整为L1范数，是特征向量的数值之和为1. </li><li>二值化<br>二值化用于将数值特征向量转换为布尔类型向量。</li><li>独热编码（其效果有些类似PCA）<br>通常需要处理的数据都是稀疏、散乱地分布在空间中，然而我们并不需要存储这些大数值。这时就需要使用独热编码（One-Hot-Encoding），可以吧独热编码看作是一种<strong>收紧</strong>（tighten）特征向量的工具。它把特征向量的每个特征与特征的非重复综述相对应，通常one-of-k的形式对每个值进行编码。特征向量的每个特征值都按照这种方式编码，当给一个特征向量的第n个特征进行编码时，编码器会遍历每个特征向量的第n个特征，然后进行非重复计数。如果非重复计数的值时K，那么久吧这个特征转换为只有一个值是1其他值都是0的K维向量。</li></ul><p><strong>关于这四类数据预处理具体实现参考本站<a href="http://www.hoooge.top/2018/03/24/监督学习/" target="_blank" rel="noopener">监督学习实例</a></strong></p><h2 id="相关分析"><a href="#相关分析" class="headerlink" title="相关分析"></a>相关分析</h2><p>这里要养成画出相关系数矩阵的好习惯。（Python中可以直接调用pd.corr()函数查看每个变量间的交互相关性。）如果多个自变量相关性比较大可以考虑适当剔除某些自变量。（向高斯马尔可夫定理靠拢）</p><h2 id="分测试集训练集"><a href="#分测试集训练集" class="headerlink" title="分测试集训练集"></a>分测试集训练集</h2><p>一般采用2/8分或者3/7分，略。</p><h2 id="跑一次模型"><a href="#跑一次模型" class="headerlink" title="跑一次模型"></a>跑一次模型</h2><h3 id="F检验"><a href="#F检验" class="headerlink" title="F检验"></a><a href="https://baike.baidu.com/item/F检验/9910842?fr=aladdin" target="_blank" rel="noopener">F检验</a></h3><p>F检验（F-test），最常用的别名叫做联合假设检验（英语：joint hypotheses test），此外也称方差比率检验、方差齐性检验。它是一种在零假设（null hypothesis, H0）之下，统计值服从F-分布的检验。其通常是用来分析用了超过一个参数的统计模型，以判断该模型中的全部或一部分参数是否适合用来估计母体。</p><h3 id="T检验"><a href="#T检验" class="headerlink" title="T检验"></a><a href="https://baike.baidu.com/item/t检验/9910799?fr=aladdin" target="_blank" rel="noopener">T检验</a></h3><p>t检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。它与f检验、卡方检验并列。</p><ul><li><strong>单总体t检验</strong>是检验一个样本平均数与一个已知的总体平均数的差异是否显著。</li><li><strong>双总体t检验</strong>是检验两个样本平均数与其各自所代表的总体的差异是否显著。双总体t检验又分为两种情况，一是独立样本t检验，一是配对样本t检验。</li></ul><h3 id="R方、均方差、解释得分等"><a href="#R方、均方差、解释得分等" class="headerlink" title="R方、均方差、解释得分等"></a>R方、均方差、解释得分等</h3><p><strong>参考本站<a href="http://www.hoooge.top/2018/03/24/监督学习/" target="_blank" rel="noopener">计算回归准确性</a></strong></p><h3 id="召回率、查准率、F1Score"><a href="#召回率、查准率、F1Score" class="headerlink" title="召回率、查准率、F1Score"></a>召回率、查准率、F1Score</h3><p><strong>准确率(Precision)</strong> = 预测到的相关的 / 预测到的相关的+预测到的不相关的<br><strong>召回率(Recall)</strong> = 预测到的相关的 / 预测到的相关的+没有被预测到的相关的</p><p><strong>参考本站<a href="http://www.hoooge.top/2018/03/28/sklearn1/" target="_blank" rel="noopener">学习曲线、查准率和召回率</a></strong></p><h2 id="残差分析"><a href="#残差分析" class="headerlink" title="残差分析"></a>残差分析</h2><p>这里首先引入马尔可夫假定，后面的残差相关调整多数是为了向这一定理靠拢。高斯—马尔可夫定理（Gauss–Markov theory）是指在给定经典线性回归的假定下，最小二乘估计量是具有最小方差的线性无偏估计量的这一定理。</p><h3 id="马尔可夫假定"><a href="#马尔可夫假定" class="headerlink" title="马尔可夫假定"></a>马尔可夫假定</h3><p>高斯–马尔可夫定理的意义在于，当经典假定成立时，我们不需要再去寻找其它无偏估计量，没有一个会优于普通最小二乘估计量。也就是说，如果存在一个好的线性无偏估计量，这个估计量的方差最多与普通最小二乘估计量的方差一样小，不会小于普通最小二乘估计量的方差。</p><p>高斯-马尔可夫定理总共分为对OLS（Ordinary least square）普通线性方程有5个假设。</p><ol><li>Assumption MLR.1（linear in parameters): 假设一要求所有的母集团参数（population parameters）为常数，用来保证模型为线性关系。即如果<strong>母集团方程为y=a+b1x1+b2x2+…+bkxk+u, 所有的a, b1,b2…bk必须为常数。</strong>同时u为无法检测的误差项，即实验过程中模型没有包含的因素。</li><li>Assumption MLR.2 (Random sampling)假设二： 假设我们有n个调查的样本，那么这n个样本必须是从母集团里面<strong>随机抽样得出的</strong>。以假设一的方程为例，{（xi1,xi2, xi3…..xik,yi): i=1,2,3…n}</li><li>Assumption MLR.3 （No perfect collinearity)假设三：在样本（母集团）中， 没有独立变量（independent variable）是常数，并且<strong>独立变量之间不能有完全共线性</strong>。（根据矩阵方程的定义，方程会无解）</li><li>Assumption MLR.4 (Zero conditional mean)假设四： <strong>母集团方程的误差项的均值为 0</strong>，并且均值不受到独立变量的影响，可以表示为：E(U/ X1, X2…Xk)=0<br>5.Assumption MLR.5 (Homoscedasticity): 假设五：<strong>同方差性</strong>， 误差项u的方差不受到独立变量的影响为一个固定不变的值，可以表示为： Var(u/X1,X2…Xk)=σ [1] </li></ol><h3 id="正态性"><a href="#正态性" class="headerlink" title="正态性"></a>正态性</h3><p>在R中可以画出<a href="https://baike.baidu.com/item/QQPlot图/4050730?fr=aladdin" target="_blank" rel="noopener">QQPlot图</a>。<br><strong>P-P图</strong>是根据变量的累积概率对应于所指定的理论分布累积概率绘制的散点图，用于直观地检测样本数据是否符合某一概率分布。如果被检验的数据符合所指定的分布，则代表样本数据的点应当基本在代表理论分布的对角线上。<br><strong>Q-Q图</strong>的结果与P-P图非常相似，只是P-P图是用分布的累计比，而Q-Q图用的是分布的分位数来做检验。和P-P图一样，如果数据为正态分布，则在Q-Q正态分布图中，数据点应基本在图中对角线上。</p><h3 id="异方差性"><a href="#异方差性" class="headerlink" title="异方差性"></a><a href="https://baike.baidu.com/item/异方差性/3206526?fr=aladdin" target="_blank" rel="noopener">异方差性</a></h3><p>异方差性是计量经济学术语，是相对于同方差而言的。所谓同方差，是为了保证回归参数估计量具有良好的统计性质，经典线性回归模型的一个重要假定：总体回归函数中的随机误差项满足同方差性，即它们都有相同的方差。如果这一假定不满足，即：随机误差项具有不同的方差，则称线性回归模型存在异方差性。这里的目的是排除样本自变量之间的共线性。常用的解决办法有：</p><ul><li><strong>BP检验</strong></li><li><strong>White检验</strong>（精度更高，但需要花费大量自由度，因此更适合样本数据量大的数据集）</li><li><strong>加权最小二乘</strong></li></ul><h3 id="共线性"><a href="#共线性" class="headerlink" title="共线性"></a><a href="https://baike.baidu.com/item/共线性/4021508?fr=aladdin" target="_blank" rel="noopener">共线性</a></h3><p><strong>共线性</strong>，即同线性或同线型。统计学中，共线性即多重共线性。<br><strong>多重共线性</strong>（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。<br>一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。完全共线性的情况并不多见，一般出现的是在一定程度上的共线性，即近似共线性。<br><strong>VIF 查看方差膨胀因子</strong><br>多重共线性使参数估计值的方差增大，1/(1-r2)为方差膨胀因子(Variance Inflation Factor, VIF)如果方差膨胀因子值越大，说明共线性越强。<br><strong>解决办法</strong></p><ul><li>排除引起共线性的变量<ul><li>主成分分析法可以完全消灭共线性，但缺点是无法保留现有自变量的属性，即标签会丢失。</li></ul></li><li>减小参数估计量的方差：缺点是一定会让模型有偏<ul><li>岭回归</li><li>Lasso</li><li>弹性网络（以上两者的结合）</li></ul></li></ul><h3 id="内生性"><a href="#内生性" class="headerlink" title="内生性"></a>内生性</h3><p>内生性就是模型中的一个或多个解释变量与随机扰动项相关。<br>导致原因为：<br>1：<strong>遗漏变量</strong>，且遗漏变量与引入模型的其他变量相关。<br>2：解释变量和被解释变量相互作用，相互影响，互为因果。</p><p>解决办法：<a href="https://baike.baidu.com/item/内生性/9680257?fr=aladdin" target="_blank" rel="noopener">工具变量估计</a></p><h3 id="异常值二次处理"><a href="#异常值二次处理" class="headerlink" title="异常值二次处理"></a>异常值二次处理</h3><p>略。</p><h3 id="参数的调优"><a href="#参数的调优" class="headerlink" title="参数的调优"></a>参数的调优</h3><p>利用梯度下降算法对参数进行调优，后续补充。</p><h3 id="自变量优化"><a href="#自变量优化" class="headerlink" title="自变量优化"></a>自变量优化</h3><p>这里自变量优化的核心思想是：<strong>让各个自变量的变化趋势保持其“独立和准确性“。</strong></p><ul><li>多分类自变量的因子变换：保持各自变量截距的独立性</li><li>交互项：保持各自变量斜率的独立性</li><li>高次项：保持各自变量曲率的独立性</li></ul><p>在sklearn中，PolynomialFeatures算法包即是这里交互项和高次项的算法实现。<br>具体交互项和高次项中如何取参，或可参考杜宾两步法？<br>参考本站<a href="http://www.hoooge.top/2018/03/24/监督学习/" target="_blank" rel="noopener">监督学习：创建多项式回归器</a></p><h2 id="验证模型"><a href="#验证模型" class="headerlink" title="验证模型"></a>验证模型</h2><h3 id="逐步回归"><a href="#逐步回归" class="headerlink" title="逐步回归"></a>逐步回归</h3><p>逐步回归的基本思想是将变量逐个引入模型，每引入一个解释变量后都要进行F检验，并对已经选入的解释变量逐个进行t检验，当原来引入的解释变量由于后面解释变量的引入变得不再显著时，则将其删除。这是一个反复的过程，直到既没有显著的解释变量选入回归方程，也没有不显著的解释变量从回归方程中剔除为止。以保证最后所得到的解释变量集是最优的。<br>依据上述思想，可利用逐步回归筛选并剔除引起多重共线性的变量，其具体步骤如下：先用被解释变量对每一个所考虑的解释变量做简单回归，然后以对被解释变量贡献最大的解释变量所对应的回归方程为基础，再逐步引入其余解释变量。经过逐步回归，使得最后保留在模型中的解释变量既是重要的，又没有严重多重共线性。</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证（K-fold cross-validation）的基本思想是把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对分类器进行训练,再利用验证集来测试训练得到的模型(model),以此来做为评价分类器的性能指标。</p><p>Python的sklearn中有Kfold可以直接调用进行交叉验证</p><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p><strong>参考本站<a href="http://www.hoooge.top/2018/03/28/sklearn1/" target="_blank" rel="noopener">学习曲线、查准率和召回率</a></strong></p><h2 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h2><p>最后一步，模型测试。拿之前切割出来的测试集Just Run It！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;本文参考:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CDA徐杨老师&lt;/strong&gt;授课内容梳理&lt;/li&gt;
&lt;li&gt;scikit-learn 机器学习常用算法原理及编程实战 ——黄永昌&lt;/li&gt;
&lt;li&gt;Python机器学习经典实例 —
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="Econometrics" scheme="http://www.ihoge.cn/tags/Econometrics/"/>
    
      <category term="计量经济" scheme="http://www.ihoge.cn/tags/%E8%AE%A1%E9%87%8F%E7%BB%8F%E6%B5%8E/"/>
    
  </entry>
  
  <entry>
    <title>KNN-糖尿病预测-学习曲线的应用</title>
    <link href="http://www.ihoge.cn/2018/KNN.html"/>
    <id>http://www.ihoge.cn/2018/KNN.html</id>
    <published>2018-03-29T09:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h2><p>KNN算法的核心思想是为预测样本的类别，即使最邻近的k个邻居中类别占比最高的的类别：</p><p>假设X_test为未标记的数据样本，X_train为已标记类别的样本，算法原理伪代码如下：</p><ul><li>遍历X_train中所有样本，计算每个样本与X_test的距离，并保存在Distance数组中</li><li>对Distance数组进行排序，取距离最近的k个点，记为X_knn</li><li>在X_knn中统计每个类别的个数</li><li>代表记得样本的类别，就是在X_knn中样本最多的类别</li></ul><ul><li>KNeighborsClassifier</li><li>KNeighborsRegressor</li><li>RadiusNeighborsClassifier</li><li>RadiusNeighborsRegressor</li></ul><h3 id="算法优缺点"><a href="#算法优缺点" class="headerlink" title="算法优缺点"></a>算法优缺点</h3><p><strong>优点：</strong>准确性高，对异常值和噪声有较高的容忍度</p><p><strong>缺点：</strong>计算量大，对内存的需求也较大</p><h3 id="算法参数（-k-）"><a href="#算法参数（-k-）" class="headerlink" title="算法参数（$k$）"></a>算法参数（$k$）</h3><p><strong>$k$越大：模型偏差越大，对噪声越不敏感。过大是造成欠拟合</strong></p><p><strong>$k$越小：模型的方差就会越大。太小是会造成过拟合</strong></p><h3 id="算法的变种"><a href="#算法的变种" class="headerlink" title="算法的变种"></a>算法的变种</h3><p><strong>增加邻居的权重：</strong>默认情况下X_knn的权重相等，我们可以指定算法的<code>weights</code>参数调整成距离越近权重越大</p><p><strong>使用一定半径内的点取代距离最近的$k$个点</strong>，<code>RadiusNeighborsClassifier</code>类实现了这个算法</p><h2 id="使用knn进行分类"><a href="#使用knn进行分类" class="headerlink" title="使用knn进行分类"></a>使用knn进行分类</h2><ul><li><code>sklearn.neighbors.KNeighborsClassifier</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="comment"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span></span><br><span class="line">centers = [[<span class="number">-2</span>,<span class="number">2</span>], [<span class="number">2</span>,<span class="number">2</span>], [<span class="number">0</span>,<span class="number">4</span>]]</span><br><span class="line">X , y = make_blobs(n_samples=<span class="number">100</span>, centers=centers, random_state=<span class="number">0</span>, cluster_std=<span class="number">0.60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出数据</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>), dpi=<span class="number">100</span>)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">10</span>, cmap=<span class="string">'cool'</span>)</span><br><span class="line">plt.scatter(c[:,<span class="number">0</span>], c[:,<span class="number">1</span>], s=<span class="number">50</span>, marker=<span class="string">'^'</span>, c=<span class="string">'red'</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/3.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">k = <span class="number">5</span></span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=k)</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure><pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,           metric_params=None, n_jobs=1, n_neighbors=5, p=2,           weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X_test = np.array([0, 2]).reshape(1,-1)</span></span><br><span class="line">X_test = [[<span class="number">0</span>,<span class="number">2</span>]]</span><br><span class="line">y_test = clf.predict(X_test)</span><br><span class="line">neighbors = clf.kneighbors(X_test, return_distance=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="comment"># 生成n_samples个训练样本，分布在centers参数指定的中心点周围。 cluster_std为标准差，指定生成的点分布的稀疏程度</span></span><br><span class="line">centers = [[<span class="number">-2</span>,<span class="number">2</span>], [<span class="number">2</span>,<span class="number">2</span>], [<span class="number">0</span>,<span class="number">4</span>]]</span><br><span class="line">X , y = make_blobs(n_samples=<span class="number">100</span>, centers=centers, random_state=<span class="number">0</span>, cluster_std=<span class="number">0.60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出数据</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>), dpi=<span class="number">100</span>)</span><br><span class="line">c = np.array(centers)</span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c=y, s=<span class="number">10</span>, cmap=<span class="string">'cool'</span>)  <span class="comment"># 样本</span></span><br><span class="line">plt.scatter(c[:,<span class="number">0</span>], c[:,<span class="number">1</span>], s=<span class="number">50</span>, marker=<span class="string">'^'</span>, c=<span class="string">'red'</span>) <span class="comment"># 中心点</span></span><br><span class="line">plt.scatter(X_test[<span class="number">0</span>][<span class="number">0</span>], X_test[<span class="number">0</span>][<span class="number">1</span>], marker=<span class="string">'x'</span>, s=<span class="number">50</span>, c=<span class="string">'blue'</span>) <span class="comment"># 中心点</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> neighbors[<span class="number">0</span>]:</span><br><span class="line">    plt.plot([X[i][<span class="number">0</span>], X_test[<span class="number">0</span>][<span class="number">0</span>]], </span><br><span class="line">             [X[i][<span class="number">1</span>], X_test[<span class="number">0</span>][<span class="number">1</span>]],</span><br><span class="line">             <span class="string">'k--'</span>, linewidth=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/2.png" alt="png"></p><h2 id="KNN-回归拟合"><a href="#KNN-回归拟合" class="headerlink" title="KNN 回归拟合"></a>KNN 回归拟合</h2><p><code>sklearn.neighbors.KNeighborsRegressor</code></p><h3 id="模型展示"><a href="#模型展示" class="headerlink" title="模型展示"></a>模型展示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">n = <span class="number">50</span></span><br><span class="line">X = <span class="number">5</span> * np.random.rand(n ,<span class="number">1</span>)</span><br><span class="line">y = np.cos(X).ravel()</span><br><span class="line"><span class="comment"># 添加一些噪声</span></span><br><span class="line">y += <span class="number">0.2</span> * np.random.rand(n) - <span class="number">0.1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">5</span></span><br><span class="line">knn = KNeighborsRegressor(k)</span><br><span class="line">knn.fit(X, y)</span><br></pre></td></tr></table></figure><pre><code>KNeighborsRegressor(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,          metric_params=None, n_jobs=1, n_neighbors=5, p=2,          weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">T = np.linspace(<span class="number">0</span>,<span class="number">5</span>, <span class="number">500</span>)[:, np.newaxis]</span><br><span class="line">y_pred = knn.predict(T)</span><br><span class="line">knn.score(X,y)</span><br></pre></td></tr></table></figure><pre><code>0.9909058023770559</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plt.scatter(X, y, label=<span class="string">'data'</span>, s=<span class="number">10</span>)</span><br><span class="line">plt.scatter(T, y_pred, label=<span class="string">'prediction'</span>, lw=<span class="number">4</span>, s=<span class="number">0.1</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.title(<span class="string">"KNeighborsRegressor (k=%i)"</span> % k)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/4.png" alt="png"></p><h2 id="糖尿病预测"><a href="#糖尿病预测" class="headerlink" title="糖尿病预测"></a>糖尿病预测</h2><p>总共有768个数据、8个特征，其中Outcome为标记值（1表示有糖尿病）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'code/datasets/pima-indians-diabetes/diabetes.csv'</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">y = data.iloc[:,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><h3 id="模型比较-–KFold和cross-val-score"><a href="#模型比较-–KFold和cross-val-score" class="headerlink" title="模型比较 –KFold和cross_val_score()"></a>模型比较 –KFold和cross_val_score()</h3><ul><li>分别使用普通KNN，加权重KNN，和指定权重的KNN分别对数据拟合计算评分</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line"></span><br><span class="line">models = []</span><br><span class="line">models.append((<span class="string">"KNN"</span>, KNeighborsClassifier(n_neighbors=<span class="number">10</span>)))</span><br><span class="line">models.append((<span class="string">"KNN + weights"</span>, KNeighborsClassifier(</span><br><span class="line">    n_neighbors=<span class="number">10</span>, weights=<span class="string">"distance"</span>)))</span><br><span class="line">models.append((<span class="string">"Radius Neighbors"</span>, RadiusNeighborsClassifier(n_neighbors=<span class="number">10</span>, radius=<span class="number">500.0</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    results.append((name, model.score(X_test, y_test)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(results)):</span><br><span class="line">    print(<span class="string">"name:&#123;&#125;; score:&#123;&#125;"</span>.format(results[i][<span class="number">0</span>], results[i][<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>name:KNN; score:0.7207792207792207name:KNN + weights; score:0.6818181818181818name:Radius Neighbors; score:0.6558441558441559</code></pre><ul><li>此时单从得分上看，普通的KNN性能是最好的，但是我们的训练样本和测试样本是随机分配的，不同的训练集、测试集会造成不同得分。</li><li>为了消除随机样本集对得分结果可能的影响，scikit-learn提供了<strong><code>KFold和cross_val_score()</code></strong>函数来处理这个问题</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> name , model <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">10</span>)</span><br><span class="line">    cv_result = cross_val_score(model, X, y, cv=kfold)  <span class="comment"># 这里要给模型全部的样本集</span></span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(results)):</span><br><span class="line">    print(<span class="string">"name:&#123;&#125;; cross_val_score:&#123;&#125;"</span>.format(results[i][<span class="number">0</span>], results[i][<span class="number">1</span>].mean()))</span><br></pre></td></tr></table></figure><pre><code>name:KNN; cross_val_score:0.74865003417635name:KNN + weights; cross_val_score:0.7330485304169514name:Radius Neighbors; cross_val_score:0.6497265892002735</code></pre><p>结果显示还是普通KNN更适合该数据集。</p><h3 id="用查准率和召回率以及F1对该模型进行评估："><a href="#用查准率和召回率以及F1对该模型进行评估：" class="headerlink" title="用查准率和召回率以及F1对该模型进行评估："></a>用查准率和召回率以及F1对该模型进行评估：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score, precision_score, recall_score</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(<span class="number">10</span>)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"该模型查准率为："</span>, precision_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"该模型召回率为："</span>, recall_score(y_test, y_pred))</span><br><span class="line">print(<span class="string">"该模型F1_score为："</span>, f1_score(y_test, y_pred))</span><br></pre></td></tr></table></figure><pre><code>该模型查准率为： 0.6086956521739131该模型召回率为： 0.5283018867924528该模型F1_score为： 0.5656565656565657</code></pre><h3 id="模型的训练及分析-–-学习曲线"><a href="#模型的训练及分析-–-学习曲线" class="headerlink" title="模型的训练及分析 – 学习曲线"></a>模型的训练及分析 – 学习曲线</h3><p>看起来还是普通KNN更优一些。</p><p>下面就选择用普通KNN算法模型对数据集进行训练，并查看训练样本的拟合情况及对策测试样本的预测准确性：</p><p><code>from sklearn.model_selection import learning_curverain_sizes, train_scores, test_scores = learning_curve(</code></p><p>输入：</p><pre><code>- estimator : 你用的分类器。- title : 表格的标题。- X : 输入的feature，numpy类型- y : 输入的target vector- ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点- cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)- n_jobs : 并行的的任务数(默认1))</code></pre><p>输出：</p><pre><code>- train_sizes_abs :训练样本数- train_scores:训练集上准确率- test_scores:交叉验证集上的准确率) </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">2</span>)</span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line">train_score = knn.score(X_train, y_train)</span><br><span class="line">test_score = knn.score(X_test, y_test)</span><br><span class="line">print(<span class="string">'训练集得分：'</span>,train_score)</span><br><span class="line">print(<span class="string">'测试集得分：'</span>,test_score)</span><br></pre></td></tr></table></figure><pre><code>训练集得分： 0.8517915309446255测试集得分： 0.6948051948051948</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"><span class="comment"># from common.utils import plot_learning_curve</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learn_curve</span><span class="params">(estimator, title, X, y, ylim = None, cv=None, n_jobs=<span class="number">1</span>, train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.</span>, <span class="number">10</span>)</span>)</span>:</span></span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(<span class="string">"train exs"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    </span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, </span><br><span class="line">                     train_score_mean + train_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'r'</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, </span><br><span class="line">                     test_score_mean + test_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>, label=<span class="string">'train score训练得分'</span>)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, <span class="string">'o-'</span>, color=<span class="string">'g'</span>, label=<span class="string">'cross-validation score交叉验证得分'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">'code/datasets/pima-indians-diabetes/diabetes.csv'</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">y = data.iloc[:,<span class="number">8</span>]</span><br><span class="line"></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plot_learn_curve(KNeighborsClassifier(<span class="number">2</span>),<span class="string">"KNN score"</span>,</span><br><span class="line">                X, y, ylim=(<span class="number">0.5</span>, <span class="number">1</span>), cv=cv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/5.png" alt="png"><br>当训练集和测试集的误差收敛但却很高时，为高偏差。<br>左上角的偏差很高，训练集和验证集的准确率都很低，很可能是欠拟合。<br>我们可以增加模型参数，比如，构建更多的特征，减小正则项。<br>此时通过增加数据量是不起作用的。</p><p>当训练集和测试集的误差之间有大的差距时，为高方差。<br>当训练集的准确率比其他独立数据集上的测试结果的准确率要高时，一般都是过拟合。<br>右上角方差很高，训练集和验证集的准确率相差太多，应该是过拟合。<br>我们可以增大训练集，降低模型复杂度，增大正则项，或者通过特征选择减少特征数。</p><p>理想情况是是找到偏差和方差都很小的情况，即收敛且误差较小。</p><h3 id="特征选择及数据可视化"><a href="#特征选择及数据可视化" class="headerlink" title="特征选择及数据可视化"></a>特征选择及数据可视化</h3><p><strong>使用sklearn.feature_selection.SelectKBest选择相关性最大的两个特征</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.neighborse <span class="keyword">import</span> KN</span><br><span class="line"></span><br><span class="line">selector = SelectKBest(k=<span class="number">2</span>)</span><br><span class="line">X_new = selector.fit_transform(X,y)</span><br><span class="line">X_new[<span class="number">0</span>:<span class="number">5</span>]  <span class="comment">#把相关性最大的两个特征放到X_new里并查看前5个数据样本</span></span><br></pre></td></tr></table></figure><pre><code>array([[148. ,  33.6],       [ 85. ,  26.6],       [183. ,  23.3],       [ 89. ,  28.1],       [137. ,  43.1]])</code></pre><ul><li>使用相关性最大的两个特征，对3种不同的KNN算法进行检验</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier</span><br><span class="line">models = []</span><br><span class="line">models.append((<span class="string">"KNN"</span>, KNeighborsClassifier(n_neighbors=<span class="number">5</span>)))</span><br><span class="line">models.append((<span class="string">"KNN + weights"</span>, KNeighborsClassifier(</span><br><span class="line">    n_neighbors=<span class="number">5</span>, weights=<span class="string">"distance"</span>)))</span><br><span class="line">models.append((<span class="string">"Radius Neighbors"</span>, RadiusNeighborsClassifier(n_neighbors=<span class="number">5</span>, radius=<span class="number">500.0</span>)))</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">    kfold = KFold(n_splits=<span class="number">10</span>)</span><br><span class="line">    cv_result = cross_val_score(model, X_new, y, cv=kfold)</span><br><span class="line">    results.append((name, cv_result))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(results)):</span><br><span class="line">    print(<span class="string">"name: &#123;&#125;; cross_val_score: &#123;&#125;"</span>.format(results[i][<span class="number">0</span>], results[i][<span class="number">1</span>].mean()))</span><br></pre></td></tr></table></figure><pre><code>name: KNN; cross_val_score: 0.7369104579630894name: KNN + weights; cross_val_score: 0.7199419002050581name: Radius Neighbors; cross_val_score: 0.6510252904989747</code></pre><p>从输出结果来看，还是普通KNN的准确性更高，与所有特征放到一起训练的准确性差不多，这也侧面证明了SelectKNBest特征选取的准确性。</p><p>回到目标上来，我们是想看看为什么KNN不能很好的拟合训练样本。现在我们至于2个特征可以很方便的在二维坐标上画出所有的训练样本，观察这些数据分布情况</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plt.ylabel(<span class="string">"BMI"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Glucose"</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X_new[y==<span class="number">0</span>][:,<span class="number">0</span>], X_new[y==<span class="number">0</span>][:,<span class="number">1</span>], marker=<span class="string">'o'</span>, s=<span class="number">10</span>)</span><br><span class="line">plt.scatter(X_new[y==<span class="number">1</span>][:,<span class="number">0</span>], X_new[y==<span class="number">1</span>][:,<span class="number">1</span>], marker=<span class="string">'^'</span>, s=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/6.png" alt="png"></p><p>横坐标是血糖值，纵坐标是BMI值反应身体肥胖情况。在数据密集的区域，代表糖尿病的阴性和阳性的样本几乎重叠到了一起。这样就很直观的看到，KNN在糖尿病预测的这个问题上无法达到很高的预测准确性</p><h2 id="关于如何特高KNN算法的运算效率"><a href="#关于如何特高KNN算法的运算效率" class="headerlink" title="关于如何特高KNN算法的运算效率"></a>关于如何特高KNN算法的运算效率</h2><ul><li><code>K-D Tree 数据结构  ——Bentley，J.L.，Communications of the ACM(1975)</code></li><li><code>Ball Tree（对K-D Tree的优化） ——Five balltree construction algorithms</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;算法原理&quot;&gt;&lt;a href=&quot;#算法原理&quot; class=&quot;headerlink&quot; title=&quot;算法原理&quot;&gt;&lt;/a&gt;算法原理&lt;/h2&gt;&lt;p&gt;KNN算法的核心思想是为预测样本的类别，即使最邻近的k个邻居中类别占比最高的的类别：&lt;/
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机、学习曲线、查准率和召回率</title>
    <link href="http://www.ihoge.cn/2018/sklearn1.html"/>
    <id>http://www.ihoge.cn/2018/sklearn1.html</id>
    <published>2018-03-28T09:05:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="手写字体识别–支持向量机"><a href="#手写字体识别–支持向量机" class="headerlink" title="手写字体识别–支持向量机"></a>手写字体识别–支持向量机</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">digits = datasets.load_digits()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = <span class="number">0.20</span>, random_state=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.metrics <span class="keyword">as</span> sm</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"平均绝对误差（mean absolute error） ："</span></span><br><span class="line">      , round(sm.mean_absolute_error(y_test, y_pred), <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"均方误差（mean squared error） ："</span></span><br><span class="line">     , round(sm.mean_squared_error(y_test, y_pred), <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"中位数绝对误差（median absolute error） ："</span></span><br><span class="line">     , round(sm.median_absolute_error(y_test, y_pred), <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"解释方差分（explained variance score） ："</span></span><br><span class="line">     , round(sm.explained_variance_score(y_test, y_pred), <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"R方得分（R2 score） ："</span></span><br><span class="line">     , round(sm.r2_score(y_test, y_pred)))</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure><pre><code>平均绝对误差（mean absolute error） ： 0.08均方误差（mean squared error） ： 0.33中位数绝对误差（median absolute error） ： 0.0解释方差分（explained variance score） ： 0.96R方得分（R2 score） ： 1.00.9777777777777777</code></pre><h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">n = <span class="number">200</span></span><br><span class="line">X = np.linspace(<span class="number">0</span>,<span class="number">1</span>,n)</span><br><span class="line">y = np.sqrt(X) + <span class="number">0.2</span>*np.random.rand(n) <span class="number">-0.1</span></span><br><span class="line">X = X.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">y = y.reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">poly_model</span><span class="params">(degree=<span class="number">1</span>)</span>:</span></span><br><span class="line">    poly_features = PolynomialFeatures(degree = degree, include_bias=<span class="keyword">False</span>)</span><br><span class="line">    linear_regr = LinearRegression()</span><br><span class="line">    <span class="comment"># 这是一个流水线，先增加多项式阶数，再用线性回归算法来拟合数据</span></span><br><span class="line">    pipeline = Pipeline([</span><br><span class="line">        (<span class="string">'poly_features'</span>, poly_features), (<span class="string">'linear_regr'</span>, linear_regr)</span><br><span class="line">    ])</span><br><span class="line">    <span class="keyword">return</span> pipeline</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learn_curve</span><span class="params">(estimator, title, X, y, ylim = None, cv=None, n_jobs=<span class="number">1</span>, train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.</span>, <span class="number">5</span>)</span>)</span>:</span></span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(<span class="string">"train exs"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_score_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_score_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_score_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    </span><br><span class="line">    plt.fill_between(train_sizes, train_score_mean - train_score_std, </span><br><span class="line">                     train_score_mean + train_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'r'</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_score_mean - test_score_std, </span><br><span class="line">                     test_score_mean + test_score_std, alpha=<span class="number">0.1</span>, color=<span class="string">'g'</span>)</span><br><span class="line">    plt.plot(train_sizes, train_score_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>, label=<span class="string">'train score训练得分'</span>)</span><br><span class="line">    plt.plot(train_sizes, test_score_mean, <span class="string">'o-'</span>, color=<span class="string">'g'</span>, label=<span class="string">'cross-validation score交叉验证得分'</span>)</span><br><span class="line">    </span><br><span class="line">    plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了让学习曲线更平滑，计算10次交叉验证数据的分数</span></span><br><span class="line">cv = ShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line">titles = [<span class="string">'under fitting'</span>, <span class="string">'learning curves'</span>, <span class="string">'over fitting'</span>]</span><br><span class="line"></span><br><span class="line">degrees = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">18</span>,<span class="number">4</span>), dpi=<span class="number">150</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(degrees)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, i+<span class="number">1</span>)</span><br><span class="line">    plot_learn_curve(poly_model(degrees[i]), titles[i],</span><br><span class="line">                    X, y, ylim=(<span class="number">0.75</span>, <span class="number">1.01</span>), cv=cv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/sklearn/1.png" alt="png"></p><p><strong>过拟合</strong>： 模型对训练集的准确性高，其成本比较低；对交叉验证数据集的准确性低，其成本比较高</p><ul><li>获取更多的训练数据</li><li>减少输入的特征数量</li></ul><p><strong>欠拟合</strong>： 模型对训练集的准确性低，其成本比较高；对交叉验证数据集的准确性低，其成本比较高</p><ul><li>增加有价值的特征</li><li>增加多项式特征<br>  -如原数据只有$x_1, x_2$，优化后可以增加特征变成，$x_1,x_2,x_1x_2,x_1^2,x_2^2$。这样即可增加模型复杂度</li></ul><h2 id="查准率和召回率"><a href="#查准率和召回率" class="headerlink" title="查准率和召回率"></a>查准率和召回率</h2><ul><li>准确率(Precision) =  预测到的相关的 / 预测到的相关的+预测到的不相关的</li><li>召回率(Recall)      =  预测到的相关的 / 预测到的相关的+没有被预测到的相关的</li></ul><p>在<code>scikit-learn</code>中，评估模型性能的算法都在<code>sklearn.metrics</code>包里。</p><p>其中计算查准率和召回率的<code>API</code>分别为<code>sklearn.metrics.precision_score()</code>和<code>sklearn.metrics.recall_score()</code></p><p>如果有一个算法的查准率是0.5，召回率是0.4；另外一个查准率是0.02，召回率是1.0,那么那个算法更好？</p><p>为了解决这个问题我们引入了$F_1Score$的概念：$$F_1Score = 2*PR / (P+R)$$<br><code>sklearn中对应的算法包是：sklearn.metrics.f1_score()</code></p><p><strong>但是这个方法好想只接受二值化的数据类型</strong>([0,1] 或者[1,2])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics </span><br><span class="line"></span><br><span class="line">x = np.random.randint(<span class="number">1</span>,<span class="number">3</span>,<span class="number">100</span>)</span><br><span class="line">y = np.random.randint(<span class="number">1</span>,<span class="number">3</span>,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">print(metrics.precision_score(x, y))</span><br><span class="line">print(metrics.recall_score(x, y))</span><br><span class="line">print(metrics.f1_score(x, y))</span><br></pre></td></tr></table></figure><pre><code>0.65957446808510630.55357142857142860.6019417475728156</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;手写字体识别–支持向量机&quot;&gt;&lt;a href=&quot;#手写字体识别–支持向量机&quot; class=&quot;headerlink&quot; title=&quot;手写字体识别–支持向量机&quot;&gt;&lt;/a&gt;手写字体识别–支持向量机&lt;/h2&gt;&lt;figure class=&quot;
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？</title>
    <link href="http://www.ihoge.cn/2018/hadoop.html"/>
    <id>http://www.ihoge.cn/2018/hadoop.html</id>
    <published>2018-03-26T16:28:59.000Z</published>
    <updated>2018-04-06T17:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？<br>这是知乎上某大神的解释：</p><p><strong>学习很重要的是能将纷繁复杂的信息进行归类和抽象。</strong><br>对应到大数据技术体系，虽然各种技术百花齐放，层出不穷，但大数据技术本质上无非解决4个核心问题：</p><ul><li>存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka；</li><li>计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等；</li><li>查询，海量数据怎样快速查询？主要为Nosql和Olap，Nosql主要包括Hbase、 Cassandra 等，其中olap包括kylin、impla等，其中Nosql主要解决随机查询，Olap技术主要解决关联查询；挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等；</li></ul><p><strong>大数据技术生态其实是一个江湖….</strong><br>在一个夜黑风高的晚上，江湖第一大帮会Google三本阵法修炼秘籍流出，大数据技术江湖从此纷争四起、永无宁日…<br>这三本秘籍分别为：</p><ul><li>《Google file system》：论述了怎样借助普通机器有效的存储海量的大数据；</li><li>《Google MapReduce》：论述了怎样快速计算海量的数据；</li><li>《Google BigTable》：论述了怎样实现海量数据的快速查询；</li></ul><p><strong>以上三篇论文秘籍是大数据入门的最好文章，通俗易懂，先看此三篇再看其它技术；</strong></p><p>在Google三大秘籍流出之后，江湖上，致力于武学开放的apache根据这三本秘籍分别研究出了对应的武学巨著《hadoop》，并开放给各大门派研习。<br>Hadoop包括三大部分，分别是hdfs、MapReduce和hbase：</p><ul><li>hdfs解决大数据的存储问题。</li><li>mapreduce解决大数据的计算问题。</li><li>hbase解决大数据量的查询问题。</li></ul><p>之后，在各大门派的支持下，Hadoop不断衍生和进化各种分支流派，其中最激烈的当属计算技术，其次是查询技术。存储技术基本无太多变化，hdfs一统天下。以下为大概的演进：</p><ul><li>1，传统数据仓库派说你mapreduce修炼太复杂，老子不会编程，老子以前用sql吃遍天下，为了将这拨人收入门下，并降低大数据修炼难度，遂出了hive，pig、impla等SQL ON Hadoop的简易修炼秘籍；</li><li>2，伯克利派说你MapReduce只重招数，内力无法施展，且不同的场景需要修炼不同的技术，太过复杂，于是推出基于内力（内存）的《Spark》，意图解决所有大数据计算问题。</li><li>3，流式计算相关门派说你hadoop只能憋大招（批量计算），太麻烦，于是出了SparkStreaming、Storm，S4等流式计算技术，能够实现数据一来就即时计算。</li><li>4，apache看各大门派纷争四起，推出flink，想一统流计算和批量计算的修炼；</li></ul><p>以上，如有帮助，别忘了点个赞，谢谢</p><p>作者：有点文<br>链接：<a href="https://www.zhihu.com/question/27974418/answer/156227565" target="_blank" rel="noopener">https://www.zhihu.com/question/27974418/answer/156227565</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;如何用形象的比喻描述大数据的技术生态？Hadoop、Hive、Spark 之间是什么关系？&lt;br&gt;这是知乎上某大神的解释：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;学习很重要的是能将纷繁复杂的信息进行归类和抽象。&lt;/strong&gt;&lt;br&gt;对应到大数
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
  </entry>
  
</feed>
