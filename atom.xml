<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Student</title>
  
  <subtitle>Life is short</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.ihoge.cn/"/>
  <updated>2018-06-01T14:45:35.230Z</updated>
  <id>http://www.ihoge.cn/</id>
  
  <author>
    <name>刘知行</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BF神经网络（二）预测共享单车使用情况</title>
    <link href="http://www.ihoge.cn/2018/BFNN.html"/>
    <id>http://www.ihoge.cn/2018/BFNN.html</id>
    <published>2018-06-01T04:20:21.000Z</published>
    <updated>2018-06-01T14:45:35.230Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>传送门：<br><a href="http://ihoge.cn/2018/DL_BFNN.html" target="_blank" rel="noopener">BF神经网络（一）算法原理与实现</a></p><h1 id="BF神经网络预测共享单车使用情况"><a href="#BF神经网络预测共享单车使用情况" class="headerlink" title="BF神经网络预测共享单车使用情况"></a>BF神经网络预测共享单车使用情况</h1><h2 id="BF实现算法基础"><a href="#BF实现算法基础" class="headerlink" title="BF实现算法基础"></a>BF实现算法基础</h2><p>BF神经网络算法的原理及实现参考前一篇博客：<a href="http://ihoge.cn/2018/DL_BFNN.html" target="_blank" rel="noopener">http://ihoge.cn/2018/DL_BFNN.html</a></p><h2 id="加载和准备数据"><a href="#加载和准备数据" class="headerlink" title="加载和准备数据"></a>加载和准备数据</h2><p>构建神经网络的关键一步是正确地准备数据。不同尺度级别的变量使网络难以高效地掌握正确的权重。我们在下方已经提供了加载和准备数据的代码。你很快将进一步学习这些代码！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_path = <span class="string">'Bike-Sharing-Dataset/hour.csv'</span></span><br><span class="line"></span><br><span class="line">rides = pd.read_csv(data_path)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rides.head()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278638061796.jpg" alt=""></p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><p>此数据集包含的是从 2011 年 1 月 1 日到 2012 年 12 月 31 日期间每天每小时的骑车人数。骑车用户分成临时用户和注册用户，cnt 列是骑车用户数汇总列。你可以在上方看到前几行数据。</p><p>下图展示的是数据集中前 10 天左右的骑车人数（某些天不一定是 24 个条目，所以不是精确的 10 天）。你可以在这里看到每小时租金。这些数据很复杂！周末的骑行人数少些，工作日上下班期间是骑行高峰期。我们还可以从上方的数据中看到温度、湿度和风速信息，所有这些信息都会影响骑行人数。你需要用你的模型展示所有这些数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rides[:<span class="number">24</span>*<span class="number">10</span>].plot(x=<span class="string">'dteday'</span>, y=<span class="string">'cnt'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278473558848.jpg" alt=""></p><h3 id="虚拟变量（哑变量）"><a href="#虚拟变量（哑变量）" class="headerlink" title="虚拟变量（哑变量）"></a>虚拟变量（哑变量）</h3><p>下面是一些分类变量，例如季节、天气、月份。要在我们的模型中包含这些数据，我们需要创建二进制虚拟变量。用 Pandas 库中的 <code>get_dummies()</code> 就可以轻松实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dummy_fields = [<span class="string">'season'</span>, <span class="string">'weathersit'</span>, <span class="string">'mnth'</span>, <span class="string">'hr'</span>, <span class="string">'weekday'</span>]</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> dummy_fields:</span><br><span class="line">    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=<span class="keyword">False</span>)</span><br><span class="line">    rides = pd.concat([rides, dummies], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fields_to_drop = [<span class="string">'instant'</span>, <span class="string">'dteday'</span>, <span class="string">'season'</span>, <span class="string">'weathersit'</span>, </span><br><span class="line">                  <span class="string">'weekday'</span>, <span class="string">'atemp'</span>, <span class="string">'mnth'</span>, <span class="string">'workingday'</span>, <span class="string">'hr'</span>]</span><br><span class="line">data = rides.drop(fields_to_drop, axis=<span class="number">1</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278639194549.jpg" alt=""></p><h3 id="调整目标变量"><a href="#调整目标变量" class="headerlink" title="调整目标变量"></a>调整目标变量</h3><p>为了更轻松地训练网络，我们将对每个连续变量标准化，即转换和调整变量，使它们的均值为 0，标准差为 1。</p><p>我们会保存换算因子，以便当我们使用网络进行预测时可以还原数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">quant_features = [<span class="string">'casual'</span>, <span class="string">'registered'</span>, <span class="string">'cnt'</span>, <span class="string">'temp'</span>, <span class="string">'hum'</span>, <span class="string">'windspeed'</span>]</span><br><span class="line"><span class="comment"># Store scalings in a dictionary so we can convert back later</span></span><br><span class="line">scaled_features = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> quant_features:</span><br><span class="line">    mean, std = data[each].mean(), data[each].std()</span><br><span class="line">    scaled_features[each] = [mean, std]</span><br><span class="line">    data.loc[:, each] = (data[each] - mean)/std</span><br></pre></td></tr></table></figure><h3 id="将数据拆分为训练、测试和验证数据集"><a href="#将数据拆分为训练、测试和验证数据集" class="headerlink" title="将数据拆分为训练、测试和验证数据集"></a>将数据拆分为训练、测试和验证数据集</h3><p>我们将大约最后 21 天的数据保存为测试数据集，这些数据集会在训练完网络后使用。我们将使用该数据集进行预测，并与实际的骑行人数进行对比。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save data for approximately the last 21 days </span></span><br><span class="line">test_data = data[<span class="number">-21</span>*<span class="number">24</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now remove the test data from the data set </span></span><br><span class="line">data = data[:<span class="number">-21</span>*<span class="number">24</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Separate the data into features and targets</span></span><br><span class="line">target_fields = [<span class="string">'cnt'</span>, <span class="string">'casual'</span>, <span class="string">'registered'</span>]</span><br><span class="line">features, targets = data.drop(target_fields, axis=<span class="number">1</span>), data[target_fields]</span><br><span class="line">test_features, test_targets = test_data.drop(target_fields, axis=<span class="number">1</span>), test_data[target_fields]</span><br></pre></td></tr></table></figure><p>我们将数据拆分为两个数据集，一个用作训练，一个在网络训练完后用来验证网络。因为数据是有时间序列特性的，所以我们用历史数据进行训练，然后尝试预测未来数据（验证数据集）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hold out the last 60 days or so of the remaining data as a validation set</span></span><br><span class="line">train_features, train_targets = features[:<span class="number">-60</span>*<span class="number">24</span>], targets[:<span class="number">-60</span>*<span class="number">24</span>]</span><br><span class="line">val_features, val_targets = features[<span class="number">-60</span>*<span class="number">24</span>:], targets[<span class="number">-60</span>*<span class="number">24</span>:]</span><br></pre></td></tr></table></figure><h2 id="开始构建网络"><a href="#开始构建网络" class="headerlink" title="开始构建网络"></a>开始构建网络</h2><p>下面你将构建自己的网络。我们已经构建好结构和反向传递部分。你将实现网络的前向传递部分。还需要设置超参数：学习速率、隐藏单元的数量，以及训练传递数量。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15278473911244.jpg" alt=""></p><p>该网络有两个层级，一个隐藏层和一个输出层。隐藏层级将使用 S 型函数作为激活函数。输出层只有一个节点，用于递归，节点的输出和节点的输入相同。即激活函数是 $f(x)=x$。这种函数获得输入信号，并生成输出信号，但是会考虑阈值，称为激活函数。我们完成网络的每个层级，并计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。这一流程叫做前向传播（forward propagation）。</p><p>我们在神经网络中使用权重将信号从输入层传播到输出层。我们还使用权重将错误从输出层传播回网络，以便更新权重。这叫做反向传播（backpropagation）。</p><blockquote><p><strong>提示</strong>：你需要为反向传播实现计算输出激活函数 ($f(x) = x$) 的导数。如果你不熟悉微积分，其实该函数就等同于等式 $y = x$。该等式的斜率是多少？也就是导数 $f(x)$。</p></blockquote><p>你需要完成以下任务：</p><ol><li>实现 S 型激活函数。将 <code>__init__</code> 中的 <code>self.activation_function</code>  设为你的 S 型函数。</li><li>在 <code>train</code> 方法中实现前向传递。</li><li>在 <code>train</code> 方法中实现反向传播算法，包括计算输出错误。</li><li>在 <code>run</code> 方法中实现前向传递。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">( self, input_nodes, hidden_nodes, output_nodes, learning_rate )</span>:</span></span><br><span class="line">        <span class="comment"># Set the number of nodes in input, hidden and output layers</span></span><br><span class="line">        self.input_nodes = input_nodes</span><br><span class="line">        self.hidden_nodes = hidden_nodes</span><br><span class="line">        self.output_nodes = output_nodes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set the learning rate</span></span><br><span class="line">        self.lr = learning_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize weights</span></span><br><span class="line">        self.weights_input_to_hidden = np.random.normal( <span class="number">0.0</span>, self.input_nodes**<span class="number">-0.5</span>, (self.input_nodes, self.hidden_nodes) )</span><br><span class="line">        self.weights_hidden_to_output = np.random.normal( <span class="number">0.0</span>, self.hidden_nodes**<span class="number">-0.5</span>, (self.hidden_nodes, self.output_nodes) )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Activation function</span></span><br><span class="line">        self.activation_function = <span class="keyword">lambda</span> x : <span class="number">1</span> / ( <span class="number">1</span> + np.exp(-x) )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, features, targets)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Arguments</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        features: 2D array, each row is one data record, each column is a feature</span></span><br><span class="line"><span class="string">        targets: 1D array of target values</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"></span><br><span class="line">        n_record = features.shape[<span class="number">0</span>]</span><br><span class="line">        delta_weights_i_h = np.zeros( self.weights_input_to_hidden.shape )</span><br><span class="line">        delta_weights_h_o = np.zeros( self.weights_hidden_to_output.shape )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> zip( features, targets ):</span><br><span class="line">            hidden_inputs = np.dot( X, self.weights_input_to_hidden )</span><br><span class="line">            hidden_outputs = self.activation_function( hidden_inputs )</span><br><span class="line"></span><br><span class="line">            final_inputs = np.dot( hidden_outputs, self.weights_hidden_to_output )</span><br><span class="line">            final_outputs = final_inputs</span><br><span class="line"></span><br><span class="line">            error = y - final_outputs</span><br><span class="line"></span><br><span class="line">            output_error_term = error <span class="comment"># error * 1</span></span><br><span class="line"></span><br><span class="line">            hidden_error = np.dot( self.weights_hidden_to_output, output_error_term )</span><br><span class="line"></span><br><span class="line">            hidden_error_term = hidden_error * hidden_outputs * (<span class="number">1</span> - hidden_outputs) <span class="comment"># f'(hidden_input)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Weight step</span></span><br><span class="line">            delta_weights_i_h += hidden_error_term * X[:,<span class="keyword">None</span>]</span><br><span class="line">            </span><br><span class="line">            delta_weights_h_o += output_error_term * hidden_outputs[:,<span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update the weights</span></span><br><span class="line">        self.weights_input_to_hidden += self.lr * delta_weights_i_h/n_record</span><br><span class="line">        self.weights_hidden_to_output += self.lr * delta_weights_h_o/n_record</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        hidden_inputs = np.dot( features, self.weights_input_to_hidden )</span><br><span class="line">        hidden_output = self.activation_function( hidden_inputs )</span><br><span class="line"></span><br><span class="line">        final_inputs = np.dot( hidden_output, self.weights_hidden_to_output )</span><br><span class="line">        final_outputs = final_inputs</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MSE</span><span class="params">(y, Y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.mean((y-Y)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>运行这些单元测试，检查你的网络实现是否正确。这样可以帮助你确保网络已正确实现，然后再开始训练网络。这些测试必须成功才能通过此项目。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> unittest</span><br><span class="line"></span><br><span class="line">inputs = np.array([[<span class="number">0.5</span>, <span class="number">-0.2</span>, <span class="number">0.1</span>]])</span><br><span class="line">targets = np.array([[<span class="number">0.4</span>]])</span><br><span class="line">test_w_i_h = np.array([[<span class="number">0.1</span>, <span class="number">-0.2</span>],</span><br><span class="line">                       [<span class="number">0.4</span>, <span class="number">0.5</span>],</span><br><span class="line">                       [<span class="number">-0.3</span>, <span class="number">0.2</span>]])</span><br><span class="line">test_w_h_o = np.array([[<span class="number">0.3</span>],</span><br><span class="line">                       [<span class="number">-0.1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestMethods</span><span class="params">(unittest.TestCase)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line">    <span class="comment"># Unit tests for data loading</span></span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_data_path</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test that file path to dataset has been unaltered</span></span><br><span class="line">        self.assertTrue(data_path.lower() == <span class="string">'bike-sharing-dataset/hour.csv'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_data_loaded</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test that data frame loaded</span></span><br><span class="line">        self.assertTrue(isinstance(rides, pd.DataFrame))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line">    <span class="comment"># Unit tests for network functionality</span></span><br><span class="line">    <span class="comment">##########</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_activation</span><span class="params">(self)</span>:</span></span><br><span class="line">        network = NeuralNetwork(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0.5</span>)</span><br><span class="line">        <span class="comment"># Test that the activation function is a sigmoid</span></span><br><span class="line">        self.assertTrue(np.all(network.activation_function(<span class="number">0.5</span>) == <span class="number">1</span>/(<span class="number">1</span>+np.exp(<span class="number">-0.5</span>))))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_train</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test that weights are updated correctly on training</span></span><br><span class="line">        network = NeuralNetwork(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0.5</span>)</span><br><span class="line">        network.weights_input_to_hidden = test_w_i_h.copy()</span><br><span class="line">        network.weights_hidden_to_output = test_w_h_o.copy()</span><br><span class="line">        </span><br><span class="line">        network.train(inputs, targets)</span><br><span class="line">        self.assertTrue(np.allclose(network.weights_hidden_to_output, </span><br><span class="line">                                    np.array([[ <span class="number">0.37275328</span>], </span><br><span class="line">                                              [<span class="number">-0.03172939</span>]])))</span><br><span class="line">        self.assertTrue(np.allclose(network.weights_input_to_hidden,</span><br><span class="line">                                    np.array([[ <span class="number">0.10562014</span>, <span class="number">-0.20185996</span>], </span><br><span class="line">                                              [<span class="number">0.39775194</span>, <span class="number">0.50074398</span>], </span><br><span class="line">                                              [<span class="number">-0.29887597</span>, <span class="number">0.19962801</span>]])))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_run</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># Test correctness of run method</span></span><br><span class="line">        network = NeuralNetwork(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0.5</span>)</span><br><span class="line">        network.weights_input_to_hidden = test_w_i_h.copy()</span><br><span class="line">        network.weights_hidden_to_output = test_w_h_o.copy()</span><br><span class="line"></span><br><span class="line">        self.assertTrue(np.allclose(network.run(inputs), <span class="number">0.09998924</span>))</span><br><span class="line"></span><br><span class="line">suite = unittest.TestLoader().loadTestsFromModule(TestMethods())</span><br><span class="line">unittest.TextTestRunner().run(suite)</span><br></pre></td></tr></table></figure><pre><code>.....----------------------------------------------------------------------Ran 5 tests in 0.008sOK&lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt;</code></pre><h2 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h2><p>现在你将设置网络的超参数。策略是设置的超参数使训练集上的错误很小但是数据不会过拟合。如果网络训练时间太长，或者有太多的隐藏节点，可能就会过于针对特定训练集，无法泛化到验证数据集。即当训练集的损失降低时，验证集的损失将开始增大。</p><p>你还将采用随机梯度下降 (SGD) 方法训练网络。对于每次训练，都获取随机样本数据，而不是整个数据集。与普通梯度下降相比，训练次数要更多，但是每次时间更短。这样的话，网络训练效率更高。稍后你将详细了解 SGD。</p><h3 id="选择迭代次数"><a href="#选择迭代次数" class="headerlink" title="选择迭代次数"></a>选择迭代次数</h3><p>也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。</p><h3 id="选择学习速率"><a href="#选择学习速率" class="headerlink" title="选择学习速率"></a>选择学习速率</h3><p>速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。</p><h3 id="选择隐藏节点数量"><a href="#选择隐藏节点数量" class="headerlink" title="选择隐藏节点数量"></a>选择隐藏节点数量</h3><p>隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment">### Set the hyperparameters here ###</span></span><br><span class="line">iterations = <span class="number">2000</span></span><br><span class="line">learning_rate = <span class="number">0.7</span></span><br><span class="line">hidden_nodes = <span class="number">15</span></span><br><span class="line">output_nodes = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">N_i = train_features.shape[<span class="number">1</span>]</span><br><span class="line">network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)</span><br><span class="line"></span><br><span class="line">losses = &#123;<span class="string">'train'</span>:[], <span class="string">'validation'</span>:[]&#125;</span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(iterations):</span><br><span class="line">    <span class="comment"># Go through a random batch of 128 records from the training data set</span></span><br><span class="line">    batch = np.random.choice(train_features.index, size=<span class="number">128</span>)</span><br><span class="line">    X, y = train_features.ix[batch].values, train_targets.ix[batch][<span class="string">'cnt'</span>]</span><br><span class="line">                             </span><br><span class="line">    network.train(X, y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Printing out the training progress</span></span><br><span class="line">    train_loss = MSE(network.run(train_features).T, train_targets[<span class="string">'cnt'</span>].values)</span><br><span class="line">    val_loss = MSE(network.run(val_features).T, val_targets[<span class="string">'cnt'</span>].values)</span><br><span class="line">    sys.stdout.write(<span class="string">"\rProgress: &#123;:2.1f&#125;"</span>.format(<span class="number">100</span> * ii/float(iterations)) \</span><br><span class="line">                     + <span class="string">"% ... Training loss: "</span> + str(train_loss)[:<span class="number">5</span>] \</span><br><span class="line">                     + <span class="string">" ... Validation loss: "</span> + str(val_loss)[:<span class="number">5</span>])</span><br><span class="line">    sys.stdout.flush()</span><br><span class="line">    </span><br><span class="line">    losses[<span class="string">'train'</span>].append(train_loss)</span><br><span class="line">    losses[<span class="string">'validation'</span>].append(val_loss)</span><br></pre></td></tr></table></figure><pre><code>Progress: 100.0% ... Training loss: 0.092 ... Validation loss: 0.186</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(losses[<span class="string">'train'</span>], label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(losses[<span class="string">'validation'</span>], label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">_ = plt.ylim()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278475026241.jpg" alt=""></p><h2 id="检查预测结果"><a href="#检查预测结果" class="headerlink" title="检查预测结果"></a>检查预测结果</h2><p>使用测试数据看看网络对数据建模的效果如何。如果完全错了，请确保网络中的每步都正确实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">mean, std = scaled_features[<span class="string">'cnt'</span>]</span><br><span class="line">predictions = network.run(test_features).T*std + mean</span><br><span class="line">ax.plot(predictions[<span class="number">0</span>], label=<span class="string">'Prediction'</span>)</span><br><span class="line">ax.plot((test_targets[<span class="string">'cnt'</span>]*std + mean).values, label=<span class="string">'Data'</span>)</span><br><span class="line">ax.set_xlim(right=len(predictions))</span><br><span class="line">ax.legend()</span><br><span class="line"></span><br><span class="line">dates = pd.to_datetime(rides.ix[test_data.index][<span class="string">'dteday'</span>])</span><br><span class="line">dates = dates.apply(<span class="keyword">lambda</span> d: d.strftime(<span class="string">'%b %d'</span>))</span><br><span class="line">ax.set_xticks(np.arange(len(dates))[<span class="number">12</span>::<span class="number">24</span>])</span><br><span class="line">_ = ax.set_xticklabels(dates[<span class="number">12</span>::<span class="number">24</span>], rotation=<span class="number">45</span>)</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278475263318.jpg" alt=""></p><h2 id="可选：思考下你的结果（我们不会评估这道题的答案）"><a href="#可选：思考下你的结果（我们不会评估这道题的答案）" class="headerlink" title="可选：思考下你的结果（我们不会评估这道题的答案）"></a>可选：思考下你的结果（我们不会评估这道题的答案）</h2><p>请针对你的结果回答以下问题。模型对数据的预测效果如何？哪里出现问题了？为何出现问题呢？</p><blockquote><p><strong>注意</strong>：你可以通过双击该单元编辑文本。如果想要预览文本，请按 Control + Enter</p></blockquote><h4 id="请将你的答案填写在下方"><a href="#请将你的答案填写在下方" class="headerlink" title="请将你的答案填写在下方"></a>请将你的答案填写在下方</h4><p>问题1:Dec 24 开始至Dec 28这四天的预测误差比较大的原因？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;传送门：&lt;br&gt;&lt;a href=&quot;http://ihoge.cn/2018/DL_BFNN.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BF神经网络（一）算法原理与实现&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;BF神
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="BF神经网络" scheme="http://www.ihoge.cn/tags/BF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>BF神经网络（一）算法原理与实现</title>
    <link href="http://www.ihoge.cn/2018/DL_BFNN.html"/>
    <id>http://www.ihoge.cn/2018/DL_BFNN.html</id>
    <published>2018-05-30T04:20:21.000Z</published>
    <updated>2018-06-01T14:44:40.432Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>传送门：<br><a href="http://ihoge.cn/2018/DL_BFNN.html" target="_blank" rel="noopener">BF神经网络（二）预测共享单车使用情况</a></p><h2 id="一、反向传播算法基本原理"><a href="#一、反向传播算法基本原理" class="headerlink" title="一、反向传播算法基本原理"></a>一、反向传播算法基本原理</h2><p>如何让多层神经网络学习呢？我们已了解了使用梯度下降来更新权重，反向传播算法则是它的一个延伸。以一个两层神经网络为例，可以使用链式法则计算输入层-隐藏层间权重的误差。</p><p>要使用梯度下降法更新隐藏层的权重，你需要知道各隐藏层节点的误差对最终输出的影响。每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。</p><p>例如，输出层每个输出节点$k$的误差是$\delta^o_k$，隐藏节点$j$的误差即为输出误差乘以输出层-隐藏层间的权重矩阵以及梯度。</p><p>$$\delta ^h_j=\sum W_{jk}\delta^o_kf’(h_j)$$</p><p>然后，梯度下降与之前相同，只是用新的误差：<br>$$\Delta w_{ij}=\eta \delta^h_jx_i $$</p><p>其中 $w_{ij}$ 是输入和隐藏层之间的权重， $x_i$ 是输入值。这个形式可以表示任意层数。权重更新步长等于步长乘以层输出误差再乘以该层的输入值。<br>$$\Delta w_{qp}=\eta\delta_{output}V_{input}$$<br>现在，你有了输出误差，$\delta_{output}$，便可以反向传播这些误差了。$V_{input}$ 是该层的输入，比如经过隐藏层激活函数的输出值。</p><h2 id="二、范例"><a href="#二、范例" class="headerlink" title="二、范例"></a>二、范例</h2><h3 id="1-单节点误差项"><a href="#1-单节点误差项" class="headerlink" title="1. 单节点误差项"></a>1. 单节点误差项</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15278487531883.jpg" alt=""><br>假设我们试着训练一些二进制数据，目标值是 $y=1$。我们从正向传播开始，首先计算输入到隐藏层节点：</p><p>$h=\sum_iw_ix_i=0.1 \times 0.4 - 0.2\times 0.3=-0.02 $</p><p>以及隐藏层节点的输出：</p><p>$a=f(h)=sigmoid(-0.02)=0.495$</p><p>然后将其作为输出节点的输入，该神经网络的输出可表示为：</p><p>$\hat y=f(W \cdot a)=sigmoid(0.1 \times0.495)=0.512$</p><p>基于该神经网络的输出，就可以使用反向传播来更新各层的权重了。sigmoid 函数的导数$\hat y=f(W \cdot a)(1-f(W\cdot a))$，输出节点的误差项(error term)可表示为：</p><p>$\delta^o=(y-\hat y)f’(W\cdot a)=(1-0.512)\times0.512\times (1-0.512)=0.122 $</p><p>现在我们要通过反向传播来计算隐藏节点的误差项。这里我们把输出节点的误差项与隐藏层到输出层的权重 $W$ 相乘。隐藏节点的误差项$\delta ^h_j=\sum W_{jk}\delta^o_kf’(h_j)$，因为该案例只有一个隐藏节点，这就比较简单了</p><p>$\delta ^h=W\delta^of’(h)=0.1 \times 0.122 \times 0.495 \times(1-0.495)=0.003$</p><p>有了误差，就可以计算梯度下降步长了。隐藏层-输出层权重更新步长是学习速率乘以输出节点误差再乘以隐藏节点激活值。</p><p>$\Delta W=\eta\delta^oa=0.5\times0.122\times 0.495=0.0302 $</p><p>然后，输入-隐藏层权重 $w_i$ 是学习速率乘以隐藏节点误差再乘以输入值。</p><p>$\Delta w_i=\eta\delta^ox_i=(0.5\times0.003\times0.1，0.5\times0.003\times0.3)=(0.00015，0.00045)$</p><p>从这个例子中你可以看到 sigmoid 做激活函数的一个缺点。sigmoid 函数导数的最大值是 0.25，因此输出层的误差被减少了至少 75%，隐藏层的误差被减少了至少 93.75%！如果你的神经网络有很多层，使用 sigmoid 激活函数会很快把靠近输入层的权重步长降为很小的值，该问题称作<strong>梯度消失</strong>。后面的课程中你会学到在这方面表现更好，也被广泛用于最新神经网络中的其它激活函数。</p><h3 id="2-多节点误差项时的情况"><a href="#2-多节点误差项时的情况" class="headerlink" title="2. 多节点误差项时的情况"></a>2. 多节点误差项时的情况</h3><p>大多数时候我们需要创建多个隐藏层多几个节点，因此针对这种情况，现在在更新权重时，我们需要考虑隐藏层 每个节点 的误差 $\delta _j$</p><p>$\Delta w_{ij}=\eta\delta_jx_i$</p><p>首先，会有不同数量的输入和隐藏节点，所以试图把误差与输入当作行向量来乘会报错。</p><p>另外，现在$w_{ij}$是一个矩阵，所以右侧对应也应该有跟左侧同样的维度。幸运的是，NumPy 这些都能搞定。如果你用一个列向量数组和一个行向量数组相乘，它会把列向量的第一个元素与行向量的每个元素相乘，组成一个新的二维数组的第一行。列向量的每一个元素依次重复该过程，最后你会得到一个二维数组，形状是 <code>(len(column_vector)</code>,<code>len(row_vector))</code>。</p><pre><code>hidden_error*inputs[:,None]array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])</code></pre><p>这正好是我们计算权重更新的步长的方式。跟以前一样，如果你的输入是一个一行的二维数组，也可以用 <code>hidden_error*inputs.T，</code>但是如果 inputs 是一维数组，就不行了（这时可以使用上述方法<code>hidden_error*inputs[:,None]</code>）。</p><h2 id="三、反向传播练习"><a href="#三、反向传播练习" class="headerlink" title="三、反向传播练习"></a>三、反向传播练习</h2><p>接下来你将用代码来实现一次两个权重的反向传播更新。我们提供了正向传播的代码，你来实现反向传播的部分。</p><p>要做的事</p><ul><li>计算网络输出误差</li><li>计算输出层误差项</li><li>用反向传播计算隐藏层误差项</li><li>计算反向传播误差的权重更新步长</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate sigmoid</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.2</span>])</span><br><span class="line">target = <span class="number">0.6</span></span><br><span class="line">learnrate = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">weights_input_hidden = np.array([[<span class="number">0.5</span>, <span class="number">-0.6</span>],</span><br><span class="line">                                 [<span class="number">0.1</span>, <span class="number">-0.2</span>],</span><br><span class="line">                                 [<span class="number">0.1</span>, <span class="number">0.7</span>]])</span><br><span class="line"></span><br><span class="line">weights_hidden_output = np.array([<span class="number">0.1</span>, <span class="number">-0.3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">## Forward pass</span></span><br><span class="line">hidden_layer_input = np.dot(x, weights_input_hidden)</span><br><span class="line">hidden_layer_output = sigmoid(hidden_layer_input)</span><br><span class="line"></span><br><span class="line">output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)</span><br><span class="line">output = sigmoid(output_layer_in)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Backwards pass</span></span><br><span class="line"><span class="comment">## <span class="doctag">TODO:</span> Calculate output error</span></span><br><span class="line">error = target - output</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate error term for output layer</span></span><br><span class="line">output_error_term = error * output * (<span class="number">1</span> - output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate error term for hidden layer</span></span><br><span class="line">hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \</span><br><span class="line">                    hidden_layer_output * (<span class="number">1</span> - hidden_layer_output)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate change in weights for hidden layer to output layer</span></span><br><span class="line">delta_w_h_o = learnrate * output_error_term * hidden_layer_output</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Calculate change in weights for input layer to hidden layer</span></span><br><span class="line">delta_w_i_h = learnrate * hidden_error_term * x[:, <span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Change in weights for hidden layer to output layer:'</span>)</span><br><span class="line">print(delta_w_h_o)</span><br><span class="line">print(<span class="string">'Change in weights for input layer to hidden layer:'</span>)</span><br><span class="line">print(delta_w_i_h)</span><br></pre></td></tr></table></figure><pre><code>Change in weights for hidden layer to output layer:[0.00804047 0.00555918]Change in weights for input layer to hidden layer:[[ 1.77005547e-04 -5.11178506e-04] [ 3.54011093e-05 -1.02235701e-04] [-7.08022187e-05  2.04471402e-04]]Nice job!  That&apos;s right!</code></pre><h2 id="四、实现反向传播算法"><a href="#四、实现反向传播算法" class="headerlink" title="四、实现反向传播算法"></a>四、实现反向传播算法</h2><p>现在我们知道输出层的误差是</p><p>$$\delta _k=(y_k-\hat y_k)f’(a_k)$$</p><p>隐藏层误差为：</p><p>$$\delta <em>j=\sum[w</em>{jk}\delta_k]f’(h_j)$$<br>现在我们只考虑一个简单神经网络，它只有一个隐藏层和一个输出节点。这是通过反向传播更新权重的算法概述：</p><p>1) 初始化权重。为了简单起见，这里初始化为0</p><ul><li>输入到隐藏册的权重$\Delta w_{ij}=0$</li><li>隐藏层到输出层权重$\Delta W_j=0$</li></ul><p>2) 对训练数据中的每个点执行</p><ul><li>让它正向通过网络，计算出$\hat y$</li><li>计算输出节点的误差梯度 $\delta^o=(y-\hat y)f’(z)$这里$z=\sum _jW_ja_j$是输出节点的输入。</li><li>误差传播到隐藏层 $\delta^h_j=\delta^oW_jf’(h_j)$</li><li>更新权重步长：<ul><li>$\Delta W_j=\Delta W_j+\delta^oa_j$</li><li>$\Delta w_{ij}=\Delta w_{ij}+\delta_j^ha_i$</li></ul></li></ul><p>3) 更新权重，其中$\eta $是学习率，$m$ 是数据点的数量：</p><ul><li>$W_j=W_j+\eta\Delta W_j/m$</li><li>$w_{ij}=w_{ij}+\eta\Delta w_{ij}/m$</li></ul><p>4) 重复以上过程$e$代。</p><p><strong>代码实现例子</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">admissions = pd.read_csv(<span class="string">'binary.csv'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.concat([admissions, pd.get_dummies(admissions[<span class="string">'rank'</span>], prefix=<span class="string">'rank'</span>)], axis=<span class="number">1</span>)</span><br><span class="line">data = data.drop(<span class="string">'rank'</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> field <span class="keyword">in</span> [<span class="string">'gre'</span>, <span class="string">'gpa'</span>]:</span><br><span class="line">    mean, std = data[field].mean(), data[field].std()</span><br><span class="line">    data.loc[:,field] = (data[field]-mean)/std</span><br><span class="line">    </span><br><span class="line">np.random.seed(<span class="number">21</span>)</span><br><span class="line">sample = np.random.choice(data.index, size=int(len(data)*<span class="number">0.9</span>), replace=<span class="keyword">False</span>)</span><br><span class="line">data, test_data = data.ix[sample], data.drop(sample)</span><br><span class="line"></span><br><span class="line">features, targets = data.drop(<span class="string">'admit'</span>, axis=<span class="number">1</span>), data[<span class="string">'admit'</span>]</span><br><span class="line">features_test, targets_test = test_data.drop(<span class="string">'admit'</span>, axis=<span class="number">1</span>), test_data[<span class="string">'admit'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#=================================</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate sigmoid</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">n_hidden = <span class="number">2</span>  <span class="comment"># number of hidden units</span></span><br><span class="line">epochs = <span class="number">900</span></span><br><span class="line">learnrate = <span class="number">0.005</span></span><br><span class="line"></span><br><span class="line">n_records, n_features = features.shape</span><br><span class="line">last_loss = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">weights_input_hidden = np.random.normal(scale=<span class="number">1</span> / n_features ** <span class="number">.5</span>,</span><br><span class="line">                                        size=(n_features, n_hidden))</span><br><span class="line">weights_hidden_output = np.random.normal(scale=<span class="number">1</span> / n_features ** <span class="number">.5</span>,</span><br><span class="line">                                         size=n_hidden)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    del_w_input_hidden = np.zeros(weights_input_hidden.shape)</span><br><span class="line">    del_w_hidden_output = np.zeros(weights_hidden_output.shape)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(features.values, targets):</span><br><span class="line"></span><br><span class="line">        hidden_input = np.dot(x, weights_input_hidden)</span><br><span class="line">        hidden_output = sigmoid(hidden_input)</span><br><span class="line"></span><br><span class="line">        output = sigmoid(np.dot(hidden_output,</span><br><span class="line">                                weights_hidden_output))</span><br><span class="line"></span><br><span class="line">        error = y - output</span><br><span class="line"></span><br><span class="line">        output_error_term = error * output * (<span class="number">1</span> - output)</span><br><span class="line"></span><br><span class="line">        hidden_error = np.dot(output_error_term, weights_hidden_output)</span><br><span class="line"></span><br><span class="line">        hidden_error_term = hidden_error * hidden_output * (<span class="number">1</span> - hidden_output)</span><br><span class="line"></span><br><span class="line">        del_w_hidden_output += output_error_term * hidden_output</span><br><span class="line">        del_w_input_hidden += hidden_error_term * x[:, <span class="keyword">None</span>]</span><br><span class="line"></span><br><span class="line">    weights_input_hidden += learnrate * del_w_input_hidden / n_records</span><br><span class="line">    weights_hidden_output += learnrate * del_w_hidden_output / n_records</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> e % (epochs / <span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">        hidden_output = sigmoid(np.dot(x, weights_input_hidden))</span><br><span class="line">        out = sigmoid(np.dot(hidden_output,</span><br><span class="line">                             weights_hidden_output))</span><br><span class="line">        loss = np.mean((out - targets) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> last_loss <span class="keyword">and</span> last_loss &lt; loss:</span><br><span class="line">            print(<span class="string">"Train loss: "</span>, loss, <span class="string">"  WARNING - Loss Increasing"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Train loss: "</span>, loss)</span><br><span class="line">        last_loss = loss</span><br><span class="line"></span><br><span class="line">hidden = sigmoid(np.dot(features_test, weights_input_hidden))</span><br><span class="line">out = sigmoid(np.dot(hidden, weights_hidden_output))</span><br><span class="line">predictions = out &gt; <span class="number">0.5</span></span><br><span class="line">accuracy = np.mean(predictions == targets_test)</span><br><span class="line">print(<span class="string">"Prediction accuracy: &#123;:.3f&#125;"</span>.format(accuracy))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;传送门：&lt;br&gt;&lt;a href=&quot;http://ihoge.cn/2018/DL_BFNN.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BF神经网络（二）预测共享单车使用情况&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="BF神经网络" scheme="http://www.ihoge.cn/tags/BF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法（1）</title>
    <link href="http://www.ihoge.cn/2018/clustering.html"/>
    <id>http://www.ihoge.cn/2018/clustering.html</id>
    <published>2018-05-28T04:20:21.000Z</published>
    <updated>2018-05-28T08:01:37.166Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><hr><p>本文内容包含：<br>    一、基本概念<br>    二、K-means<br>    三、凝聚层次聚类<br>    四、DBSCAN密度聚类<br>    五、簇评估</p><hr><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>聚类分析仅根据数据内部发现的描述对象及其关系信息将数据对象分组。组件的相似性（如凝聚度）越大，组件差别越大（如分离度），聚类就越好。</p><h3 id="1-不同的聚类类型"><a href="#1-不同的聚类类型" class="headerlink" title="1. 不同的聚类类型"></a>1. 不同的聚类类型</h3><h4 id="1-1-层次的和划分的"><a href="#1-1-层次的和划分的" class="headerlink" title="1.1 层次的和划分的"></a>1.1 层次的和划分的</h4><p>其区别是簇的集合是嵌套的还是非嵌套的。</p><ul><li><strong>层次的</strong>： 适用于小数据。可以形成相似度层次图谱，便于直观地确定类之间的划分。  </li><li><strong>划分的</strong>： 适用于大数据。将观测分为预先指定不重叠的类。但不能提供类相似度信息，需要实现决定<code>聚类个数</code>和<code>初始位置</code>。（如K-means）</li></ul><h4 id="1-2-互斥的、重叠的与模糊的"><a href="#1-2-互斥的、重叠的与模糊的" class="headerlink" title="1.2 互斥的、重叠的与模糊的"></a>1.2 互斥的、重叠的与模糊的</h4><p><strong>互斥</strong>： 互斥表示一个对象最多只属于一个类。<br><strong>重叠</strong>： 重叠表示一个对象可以被指定为多个类。没个对象以 1（绝对属于）和 0（绝对不属于）的概率属于任何一个集合，且隶属概率之和为1。</p><h4 id="1-3-完全的与部分的"><a href="#1-3-完全的与部分的" class="headerlink" title="1.3 完全的与部分的"></a>1.3 完全的与部分的</h4><p><strong>完全的</strong>： 将每个对象指派到一个簇<br><strong>部分的</strong>： 忽略掉不感兴趣的背景（噪音或者离群点）</p><h3 id="2-部分聚类的方法"><a href="#2-部分聚类的方法" class="headerlink" title="2. 部分聚类的方法"></a>2. 部分聚类的方法</h3><p><strong>K-means</strong>： 是基于原型的、划分的聚类技术。适用于大数据。将观测分为预先指定不重叠的类。但不能提供类相似度信息，需要实现决定<code>聚类个数</code>和<code>初始位置</code>。<br><strong>凝聚的层次聚类</strong>： 开始每个点作为单个簇；然后重复地合并两个最靠近的簇直到产生单个的、包含所有点的簇。<br><strong>两步聚类</strong>： 适用于大样本数据。以 <code>BIRCH</code> 方法为代表，是层次聚类和 k-means 的结合。具有运算速度快、不需要大量递归运算，节省存储空间的优点。<br><strong>基于密度的聚类</strong>： 适用于大样本数据，对噪音不敏感。基于划分的方法适用于样本形态为球状簇时的情况，当分布不规则时，需要使用该类方法。最常见的是 <code>BDSCAN</code>聚类。</p><h2 id="二、K-means"><a href="#二、K-means" class="headerlink" title="二、K-means"></a>二、K-means</h2><h3 id="1-算法步骤"><a href="#1-算法步骤" class="headerlink" title="1. 算法步骤"></a>1. 算法步骤</h3><blockquote><p><1> 选择$K$个点作为初始质心</1></p><p><2> Repeat：</2></p><p><3> 将每个点指派到最近的质心，形成$K$个簇</3></p><p><4> 重新计算每个簇的质心</4></p><p><5> Until: 质心不发生变化终止</5></p></blockquote><h3 id="2-距离的度量"><a href="#2-距离的度量" class="headerlink" title="2. 距离的度量"></a>2. 距离的度量</h3><p><a href="http://ihoge.cn/2018/KNN1.html" target="_blank" rel="noopener"><strong>闵可夫斯基距离</strong></a></p><p>闵可夫斯基距离不是一种距离，而是一类距离的定义。对于 n 维空间中的两个点 $x(x_1,x_2,x_3,…,x_n)$和$y(y_1,y_2,y_3,…,y_n)$，那么$x$和$y$亮点之间的闵可夫斯基距离为：$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{p}}}$$<br>其中p是一个可变参数：</p><ul><li>当p=1时，被称为<strong>曼哈顿距离</strong>；</li><li>当p=2时，被称为<strong>欧式距离</strong>；</li><li>当p=$\infty $时，被称为切比雪夫距离。</li></ul><p><strong>余弦相似度</strong>：</p><p>$$cos(\Theta )=\frac {a^Tb}{|a|*|b|}$$</p><ul><li>$a,b$表示两个向量，$|a|$和$|b|$表示向量的模。 余弦相似度一般衡量两个向量的相似情况，常用与文本处理。余弦角越小越相似。 </li></ul><p><strong>杰卡德（Jaccard）相似系数</strong> </p><p>$$J(A,B)=\frac {|A \bigcap B|}{|A \bigcup B|}$$</p><ul><li>这里，$A、B$表示集合，$A \bigcap B$表示两个集合公共元素的个数，$A \bigcup B$表示两个集合并集元素的个数。 Jaccard 相似系数适用于度量两个集合的相似程度，取值在 0～1 之间，越大越相似。在推荐系统中常用衡量客户或商品的相似度。</li></ul><h3 id="3-变量标准化"><a href="#3-变量标准化" class="headerlink" title="3. 变量标准化"></a>3. 变量标准化</h3><p>在聚类前，通常需要对个连续变量进行标准化，因为方差大的变量比方差晓得变量对距离或相似度的影响更大，从而对聚类结果的影响更大。</p><p>常用的方法有：</p><p><strong>正态标准化</strong>：$x_i=\frac {x_i-mean(X)}{std(X}$<br><strong>归一化</strong>：$x_i=\frac {x_i-min(X)}{max(X)-min(X)}$</p><h3 id="4-变量的维度分析"><a href="#4-变量的维度分析" class="headerlink" title="4. 变量的维度分析"></a>4. 变量的维度分析</h3><p>假设一组变量中，一个维度有5个变量，二另一个维度只有1个变量，则第一个维度的权重被明显提高了。一般情况下，每个维度上使用的变量个数应该是一样的，不过分析人员要结合具体场景不同维度提供不同数量的变量个数，这相当于加大了一些维度的权重。</p><p>除了机遇业务定义进行变量的选择，另一种常用的方法是在聚类之前进行主成分分析。</p><h3 id="5-质心的目标函数"><a href="#5-质心的目标函数" class="headerlink" title="5. 质心的目标函数"></a>5. 质心的目标函数</h3><h4 id="5-1-SSE-误差平方和"><a href="#5-1-SSE-误差平方和" class="headerlink" title="5.1 SSE 误差平方和"></a>5.1 SSE 误差平方和</h4><p>聚类的目标通常用一个目标函数表示，该函数依赖于点之间，或点到簇的质心的临近性；<br>如，考虑临近性度量为欧几里得距离的数据，我们使用<strong>误差平方和（SSE）</strong>作为度量聚类质量的目标函数，即<strong>最小化簇中点到质心的距离平方和</strong>。 SSE也称散布（scatter），定义如下：<br>$$SSE=∑^K_{i=1}∑_{x\in C_i}dist(c_i,x)^2$$<br>其中，$dist$是欧几里得空间中两个对象之间的标准欧几里得距离。</p><p>给定这些假设，实际上可以证明：对 SSE 求导，另导数为 0 求解 $c_k$<strong>使簇的 SSE 最小的质心是均值</strong>；</p><p>$$\frac {\partial }{\partial c_k}SSE =\frac {\partial }{\partial c_k}∑^K_{i=1}∑_{x\in C_i}(c_i,x)^2=0 $$</p><p>最终得到： $$∑_{x\in C_k}2(c_k-x_k)=0\Longrightarrow m_kc_k=∑_{x\in C_k}x_k \Longrightarrow c_k = \frac 1{m_k}∑_{x\in C_k}x_k$$</p><p><strong>文档数据</strong></p><p>考虑文档数据和余弦相似性度量。这里我们假定文档数据用文档——词矩阵表示，我们的目标是最大化簇中文档与簇的质心的相似性；该量乘坐簇的<code>凝聚度（cohesion）</code>。对于该目标，可以证明，与欧几里得数据一样，簇的质心是均值。总 SSE 的类似量是总凝聚度（total cohesion）：<br>$$Total Cohesion=∑^K_{i=1}∑_{x\in C_i}cosine(c_i,x)$$</p><p>关于凝聚度的知识，会在下文模型评估里面详细介绍</p><h4 id="5-2-SAE-绝对误差和"><a href="#5-2-SAE-绝对误差和" class="headerlink" title="5.2 SAE 绝对误差和"></a>5.2 SAE 绝对误差和</h4><p>为了表明$K$均值可以用各种不同的目标函数，我们考虑如何将数据分成$K$个簇，使得<strong>点到其簇中心的曼哈顿距离之和最小</strong>。如下式绝对误差和（SAE）</p><p>$$SAE = ∑^K_{i=1}∑_{x \in C_i}|c_i-x|$$</p><p>$$\frac {\partial }{\partial c_k}SAE =\frac {\partial }{\partial c_k}∑^K_{i=1}∑_{x\in C_i}|c_i-x|=0 $$</p><p>最终得到： $$∑_{x\in C_k}\frac {\partial }{\partial c_k}|c_k-x|=0\Longrightarrow ∑_{x\in C_k}sign(x-c_k )=0 \Longrightarrow c_k=median{x\in C_k}$$<br>即簇中点的中位数。一组点的中位数的计是直截了当的，并且减少受离群值的影响。</p><h4 id="5-3-常见的邻近度、质心和目标函数组合"><a href="#5-3-常见的邻近度、质心和目标函数组合" class="headerlink" title="5.3 常见的邻近度、质心和目标函数组合"></a>5.3 常见的邻近度、质心和目标函数组合</h4><table><thead><tr><th>邻近度函数</th><th>质心</th><th>目标函数</th></tr></thead><tbody><tr><td>曼哈顿距离</td><td>中位数</td><td>最小化对象与质心的绝对误差和SAE</td></tr><tr><td>平方欧几里得距离</td><td>均值</td><td>最小化对象与质心的误差平方和SSE</td></tr><tr><td>余弦</td><td>均值</td><td>最大化对象与质心的余弦相似度和</td></tr><tr><td>Bregman散度</td><td>均值</td><td>最小化对象到质心的Bregman散度和</td></tr></tbody></table><p><code>Bregman散度</code>实际上是一类紧邻性度量，包括平方欧几里得距离。<code>Bregman散度函数</code>的重要性在于，任意这类函数都可以用作以均值为质心的 K-means 类型的聚类算法的基础。</p><h3 id="6-选择初始质心"><a href="#6-选择初始质心" class="headerlink" title="6. 选择初始质心"></a>6. 选择初始质心</h3><p>当质心随机初始化时，K-means 将产生不同的总 SEE。选择适当的初始质心是基本 K-menas 过程的关键步骤。常见的是随机选取，但这种情况下簇的质量常常很差。考虑什么情况下选择的初始质心能找到最优解？答案是：每个簇刚好分到一个质心。事实证明发生这种情况的概率是非常非常低的。</p><p>常见一种技术是：多次运行，然后选取具有最小 SEE 的簇集。该策略虽然简单，但是效果可能不太好，依然是概率事件。</p><p>另一种有效的技术是：取一个样本，并使用层次聚类技术对他聚类。从层次聚类中提取 $K$ 个簇，并用这些簇的质心作为初始质心。该方法虽然有效，但仅对下列情况有效：（1）样本相对较小，例如数百到数千（层次聚类开销较大）；（2）$K$ 相对与样本大小较小。</p><p>还有一种方法是：随机选择第一个点或者所有点到质心作为第一个点。然后对于每个候机初始质心，选择里已经选取的初始质心最远的点，并且把该方法应用与点样本。 这样可以大大缓解可能会选择离群点作为质心的可能，并且大大减小计算量。</p><p>另外，我们也可以采用对初始化问题不太敏感的 K-means 的变种，<strong>二分K-means</strong>、使用后处理来“修补”<br>所产生的簇集</p><h3 id="7-时间复杂性和空间复杂性"><a href="#7-时间复杂性和空间复杂性" class="headerlink" title="7. 时间复杂性和空间复杂性"></a>7. 时间复杂性和空间复杂性</h3><ul><li><p>所需空间：$O((m+K)n)$，m 是点数， n 是属性数</p></li><li><p>所需时间：$O(I<em>K</em>m*n)$，$I$ 是收敛所需迭代次数，通常很小，可以是有界的。</p></li></ul><h3 id="8-K-means-其他问题"><a href="#8-K-means-其他问题" class="headerlink" title="8. K-means 其他问题"></a>8. K-means 其他问题</h3><h4 id="8-1-处理空簇"><a href="#8-1-处理空簇" class="headerlink" title="8.1 处理空簇"></a>8.1 处理空簇</h4><p>K-means 存在的问题之一是：如果所有的点在指派的步骤都为分配到某个簇，就会得到空簇。这种情况下需要选择一个替补质心，否则误差将会偏大。</p><ul><li>方法一： 选择一个距离当前任何质心最远的点</li><li>方法二： 从具有最大 SSE 的簇中选择一个替补质心。浙江分裂簇并降低聚类的总 SSE。</li></ul><h4 id="8-2-离群点"><a href="#8-2-离群点" class="headerlink" title="8.2 离群点"></a>8.2 离群点</h4><p>当然我们想到的第一反应是删除离群点，但是有些聚类应用，不能删除离群点。在某些情况下（财经分析），明显的离群点可能是最令人感兴趣的点。</p><p>那么问题来了，如何识别离群点？</p><ul><li>方法一：聚类前删除离群点</li><li>方法二：后处理离群点。如删除那些具有不寻常影响的点（尤其是多次运行算法时），另外也可以删除那些很小的簇，他们尝尝代表离群点的组。</li></ul><h4 id="8-3-后处理降低-SSE"><a href="#8-3-后处理降低-SSE" class="headerlink" title="8.3 后处理降低 SSE"></a>8.3 后处理降低 SSE</h4><ul><li><p><strong>增加簇个数</strong></p><ul><li>分裂一个簇：通常选择具有最大 SSE 的簇，页可以分裂在特定属性上具有最大标准差的簇。</li><li>引进一个新的质心：通常选择离所有质心最远的点。</li></ul></li><li><p><strong>减少簇个数</strong></p><ul><li>拆散一个簇： 通常选择拆散使总 SSE 增加最少的簇， 删除对应的质心</li><li>合并两个簇： 通常选择合并质心最接近的两个簇，或者合并两个导致总 SSE 增加最少的簇。这两种方法与层次聚类使用的方法相同，分别乘坐质心方法和 Ward 方法。</li></ul></li></ul><h3 id="9-二分-K-means"><a href="#9-二分-K-means" class="headerlink" title="9. 二分 K-means"></a>9. 二分 K-means</h3><p>二分 K-means 算法时基于 K-means 算法的直接扩充，它基于一种简单想法：为了得到 K 个簇，将所有点的集合分裂成两个簇，从这些簇中选取一个继续分裂，如此下去，知道产生 K 个簇。</p><p>算法实现步骤：</p><blockquote><p><1> 初始化簇表，是指包含有所有的点组成的簇。</1></p><p><2> <strong>Repeat</strong>：</2></p><blockquote><p><3>   从簇表中取出一个簇</3></p></blockquote><p><4>   对选出的簇进行多次二分“实验”</4></p><p><5>   for i = 1 to 试验次数 do：</5></p><p><6>       使用基本 K-means，二分选定的簇</6></p><p><7>   end for</7></p><p><8>   从二分实验中选择具有最小 SSE 的两个簇</8></p><p><9>   将这两个簇添加到簇表中</9></p></blockquote><blockquote><p><10>  <strong>Until</strong> 簇表包含 K 个簇。  </10></p></blockquote><p>待分裂的簇有许多不同的选择方法。可以选择最大的簇，选择具有最大 SSE 的簇，或者使用一个基于大小和 SSE 的标准进行选择。不同的选择导致不同的簇。</p><p>我们通常使用结果簇的质心作为基本 K-means 的初始质心，对结果逐步求精。 因为尽管 K-means 可以保证找到使 SSE 局部最小的聚类，但是自二分 K-means 算法中，我们“局部地”使用了 K-means ，即二分个体簇。因此，最终的簇集并不代表使 SSE 局部最小的聚类。</p><h3 id="10-K-means优缺点"><a href="#10-K-means优缺点" class="headerlink" title="10. K-means优缺点"></a>10. K-means优缺点</h3><h4 id="10-1-优点"><a href="#10-1-优点" class="headerlink" title="10.1 优点"></a>10.1 优点</h4><ul><li>简单并且可以用于各种数据类型；</li><li>具备适合的空间复杂度和计算负责度，适用于大样本数据；</li><li><p>K-means 某些变种甚至更有效 （二分K-means）且不受初始化问题影响。</p><h4 id="10-2-缺点"><a href="#10-2-缺点" class="headerlink" title="10.2 缺点"></a>10.2 缺点</h4></li><li><p>不能处理非球形簇、不同尺寸和不同密度的簇；</p></li><li>对离群点敏感；</li><li>K-means 仅限于具有中心（质心）概念的数据。有一种相关的 K-中心点聚类技术没有这种限制，但是开销更大。</li></ul><h2 id="三、凝聚层次聚类"><a href="#三、凝聚层次聚类" class="headerlink" title="三、凝聚层次聚类"></a>三、凝聚层次聚类</h2><p>有两种产生层次聚类的基本方法：</p><ul><li>凝聚的： 从点作为个体簇开始，每一步合并两个最接近的簇</li><li>分裂的： 从包含所有的点某个簇开始，每一步分裂一个簇，知道今生下单点簇</li></ul><p>到目前为之，凝聚层次聚类最常见，这里只讨论这类方法。</p><h3 id="1-基本聚类层次聚类算法"><a href="#1-基本聚类层次聚类算法" class="headerlink" title="1. 基本聚类层次聚类算法"></a>1. 基本聚类层次聚类算法</h3><blockquote><p><1> 如果需要，计算邻近度矩阵</1></p><p><2> <strong>Repeat:</strong></2></p><blockquote><p><3> 合并最接近的两个簇</3></p><p><4> 更新临近性矩阵，以反映新的簇与原来簇之间的临近性</4></p></blockquote></blockquote><blockquote><p><5> <strong>Until：</strong> 仅剩下一个簇</5></p></blockquote><h3 id="2-定义簇之间的临近性"><a href="#2-定义簇之间的临近性" class="headerlink" title="2. 定义簇之间的临近性"></a>2. 定义簇之间的临近性</h3><ul><li><strong>MIN</strong>：MIN定义簇的邻近度为不同簇的两个最近点之间的距离，也叫做<strong>单链（sigle link）</strong></li><li><strong>MAX</strong>：MAX定义簇的邻近度为不同簇的两个最远点之间的距离，也叫做<strong>全链（complete link）</strong></li><li><strong>组平均</strong>：它定义簇邻近度为取自不同簇的所有点对邻近度的平均值。</li></ul><p>如果我们取基于原型的观点，簇用质心代表，则不同簇邻近度定义就更加自然。</p><ul><li><strong>簇质心间邻近度</strong>：两个簇质心之间的距离</li><li><strong>Ward方法</strong>：同样嘉定簇用气质心代表，但他使用合并两个簇导致 SSE 的增量来度量两个簇之间的临近性。像 K-means 一样， Ward 方法也是图最小化点到其簇质心的距离平方和。</li></ul><h3 id="3-时间和空间复杂性"><a href="#3-时间和空间复杂性" class="headerlink" title="3. 时间和空间复杂性"></a>3. 时间和空间复杂性</h3><ul><li><strong>空间复杂度</strong>： $O(m^2)$</li><li><strong>时间复杂度</strong>： $O(m^2)$时间计算邻近度矩阵，基于以上算法所需总时间为$O(m^2log(m))$</li></ul><p>基本层次聚类使用邻近度矩阵，这需要存储$m^2/2$个邻近度（假定邻近度矩阵是对称的），其中$m$是数据点的个数。记录簇所需要的空间正比于簇的个数$m-1$，不包括单点簇。因此空间复杂度是$O(m^2)$。</p><p>上面步骤中<3>和<4>涉及$m-1$次迭代，因为开始有$m$个簇，每次迭代合并两个簇。如果邻近度矩阵采用线性搜索，则对于第$i$次迭代，步骤3需要$O(m-i+1)^2$的时间，步骤<4>只需要$O(m-i+1)$时间，在合并两个簇后更新邻近度矩阵（对于我们考虑的技术，簇合并至影响$O(m-i+1)$个邻近度）不做修改，时间负责度将为$O(m^3)$。如果某个簇到其他所有簇的距离存放在一个有序表或堆中，则查找两个最近簇的开销可能降低到$O(m-i+1)$，然而，由于维护有序表或堆的附加开销，基于以上算法的层次聚类所需总时间是$O(m^2log(m))$。</4></4></3></p><p>由此可见，层次聚类算法需要消耗大量的存储和计算资源，严重限制了它所能处理的数据集的大小。</p><h3 id="4-层次聚类的主要问题"><a href="#4-层次聚类的主要问题" class="headerlink" title="4. 层次聚类的主要问题"></a>4. 层次聚类的主要问题</h3><h4 id="4-1-缺乏全局目标函数"><a href="#4-1-缺乏全局目标函数" class="headerlink" title="4.1 缺乏全局目标函数"></a>4.1 缺乏全局目标函数</h4><p>也就是说凝聚层次聚类技术使用各种标准，在每一步局部地确定哪些簇应当合并（或分裂，对于分裂方法）。这种方法产生的聚类算法避开了解决困难的组合优化能力。此外这样的方法没有局部极小问题或很难选择初始化的问题。当然，在许多情况下$O(m^2)$的空间复杂度和$O(m^2log(m))$的事件负责度也阻碍了它的应用。</p><h4 id="4-2-处理不同大小簇的能力"><a href="#4-2-处理不同大小簇的能力" class="headerlink" title="4.2 处理不同大小簇的能力"></a>4.2 处理不同大小簇的能力</h4><p>对于如何处理待合并的簇对的相对大小（鄙人理解为权值，该讨论仅适用于涉及求和的簇临近性方法，如质心、Ward方法和组平均）有两种方法：</p><ul><li><strong>加权</strong>：平等的对待所有簇，赋予不同大小的簇中点不同的权值</li><li><strong>非加权</strong>： 赋予不同大小簇中点相同的权值 </li></ul><h4 id="4-3-合并不可逆"><a href="#4-3-合并不可逆" class="headerlink" title="4.3 合并不可逆"></a>4.3 合并不可逆</h4><p>对于合并两个簇，凝聚层次聚类算法去相遇作出有好的局部决策。然而，一旦做出合并决策，以后就不能撤销。这种方法阻碍了局部最优标准变成全局最优标准。</p><p>有一些技术是图克服“合并不可逆”这一限制，一种通过移动树的分支以改善全局目标函数；另一种使用划分聚类技术（如K-means）来创建许多小簇，然后从这些小簇出发进行层次聚类。<br>看似第二种更可取</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>该算法缺点上面描述的已经很详细了，通常使用这类算法是因为基本应用需要层次结构，此外有些研究表明这些算法能产生高质量的聚类。然而就计算量和存储量而言，凝聚层次聚类算法是昂贵的，合并时不可逆的，对于噪声、高维数据（如文档数据），这也可能造成问题。</p><p>首先使用其他技术（K-均值）进行部分聚类，这两个问题都可以在某种程度上加以解决。</p><h2 id="四、DBSCAN-基于密度的聚类"><a href="#四、DBSCAN-基于密度的聚类" class="headerlink" title="四、DBSCAN 基于密度的聚类"></a>四、DBSCAN 基于密度的聚类</h2><p> 基于密度的聚类寻找被低密度区域分离的高密度区域。DBSCAN 是一种简单、有效的基于密度的聚类算法。</p><h3 id="1-基本名词"><a href="#1-基本名词" class="headerlink" title="1. 基本名词"></a>1. 基本名词</h3><ul><li><strong>核心点</strong>： 如果该点满足给定的邻域内（半径为$Eps$的范围内）的点的个数超过给定的阈值$Minpts$，则该点为满足该条件下的核心点。</li><li><strong>边界点</strong>： 边界点落在某个核心点的邻域内，同时边界点可能落在多个核心点的邻域内。</li><li><strong>噪声点</strong>： 噪声点既非核心点，也不是边界点</li></ul><h3 id="2-SBSCAN算法步骤"><a href="#2-SBSCAN算法步骤" class="headerlink" title="2. SBSCAN算法步骤"></a>2. SBSCAN算法步骤</h3><blockquote><p><1> 将所有点标记为核心点、边界点或噪音点</1></p><p><2> 删除噪音点</2></p><p><3> 为距离在 $Eps$ 之内的所有核心点之间赋予一条边 </3></p><p><4> 每组联通的核心点形成一个簇</4></p><p><5> 将每个边界点指派到一个与之关联的核心点的簇中</5></p></blockquote><h3 id="3-时间和空间复杂度"><a href="#3-时间和空间复杂度" class="headerlink" title="3. 时间和空间复杂度"></a>3. 时间和空间复杂度</h3><ul><li><strong>时间复杂度</strong>： $O(m <em> 找出邻域中的点所需要的时间)$，在最坏的情况下时间复杂度为$O(m^2)$；当数据结构为 KD树，可以有效地检索特定点给顶距离内的所有点，时间复杂度降低到$O(m</em>log(m))$</li><li><strong>空间复杂度</strong>： $O(m)$，每个点只需维护少量数据，即簇标号和每个点是核心点、边界点还是噪音点的标示。</li></ul><h3 id="4-选择-DBSCAN-参数"><a href="#4-选择-DBSCAN-参数" class="headerlink" title="4. 选择 DBSCAN 参数"></a>4. 选择 DBSCAN 参数</h3><ul><li><p><strong>$k-$距离</strong>：如何选择 $Eps$ 和 $MinPts$ 参数，基本的方法是观察点到它的第 $k$ 个最近邻的距离。</p><p>考虑下，如何 $k$ 不大于簇个数的话， $k-距离$ 相对较小，反之对于不咋簇中的点（噪音点或异常值）则$k距离$较大。因此对于参数的选取我们可以利用这点进行作图：先选取一个 $k$ (一般为4)，计算所有点的$k-距离$，并递增排序，画出曲线图，则我们会看到$k-距离的变化$，并依照此图选择出合适的 $MinPts$ 参数，即对应拐点的位置。</p></li></ul><h3 id="5-优点和缺点"><a href="#5-优点和缺点" class="headerlink" title="5. 优点和缺点"></a>5. 优点和缺点</h3><ul><li>对噪声不敏感，而且能处理任意形状和大小的簇， DBSCAN 可以发现使用 K 均值不能发现的许多簇。</li><li>当簇的密度变化太大时， DBSCAN 就会有麻烦。 对于高维数据也会有问题，因为对于这样的数据，密度定义更困难。最后，当邻近计算需要计算所有的点对邻近度时（对于高维数据，常常如此），DBSCAN 的开销可能是很大的。</li></ul><h2 id="五、簇评估"><a href="#五、簇评估" class="headerlink" title="五、簇评估"></a>五、簇评估</h2><h3 id="1-簇评估的基本目标和度量"><a href="#1-簇评估的基本目标和度量" class="headerlink" title="1. 簇评估的基本目标和度量"></a>1. 簇评估的基本目标和度量</h3><p>簇评估的一些重要问题包含：</p><ol><li>确定数据集的<strong>聚类趋势</strong>，即识别数据中是否实际存在非随机结构</li><li>确定正确的簇个数</li><li>不引用附加的信息，评估聚类分析结果对数据的拟合情况</li><li>将聚类分析结果与已知客观结果相比较</li><li>比较两个簇集，确定哪个更好</li></ol><p>用于评估簇的各方面的评估度量或指标一般分成如下三类：</p><ul><li><strong>非监督的</strong>：非监督度量通常称为<strong>内部指标</strong>，仅使用出现在数据集中的信息。如 SSE <ul><li><em>凝聚性</em>：度量簇中对象的密切关系，越大越好</li><li><em>分离性</em>：度量簇与簇之间的分离度，也是越大越好</li></ul></li><li><strong>监督的</strong>：例如监督指标的 熵，它度量簇表好与外部提供标号的匹配程度。通常称为<strong>外部指标</strong></li><li><strong>相对的</strong>：不是一种单独的簇评估度量类型，而是度量的一种具体使用。如使用 SSE 和 熵 对两个 K-means 聚类进行比较。</li></ul><h3 id="2-划分聚类的非监督簇评估"><a href="#2-划分聚类的非监督簇评估" class="headerlink" title="2.划分聚类的非监督簇评估"></a>2.划分聚类的非监督簇评估</h3><h4 id="2-1-凝聚的和分离度"><a href="#2-1-凝聚的和分离度" class="headerlink" title="2.1 凝聚的和分离度"></a>2.1 凝聚的和分离度</h4><p>通常，将 K 个簇的集合的总体有效性表示为个体簇有效性的加权和：</p><p>$$overall_validity=∑^K_{i=1}w_i*validity(C_i)$$</p><p>其中，$validity$函数可以是凝聚度、分离度，或者这些量的组合。权值将因簇有效性的度量而异。在某些情况下，全职可以简单地取 1 或者簇的大小；而在其他情况下，他们反应更复杂的性质。如凝聚度的平方根。如果有效性函数是凝聚度，则值越高越好。如果是分离度，则簇与簇之间越大越好；簇中点越小越好。</p><p>凝聚度和分离度是分两种情况下考虑的，一个是基于图的观点。</p><p><strong>基于图的观点：</strong></p><p>该情况下度量的取值需要簇中所有的点加入运算求出平均距离或者总距离来对凝聚程度和分离程度作出度量。（一个簇的话是计算簇内两两之间距离，两个簇的话计算其中一个簇内每个点到另一个簇内所有点的距离，因此这种情况下计算量往往比较大）<br>$$基于图的凝聚度SSE=\frac 1{2m}∑_{x\in C_i}∑_{y\in C_i}dist(x,y)^2$$<br>$$基于原型的凝聚度SSE= ∑_{x\in C_i}dist(c_i,x)^2$$<br><strong>基于原型的观点：</strong></p><p>这个比较好理解，就是取每个簇的质心（或中心点）作为该簇的代表。簇内凝聚度计算的是簇内每个点到质心的距离；分离性度量的是每个簇质心到总体质心的距离或者每个簇两两之间质心的距离。</p><p>$$总分离度 SSB=∑_{i=1}^Km_idist(c_i,c)^2=\frac 1{2K}∑^K_{i=1}∑^K_{j=1}\frac mKdist(c_i,c_j)^2$$<br>其中 $c_i$ 是第 $i$ 个簇的均值，$c$ 是总均值。总 $SSB$ 越高，簇之间的分离性越好。</p><p><strong>凝聚度和分离度之间的关系：</strong></p><p>可以证明总 $SSE$ 和总 $SSB$ 之和是一个常数，它等于总平方和（$TSS$，即每个点到总体数据均值距离的平方和）：</p><p>$$TSS=SSE+SSB$$<br>因此，<strong>最小化SSW(凝聚度)等价于最大化SSB（分离度）</strong>。</p><h4 id="2-2-轮廓系数"><a href="#2-2-轮廓系数" class="headerlink" title="2.2 轮廓系数"></a>2.2 轮廓系数</h4><p>流行的轮廓系数方法结合了凝聚度和分离度，该方法分三部分组成：</p><blockquote><p><1>    对于第 $i$ 个对象，计算它到簇中所有其他对象的平均距离，记作$a_i$。</1></p><p><2>    对于第 $i$ 个对象和不包含该对象的任意簇，计算该对象到给定簇中所有对象的平均距离。关于所有的簇，找出最小值，记作 $b_i$。</2></p><p><3>    对于第 $i$ 个对象，轮廓系数是 $s_i=\frac {(b_i-a_i)}{max(a_i,b_i)}$。</3></p></blockquote><p>轮廓系数取值在 -1 到 1 之间变化，但我们不希望取负值因为负值代表着簇中平均距离大于到其他簇的最小平均距离是不合理的。我们希望的是 $b_i&gt;a_i即上面的max(a_i,b_i)=b_i$，并且 $a_i$ 越接近 0 越好，因为当 $a_i=0$ 时轮廓系数取其最大值1。</p><p>我们可以简单地取簇中点的轮廓系数取平均值，计算簇的平均轮廓系数。通过计算所有点的平均轮廓系数，可以得到聚类优良性的总度量。</p><h4 id="2-3-邻近度矩阵"><a href="#2-3-邻近度矩阵" class="headerlink" title="2.3 邻近度矩阵"></a>2.3 邻近度矩阵</h4><p>数据挖掘导论参考<p337></p337></p><h3 id="3-层次聚类的非监督度量"><a href="#3-层次聚类的非监督度量" class="headerlink" title="3. 层次聚类的非监督度量"></a>3. 层次聚类的非监督度量</h3><p><strong>共性分类距离</strong>：凝聚层次聚类技术首次将对象放在同一个簇时的邻近度。即，如果在凝聚层次聚类进行的某个时刻，两个合并的簇之间的最小距离时 0.1，则其中一个簇中的所有点关于另一个簇中各点的共性分类距离都是 0.1。</p><p><strong>共性相关系数</strong>：是该矩阵与原来的相异度矩阵的项之间的相关度，是（特定类型的）层次聚类对数据拟合程度的标准度量。该度量最常见的应用是评估对于特定的数据类型，那种类型的层次聚类最好。</p><p><strong>霍普金斯(Hopkins)统计量</strong></p><p>对于该方法度量的是<strong>聚类趋势</strong>。我们随机产生 p 个随机分布在数据空间上的点，并且也抽取 p 个世纪数据点。 对于这两个点集没我们找出每个点到原数据集的最近邻距离。 设 $u_i$是人工产生的点的最近邻距离，$w_i$是样本点到源数据集的最近邻距离。则 Hopkins 统计量：<br>$$H=\frac {∑^p_{i=1}w_i}{∑^p_{i=1}u_i+∑^p_{i=1}w_i} $$</p><p>如果随机产生的点与样本点具有大致相同的最近邻距离，则 $H$ 将在 0.5 左右。$H$ 越接近 0 或者 1 分别表明数据是高度聚类的和数据在数据空间是有规律分布的。</p><p><strong>簇有效性面向分类的度量</strong></p><ul><li><code>熵</code>：每个簇由单个累的对象组成的程度。对于每个簇，首先计算数据的类分布，即对于簇 $i$，计算成员属于类 $j$ 的概率 $P_{ij}=m_{ij}/m_i$，其中$m_i$是簇 $i$ 中对象的个数，而 $m_{ij}$是簇 $i$ 中类 $j$ 的对象个数。使用类分布，用标准公式 $e_i=-∑^L_{j-1}P_{ij}log_2P_{ij}$ 计算每个簇的熵。 其中$L$是类的个数。 簇集合的总熵用每个簇的熵的加权和计算，即$e=∑^K_{i=1}\frac {m_i}{m}e_i$，其中$K$是簇的个数，而$m$是数据点的总数。</li><li><code>纯度</code>：簇包含单个累的对象的另一种度量程序。使用前面的属于，簇 $i$ 的纯度是 $P_i=\max_j P_{ij}$，而聚类的总纯度是$purity=∑^K_{i=1}=\frac {m_i}mPi$</li><li><code>精度</code>：簇中一个特定的对象所占的比例。簇$i$关于类$j$的精度是$precision(i,j)=P_{ij}$</li><li><code>召回率</code>：簇包含一个特地昂累的所有对象的程度。簇$i$关于类的召回率是$recall(i,j)=m_{ij}/m_j$，其中$m_j$是类$j$的对象个数。</li><li><strong>F度量</strong>：精度和召回率的组合，度量在多大程度上，簇只包含一个特定类型累的对象和包含该累的所有对象。簇 $i$关于类$j$的$F$度量是$F(i,j)=2 \frac{精度*召回率}{精度+召回率}$</li></ul><p>《未完待续…》</p><h3 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h3><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;hr&gt;
&lt;p&gt;本文内容包含：&lt;br&gt;    一、基本概念&lt;br&gt;    二、K-means&lt;br&gt;    三、凝聚层次聚类&lt;br&gt;    四、DBSCAN密度聚类&lt;br&gt;    五、簇评估&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;一、基本概念&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="聚类" scheme="http://www.ihoge.cn/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>深度学习——Tensorflow（1）</title>
    <link href="http://www.ihoge.cn/2018/tensorflow1.html"/>
    <id>http://www.ihoge.cn/2018/tensorflow1.html</id>
    <published>2018-05-26T04:20:21.000Z</published>
    <updated>2018-05-28T07:45:21.373Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、Tensorflow简单示例"><a href="#一、Tensorflow简单示例" class="headerlink" title="一、Tensorflow简单示例"></a>一、Tensorflow简单示例</h2><h3 id="1-基本运算"><a href="#1-基本运算" class="headerlink" title="1. 基本运算"></a>1. 基本运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义一个常量</span></span><br><span class="line">a = tf.constant([<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line"><span class="comment">#定义一个变量</span></span><br><span class="line">x = tf.Variable([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个加法op</span></span><br><span class="line">add = tf.add(a,x)</span><br><span class="line"><span class="comment">#定义一个减法</span></span><br><span class="line">sub = tf.subtract(a,x)</span><br><span class="line"><span class="comment">#定义一个乘法op</span></span><br><span class="line">mul = tf.multiply(a,x)</span><br><span class="line"><span class="comment">#定义初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment">#定义多个操作</span></span><br><span class="line">add2 = tf.add(a,add)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(<span class="string">"加法："</span>,sess.run(add)) <span class="comment">#执行加法</span></span><br><span class="line">    print(<span class="string">"减法："</span>,sess.run(sub)) <span class="comment">#执行减法</span></span><br><span class="line">    print(<span class="string">"乘法："</span>,sess.run(mul)) <span class="comment">#执行乘法</span></span><br><span class="line">    <span class="comment">#同时执行乘法op和加法op</span></span><br><span class="line">    result = sess.run([add,add2,sub,mul])</span><br><span class="line">    print(<span class="string">"执行多个："</span>,result)</span><br></pre></td></tr></table></figure><h3 id="2-使用占位符"><a href="#2-使用占位符" class="headerlink" title="2. 使用占位符"></a>2. 使用占位符</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Feed：先定义占位符，等需要的时候再传入数据</span></span><br><span class="line"><span class="comment">#创建占位符</span></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"><span class="comment">#定义乘法op</span></span><br><span class="line">output = tf.multiply(input1,input2)</span><br><span class="line">add = tf.add(input1,input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#feed的数据以字典的形式传入</span></span><br><span class="line">    print(sess.run(add, feed_dict=&#123;input1:[<span class="number">8.</span>],input2:[<span class="number">2.</span>]&#125;))</span><br></pre></td></tr></table></figure><h2 id="二、Tensorflow简单回归模型"><a href="#二、Tensorflow简单回归模型" class="headerlink" title="二、Tensorflow简单回归模型"></a>二、Tensorflow简单回归模型</h2><h3 id="1-最简单的线性回归模型"><a href="#1-最简单的线性回归模型" class="headerlink" title="1. 最简单的线性回归模型"></a>1. 最简单的线性回归模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用numpy生成100个随机点</span></span><br><span class="line"><span class="comment">#样本点</span></span><br><span class="line">x_data = np.random.rand(<span class="number">100</span>)</span><br><span class="line">y_data = x_data*<span class="number">0.1</span> + <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#构造一个线性模型</span></span><br><span class="line">d = tf.Variable(<span class="number">1.1</span>)</span><br><span class="line">k = tf.Variable(<span class="number">0.5</span>)</span><br><span class="line">y = k*x_data + d</span><br><span class="line"></span><br><span class="line"><span class="comment">#二次代价函数&lt;均方差&gt;</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y_data,y)</span><br><span class="line"><span class="comment">#定义一个梯度下降法来进行训练的优化器</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.3</span>)</span><br><span class="line"><span class="comment">#最小化代价函数</span></span><br><span class="line">train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train)</span><br><span class="line">        <span class="keyword">if</span> step%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            print(step,sess.run([k,d]))</span><br></pre></td></tr></table></figure><h3 id="2-非线性回归的问题"><a href="#2-非线性回归的问题" class="headerlink" title="2. 非线性回归的问题"></a>2. 非线性回归的问题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用numpy生成200个随机点</span></span><br><span class="line">x_data = np.linspace(<span class="number">-0.5</span>,<span class="number">0.5</span>,<span class="number">200</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.015</span>,x_data.shape)</span><br><span class="line">y_data = np.square(x_data) + noise</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义两个placeholder，列数为1，行数未知</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络结构：1-20-1，一个输入一个输出一个隐藏层包含20个神经元</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络中间层</span></span><br><span class="line">Weights_L1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">20</span>])) <span class="comment"># 初始化1行20列权值</span></span><br><span class="line">biases_L1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">20</span>])) <span class="comment"># 初始化1行20列偏置</span></span><br><span class="line">Wx_plus_b_L1 = tf.matmul(x,Weights_L1) + biases_L1 <span class="comment"># 计算神经元信号</span></span><br><span class="line">L1 = tf.nn.tanh(Wx_plus_b_L1) <span class="comment"># 使用激活函数计算神经元输出信号</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义神经网络输出层</span></span><br><span class="line">Weights_L2 = tf.Variable(tf.random_normal([<span class="number">20</span>,<span class="number">1</span>]))</span><br><span class="line">biases_L2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">Wx_plus_b_L2 = tf.matmul(L1,Weights_L2) + biases_L2</span><br><span class="line">prediction = tf.nn.tanh(Wx_plus_b_L2)</span><br><span class="line"><span class="comment"># prediction = Wx_plus_b_L2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#二次代价函数</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line"><span class="comment">#使用梯度下降法最小化代价函数训练</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#变量初始化</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        sess.run(train_step,feed_dict=&#123;x:x_data,y:y_data&#125;)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="comment">#获得预测值</span></span><br><span class="line">    prediction_value = sess.run(prediction,feed_dict=&#123;x:x_data&#125;)</span><br><span class="line">    <span class="comment">#画图</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.scatter(x_data,y_data)</span><br><span class="line">    plt.plot(x_data,prediction_value,<span class="string">'r-'</span>,lw=<span class="number">5</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><pre><code>这里思考下中间层和输出层的激活函数的选取问题。1. 中间层和输出层的激活函数均采用tanh函数，当迭代1000次时，数据拟合效果良好；输出层激活函数换成恒等函数时，效果会更好一点。2. 这里使用sigmoid函数或者softmax函数，当迭代1000次时，无法拟合。事实证明在这个数据集里sigmoid函数和softmax函数均不能作为输出层的激活函数。当输出层激活函数为softmax时预测值恒为1这个很好理解；同理sigmoid此类函数收到输出值域的限制，在该数据里是无法用来作为输出激活函数的。3. 经过有限次的测试发现，很对该数据情况下输出层的激活函数可以使用tanh、softsign和恒等函数；其中恒等激活函数表现最好（个人考虑是因为该数据非常简单）。4. 经过有限次的测试发现，sigmoid、softmax、softsign、tanh均可作为该数据情况下的中间层激活函数（恒等函数除外）。其中tanh和softsign拟合的最快但softsign效果不好；sigmoid和softmax函数拟合较慢。随着迭代次数增加到20000次，最终都能很好地拟合数据。5. sigmoid作为激活函数对神经炎要求的数量一般情况下要比tanh高。</code></pre><h2 id="三、Tensorflow分类模型"><a href="#三、Tensorflow分类模型" class="headerlink" title="三、Tensorflow分类模型"></a>三、Tensorflow分类模型</h2><p>本节用到Tensorflow自带的 mnist 数据集。这里使用独热编码将多元回归的问题转换成10个数值的二元分类问题。使用softmax作为输出层激活函数的意义在于将输出的概率数组归一化并凸显概率最大的值。当然这里也可以使用sigmoid或其他作为输出层激活函数。</p><h3 id="1-简单的MNIST数据集分类"><a href="#1-简单的MNIST数据集分类" class="headerlink" title="1. 简单的MNIST数据集分类"></a>1. 简单的MNIST数据集分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义两个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个简单的神经网络</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(x,W)+b)</span><br><span class="line"></span><br><span class="line"><span class="comment">#二次代价函数</span></span><br><span class="line">loss = tf.losses.mean_squared_error(y,prediction)</span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction) </span><br><span class="line"><span class="comment">#使用梯度下降法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.2</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中。</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率。</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment">#epoch：所有数据训练一次，就是一个epoch周期</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">        <span class="comment">#batch：一般为32，64个数据</span></span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        </span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        print(<span class="string">"Iter "</span> + str(epoch) + <span class="string">",Testing Accuracy "</span> + str(acc))</span><br></pre></td></tr></table></figure><h3 id="2-过拟合解决及梯度下降优化器"><a href="#2-过拟合解决及梯度下降优化器" class="headerlink" title="2. 过拟合解决及梯度下降优化器"></a>2. 过拟合解决及梯度下降优化器</h3><p>Dropout采用随机的方式“做空”神经元的权重，L1正则化采用的是“做空”贡献非常小的神经元权重，L2正则化是消弱每个神经元的权重让每个都有少许的贡献。<br>在神经网络中它们之间也可以结合使用，dropout应用较多些。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义三个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 784-1000-500-10</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,<span class="number">1000</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">1000</span>])+<span class="number">0.1</span>)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob) </span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([<span class="number">1000</span>,<span class="number">500</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">500</span>])+<span class="number">0.1</span>)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob) </span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([<span class="number">500</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#同样这里也可以使用正则项</span></span><br><span class="line"><span class="comment">#l2_loss = tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + #tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2) + tf.nn.l2_loss(W3) + #tf.nn.l2_loss(b3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"></span><br><span class="line"><span class="comment">#正则后的交叉熵代价函数</span></span><br><span class="line"><span class="comment">#loss = tf.losses.softmax_cross_entropy(y,prediction) + #0.0005*l2_loss #这里0.0005为学习率</span></span><br><span class="line"><span class="comment">#使用梯度下降法</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_step = tf.train.AdamOptimizer(0.001).minimize(loss)# 使用优化器的梯度下降，同时还有其他很多种基于梯度下降的优化。这里的学习率取值比传统的梯度下降法要小</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">31</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">0.5</span>&#125;) <span class="comment">#这里keep_prob:0.5 表示保留50%的神经元，这里把另它为1的时候保留所有神经元测试结果准确率提高了2个百分点，同时相对应的计算量也增大了</span></span><br><span class="line">        </span><br><span class="line">        test_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        train_acc = sess.run(accuracy,feed_dict=&#123;x:mnist.train.images,y:mnist.train.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">"Iter "</span> + str(epoch) + <span class="string">",Testing Accuracy "</span> + str(test_acc) +<span class="string">",Training Accuracy "</span> + str(train_acc))</span><br></pre></td></tr></table></figure><h3 id="3-神经网络优化"><a href="#3-神经网络优化" class="headerlink" title="3. 神经网络优化"></a>3. 神经网络优化</h3><p>这里的优化方式是不断减小学习率，使得在极小值附近迭代速度放缓，解决因学习率过大反复震荡无法拟合的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义三个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line">keep_prob=tf.placeholder(tf.float32)</span><br><span class="line">lr = tf.Variable(<span class="number">0.001</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 784-500-300-10</span></span><br><span class="line"><span class="comment">#创建一个神经网络</span></span><br><span class="line">W1 = tf.Variable(tf.truncated_normal([<span class="number">784</span>,<span class="number">500</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">500</span>])+<span class="number">0.1</span>)</span><br><span class="line">L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)</span><br><span class="line">L1_drop = tf.nn.dropout(L1,keep_prob) </span><br><span class="line"></span><br><span class="line">W2 = tf.Variable(tf.truncated_normal([<span class="number">500</span>,<span class="number">300</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">300</span>])+<span class="number">0.1</span>)</span><br><span class="line">L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)</span><br><span class="line">L2_drop = tf.nn.dropout(L2,keep_prob) </span><br><span class="line"></span><br><span class="line">W3 = tf.Variable(tf.truncated_normal([<span class="number">300</span>,<span class="number">10</span>],stddev=<span class="number">0.1</span>))</span><br><span class="line">b3 = tf.Variable(tf.zeros([<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(L2_drop,W3)+b3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(lr).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">        sess.run(tf.assign(lr, <span class="number">0.001</span> * (<span class="number">0.95</span> ** epoch)))</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        </span><br><span class="line">        learning_rate = sess.run(lr)</span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">", Testing Accuracy= "</span> + str(acc) + <span class="string">", Learning Rate= "</span> + str(learning_rate))</span><br></pre></td></tr></table></figure><h2 id="四、CNN卷积神经网络"><a href="#四、CNN卷积神经网络" class="headerlink" title="四、CNN卷积神经网络"></a>四、CNN卷积神经网络</h2><p>以上的案例采用的都是BP神经网络。考虑一张图片像素为100*100，则需要一万个输入神经元，若隐藏层也有一万个神经元则需要训练一亿个参数，这不仅需要更多计算昂还需要大量额训练样本用来“求解”。因此下面我们考虑用卷积神经网络来解决这个问题。</p><ul><li>CNN通过<strong>局部感受野</strong>和<strong>权值共享</strong>减少了神经网络需要训练的参数（权值）的个数。</li><li><p>卷积核/滤波器<br><img src="media/15273889821782.jpg" alt=""></p></li><li><p>卷积Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li><li><p>池化</p><ul><li>max-pooling 提取卷积后特征的最大值也就是最重要的特征，进一步压缩参数</li><li>mean-pooling </li><li>随机-pooling<br><img src="media/15273890234010.jpg" alt=""></li></ul></li><li><p>池化Padding</p><ul><li>SAME PADDING</li><li>VALID PADDING</li></ul></li></ul><p>下面看一个 CNN 卷积神经网络用于 MINIST 数据的分类问题。在CPU上运行比较耗时，16G内存的Mac-Pro大概两三分钟一个周期。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个批次的大小</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#计算一共有多少个批次</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size</span><br><span class="line"><span class="comment">#定义两个placeholder</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])<span class="comment">#28*28</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化权值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape,stddev=<span class="number">0.1</span>)<span class="comment">#生成一个截断的正态分布</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化偏置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>,shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">#卷积层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x,W)</span>:</span></span><br><span class="line">    <span class="comment">#x input tensor of shape `[batch, in_height, in_width, in_channels]`</span></span><br><span class="line">    <span class="comment">#W filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]</span></span><br><span class="line">    <span class="comment">#`strides[0] = strides[3] = 1`. strides[1]代表x方向的步长，strides[2]代表y方向的步长</span></span><br><span class="line">    <span class="comment">#padding: A `string` from: `"SAME", "VALID"`</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x,W,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#池化层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment">#ksize [1,x,y,1]</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x,ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#改变x的格式转为4D的格式[batch, in_height, in_width, in_channels]`</span></span><br><span class="line">x_image = tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第一个卷积层的权值和偏置</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])<span class="comment">#5*5的采样窗口，32个卷积核从1个平面抽取特征</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])<span class="comment">#每一个卷积核一个偏置值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把x_image和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)<span class="comment">#进行max-pooling</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第二个卷积层的权值和偏置</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])<span class="comment">#5*5的采样窗口，64个卷积核从32个平面抽取特征</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])<span class="comment">#每一个卷积核一个偏置值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把h_pool1和权值向量进行卷积，再加上偏置值，然后应用于relu激活函数</span></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)<span class="comment">#进行max-pooling</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#28*28的图片第一次卷积后还是28*28，第一次池化后变为14*14</span></span><br><span class="line"><span class="comment">#第二次卷积后为14*14，第二次池化后变为了7*7</span></span><br><span class="line"><span class="comment">#进过上面操作后得到64张7*7的平面</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第一个全连接层的权值</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>,<span class="number">1024</span>])<span class="comment">#上一层有7*7*64个神经元，全连接层有1024个神经元</span></span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])<span class="comment">#1024个节点</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把池化层2的输出扁平化为1维</span></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2,[<span class="number">-1</span>,<span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line"><span class="comment">#求第一个全连接层的输出</span></span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#keep_prob用来表示神经元的输出概率</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化第二个全连接层</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>,<span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算输出</span></span><br><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#交叉熵代价函数</span></span><br><span class="line">cross_entropy = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"><span class="comment">#使用AdamOptimizer进行优化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line"><span class="comment">#结果存放在一个布尔列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(prediction,<span class="number">1</span>),tf.argmax(y,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">21</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys,keep_prob:<span class="number">0.7</span>&#125;)</span><br><span class="line"></span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels,keep_prob:<span class="number">1.0</span>&#125;)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">", Testing Accuracy= "</span> + str(acc))</span><br></pre></td></tr></table></figure><h2 id="五、RNN递归神经网络"><a href="#五、RNN递归神经网络" class="headerlink" title="五、RNN递归神经网络"></a>五、RNN递归神经网络</h2><p>RNN 一个重要的用法就是通过递归的调用神经元，利用之前的信息来决策当前的问题。如下图所示：</p><p><img src="media/15273903144616.jpg" alt=""></p><p>RNN 递归的次数由多少批次（上图中的$t$来确定）。本质上还是类似 BP 神经网络，因此随着不断的迭代会存在梯度衰减的问题。 RNN 发明出来的时候还没有出现 Relu 函数，因此针对梯度衰减的问题推出了一种优化的模型 ：长段时间记忆模型（LSTM）。<br><img src="media/15273909839848.jpg" alt=""></p><h2 id="六、LSTM长段时间记忆"><a href="#六、LSTM长段时间记忆" class="headerlink" title="六、LSTM长段时间记忆"></a>六、LSTM长段时间记忆</h2><p>LSTM 原理是通过增加输入控制（Input Gate）、遗忘控制（Forget Gate）和输出控制（Output Gate）来快速遗忘不重要的信息持久化重要的信息，具体控制方式如下图所示：<br><img src="media/15273902768081.jpg" alt=""><br><img src="media/15273917640640.jpg" alt=""><br><img src="media/15273917751199.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>,one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入图片是28*28</span></span><br><span class="line">n_inputs = <span class="number">28</span> <span class="comment">#输入一行，一行有28个数据</span></span><br><span class="line">max_time = <span class="number">28</span> <span class="comment">#一共28行</span></span><br><span class="line">lstm_size = <span class="number">100</span> <span class="comment">#隐层单元</span></span><br><span class="line">n_classes = <span class="number">10</span> <span class="comment"># 10个分类</span></span><br><span class="line">batch_size = <span class="number">50</span> <span class="comment">#每批次50个样本</span></span><br><span class="line">n_batch = mnist.train.num_examples // batch_size <span class="comment">#计算一共有多少个批次</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这里的none表示第一个维度可以是任意的长度</span></span><br><span class="line">x = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">784</span>])</span><br><span class="line"><span class="comment">#正确的标签</span></span><br><span class="line">y = tf.placeholder(tf.float32,[<span class="keyword">None</span>,<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化权值</span></span><br><span class="line">weights = tf.Variable(tf.truncated_normal([lstm_size, n_classes], stddev=<span class="number">0.1</span>))</span><br><span class="line"><span class="comment">#初始化偏置值</span></span><br><span class="line">biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[n_classes]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义RNN网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RNN</span><span class="params">(X,weights,biases)</span>:</span></span><br><span class="line">    <span class="comment"># inputs=[batch_size, max_time, n_inputs]</span></span><br><span class="line">    inputs = tf.reshape(X,[<span class="number">-1</span>,max_time,n_inputs])</span><br><span class="line">    <span class="comment">#定义LSTM</span></span><br><span class="line">    lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_size)</span><br><span class="line"><span class="comment">#    final_state[state, batch_size, cell.state_size]</span></span><br><span class="line"><span class="comment">#    final_state[0]是cell state</span></span><br><span class="line"><span class="comment">#    final_state[1]是hidden_state</span></span><br><span class="line"><span class="comment">#    outputs: The RNN output `Tensor`.</span></span><br><span class="line"><span class="comment">#       If time_major == False (default), this will be a `Tensor` shaped:</span></span><br><span class="line"><span class="comment">#         `[batch_size, max_time, cell.output_size]`.</span></span><br><span class="line"><span class="comment">#       If time_major == True, this will be a `Tensor` shaped:</span></span><br><span class="line"><span class="comment">#         `[max_time, batch_size, cell.output_size]`.</span></span><br><span class="line">    outputs,final_state = tf.nn.dynamic_rnn(lstm_cell,inputs,dtype=tf.float32)</span><br><span class="line">    results = tf.nn.softmax(tf.matmul(final_state[<span class="number">1</span>],weights) + biases)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">#计算RNN的返回结果</span></span><br><span class="line">prediction= RNN(x, weights, biases)  </span><br><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">loss = tf.losses.softmax_cross_entropy(y,prediction)</span><br><span class="line"><span class="comment">#使用AdamOptimizer进行优化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-3</span>).minimize(loss)</span><br><span class="line"><span class="comment">#结果存放在一个布尔型列表中</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>),tf.argmax(prediction,<span class="number">1</span>))<span class="comment">#argmax返回一维张量中最大的值所在的位置</span></span><br><span class="line"><span class="comment">#求准确率</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))<span class="comment">#把correct_prediction变为float32类型</span></span><br><span class="line"><span class="comment">#初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">11</span>):</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_batch):</span><br><span class="line">            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)</span><br><span class="line">            sess.run(train_step,feed_dict=&#123;x:batch_xs,y:batch_ys&#125;)</span><br><span class="line">        </span><br><span class="line">        acc = sess.run(accuracy,feed_dict=&#123;x:mnist.test.images,y:mnist.test.labels&#125;)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Iter "</span> + str(epoch) + <span class="string">", Testing Accuracy= "</span> + str(acc))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、Tensorflow简单示例&quot;&gt;&lt;a href=&quot;#一、Tensorflow简单示例&quot; class=&quot;headerlink&quot; title=&quot;一、Tensorflow简单示例&quot;&gt;&lt;/a&gt;一、Tensorflow简单示例&lt;/h2
      
    
    </summary>
    
      <category term="深度学习" scheme="http://www.ihoge.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow" scheme="http://www.ihoge.cn/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>提升树（Boosting tree）算法总结</title>
    <link href="http://www.ihoge.cn/2018/boosting.html"/>
    <id>http://www.ihoge.cn/2018/boosting.html</id>
    <published>2018-05-20T04:20:21.000Z</published>
    <updated>2018-05-28T07:52:54.331Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>本文是综合了之前的以往多个笔记汇总而成，内容包含：</p><pre><code>一、Boosting基本概念二、前向分步加法模型    1. 加法模型    2. 前向分步算法三、AdaBoost    1. 算法解析    2. 模型构建    3. 算法缺点四、二叉分类树五、回归分类树    1. 算法解析    2. 模型构建六、梯度提升树（GBDT）    1. 算法解析    2. 模型构建七、XGBoost    1. 原理详解    2. 目标函数    3. 学习过程    4. 损失函数    5. 正则化    6. 决策树的构建    7. 流程步骤    8. 优缺点八、总结    1. Boosting家族    2. AdaBoost    3. 回归提升树和AdaBoost    4. GBDT和回归提升树       5. XGBoost和GBDT    6. 参考文献</code></pre><p>提升(Boosting)是集成学习方法里的一个重要方法，其主要思想是将弱分类器组装成一个强分类器。在 PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p><p>提升树模型实际采用加法模型（即基函数的线性组合）与前向分步算法，以决策树为基函数的提升方法称为提升树（Boosting Tree）。</p><p><strong>对分类问题决策树是二叉分类树</strong>，<strong>对回归问题决策树是二叉回归树</strong>。</p><p>提升树模型可以表示为决策树的加法模型：<br>$$f_M(x)=∑^M_{i=1}T(x;\Theta _m)$$其中$T(x;\Theta _m)$表示决策树；$\Theta_m$表示决策树的参数；$M$为树的个数。</p><p>不同问题的提升树学习算法，其主要区别在于损失函数不同。<strong>平方损失函数常用于回归问题，用指数损失函数用于分类问题，以及绝对损失函数用于决策问题</strong>。</p><blockquote><p>由于树的线性组合可以很好的拟合训练数据，即使数据中的输入和输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。</p></blockquote><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>提升（Boosting）方法是一类应用广泛且非常有效的统计学习方法。</p><p><strong>它基于这样一种思想</strong>：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠顶个诸葛亮”的道理。</p><p><strong>强可学习</strong>：如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的</p><p><strong>弱可学习</strong>：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的</p><p><strong>AdaBoost算法</strong>：那么如何将弱学习算法提升为强学习算法呢？关于提升方法的研究很多，有很多算法被提出。最具代表性的是AdaBoost算法（AdaBoost algorithm）</p><p><strong>Boosting算法的两个核心问题</strong></p><ol><li><p><em>在每一轮如何改变训练数据的权值或概率分布</em></p><p> <code>通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。</code>。</p></li><li><p><em>如何将弱分类器组合成一个强分类器</em></p><p> <code>通过加法模型将弱分类器进行线性组合，比如 AdaBoost 通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。</code>。</p></li></ol><blockquote><p>AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里。<br>AdaBoost算法是损失函数为指数函数时的Boosting算法</p></blockquote><h2 id="二、前向分步加法模型（Forward-Stagewise-Additive-Modeling）"><a href="#二、前向分步加法模型（Forward-Stagewise-Additive-Modeling）" class="headerlink" title="二、前向分步加法模型（Forward Stagewise Additive Modeling）"></a>二、前向分步加法模型（Forward Stagewise Additive Modeling）</h2><h3 id="1-加法模型"><a href="#1-加法模型" class="headerlink" title="1. 加法模型"></a>1. 加法模型</h3><p>（形为$Y=I+U+T+K$的模型为加法模型）<br>$$f(x)=∑^M_{m=1}\beta _mb(x;\gamma  _m)$$ 其中，$b(x;\gamma_m)$为基函数，$\beta_m$为基函数的系数。</p><h3 id="2-前向分步算法"><a href="#2-前向分步算法" class="headerlink" title="2. 前向分步算法"></a>2. 前向分步算法</h3><p>在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$称为经验风险极小化，即损失函数极小化的问题：<br>$$min_{(\beta_m,\gamma_m)}∑^N_{i=1}L(y_i,∑^M_{m=1}\beta _mb(x_i;\gamma_m )) $$</p><p>通常这是一个复杂的优化问题。前向分布算法（forward stagwise algorithm）求解这一优化问题的思路是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式$f(x)=∑^M_{m=1}\beta _mb(x;\gamma  _m)$，那么就可以简化优化的复杂度。</p><p>具体地，每步只需优化如下损失函数:$$min_{\beta, \gamma}∑^M_{i=1}L(y_i,\beta b(x_i;\gamma))$$</p><p><strong>前向分布算法步骤</strong>如下：</p><blockquote><p><strong>输入</strong>：训练数据集$D={(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_N,y_N)}$;损失函数$L(y,f(x))$；基函数集$(\beta (x;\gamma))$;<br><strong>输出</strong>：加法模型$f(x)$  </p><p>（1）初始化$f_0(x)=0$  </p><p>（2）对于$k=1,2,…,K$<br>（a）极小化损失函数，得到$\beta_m ,\gamma_m$:<br>    $$(\beta _m,\gamma_m)=argmin_{(\beta,\gamma)}∑^N_{i=1}L(y_i,f_{m-1}(x_i)+\beta b(x_i,\gamma))$$<br>（b）更新<br>    $$f_m(x)=f_{m-1}(x)+\beta _mb(x;\gamma_m)$$    </p><p>（3）得到加法模型<br>$$f(x)=f_M(x)=∑^M_{m=1}\beta_mb(x;\gamma_m)$$<br>这样。前向分步算法将同时求解从$m=1$到$m=M$所有参数$\beta_m,\gamma_m$的优化问题简化为逐次求解$\beta_m,\gamma_m$的优化问题。<br>前向分布算法学习的是加法模型，当基函数为基本分类器是，该加法模型等价于Adaboost的最终分类器。（AdaBoost算法参数迭代公式就是由此而来）</p><p><strong>基学习器</strong>：基函数为同一类型；反之称为<strong>组件学习器</strong>或直接成个体学习器</p></blockquote><h2 id="三、AdaBoost"><a href="#三、AdaBoost" class="headerlink" title="三、AdaBoost"></a>三、AdaBoost</h2><p>AdaBoost算法是前向分步算法的特例，其模型是由基本分类器组成的加法模型，损失函数是指数函数。</p><h3 id="1-AdaBoost算法解析"><a href="#1-AdaBoost算法解析" class="headerlink" title="1. AdaBoost算法解析"></a>1. AdaBoost算法解析</h3><p> <strong>AdaBoost模型是弱分类器的线性组合：</strong><br>    $$f(x)=∑^M_{m=1}\alpha _mG_m(x)$$</p><ul><li>$M$表示该提升树共有$M$个弱分类器组成</li><li>$G_m(x)$表示第$m$个弱分类器</li><li>$\alpha_m$为第$m$个弱分类器的参数（反应该分类器的重要性）</li></ul><p>Adaboost算法在分类问题中的主要特点：通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。 AdaBoost－算法描述（伪代码）如下：</p><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in \chi ⊆R^n,y_i\in Y={-1,1}$，弱学习算法$G_m(x)$;<br><strong>输出：</strong>最终强化算法分类器$G(x)$<br>（1）初始化训练数据总和为1的权值分布：（初始权重为归一化后的均值既$\frac 1N$）<br>$$D_1=(w_{11},…,w_{1i},…w_{1N}),w_{1i}=\frac 1N, i=1,2,…N$$<br>（2）对$m=1,2,…M$：（弱分类器的个数）<br>（a）使用具有权值分布的$D_m$的训练数据集学习，得到基本分类器：(数据集$X$到{-1,1}的映射)<br>$$G_m(x):X-&gt;{-1,1}$$<br>（b）计算$Gm(x)$在训练数据集上的分类误差率：（公式不够简洁明了，其实总结下来非常好理解：误差率$e_m$=误分类样本的权值之和）<br>$$e_m=∑^N_{i=1}P(G_m(x_i)≠y_i)=∑^N_{i=1}w_{mi}I(G_m(x_i)≠y_i)$$<br>我们来考虑下误差$e_m$的取值空间：由于训练集权制之和为1，因此误差$0≤e_m≤1$。但是这样还不够。因为我们在选择分裂阈值的时候会选择一个最优或局部最优的取值来分裂，且当$e_m=0.5$是表明该分裂阈值对预测无贡献。因此最终得到的$e_m$的实际取值应小于$e_m≤0.5$。<br>所以最终：$0≤e_m≤0.5$，且每次迭代误差$e_m$递减。这点对下面的参数理解很重要。<br>（c）计算$G_m(x)$的系数:(这里对数为自然对数)<br>$$\alpha_m=\frac 12log\frac{1-e_m}{e_m} $$<br>那么问题来了，为什么要用这个公式来计算更新每个基分类器的参数？我们先画个图出来观察下这个函数。（其中y轴为$\alpha _m$，x轴为误差$e_m$）</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266077461675.jpg" alt=""></p><p>由（2-b）我们得到误差$e_m$的取值范围为$0≤e_m&lt;0.5$，结合该图可以可知$0&lt;\alpha_m&lt;+\infty $。<br>另外可以发现，通过该函数的转换，弱分类器$G_m(x)$的误差的越小，参数$\alpha_m$越大。即实现了<strong><code>给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</code></strong><br>（d）更新训练数据集的权值分布：（该权值决定数据集的重要性，并让误差的计算变得简单）<br>$$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…w_{m+1,N})$$<br>$$w_{m+1,i}=\frac {w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x-i)),i=1,2,…N$$<br>这里$y_i={-1,1} $为真实值，$G_m(x_i)={-1,1}$为预测值。当预测正确时$y_iG_m(x_i)$为1，反之为-1。<br>令$\delta_{m_i}=\alpha_my_iG_m(x_i)$，$\theta_{mi}=\frac {w_{mi}}{Z_m}$(把它看作一个用于归一化权值的加权平均常数)。权重$w_{m+1,i}$的更新函数可以简化为$$w_{m+1,i}=\theta_{mi}exp(\delta <em>{mi}),i=1,2,…N$$画出$y=w</em>{m+1,i}=exp(\delta_{mi})$的图形来看一下：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266159564936.jpg" alt=""></p><p>由于$0&lt;\alpha_m&lt;1$，所以$-1&lt;\delta_{m,i }&lt;1$。且<strong>使得预测错误的数据集样本点更新后的权重变大，预测正确的权值变小，然后对所有权值进行归一化</strong>。这就是该函数实现的作用。(图中y=1是当$\alpha$无限接近于0时的情况：解释为，当$\alpha_m$权值越大，权重$w_{m+1,i}$更新改变的效果越明显。)<br>这里，$Z_m$是规范化因子，目的是使各数据集的权重进行归一化。理解为$Z_m$=更新后的各数据集权重之和。<br>$$Z_m=∑^N_{i=1}w_{mi}exp(-\alpha_my_iG_m(x_i))$$<br>（3）构建基本分类器的新型组合$f(x)=∑^M_{m=1}\alpha_mG_m(x)$，即：<br>$$G(x)=sign(f(x))=sign(∑^M_{m=1}\alpha_mG_m(x))$$<br>函数$sign()$的意义是将正数判别为1，负数判别为-1，最终达到分类的目的。如图：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266164023757.jpg" alt=""></p></blockquote><h3 id="2-AdaBoost构建步骤"><a href="#2-AdaBoost构建步骤" class="headerlink" title="2. AdaBoost构建步骤"></a>2. AdaBoost构建步骤</h3><blockquote><p>上面解释了AdaBoost算法的具体内容。这里写出它的分布实现步骤再对上文算法加深下理解：<br>（1）假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$。<br>（2）AdaBoost反复学习基本分类器，在每一轮$m＝1,2,…,M$顺次地执行下列操作：<br>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$<br>（b）计算基本分类器$G_m(x)$再加权训练数据集上的分类误差率（即误分类样本的权值之和。这里要注意$w_{mi}$表示第$m$轮中第$i$个实例的权值，且权值之和为1，即$∑^N_{i=1}w_{mi}=1$）：<br>$$e_m=P(G_m(x_i)≠y_i)=∑_{G_m(x_i)≠y_i}w_{mi}$$<br>（c）计算基本分类器$G_m (x)$的系数$\alpha_m$。$alpha_m$表示$G_m(x)$在最终分类器中的重要性。由上面（2-c）可知，<strong>当$e_m≤1/2$时，$alpha_m≥0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的分类器在最终分类器中的作用越大。</strong><br>（d）更新训练数据的权值分布为下一轮作准备。式（2-d）的权重更新函数可以写成：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266248077288.jpg" alt="">    </p><p> 由此可知，被基本分类器$G_m (x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大$e^{(2\alpha_m)}=\frac{e_m}{1-e_m} $倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。<br>（3）线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$ 表示了基本分类器$G_m (x)$的重要性，这里，所有$\alpha_m$ 之和并不为1。$f(x)$的符号决定实例x的类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p></blockquote><h3 id="3-AdaBoost算法缺点"><a href="#3-AdaBoost算法缺点" class="headerlink" title="3. AdaBoost算法缺点"></a>3. AdaBoost算法缺点</h3><p><strong>1. 常规AdaBoost算法只能处理二分类问题</strong></p><p>MultiBoost工具的侧重点不同于XGBoost，是Adaboost算法的多分类版本实现，更偏向于解决multi-class / multi-label / multi-task的分类问题。</p><p><strong>2. 对异常值敏感</strong></p><p>指数损失存在的一个问题是不断增加误分类样本的权重（指数上升）。如果数据样本是异常点（outlier），会极大的干扰后面基本分类器学习效果；</p><p><strong>3. 模型无法用于概率估计</strong></p><p>对于取值为$\hat y \in {-1,1}$的随机变量说，$e^{-\hat yf}$不是任何概率密度函数的对数形式，模型$f(x)$的结果无法用概率解释。<br>MLAPP中的原话：$e^{-\hat y f}$is not the logarithm of any pmf for binary variables $\hat y \in {-1,1}$; consequently we cannot recover probability estimate from $f(x)$.”</p><h2 id="四、二叉分类提升树（如AdaBoost）"><a href="#四、二叉分类提升树（如AdaBoost）" class="headerlink" title="四、二叉分类提升树（如AdaBoost）"></a>四、二叉分类提升树（如AdaBoost）</h2><p>对于二类分类问题，提升树算法只需要将AdaBoost算法例子中的基本分类器限制为二叉分类树即可，可以说<strong>此时的决策树算法时AdaBoost算法的特殊情况</strong>。</p><p><strong>二叉分类树中用基尼指数作为最优特征选择的度量标准。</strong></p><p>在实际操作中，通过遍历所有特征（如果是连续值，需做离散化）及其取值，选择基尼指数最小所对应的特征和特征值。</p><h2 id="五、二叉回归提升树"><a href="#五、二叉回归提升树" class="headerlink" title="五、二叉回归提升树"></a>五、二叉回归提升树</h2><p><strong>二叉回归树采用平方误差最小化作为特征选择和切分点选择的依据</strong>。</p><p>下面要解决的问题是：如何划分特征空间？</p><p>一个启发式的方式就是选择特征空间中第$m$个特征$f_m$和它的取值$s$,作为划分特征和划分点，然后寻找最优划分特征$f_m$和最优划分点$s$。</p><p><strong>具体操作就是遍历所有未划分过的特征集合和对应的取值（集合）</strong>求解得出另损失函数最小的参数$f_m和s$。</p><h3 id="1-回归问题提升树算法解析"><a href="#1-回归问题提升树算法解析" class="headerlink" title="1. 回归问题提升树算法解析"></a>1. 回归问题提升树算法解析</h3><p>对于二类分类问题，提升树算法只需将AdaBoost算法中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况，这里不再细述。</p><p>已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$ \chi $为输入空间，$y_i \in Y ⊆ R$，$Y$为输出空间。如果将输入空间$\chi $划分为$J$个互不相交的区域$R_1,R_2,…R_J$，并且在每个区域上确定输出的敞亮$c_j$，那么树可以表示为：$$T(x; \Theta)=∑^J_{j=1}c_jI(x \in R_j)$$</p><p>其中参数$\Theta ={(R_1,c_1),(R_3,c_2),…,(R_J,c_J)}$表示树的区域划分和各区域上的常数。$J$是回归树的复杂度即叶节点的个数。</p><blockquote><p>$f_0(x)=0$<br>$f_m(x)=f_{m-1}(x)+T(x;\Theta_m),m=1,2,…,M$<br>$f(x)=f_M(x)=∑^M_{m=1}T(x;\Theta_m)$</p><p>在前向分步算法的第$m$步，给定当前模型$f_{m-1}(x) $，需求解：</p><p>$Pred (\Theta_m)$<br>$=argmin_{(\Theta_m)}∑^N_{i=1}L(y_i,f_m(x_i))$<br>$=argmin_{(\Theta_m)}∑^N_{i=1}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m ))$<br>得到$Pred(\Theta _m)$，即第$m$棵树的参数。</p><p>当采用平方误差损失函数$L(y,f(x))=(y-f(x))^2$时，其损失变为：</p><p>$L(y,f_{m-1}(x)+T(x;\Theta_m ) )$<br>$=[y-f_{m-1}(x)-T(x;\Theta_m )]^2$<br>$=[\gamma -T(x;\Theta_m)]^2$</p><p>这里$\gamma=y-f_{m-1}(x)$<strong>是当前模型拟合数据的残差（这点很重要）</strong>。所以，对回归问题的提升算法来说，只需要简单地拟合当前模型的残差。这样算法是很简单地。</p></blockquote><h3 id="2-回归提升树构建步骤"><a href="#2-回归提升树构建步骤" class="headerlink" title="2. 回归提升树构建步骤"></a>2. 回归提升树构建步骤</h3><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：回归提升树$f(x)$;<br>（1）初始化$f_0(x)=0 $</p><p>（2）对$m=1,2,…,M$</p><p>（a）计算或更新残差<br>$$\gamma _{mi}=y_i-f_{m-1}(x_i),i=1,2,…,N$$<br>（b）拟合残差$\gamma_{mi}$学习一个回归树，得到$T(x; \Theta_m)$<br>（c）更新$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$<br>（d）重复步骤（a～c）知道满足终止条件</p><p>（3）得到回归问题提升树<br>$$f(x)=f_M(x)=∑^M_{m=1}T(x;\Theta_m)$$</p></blockquote><h2 id="六、梯度提升树（GBDT）"><a href="#六、梯度提升树（GBDT）" class="headerlink" title="六、梯度提升树（GBDT）"></a>六、梯度提升树（GBDT）</h2><h3 id="1-梯度提升树-GBDT-算法解析"><a href="#1-梯度提升树-GBDT-算法解析" class="headerlink" title="1. 梯度提升树(GBDT)算法解析"></a>1. 梯度提升树(GBDT)算法解析</h3><p><strong>Gradient boosting</strong> 就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高。之所以称为 Gradient，是因为在添加新模型时使用了梯度下降算法来最小化的损失。<br><strong>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。</strong></p><p>损失函数的负梯度为：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15267353730899.jpg" alt=""></p><p>也就是说梯度提升树（GBDT）本质上和提升回归树类似，唯一不同的是使用损失函数的负梯度在当前模型的值取近似替代回归提升树中的残差去拟合回归树。这里算法解析部分可以参考上文回归提升树。</p><h3 id="2-梯度回归树（GBDT）构建步骤"><a href="#2-梯度回归树（GBDT）构建步骤" class="headerlink" title="2. 梯度回归树（GBDT）构建步骤"></a>2. 梯度回归树（GBDT）构建步骤</h3><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：梯度提升树$\hat{f}(x) $;(因为这里用损失函数负梯度的值去近似残差，因此使用$\hat{f}(x) $更严谨些)<br>（1）初始化：这里初始化与回归提升树略有不同<br>$$f_0(x)=argmin_c∑^N_{i=1}L(y_i,c)$$</p><p>（2）对$m=1,2,…,M$<br>（a）对$i=1,2,…,M$计算损失函数在当前模型的值作为残差$\gamma_{mi}$的近似</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267354146996.jpg" alt=""></p><p>（b）对$\gamma_{mi}$拟合一个回归树，得到第$m$棵树的叶节点区域$R_{mj}$，$j=1,2,…,J$<br>（c）对$j=1,2,..,J$，计算<br>$$c_{mj}=argmin_{(c)}∑_{(x_i \in R_{mj})}$$<br>（d）更新$f_m(x)=f_{m-1}+∑^J_{j=1}c_{mj}I(x\in R_{mj})$</p><p>（3）得到回归树<br>$$\hat{f}(x)=f_M(x)=∑^M_{m=1}∑^J_{j=1}c_{mj}I(c\in R_{mj})$$</p></blockquote><p><strong>算法解释</strong>：</p><ol><li>第（1）步初始化，估计使用损失函数极小化的常数值，它是只有一个根结点的树。</li><li>第（2a）步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，就是通常所说的残差；对于一般损失函数，它就是残差的近似值。</li><li>第（2b）步估计回归树叶节点区域，以拟合残差的近似值。</li><li>第（2c）步<strong>利用线性搜索估计叶节点区域的值，是损失函数极小化</strong>。</li><li>第（2d）步更新回归树，然后输出最终模型$\hat{y}(x)$。</li></ol><h3 id="3-GBDT缺点"><a href="#3-GBDT缺点" class="headerlink" title="3. GBDT缺点"></a>3. GBDT缺点</h3><p> <strong>效率低</strong>：gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。所以就有了<strong>XGBoost</strong>。</p><h2 id="七、XGBoost"><a href="#七、XGBoost" class="headerlink" title="七、XGBoost"></a>七、XGBoost</h2><h3 id="1-XGBoost原理详解"><a href="#1-XGBoost原理详解" class="headerlink" title="1. XGBoost原理详解"></a>1. XGBoost原理详解</h3><p>前面介绍了提升树算法，其实 XGBoost 就是一种特殊的提升树算法，准确的说它是一种梯度提升决策树（GBDT ，Gradient Boosting Decision Trees）。GBDT 与前面介绍的提升树方法主要的区别就是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，来拟合一颗回归树，即：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267877818108.jpg" alt=""></p><p>XGBoost 就是对 GBDT 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。</p><p>而 XGBoost 的特点就是<strong>计算速度快</strong>，<strong>模型表现好</strong>，这两点也正是这个项目的目标。</p><h3 id="2-XGBoost的目标函数"><a href="#2-XGBoost的目标函数" class="headerlink" title="2. XGBoost的目标函数"></a>2. XGBoost的目标函数</h3><p>传统 GBDT 算法的目标函数只有损失函数这一项，而 XGBoost 在此基础上进行了改进，增加了正则项以防止模型过度复杂：</p><p>$$Obj =∑^N_{i=1}L(y_i,\hat{y}<em>i)+∑</em>{m=1}^M\Omega(f_m), f_m \in F$$</p><p>在这里我们不能够使用 SGD 算法进行优化，因为我们需要寻找的新的函数 f 是一棵树，而不仅仅是一个数值向量。解决方案也是和提升树一样，采用 Boosting 的思想，<strong>从一个常量（通常是0）进行预测，每次添加一个新的预测残差的函数</strong>：<br>$f_0(x)=0$<br>$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$<br>$f_M(x)=∑^M_{m=1}T(x;\Theta_m),m=1,2,…,M$</p><p>由以上公式我们可以知道，我们所要做的唯一的一件事就是寻找一个方法来确定第$m$轮的函数$f_m(x_i)$。但是怎么确定每一轮迭代的函数$f_m(x)$呢，答案是优化！</p><h3 id="3-XGBoost算法学习过程"><a href="#3-XGBoost算法学习过程" class="headerlink" title="3. XGBoost算法学习过程"></a>3. XGBoost算法学习过程</h3><p>第$m$轮迭代的预测结果为：<br>$$\hat{y}_m(x_i)=\hat{y}_{m-1}(x_i)+f_m(x_i)$$</p><p>目标函数为：</p><p>$Obj_m =∑^N_{i=1}L(y_i,f_m(x_i))+∑_{m=1}^M\Omega(f_m)$</p><p>$=∑^N_{i=1}L(y_i,f_{m-1}(x)+f_m(x_i)+\Omega(f_m))+const$</p><p>推导到这里，应该就清楚了，我们的目标就是最小化上式去除常量的部分。那么我们首先来考虑一下损失函数为平方(Square）损失的情况，即：</p><p>$Obj_m =∑^N_{i=1}(y_i,f_{m-1}(x_i)+f_m(x_i))^2+\Omega(f_m)$</p><p>$=∑^N_{i=1}[2(f_{m-1}-y_i)f_m(x_i)+f_m(x_i)^2]+\Omega(f_m)+const$</p><p>有上述分析，我们知道对于目标函数当我们将损失函数设定为平方损失的时候，目标函数最终可转化为一个关于 $f_m(x)$ 的二次函数，这时候我们很容易对其进行优化，甚至可以求出它的解析解。</p><p>但是，为了算法的通用性和可扩展性的考虑，XGBoost 并没有指定损失函数为平方损失函数，此时我们会发现其实目标函数的表达是还是相当复杂的： </p><p>$$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+\Omega(f_m)+const$$</p><p>这时候该怎么做呢，陈天奇大神想出了我们高数中学过的泰勒展开式，具体怎么做的呢？</p><h3 id="4-泰勒展开时近似损失函数"><a href="#4-泰勒展开时近似损失函数" class="headerlink" title="4. 泰勒展开时近似损失函数"></a>4. 泰勒展开时近似损失函数</h3><p>为了更好的介绍 XGBoost 中是如何使用泰勒展开式来近似损失函数的，首先让我们回顾一下泰勒展开式的二阶形式：</p><p>$$f(x+\Delta x)=f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2+R(\Delta x)$$</p><p>其中$R(\Delta x)$表示$\Delta x$的高阶无穷小。因此，有：<br>$$f(x+\Delta x)\approx f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2$$</p><p>有了泰勒公式，我们给出如下定义：</p><p>$$g_i=\partial _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$<br>$$h_i=\partial^2 _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$</p><p>这里我们把$L(y_i,f_{m-1}(x_i))$看成是$f_{m-1}(x_i)$为自变量的函数，因此$g_i$和$h_i$为其一阶导和二阶导数（其实是偏导），并且我们将目标函数中的$f_m(x_i)$看成上式中自变量的增量$\Delta x$，因此将目标函数按$f_{m-1}(x_i)$进行泰勒展开，得到：</p><p>$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+ \Omega(f_m)+const$<br>$\approx ∑^N_{i=1}[L(y_i,f_{m-1}(x_i))+g_if_m(x_i)+\frac 12h_if^2_m(x_i)] + \Omega(f_m)+const$</p><p>去除掉常量部分，我们可以得到新的目标函数：<br>$$Obj_m=∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if^2_m(x_i)]+\Omega(f_m) $$</p><p>这样做的好处是：</p><blockquote><p><strong>理论上的好处</strong>：使得我们更加清楚的知道我们在学习什么，以及更好的收敛性；<br><strong>工程上的好处</strong> ：</p><ol><li>$g_i$和$h_i$都来自于损失函数的定义</li><li>函数的学习过程仅仅通过$g_i$和$h_i$依赖于目标函数</li><li>可以利用不同的模块分开实现不同的损失函数，例如平方损失函数和 logistic 损失函数，这样损失函数不会受限制。</li></ol></blockquote><h3 id="5-正则化的处理"><a href="#5-正则化的处理" class="headerlink" title="5. 正则化的处理"></a>5. 正则化的处理</h3><p>目标函数中正则化项存在的原因是为了限制模型的复杂度，<strong>让模型在训练集上能够取得较好的结果的前提下尽可能地简单</strong>。而前面我们也提到了，在 XGBoost 中，对于采用前向分布方法一步步迭代的优化时，我们模型的复杂度就是当前要定义的决策树的复杂度。</p><h4 id="决策树函数的定义"><a href="#决策树函数的定义" class="headerlink" title="决策树函数的定义"></a>决策树函数的定义</h4><p>为此，我们首先重新定义树：我们将树定义为一个该树中所有叶子节点的值的向量。并且，每个叶子的索引映射函数映射实例到每个叶子节点上：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268011299685.jpg" alt=""></p><h4 id="定义决策树的复杂度"><a href="#定义决策树的复杂度" class="headerlink" title="定义决策树的复杂度"></a>定义决策树的复杂度</h4><p>我们将决策树的复杂度，也就是目标函数定义如下：<br>$$\Omega (f_m)=\gamma T+\frac 12\lambda∑^T_{j=1}w_j^2$$</p><p>其中，$T$树中叶子结点的个数计算如下：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268013497837.jpg" alt=""></p><h4 id="新目标函数的优化"><a href="#新目标函数的优化" class="headerlink" title="新目标函数的优化"></a>新目标函数的优化</h4><p>首先，我们对叶子节点$j$中的实例进行如下定义：<br>$$I_j={i|q(x_i)=j }$$<br>此时目标函数为：<br>$$Obj_m\approx ∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if_m^2(x_i)]+\Omega(f_m)+const$$<br>$$=∑^N_{i=1}[g_iw_{q(x_i)}+\frac 12h_iw^2_{q(x_i)}]+\gamma T+\frac 12 \lambda ∑^T_{j=1}w^2_j$$<br>$$=∑^T_{j=1}[(∑_{i\in I_j}g_i)w_i+\frac 12(∑_{i \in I_j}h_i+\lambda)w^2_j]+\gamma T $$</p><p>首先，我们进行如下定义：</p><p>$$G_j=∑_{i \in I_j}g_i,H_j=∑_{i \in I_j}h_i$$</p><p>进一步简化目标函数：<br>$$Obj_m=∑^T_{j=1}[G_jw_j+\frac 12(H_i+\lambda)w^2_j]+\gamma T$$</p><p>众所周知，对于一元二次函数，由如下两条结论：<br>$$arg \min _xG_x + \frac 12Hx^2=-\frac GH,H&gt;0$$<br>$$\min _xG_x+\frac 12Hx^2=- \frac {G^2}{2H}$$</p><p>因此对于目标函数进行最小化，当$w_j=-\frac {G_j}{H_j+\lambda }$时，我们得到：<br>$$\min _{Obj_m}=∑^T_{j=1}[G_jw_j+\frac 12(H_j+\lambda)w^2_j]+\gamma T$$<br>$$=-\frac 12 ∑^T_{j=1}\frac {G^2_j}{H_j+\lambda }+\gamma T$$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268027278380.jpg" alt=""></p><p>至此，对于第 t 轮的一颗已经分裂好的决策树，我们能够求出其对应的最小化的目标函数。但是到目前为止，到底如何进行分裂我们还不知道具体的做法，接下来让我们一起学习 XGBoost 是如何寻找分裂点的。</p><h3 id="6-决策树构建"><a href="#6-决策树构建" class="headerlink" title="6. 决策树构建"></a>6. 决策树构建</h3><h4 id="决策树的生成策略"><a href="#决策树的生成策略" class="headerlink" title="决策树的生成策略"></a>决策树的生成策略</h4><p>对于回归决策树来说，在目标函数已经确定的情况下，接下来我们的问题是如何寻找对于当前训练样本的最优的决策树结构。当然，我们最容易想到的是穷举法。如果按照穷举法，我们列出所有的可能的决策树的结构$q$，然后基于决策树结构$q$去计算它的目标函数值，在计算完所有可能的决策树结构后，选择目标函数值最小的决策树结构$\hat q$作为最终决策树。</p><p>虽然理论上来说，穷举法能够寻找到最优的决策树结构，但是在有限的时间内我们无法去寻找到最优的决策树结构，因为可能的树结构有无穷多种。因此，在应用中我们采用的是贪心的策略来一步步地增长树的结构。也就是从根节点开始，不断地进行递归地分裂，直至在给定的准则下无法进行分裂为止。所以，接下来我们需要知道递归时如何进行合适并有效地分裂当前节点。</p><h4 id="寻找最优分裂点"><a href="#寻找最优分裂点" class="headerlink" title="寻找最优分裂点"></a>寻找最优分裂点</h4><p>对于一个叶子节点，如何进行分裂我们需要遵循的原则是分裂后的两颗子树的最优目标函数值之和要小于未分裂之前的父节点，为了叙述方便我们定义如下的目标函数值的 “增益”：\<br>$$Gain=\frac 12[\frac {G^2_L}{H_L+\lambda } + \frac {G^2_R}{H_R+\lambda } - \frac {(G_L+G_R)^2}{H_L+H_R+\lambda } -\gamma ]$$<br>上式表示的是在某个节点分裂前的目标函数值与分裂后的目标函数值的差值，由于我们的目标是寻找到最优的决策树，也就是说只有当$Gain$的值为正时我们才会选择进行分裂。</p><blockquote><p><strong>分裂点寻找方法</strong>：</p></blockquote><blockquote><ul><li>对每一个待分裂节点，枚举出所有的特征；</li><li>对于每一个特征，根据该特征将所有的实例进行排序；</li><li>使用线性扫描的方法计算该特征的每个可能的值作为分裂点时对应的 $Gain$对所有特征，使用上述扫描过程中找到的 $Gain$ 值大时特征及其对应的取值作为分裂点，将当前节点一分为二。</li></ul></blockquote><h4 id="离散变量处理"><a href="#离散变量处理" class="headerlink" title="离散变量处理"></a>离散变量处理</h4><p>传统的 GBDT 算法对连续型变量和离散型变量是进行分开处理的。例如 Spark 中的 GBDT 就是这样的，当我们的实例特征中有离散型变量的时候，就需要通过参数指定该离散型变量的种类，这样使得算法的用户友好性变得十分的糟糕。</p><p>而 XGBoost 在设计时就考虑到了这一点。实际上，我们不需要将离散型特征变量进行分开处理，XGBoost 使用 one-hot 编码的方式对离散型变量进行处理。</p><h4 id="剪枝策略"><a href="#剪枝策略" class="headerlink" title="剪枝策略"></a>剪枝策略</h4><ul><li><strong>前剪枝</strong><ul><li>当最优分裂点对应的增益值为负时停止分裂</li><li>但是这也会存在问题，即将来的某个时刻能够获取更高的增益</li></ul></li><li><strong>后剪枝</strong><ul><li>将决策树增长到它的最大深度，递归的进行剪枝，剪去那些使得增益值为负值的叶子节点。</li></ul></li></ul><h4 id="前向分步的步长"><a href="#前向分步的步长" class="headerlink" title="前向分步的步长"></a>前向分步的步长</h4><p>在 XGBoost 提升过程中，每产生一颗对当前残差最优化的决策树 $f_m(x)$时，并不是直接将决策树$f_m(x)$加入到模型中，而是对它乘以一个固定的算法参数 $\eta$之后才加入到模型中：</p><p>$$f_m(x)=f_{m-1}(x)+\eta f_m(x)$$</p><p>这样做的好处是防止单步决策树过拟合，以减少每棵树对最终木星的影响。</p><h3 id="7-XGBoost构建步骤流程"><a href="#7-XGBoost构建步骤流程" class="headerlink" title="7. XGBoost构建步骤流程"></a>7. XGBoost构建步骤流程</h3><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：梯度提升树$\hat{f}(x) = \hat f_M(x)$;</p><p>（1）初始化$f_0(x)=0$</p><p>（2）对$m=1,2,…,M$，依次进行循环迭代：</p><p>（a）对每个样本点，分别计算：<br>$$g_i= \partial _{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>$$h_i= \partial ^2_{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>（b）使用贪心策略构建一棵树$f_m(x)$，以使得下列的目标函数最小化：<br>$$Obj=-\frac 12 ∑^T_{j=1}\frac {G_j^2}{H_j+\lambda }+\gamma T$$<br>（c）更新：$f_M(x)=f_{m-1}(x)+\eta f_m(x)$<br>（3）得到XGBoost提升树：<br>$$f(x)=f_M(x)=∑^M_{m=1}\eta f_m(x)$$</p></blockquote><h3 id="8-XGBoost优缺点"><a href="#8-XGBoost优缺点" class="headerlink" title="8. XGBoost优缺点"></a>8. XGBoost优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li><strong>正则化</strong></li></ol><p>XGBoost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variancetradeoff 角度来讲，正则项降低了模型的 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。</p><ol start="2"><li><strong>并行处理</strong></li></ol><p>XGBoost 工具支持并行。Boosting 不是一种串行的结构吗?怎么并行的？注意 XGBoost 的并行不是 tree 粒度的并行，XGBoost 也是一次迭代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代的预测值）。XGBoost 的并行是在特征粒度上的。</p><p>我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost 在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p><ol start="3"><li><strong>灵活性</strong></li></ol><p>XGBoost 支持用户自定义目标函数和评估函数，只要目标函数二阶可导就行。</p><ol start="4"><li><strong>缺失值的处理</strong></li></ol><p>对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。</p><ol start="5"><li><strong>剪枝</strong></li></ol><p>XGBoost 先从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝。比起 GBM，这样不容易陷入局部最优解。</p><ol start="6"><li><strong>内置交叉验证</strong></li></ol><p>XGBoost 允许在每一轮 Boosting 迭代中使用交叉验证。因此，可以方便地获得最优 Boosting 迭代次数。而 GBM 使用网格搜索，只能检测有限个值。</p><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><p>虽然说 XGBoost 在 Kaggle 比赛中获得了不错的成绩，但并不代表 XGBoost 是一个完美的算法，它当然也有自己的缺点和不足之处:</p><ol><li><strong>算法参数过多</strong></li></ol><p>调参复杂，需要对 XGBoost 原理十分清楚才能很好的使用 XGBoost。</p><ol start="2"><li><strong>只适合处理结构化数据</strong></li></ol><p>相对于深度学习算法来说，XGBoost 算法只适合处理结构化的特征数据，而对于类似图像中的目标检测等任务重的非结构化数据没有很好的处理能力。</p><ol start="3"><li><strong>不适合处理超高维特征数据</strong></li></ol><p>XGBoost 算法对于中低维数据具有很好的处理速度和精度，但是对于例如大规模图像物体识别，或者是推荐算法的某些场景中会出现的超高维特征的数据就无能为力了，这时候我们就需要借助于深度学习等算法。</p><h2 id="八、-总结"><a href="#八、-总结" class="headerlink" title="八、 总结"></a>八、 总结</h2><h3 id="1-Boosting家族"><a href="#1-Boosting家族" class="headerlink" title="1. Boosting家族"></a>1. Boosting家族</h3><p>提升(Boosting)是集成学习方法里的一个重要方法，其主要思想是将弱分类器组装成一个强分类器。在 PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p><p><strong>AdaBoost、GBDT、XGBoost</strong>都属于（加法模型、前向分步、指数损失函数）家族，都实现了通过将多个弱学习器组合成强学习器已达到提升预测准确度的目的。实现的过程略有不同且适用于不同场景。<br>Boosting并非是一个方法，而是一类方法。这里按照损失函数的不同，将其细分为若干类算法，下表给出了4种不同损失函数对应的Boosting方法：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267180767470.jpg" alt=""></p><h3 id="2-Adaboost"><a href="#2-Adaboost" class="headerlink" title="2. Adaboost"></a>2. Adaboost</h3><p><strong>AdaBoost</strong>是一种典型的分类回归树，适合解决二分类问题。多分类问题解决起来比较麻烦（参考上文）</p><p><strong>AdaBoost</strong>是一种采用加法模型、前向分步算法、指数损失函数的提升树。可以表示为 Boosting 的前向分布算法的一个特例。 <a href="http://ihoge.cn/2018/adaboost.html" target="_blank" rel="noopener">更多内容请参考</a></p><h3 id="3-回归提升树和-Adaboost"><a href="#3-回归提升树和-Adaboost" class="headerlink" title="3. 回归提升树和 Adaboost"></a>3. 回归提升树和 Adaboost</h3><p><strong>回归提升树</strong>与<strong>AdaBoot</strong>不同的地方在于它使用的是平方损失函数来解决回归的问题，通过计算残差发现模型的不足，并不断拟合更新<strong>残差</strong>来构建新树。（这里残差的实现的功能与Adaboost有些类似）</p><h3 id="4-GBDT-和回归提升树"><a href="#4-GBDT-和回归提升树" class="headerlink" title="4. GBDT 和回归提升树"></a>4. GBDT 和回归提升树</h3><ol><li><strong>梯度提升树</strong>可以看作是特殊的一种回归提升树。它通过计算损失函数的负梯度在当前模型的值来近似回归树中的“残差”发现模型的不足，并通过拟合该“残差”构建新树。</li><li>回归提升树初始化另$f_0(x)=0$；GBDT初始化时令$f_{ 0 }(x)=arg\min _{ c } \sum _{ i=1 }^{ N }{ L(y_i,c) } $估计是损失函数极小化的常数值。</li><li>回归提升树通过平方损失函数计算残差，GBDT通过计算负梯度作为伪残差。</li></ol><h3 id="5-XGBoost-和-GBDT"><a href="#5-XGBoost-和-GBDT" class="headerlink" title="5. XGBoost 和 GBDT"></a>5. XGBoost 和 GBDT</h3><ul><li><p>Xgboost 是 GB 算法的高效实现，xgboost 中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）</p></li><li><p>xgboost在目标函数中显示的加上了正则化项</p></li><li><p>GB 中使用 Loss Function 对 f(x) 的一阶导数计算出伪残差用于学习生成$f_m$，xgboost 不仅使用到了一阶导数，还使用二阶导数</p></li><li><p>CART 回归树中寻找最佳分割点的衡量标准是最小化均方差，xgboost 寻找分割点的标准是最大化，lamda，gama 与正则化项相关</p></li></ul><h3 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h3><p>[1] 《统计学习方法》  ——李航 2012 清华大学出版社<br>[2] 《机器学习》  ——周志华 2016 清华大学出版社<br>[3] <a href="http://ihoge.cn/2018/adaboost.html" target="_blank" rel="noopener">http://ihoge.cn/2018/adaboost.html</a><br>[4] <a href="https://www.jianshu.com/nb/7305482" target="_blank" rel="noopener">https://www.jianshu.com/nb/7305482</a><br>[5] <a href="http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/#Gradient_Boosting" target="_blank" rel="noopener">http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/#Gradient_Boosting</a><br>[6] <a href="https://www.jianshu.com/p/d55f7aaac4a7" target="_blank" rel="noopener">https://www.jianshu.com/p/d55f7aaac4a7</a><br>[7] <a href="http://gitbook.cn/gitchat/column/5ac2f0509e924a1dc029dd84/topic/5ac9e0e5dbd50e7493d35d3f" target="_blank" rel="noopener">http://gitbook.cn/gitchat/column/5ac2f0509e924a1dc029dd84/topic/5ac9e0e5dbd50e7493d35d3f</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;本文是综合了之前的以往多个笔记汇总而成，内容包含：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;一、Boosting基本概念
二、前向分步加法模型
    1. 加法模型
    2. 前向分步算法
三、AdaBoost
    1. 算法解析
   
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="集合算法" scheme="http://www.ihoge.cn/tags/%E9%9B%86%E5%90%88%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost算法学习总结</title>
    <link href="http://www.ihoge.cn/2018/XGBoost.html"/>
    <id>http://www.ihoge.cn/2018/XGBoost.html</id>
    <published>2018-05-20T04:20:21.000Z</published>
    <updated>2018-05-28T07:52:58.059Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、XGBoost原理详解"><a href="#一、XGBoost原理详解" class="headerlink" title="一、XGBoost原理详解"></a>一、XGBoost原理详解</h2><p>前面介绍了提升树算法，其实 XGBoost 就是一种特殊的提升树算法，准确的说它是一种梯度提升决策树（GBDT ，Gradient Boosting Decision Trees）。GBDT 与前面介绍的提升树方法主要的区别就是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，来拟合一颗回归树，即：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15267877818108.jpg" alt=""></p><p>XGBoost 就是对 GBDT 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。</p><p>而 XGBoost 的特点就是<strong>计算速度快</strong>，<strong>模型表现好</strong>，这两点也正是这个项目的目标。</p><h2 id="二、XGBoost的目标函数"><a href="#二、XGBoost的目标函数" class="headerlink" title="二、XGBoost的目标函数"></a>二、XGBoost的目标函数</h2><p>传统 GBDT 算法的目标函数只有损失函数这一项，而 XGBoost 在此基础上进行了改进，增加了正则项以防止模型过度复杂：</p><p>$$Obj =∑^N_{i=1}L(y_i,\hat{y}<em>i)+∑</em>{m=1}^M\Omega(f_m), f_m \in F$$</p><p>在这里我们不能够使用 SGD 算法进行优化，因为我们需要寻找的新的函数 f 是一棵树，而不仅仅是一个数值向量。解决方案也是和提升树一样，采用 Boosting 的思想，<strong>从一个常量（通常是0）进行预测，每次添加一个新的预测残差的函数</strong>：<br>$f_0(x)=0$<br>$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$<br>$f_M(x)=∑^M_{m=1}T(x;\Theta_m),m=1,2,…,M$</p><p>由以上公式我们可以知道，我们所要做的唯一的一件事就是寻找一个方法来确定第$m$轮的函数$f_m(x_i)$。但是怎么确定每一轮迭代的函数$f_m(x)$呢，答案是优化！</p><h2 id="三、XGBoost算法学习过程"><a href="#三、XGBoost算法学习过程" class="headerlink" title="三、XGBoost算法学习过程"></a>三、XGBoost算法学习过程</h2><p>第$m$轮迭代的预测结果为：<br>$$\hat{y}_m(x_i)=\hat{y}_{m-1}(x_i)+f_m(x_i)$$</p><p>目标函数为：</p><p>$Obj_m =∑^N_{i=1}L(y_i,f_m(x_i))+∑_{m=1}^M\Omega(f_m)$</p><p>$=∑^N_{i=1}L(y_i,f_{m-1}(x)+f_m(x_i)+\Omega(f_m))+const$</p><p>推导到这里，应该就清楚了，我们的目标就是最小化上式去除常量的部分。那么我们首先来考虑一下损失函数为平方(Square）损失的情况，即：</p><p>$Obj_m =∑^N_{i=1}(y_i,f_{m-1}(x_i)+f_m(x_i))^2+\Omega(f_m)$</p><p>$=∑^N_{i=1}[2(f_{m-1}-y_i)f_m(x_i)+f_m(x_i)^2]+\Omega(f_m)+const$</p><p>有上述分析，我们知道对于目标函数当我们将损失函数设定为平方损失的时候，目标函数最终可转化为一个关于 $f_m(x)$ 的二次函数，这时候我们很容易对其进行优化，甚至可以求出它的解析解。</p><p>但是，为了算法的通用性和可扩展性的考虑，XGBoost 并没有指定损失函数为平方损失函数，此时我们会发现其实目标函数的表达是还是相当复杂的： </p><p>$$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+\Omega(f_m)+const$$</p><p>这时候该怎么做呢，陈天奇大神想出了我们高数中学过的泰勒展开式，具体怎么做的呢？</p><h2 id="三、泰勒展开时近似损失函数"><a href="#三、泰勒展开时近似损失函数" class="headerlink" title="三、泰勒展开时近似损失函数"></a>三、泰勒展开时近似损失函数</h2><p>为了更好的介绍 XGBoost 中是如何使用泰勒展开式来近似损失函数的，首先让我们回顾一下泰勒展开式的二阶形式：</p><p>$$f(x+\Delta x)=f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2+R(\Delta x)$$</p><p>其中$R(\Delta x)$表示$\Delta x$的高阶无穷小。因此，有：<br>$$f(x+\Delta x)\approx f(x)+f’(x)\Delta x+\frac 12f’’(x)\Delta x^2$$</p><p>有了泰勒公式，我们给出如下定义：</p><p>$$g_i=\partial _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$<br>$$h_i=\partial^2 _{f_{m-1}(x_i)}L(y_i, f_{m-1}(x_i))$$</p><p>这里我们把$L(y_i,f_{m-1}(x_i))$看成是$f_{m-1}(x_i)$为自变量的函数，因此$g_i$和$h_i$为其一阶导和二阶导数（其实是偏导），并且我们将目标函数中的$f_m(x_i)$看成上式中自变量的增量$\Delta x$，因此将目标函数按$f_{m-1}(x_i)$进行泰勒展开，得到：</p><p>$Obj_m=∑^N_{i=1}L(y_i,f_{m-1}(x_i)+f_m(x_i))+ \Omega(f_m)+const$<br>$\approx ∑^N_{i=1}[L(y_i,f_{m-1}(x_i))+g_if_m(x_i)+\frac 12h_if^2_m(x_i)] + \Omega(f_m)+const$</p><p>去除掉常量部分，我们可以得到新的目标函数：<br>$$Obj_m=∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if^2_m(x_i)]+\Omega(f_m) $$</p><p>这样做的好处是：</p><blockquote><p><strong>理论上的好处</strong>：使得我们更加清楚的知道我们在学习什么，以及更好的收敛性；<br><strong>工程上的好处</strong> ：</p><ol><li>$g_i$和$h_i$都来自于损失函数的定义</li><li>函数的学习过程仅仅通过$g_i$和$h_i$依赖于目标函数</li><li>可以利用不同的模块分开实现不同的损失函数，例如平方损失函数和 logistic 损失函数，这样损失函数不会受限制。</li></ol></blockquote><h2 id="五、正则化的处理"><a href="#五、正则化的处理" class="headerlink" title="五、正则化的处理"></a>五、正则化的处理</h2><p>目标函数中正则化项存在的原因是为了限制模型的复杂度，<strong>让模型在训练集上能够取得较好的结果的前提下尽可能地简单</strong>。而前面我们也提到了，在 XGBoost 中，对于采用前向分布方法一步步迭代的优化时，我们模型的复杂度就是当前要定义的决策树的复杂度。</p><h3 id="1-决策树函数的定义"><a href="#1-决策树函数的定义" class="headerlink" title="1. 决策树函数的定义"></a>1. 决策树函数的定义</h3><p>为此，我们首先重新定义树：我们将树定义为一个该树中所有叶子节点的值的向量。并且，每个叶子的索引映射函数映射实例到每个叶子节点上：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268011299685.jpg" alt=""></p><h3 id="2-定义决策树的复杂度"><a href="#2-定义决策树的复杂度" class="headerlink" title="2. 定义决策树的复杂度"></a>2. 定义决策树的复杂度</h3><p>我们将决策树的复杂度，也就是目标函数定义如下：<br>$$\Omega (f_m)=\gamma T+\frac 12\lambda∑^T_{j=1}w_j^2$$</p><p>其中，$T$树中叶子结点的个数计算如下：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268013497837.jpg" alt=""></p><h3 id="3-新目标函数的优化"><a href="#3-新目标函数的优化" class="headerlink" title="3. 新目标函数的优化"></a>3. 新目标函数的优化</h3><p>首先，我们对叶子节点$j$中的实例进行如下定义：<br>$$I_j={i|q(x_i)=j }$$<br>此时目标函数为：<br>$$Obj_m\approx ∑^N_{i=1}[g_if_m(x_i)+\frac 12 h_if_m^2(x_i)]+\Omega(f_m)+const$$<br>$=∑^N_{i=1}[g_iw_{q(x_i)}+\frac 12h_iw^2_{q(x_i)}]+\gamma T+\frac 12 \lambda ∑^T_{j=1}w^2_j$<br>$=∑^T_{j=1}[(∑_{i\in I_j}g_i)w_i+\frac 12(∑_{i \in I_j}h_i+\lambda)w^2_j]+\gamma T $</p><p>首先，我们进行如下定义：</p><p>$$G_j=∑_{i \in I_j}g_i,H_j=∑_{i \in I_j}h_i$$</p><p>进一步简化目标函数：<br>$$Obj_m=∑^T_{j=1}[G_jw_j+\frac 12(H_i+\lambda)w^2_j]+\gamma T$$</p><p>众所周知，对于一元二次函数，由如下两条结论：<br>$$arg \min _xG_x + \frac 12Hx^2=-\frac GH,H&gt;0$$<br>$$\min _xG_x+\frac 12Hx^2=- \frac {G^2}{2H}$$</p><p>因此对于目标函数进行最小化，当$w_j=-\frac {G_j}{H_j+\lambda }$时，我们得到：<br>$$\min _{Obj_m}=∑^T_{j=1}[G_jw_j+\frac 12(H_j+\lambda)w^2_j]+\gamma T$$<br>$=-\frac 12 ∑^T_{j=1}\frac {G^2_j}{H_j+\lambda }+\gamma T$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15268027278380.jpg" alt=""></p><p>至此，对于第 t 轮的一颗已经分裂好的决策树，我们能够求出其对应的最小化的目标函数。但是到目前为止，到底如何进行分裂我们还不知道具体的做法，接下来让我们一起学习 XGBoost 是如何寻找分裂点的。</p><h2 id="六、构建决策树"><a href="#六、构建决策树" class="headerlink" title="六、构建决策树"></a>六、构建决策树</h2><h3 id="1-决策树的生成策略"><a href="#1-决策树的生成策略" class="headerlink" title="1. 决策树的生成策略"></a>1. 决策树的生成策略</h3><p>对于回归决策树来说，在目标函数已经确定的情况下，接下来我们的问题是如何寻找对于当前训练样本的最优的决策树结构。当然，我们最容易想到的是穷举法。如果按照穷举法，我们列出所有的可能的决策树的结构$q$，然后基于决策树结构$q$去计算它的目标函数值，在计算完所有可能的决策树结构后，选择目标函数值最小的决策树结构$\hat q$作为最终决策树。</p><p>虽然理论上来说，穷举法能够寻找到最优的决策树结构，但是在有限的时间内我们无法去寻找到最优的决策树结构，因为可能的树结构有无穷多种。因此，在应用中我们采用的是贪心的策略来一步步地增长树的结构。也就是从根节点开始，不断地进行递归地分裂，直至在给定的准则下无法进行分裂为止。所以，接下来我们需要知道递归时如何进行合适并有效地分裂当前节点。</p><h3 id="2-寻找最优分裂点"><a href="#2-寻找最优分裂点" class="headerlink" title="2. 寻找最优分裂点"></a>2. 寻找最优分裂点</h3><p>对于一个叶子节点，如何进行分裂我们需要遵循的原则是分裂后的两颗子树的最优目标函数值之和要小于未分裂之前的父节点，为了叙述方便我们定义如下的目标函数值的 “增益”：\<br>$$Gain=\frac 12[\frac {G^2_L}{H_L+\lambda } + \frac {G^2_R}{H_R+\lambda } - \frac {(G_L+G_R)^2}{H_L+H_R+\lambda } -\gamma ]$$<br>上式表示的是在某个节点分裂前的目标函数值与分裂后的目标函数值的差值，由于我们的目标是寻找到最优的决策树，也就是说只有当$Gain$的值为正时我们才会选择进行分裂。</p><blockquote><p><strong>分裂点寻找方法</strong>：</p></blockquote><blockquote><ul><li>对每一个待分裂节点，枚举出所有的特征；</li><li>对于每一个特征，根据该特征将所有的实例进行排序；</li><li>使用线性扫描的方法计算该特征的每个可能的值作为分裂点时对应的 $Gain$对所有特征，使用上述扫描过程中找到的 $Gain$ 值大时特征及其对应的取值作为分裂点，将当前节点一分为二。</li></ul></blockquote><h3 id="3-离散变量处理"><a href="#3-离散变量处理" class="headerlink" title="3. 离散变量处理"></a>3. 离散变量处理</h3><p>传统的 GBDT 算法对连续型变量和离散型变量是进行分开处理的。例如 Spark 中的 GBDT 就是这样的，当我们的实例特征中有离散型变量的时候，就需要通过参数指定该离散型变量的种类，这样使得算法的用户友好性变得十分的糟糕。</p><p>而 XGBoost 在设计时就考虑到了这一点。实际上，我们不需要将离散型特征变量进行分开处理，XGBoost 使用 one-hot 编码的方式对离散型变量进行处理。</p><h3 id="4-剪枝策略"><a href="#4-剪枝策略" class="headerlink" title="4. 剪枝策略"></a>4. 剪枝策略</h3><ul><li><strong>前剪枝</strong><ul><li>当最优分裂点对应的增益值为负时停止分裂</li><li>但是这也会存在问题，即将来的某个时刻能够获取更高的增益</li></ul></li><li><strong>后剪枝</strong><ul><li>将决策树增长到它的最大深度，递归的进行剪枝，剪去那些使得增益值为负值的叶子节点。</li></ul></li></ul><h3 id="5-前向分步的步长"><a href="#5-前向分步的步长" class="headerlink" title="5. 前向分步的步长"></a>5. 前向分步的步长</h3><p>在 XGBoost 提升过程中，每产生一颗对当前残差最优化的决策树 $f_m(x)$时，并不是直接将决策树$f_m(x)$加入到模型中，而是对它乘以一个固定的算法参数 $\eta$之后才加入到模型中：</p><p>$$f_m(x)=f_{m-1}(x)+\eta f_m(x)$$</p><p>这样做的好处是防止单步决策树过拟合，以减少每棵树对最终木星的影响。</p><h2 id="七、XGBoost构建步骤"><a href="#七、XGBoost构建步骤" class="headerlink" title="七、XGBoost构建步骤"></a>七、XGBoost构建步骤</h2><blockquote><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_1 \in \chi ⊆ R^n$，$y_i \in Y ⊆ R$；<br><strong>输出</strong>：梯度提升树$\hat{f}(x) = \hat f_M(x)$;</p><p>（1）初始化$f_0(x)=0$</p><p>（2）对$m=1,2,…,M$，依次进行循环迭代：</p><p>（a）对每个样本点，分别计算：<br>$$g_i= \partial _{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>$$h_i= \partial ^2_{f_{m-1}(x_i)}L(y_i,f_{m-1}(x_i))$$<br>（b）使用贪心策略构建一棵树$f_m(x)$，以使得下列的目标函数最小化：<br>$$Obj=-\frac 12 ∑^T_{j=1}\frac {G_j^2}{H_j+\lambda }+\gamma T$$<br>（c）更新：$f_M(x)=f_{m-1}(x)+\eta f_m(x)$<br>（3）得到XGBoost提升树：<br>$$f(x)=f_M(x)=∑^M_{m=1}\eta f_m(x)$$</p></blockquote><h2 id="八、XGBoost优缺点"><a href="#八、XGBoost优缺点" class="headerlink" title="八、XGBoost优缺点"></a>八、XGBoost优缺点</h2><h3 id="1-优点"><a href="#1-优点" class="headerlink" title="1. 优点"></a>1. 优点</h3><ol><li><strong>正则化</strong></li></ol><p>XGBoost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的 score 的 L2 模的平方和。从 Bias-variancetradeoff 角度来讲，正则项降低了模型的 variance，使学习出来的模型更加简单，防止过拟合，这也是 XGBoost 优于传统 GBDT 的一个特性。</p><ol start="2"><li><strong>并行处理</strong></li></ol><p>XGBoost 工具支持并行。Boosting 不是一种串行的结构吗?怎么并行的？注意 XGBoost 的并行不是 tree 粒度的并行，XGBoost 也是一次迭代完才能进行下一次迭代的（第 t 次迭代的代价函数里包含了前面 t-1 次迭代的预测值）。XGBoost 的并行是在特征粒度上的。</p><p>我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost 在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个 block 结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p><ol start="3"><li><strong>灵活性</strong></li></ol><p>XGBoost 支持用户自定义目标函数和评估函数，只要目标函数二阶可导就行。</p><ol start="4"><li><strong>缺失值的处理</strong></li></ol><p>对于特征的值有缺失的样本，XGBoost 可以自动学习出它的分裂方向。</p><ol start="5"><li><strong>剪枝</strong></li></ol><p>XGBoost 先从顶到底建立所有可以建立的子树，再从底到顶反向进行剪枝。比起 GBM，这样不容易陷入局部最优解。</p><ol start="6"><li><strong>内置交叉验证</strong></li></ol><p>XGBoost 允许在每一轮 Boosting 迭代中使用交叉验证。因此，可以方便地获得最优 Boosting 迭代次数。而 GBM 使用网格搜索，只能检测有限个值。</p><h3 id="2-缺点"><a href="#2-缺点" class="headerlink" title="2. 缺点"></a>2. 缺点</h3><p>虽然说 XGBoost 在 Kaggle 比赛中获得了不错的成绩，但并不代表 XGBoost 是一个完美的算法，它当然也有自己的缺点和不足之处:</p><ol><li><strong>算法参数过多</strong></li></ol><p>调参复杂，需要对 XGBoost 原理十分清楚才能很好的使用 XGBoost。</p><ol start="2"><li><strong>只适合处理结构化数据</strong></li></ol><p>相对于深度学习算法来说，XGBoost 算法只适合处理结构化的特征数据，而对于类似图像中的目标检测等任务重的非结构化数据没有很好的处理能力。</p><ol start="3"><li><strong>不适合处理超高维特征数据</strong></li></ol><p>XGBoost 算法对于中低维数据具有很好的处理速度和精度，但是对于例如大规模图像物体识别，或者是推荐算法的某些场景中会出现的超高维特征的数据就无能为力了，这时候我们就需要借助于深度学习等算法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、XGBoost原理详解&quot;&gt;&lt;a href=&quot;#一、XGBoost原理详解&quot; class=&quot;headerlink&quot; title=&quot;一、XGBoost原理详解&quot;&gt;&lt;/a&gt;一、XGBoost原理详解&lt;/h2&gt;&lt;p&gt;前面介绍了提升树
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="集合算法" scheme="http://www.ihoge.cn/tags/%E9%9B%86%E5%90%88%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost简单案例实现</title>
    <link href="http://www.ihoge.cn/2018/XGBoost1.html"/>
    <id>http://www.ihoge.cn/2018/XGBoost1.html</id>
    <published>2018-05-18T12:10:59.000Z</published>
    <updated>2018-05-28T07:46:01.812Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XGBoost案例"><a href="#XGBoost案例" class="headerlink" title="XGBoost案例"></a>XGBoost案例</h1><p>本文中，我们用到印第安人发病的糖尿病数据集。<a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv" target="_blank" rel="noopener">右键点击下载</a></p><p>该数据集由8个描述患者医疗细节的输入变量和一个输出变量组成，以指示患者是否会在5年内发生糖尿病。</p><p>对于第一个XGBoost模型来说，这是一个很好的数据集，因为所有的输入变量都是数值型的，是一个简单的二元分类问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install xgboost</span><br></pre></td></tr></table></figure><h2 id="一、基础应用"><a href="#一、基础应用" class="headerlink" title="一、基础应用"></a>一、基础应用</h2><p><strong>引入 XGBoost 包</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> loadtxt</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><p><strong>加载数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">'diabetes.csv'</span>)</span><br><span class="line">X = data.iloc[:,<span class="number">0</span>:<span class="number">8</span>]</span><br><span class="line">Y = data.iloc[:,<span class="number">8</span>]</span><br></pre></td></tr></table></figure><p><strong>分训练集测试集</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seed = <span class="number">5</span></span><br><span class="line">test_size = <span class="number">0.2</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)</span><br></pre></td></tr></table></figure><p><strong>训练 XBGoost 模型</strong></p><p>xgboost 有封装好的分类器和回归器，可以直接用 XGBClassifier 建立模型</p><p><a href="http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" target="_blank" rel="noopener">XGBClassifier文档</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = XGBClassifier()</span><br><span class="line">model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><pre><code>XGBClassifier(base_score=0.5, booster=&apos;gbtree&apos;, colsample_bylevel=1,       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,       n_jobs=1, nthread=None, objective=&apos;binary:logistic&apos;, random_state=0,       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,       silent=True, subsample=1)</code></pre><p><a href="http://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" target="_blank" rel="noopener">了解更多XGBoost参数信息</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">print(<span class="string">"Accuracy: %.2f%%"</span> % (accuracy * <span class="number">100.0</span>))</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 80.52%</code></pre><h2 id="二、查看模型效果"><a href="#二、查看模型效果" class="headerlink" title="二、查看模型效果"></a>二、查看模型效果</h2><p>XGBoost 可以在模型训练时，评价模型在测试集上的表现，也可以输出每一步的得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = XGBClassifier()</span><br><span class="line">eval_set = [(X_test, y_test)]</span><br><span class="line">model.fit(X_train, y_train, early_stopping_rounds=<span class="number">10</span>, eval_metric=<span class="string">"logloss"</span>, eval_set=eval_set, verbose=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><pre><code>[0]    validation_0-logloss:0.660999Will train until validation_0-logloss hasn&apos;t improved in 10 rounds.[1]    validation_0-logloss:0.630583[2]    validation_0-logloss:0.605378......[74]    validation_0-logloss:0.421685Stopping. Best iteration:[64]    validation_0-logloss:0.417813</code></pre><p><strong>打印出Early Stopping 的点：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Stopping. Best iteration:</span><br><span class="line">[64]validation_0-logloss:0.417813</span><br></pre></td></tr></table></figure></p><h2 id="三、输出重要特征"><a href="#三、输出重要特征" class="headerlink" title="三、输出重要特征"></a>三、输出重要特征</h2><p>gradient boosting 还有一个优点是可以给出训练好的模型的特征重要性，<br>这样就可以知道哪些变量需要被保留，哪些可以舍弃</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line">plot_importance(model)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/output_15_0-1.png" alt="output_15_0"></p><h2 id="四、模型调参"><a href="#四、模型调参" class="headerlink" title="四、模型调参"></a>四、模型调参</h2><p>如何调参呢，下面是三个超参数的一般实践最佳值，可以先将它们设定为这个范围，然后画出 learning curves，再调解参数找到最佳模型：</p><ul><li>learning_rate ＝ 0.1 或更小，越小就需要加入更多弱学习器；</li><li>max_depth ＝ 2～8；</li><li>subsample ＝ 训练集的 30%～80%；</li></ul><p>更多<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15266532194941.jpg" alt=""></p><p>接下来我们用 GridSearchCV 来进行调参会更方便一些：</p><p>可以调的超参数组合有：</p><ol><li>树的个数和大小 (n_estimators and max_depth).</li><li>学习率和树的个数 (learning_rate and n_estimators).</li><li>行列的 subsampling rates (subsample, colsample_bytree and colsample_bylevel).</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = XGBClassifier()</span><br><span class="line">learning_rate = [<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]</span><br><span class="line">max_depth = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">param_grid = dict(learning_rate=learning_rate, max_depth=max_depth)</span><br><span class="line">kfold = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle=<span class="keyword">True</span>, random_state=<span class="number">7</span>)</span><br><span class="line">grid_search = GridSearchCV(model, param_grid, scoring=<span class="string">"neg_log_loss"</span>, n_jobs=<span class="number">-1</span>, cv=kfold)</span><br><span class="line">grid_result = grid_search.fit(X, Y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Best: %f using %s"</span> % (grid_result.best_score_, grid_result.best_params_))</span><br></pre></td></tr></table></figure><pre><code>Best: -0.474370 using {&apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 2}</code></pre><p><strong>显示最佳参数</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Best: -0.474370 using &#123;&apos;learning_rate&apos;: 0.1, &apos;max_depth&apos;: 2&#125;</span><br></pre></td></tr></table></figure><p>我们还可以用下面的代码打印出每一个参数组合对应的得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">means = grid_result.cv_results_[<span class="string">'mean_test_score'</span>]</span><br><span class="line">stds = grid_result.cv_results_[<span class="string">'std_test_score'</span>]</span><br><span class="line">params = grid_result.cv_results_[<span class="string">'params'</span>]</span><br><span class="line"><span class="keyword">for</span> mean, stdev, param <span class="keyword">in</span> zip(means, stds, params):</span><br><span class="line">    print(<span class="string">"平均得分：%f (得分标准差：%f) %r"</span> % (mean, stdev, param))</span><br></pre></td></tr></table></figure><pre><code>平均得分：-0.690191 (得分标准差：0.000436) {&apos;learning_rate&apos;: 0.0001, &apos;max_depth&apos;: 2}平均得分：-0.689811 (得分标准差：0.000475) {&apos;learning_rate&apos;: 0.0001, &apos;max_depth&apos;: 3}......平均得分：-0.607988 (得分标准差：0.098076) {&apos;learning_rate&apos;: 0.2, &apos;max_depth&apos;: 5}平均得分：-0.647131 (得分标准差：0.098951) {&apos;learning_rate&apos;: 0.2, &apos;max_depth&apos;: 6}</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;XGBoost案例&quot;&gt;&lt;a href=&quot;#XGBoost案例&quot; class=&quot;headerlink&quot; title=&quot;XGBoost案例&quot;&gt;&lt;/a&gt;XGBoost案例&lt;/h1&gt;&lt;p&gt;本文中，我们用到印第安人发病的糖尿病数据集。&lt;a href=&quot;https://raw
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="集合算法" scheme="http://www.ihoge.cn/tags/%E9%9B%86%E5%90%88%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost提升树学习笔记</title>
    <link href="http://www.ihoge.cn/2018/adaboost.html"/>
    <id>http://www.ihoge.cn/2018/adaboost.html</id>
    <published>2018-05-18T04:20:21.000Z</published>
    <updated>2018-05-28T07:51:05.221Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="提升方法-AdaBoost提升树学习笔记"><a href="#提升方法-AdaBoost提升树学习笔记" class="headerlink" title="提升方法-AdaBoost提升树学习笔记"></a>提升方法-AdaBoost提升树学习笔记</h1><p>作为非数学专业出身看到密密麻麻的数学公式刚开始真的是非常头疼。算法的物理逻辑的时候尚能理解，但是涉及到具体的数学公式实现就开始懵逼了：为什么要用这个公式，这个公式是怎么推到的，这个公式达到什么样的效果？<br>这里结合自己的理解和画图，用最直白的语言对每个公式作用进行解剖。</p><hr><h2 id="一、AdaBoost核心概念总结"><a href="#一、AdaBoost核心概念总结" class="headerlink" title="一、AdaBoost核心概念总结"></a>一、AdaBoost核心概念总结</h2><ol><li><p>提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些基本分类器线性组合，构成一个强分类器。代表性的提升方法是AdaBoost算法。（重点是：更新<strong>分类器的参数</strong>和<strong>训练集的权重见下2</strong>）</p><p> <strong>AdaBoost模型是弱分类器的线性组合：</strong><br> $$f(x)=∑^M_{m=1}\alpha _mG_m(x)$$</p><ul><li>$M$表示该提升树共有$M$个弱分类器组成</li><li>$G_m(x)$表示第$m$个弱分类器</li><li>$\alpha_m$为第$m$个弱分类器的参数（反应该分类器的重要性）</li></ul></li><li><p>AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，<strong>核心思想是：<code>提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值</code></strong>。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中<strong><code>给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</code></strong>。</p></li><li>AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，这说明了它作为提升方法的有效性。（<strong>每次迭代误差递减且误差$0≤\epsilon &lt;0.5$</strong>）</li><li>AdaBoost算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里，<strong><code>模型是加法模型，损失函数是指数损失，算法是前向分步算法时的二分类学习方法</code></strong>。每一步中极小化损失函数。</li><li>提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中最有效的方法之一。</li></ol><blockquote><p>AdaBoost是一种典型的提升树算法。</p></blockquote><p>通过上面的总结我们看到，AdaBoost是一个神奇的算法，以精妙的方式通过更新数据集的权重以及各个弱分类器的参数组合成一个强分类器。那么它具体是怎么做到的呢？</p><h2 id="二、AdaBoost算法学习过程"><a href="#二、AdaBoost算法学习过程" class="headerlink" title="二、AdaBoost算法学习过程"></a>二、AdaBoost算法学习过程</h2><p><strong>输入</strong>：训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中$x_i\in X ⊆R^n,y_i\in Y={-1,1}$，弱学习算法$G_m(x)$;</p><p><strong>输出：</strong>最终强化算法分类器$G(x)$<br>（1）初始化训练数据总和为1的权值分布：（初始权重为归一化后的均值既$\frac 1N$）<br>$$D_1=(w_{11},…,w_{1i},…w_{1N}),w_{1i}=\frac 1N, i=1,2,…N$$<br>（2）对$m=1,2,…M$：（弱分类器的个数）</p><p>（a）使用具有权值分布的$D_m$的训练数据集学习，得到基本分类器：(数据集$X$到{-1,1}的映射)<br>$$G_m(x):X-&gt;{-1,1}$$<br>（b）计算$Gm(x)$在训练数据集上的分类误差率：（公式不够简洁明了，其实总结下来非常好理解：误差率$e_m$=误分类样本的权值之和）<br>$$e_m=∑^N_{i=1}P(G_m(x_i)≠y_i)=∑^N_{i=1}w_{mi}I(G_m(x_i)≠y_i)$$</p><ul><li>我们来考虑下误差$e_m$的取值空间：由于训练集权制之和为1，因此误差$0≤e_m≤1$。但是这样还不够。因为我们在选择分裂阈值的时候会选择一个最优或局部最优的取值来分裂，且当$e_m=0.5$是表明该分裂阈值对预测无贡献。因此最终得到的$e_m$的实际取值应小于$e_m≤0.5$。</li><li>所以最终：$0≤e_m≤0.5$，且每次迭代误差$e_m$递减。这点对下面的参数理解很重要。</li></ul><p>（c）计算$G_m(x)$的系数:(这里对数为自然对数)<br>$$\alpha_m=\frac 12log\frac{1-e_m}{e_m} $$</p><ul><li>那么问题来了，为什么要用这个公式来计算更新每个基分类器的参数？我们先画个图出来观察下这个函数。（其中y轴为$\alpha _m$，x轴为误差$e_m$）</li></ul><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266077461675.jpg" alt=""></p><ul><li>由（2-b）我们得到误差$e_m$的取值范围为$0≤e_m&lt;0.5$，结合该图可以可知$0&lt;\alpha_m&lt;1$。</li><li>另外可以发现，通过该函数的转换，弱分类器$G_m(x)$的误差的越小，参数$\alpha_m$越大。即实现了<strong><code>给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值</code></strong></li></ul><p>（d）更新训练数据集的权值分布：（该权值决定数据集的重要性，并让误差的计算变得简单）<br>$$D_{m+1}=(w_{m+1,1},…,w_{m+1,i},…w_{m+1,N})$$<br>$$w_{m+1,i}=\frac {w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x-i)),i=1,2,…N$$</p><ul><li>这里$y_i={-1,1} $为真实值，$G_m(x_i)={-1,1}$为预测值。当预测正确时$y_iG_m(x_i)$为1，反之为-1。</li><li>令$\delta_{m_i}=\alpha_my_iG_m(x_i)$，$\theta_{mi}=\frac {w_{mi}}{Z_m}$(把它看作一个用于归一化权值的加权平均常数)。权重$w_{m+1,i}$的更新函数可以简化为$$w_{m+1,i}=\theta_{mi}exp(\delta <em>{mi}),i=1,2,…N$$画出$y=w</em>{m+1,i}=exp(\delta_{mi})$的图形来看一下：<img src="http://p6rvh6ej2.bkt.clouddn.com/15266159564936.jpg" alt="">由于$0&lt;\alpha_m&lt;1$，所以$-1&lt;\delta_{m,i }&lt;1$。且<strong>使得预测错误的数据集样本点更新后的权重变大，预测正确的权值变小，然后对所有权值进行归一化</strong>。这就是该函数实现的作用。(图中y=1是当$\alpha$无限接近于0时的情况：解释为，当$\alpha_m$权值越大，权重$w_{m+1,i}$更新改变的效果越明显。)</li><li>这里，$Z_m$是规范化因子，目的是使各数据集的权重进行归一化。理解为$Z_m$=更新后的各数据集权重之和。<br>$$Z_m=∑^N_{i=1}w_{mi}exp(-\alpha_my_iG_m(x_i))$$</li></ul><p>（3）构建基本分类器的新型组合$f(x)=∑^M_{m=1}\alpha_mG_m(x)$，即：<br>$$G(x)=sign(f(x))=sign(∑^M_{m=1}\alpha_mG_m(x))$$</p><ul><li>函数$sign()$的意义是将正数判别为1，负数判别为-1，最终达到分类的目的。如图：<img src="http://p6rvh6ej2.bkt.clouddn.com/15266164023757.jpg" alt=""></li></ul><blockquote><p><strong>参数$\alpha_m$公式及权重$w_{m+1,i} $</strong>其实是通过前向分步算法分别得到的$\alpha_m$和$G_m(x)$并使得$f_m(x)$再训练数据集$T$上的指数损失最小。具体的推导过程可参考《统计学习方法》–李航  第145～146页</p></blockquote><h2 id="三、AdaBoost算法实现步骤"><a href="#三、AdaBoost算法实现步骤" class="headerlink" title="三、AdaBoost算法实现步骤"></a>三、AdaBoost算法实现步骤</h2><p>上面解释了AdaBoost算法的具体内容。这里写出它的分布实现步骤再对上文算法加深下理解：</p><p>（1）假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$。</p><p>（2）AdaBoost反复学习基本分类器，在每一轮$m＝1,2,…,M$顺次地执行下列操作：</p><p>（a）使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$</p><p>（b）计算基本分类器$G_m(x)$再加权训练数据集上的分类误差率（即误分类样本的权值之和。这里要注意$w_{mi}$表示第$m$轮中第$i$个实例的权值，且权值之和为1，即$∑^N_{i=1}w_{mi}=1$）：<br>$$e_m=P(G_m(x_i)≠y_i)=∑_{G_m(x_i)≠y_i}w_{mi}$$</p><p>（c）计算基本分类器$G_m (x)$的系数$\alpha_m$。$alpha_m$表示$G_m(x)$在最终分类器中的重要性。由上面（2-c）可知，<strong>当$e_m≤1/2$时，$alpha_m≥0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的分类器在最终分类器中的作用越大。</strong></p><p>（d）更新训练数据的权值分布为下一轮作准备。式（2-d）的权重更新函数可以写成：</p><ul><li><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15266248077288.jpg" alt=""></p></li><li><p>由此可知，被基本分类器$G_m (x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大$e^{(2\alpha_m)}=\frac{e_m}{1-e_m} $倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。</p></li></ul><p>（3）线性组合$f(x)$实现$M$个基本分类器的加权表决。系数$\alpha_m$ 表示了基本分类器$G_m (x)$的重要性，这里，所有$\alpha_m$ 之和并不为1。$f(x)$的符号决定实例x的类，$f(x)$的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p><blockquote><p>提升方法是即采用加法模型（即基数函数的线性组合）与前向分步算法，以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。</p></blockquote><blockquote><p>喜欢本文的伙伴请关注我的博客<a href="http://ihoge.cn" target="_blank" rel="noopener">http://ihoge.cn</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;提升方法-AdaBoost提升树学习笔记&quot;&gt;&lt;a href=&quot;#提升方法-AdaBoost提升树学习笔记&quot; class=&quot;headerlink&quot; title=&quot;提升方法-AdaBoost提升树学习笔记&quot;&gt;&lt;/a&gt;提升方法-Ada
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="集合算法" scheme="http://www.ihoge.cn/tags/%E9%9B%86%E5%90%88%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>牛顿法、拟牛顿法、高斯-牛顿法、共轭梯度法推导总结</title>
    <link href="http://www.ihoge.cn/2018/newton1.html"/>
    <id>http://www.ihoge.cn/2018/newton1.html</id>
    <published>2018-05-16T04:20:21.000Z</published>
    <updated>2018-05-28T07:51:08.123Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><p>线性最小二乘问题，我们可以通过理论推导可以得到其解析解，但是对于非线性最小二乘问题，则需要依赖迭代优化的方法，牛顿算法是解决非线性最优的常见算法之一。<br>最近整理了几篇关于牛顿法及其优化算法都不甚满意，网上大多相关技术博客很多是拼凑起来的，多数不全面（某些推导中间过程被省略），或者数学公式的符号表达方式不统一，造成看起来非常凌乱。因此本文旨在对基于牛顿法的非线性最小二乘优化问题的基本概念和算法推导做个系统的梳理。</p><ul><li>基本数学名词及概念</li><li>海赛矩阵和雅可比矩阵</li><li>牛顿算法推导</li><li>拟牛顿算法（DFP和BFGS）</li><li>高斯牛顿算法</li><li>共轭梯度法</li><li>补充优化算法</li></ul><hr><h2 id="一、基本概念定义"><a href="#一、基本概念定义" class="headerlink" title="一、基本概念定义"></a>一、基本概念定义</h2><h3 id="1-非线性方程定义及最优化方法简述"><a href="#1-非线性方程定义及最优化方法简述" class="headerlink" title="1.非线性方程定义及最优化方法简述"></a>1.非线性方程定义及最优化方法简述</h3><p>指因变量与自变量之间的关系不是线性的关系，比如平方关系、对数关系、指数关系、三角函数关系等等。对于此类方程，求解n元实函数f在整个n维向量空间Rn上的最优值点往往很难得到精确解，经常需要求近似解问题。</p><p>求解该最优化问题的方法大多是逐次一维搜索的迭代算法，基本思想是在一个近似点处选定一个有利于搜索方向，沿这个方向进行一维搜索，得到新的近似点。如此反复迭代，知道满足预定的精度要求为止。根据搜索方向的取法不同，这类迭代算法可分为两类：</p><ul><li><strong>解析法</strong> 需要用目标函数的倒函数</li><li><strong>梯度法</strong> 又称最速下降法，是早期的解析法，收敛速度较慢</li><li><strong>牛顿法</strong> 收敛速度快，但不稳定，计算也较困难。高斯牛顿法基于其改进，但目标作用不同</li><li><strong>共轭梯度法</strong> 介于最速下降法与牛顿法之间。收敛较快，效果好</li><li><strong>变尺度法</strong> 效率较高，常用DFP法(Davidon Fletcher Powell)</li><li><strong>直接法</strong> 不涉及导数,只用到函数值。有交替方向法(又称坐标轮换法)、模式搜索法、旋转方向法、鲍威尔共轭方向法和单纯形加速法等。</li></ul><h3 id="2-非线性最小二乘问题"><a href="#2-非线性最小二乘问题" class="headerlink" title="2.非线性最小二乘问题"></a>2.非线性最小二乘问题</h3><p>非线性最小二乘问题来自于非线性回归，即通过观察自变量和因变量数据，求非线性目标函数的系数参数，使得函数模型与观测量尽量相似。</p><p><strong>高斯牛顿法</strong>是解决非线性最小二乘问题的最基本方法，并且它<strong>只能处理二次函数。(使用时必须将目标函数转化为二次的)</strong></p><h3 id="3-基本数学表达"><a href="#3-基本数学表达" class="headerlink" title="3.基本数学表达"></a>3.基本数学表达</h3><ol><li><p><strong>梯度(gradient)</strong></p><p> 常用 $\nabla $ 表示，由多元函数的哥哥偏导数组成的问题。以二元函数为例，其梯度为：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263936905896.jpg" alt=""></p></li><li><p><strong>海赛矩阵(Hessian matrix)</strong></p><p> 由多元函数的二阶偏导数组成的方阵，描述函数的局部曲率，以二元函数为例：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263938618855.jpg" alt=""></p></li><li><p><strong>雅可比矩阵(Jacobian matrix)</strong></p><p> 是多元函数一阶偏导数以一定方式排列成的矩阵，体现了一个可微方程与给出点的最优线性逼近。以二元函数为例：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263939236262.jpg" alt=""><br> 如果扩展多维的话$F: R_n -&gt; R_m$，则雅可比矩阵是一个$m$行$n$列的矩阵，表示为：$J_F(x_1,x_2,…,x_n)$<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263940046795.jpg" alt=""><br> 雅可比矩阵作用，如果$P$是$R_n$中的一点，$F$在$P$点可微分，那么在这一点的导数由$J_F(P)$给出，在此情况下，由$F(P)$描述的线性算子即接近点$P$的$F$的最优线性逼近，$x$逼近于$P$：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15263940899460.jpg" alt=""></p></li></ol><p>黑森和雅可比矩阵参考：<a href="http://jacoxu.com/jacobian矩阵和hessian矩阵/" target="_blank" rel="noopener">http://jacoxu.com/jacobian矩阵和hessian矩阵/</a></p><ol start="4"><li><p><strong>残差(residual)</strong></p><p> 表示实际观测值与估计值(拟合值)之间的差。</p></li></ol><hr><h2 id="二、牛顿法"><a href="#二、牛顿法" class="headerlink" title="二、牛顿法"></a>二、牛顿法</h2><p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法更快。比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。</p><p>从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。如下图是一个最小化一个目标方程的例子，红色曲线是利用牛顿法迭代求解，绿色曲线是利用梯度下降法求解。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15263961508272.jpg" alt=""></p><h3 id="1-求解方程"><a href="#1-求解方程" class="headerlink" title="1.求解方程"></a>1.求解方程</h3><p>并不是所有的方程都有求根公式，或者求根公式很复杂，导致求解困难。利用牛顿法，可以迭代求解。</p><p>原理是利用泰勒公式，在$x^{(0)}$处展开，且展开到一阶，即$f(x)=f(x^{(0)})+(x-x^{(0)})f’(x^{(0)}) $</p><p>求解方程$f(x)=0$，即$f(x^{(0)})+(x-x^{(0)})f’(x^{(0)})=0 $</p><p>求解$x=x^{(1)}=x^{(0)}-\frac {f(x^{(0)})}{f’(x^{(0)})}$</p><p>因为这是利用泰勒公式的一阶展开，$f(x)=f(x^{(0)})+(x-x^{(0)})f’(x^{(0)}) $此处并非完全相等，而是近似相等。这里求得的$x^{(1)}$并不能让$f(x)=0$，只能说$f(x^{(1)})$比$f(x^{(0)})$更接近$f(x)=0$，这样就可以通过不断迭代去逼近$f(x)$。</p><p>进而推出：$x^{(k+1)}=x^{(k)}-\frac {f(x^{(k)})}{f’(x^{(k)})}$</p><p>通过迭代，这恶搞狮子必然在$f(x^*)=0 $的时候收敛，如下图：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/1022856-20170916202719078-1588446775.gif" alt="1022856-20170916202719078-1588446775"></p><p>在最优化的问题中， 线性最优化至少可以使用单纯形法(或称不动点算法)求解， 但对于非线性优化问题， 牛顿法提供了一种求解的办法。 假设任务是优化一个目标函数$f$, 求函数$f$的极大极小问题, 可以转化为求解函数$f$的导数$f′=0$的问题, 这样求可以把优化问题看成方程求解问题。剩下的就和上面的牛顿法求解很相似了。</p><h3 id="2-一维无约束极值最优化"><a href="#2-一维无约束极值最优化" class="headerlink" title="2.一维无约束极值最优化"></a>2.一维无约束极值最优化</h3><p>$$min f(x),x\in R^1 $$</p><p>其中$x^*$为目标函数的极小点即$f’(x)$的根</p><p>首先把$f(x)$在探索点$x^{(k)}$处泰勒展开到2阶形式进行近似：<br>$$f (x)=f(x^{(k)})+f’(x^{(k)})(x-x^{(k)})+\frac 12f’’(x^{(k)})(x-x^{(k)})^2 $$<br>然后用$f(x)$的最小点作为新的探索点$x^{(k+1)}$</p><p>据此令：<br>$$f’(x)=f’(x^{(k)})+f’’(x^{(k)})(x-x^{(k)})=0 $$</p><p>求出迭代公式：<br>$$x^{(k+1)}=x^{(k)}-\frac {f’(x^{(k)})}{f’’(x^{(k)})}, k=0,1,…$$</p><p>因此，一维牛顿法最优化问题的<strong>计算步骤</strong>如下：<br>（1）给定初始点$x^{(0)}$，允许误差$\epsilon &gt;0$，置$k=0$<br>（2）如果$f’(x^{(k)})&lt; \epsilon $，则停止迭代，得到$x^{(k)}$；否则继续<br>（3）计算点$x^{(k+1)}$，$x^{(k+1)}=x^{(k)}-\frac {f(x^{(k)})}{f’’(x^{(k)})}$，置$k=k+1$，转（2）</p><p>需要注意的是，牛顿法在求极值的时候，如果初始点选取不好，则可能不收敛于极小点</p><h3 id="3-多维无约束极值最优化"><a href="#3-多维无约束极值最优化" class="headerlink" title="3.多维无约束极值最优化"></a>3.多维无约束极值最优化</h3><p>$$minf(x), x\in R^n$$<br>其中$x^*$为目标函数的极小点。</p><p>假设$f(x)$具有二阶连续偏导数，若第$k$次迭代值为$x^{(k)}$，则可将$f(x)$在$x^{(k)}$附近进行二阶泰勒展开：<br>$$f(x)=f(x^{(k)})+\nabla f(x^{(k)})^T(x-x^{(k)})+\frac 12(x-x^{(k)})^TH(x^{(k)})(x-x^{(k)})     (式一) $$<br>这里$\nabla f(x^{(k)})$是$f(x)$的梯度向量在点$x^{(k)}$的值；</p><p>$H(x^{(k)})$是$f(x)$的海赛矩阵（Hesse matrix）$H(x)=[\frac {ϑ^2f}{ϑ_{x_i}ϑ_{x_j}}]_{n*n} $在点$x^{(k)}$的值；</p><p>函数$f(x)$有极值的必要条件是在极值点处一阶导数为0，即梯度向量为0。特别是当$H(x^{(k)})$是正定矩阵时，函数$f(x)$的极值为极小值。</p><p>牛顿利用极小点的必要条件：$\nabla f(x)=0$</p><p>每次迭代中从点$x^{(k)} $开始，求目标函数的极小点，作为第$k+1$次迭代值$x^{(k+1)}$</p><p>假设：$x^{(k+1)}$满足$\nabla f(x^{(k+1)})=0 $ </p><p>由式二得：$$\nabla f(x)=\nabla f(x^{(k)})+H(x^{(k)})(x-x^{(k)})$$</p><p>由于：$$\nabla f(x^{(k+1)})=0 $$</p><p>即：$$\nabla f(x^{k)})+H(x^{(k)})(x^{(k+1)}-x^{(k)})=0 $$</p><p>得：$$x^{(k+1)}=x^{(k)}-H(x^{(k)})^{-1}\nabla f(x^{(k)})$$<br>简写为：$$x^{(k+1)}=x^{(k)}-H_k^{-1}g_k $$<br><strong>用该公式作为迭代公式的算法就是牛顿法。</strong>其中，$H_kp_k=-g_k$</p><p>下面给出<strong>计算步骤</strong>：</p><p>输入：目标函数$f(x)$，梯度为$g(x)=\nabla f(x)$，海赛矩阵$H(x)$，精度要求$\epsilon $;</p><p>输出：$f(x)$的极小点$x^*$</p><p>（1）取初始点$x^{(0)}$，置$k=0$<br>（2）计算梯度$g_k=\nabla f(x^{(k)})$<br>（3）若$||g_k||&lt;\epsilon $，停止计算，得近似解$x^*=x^{(k)}$；否则转（3）<br>（4）计算$H_k=H(x^{(k)})$，并根据迭代公式求出：$p_k=H(x^{(k)})^{-1}\nabla f(x^{(k)})$<br>（5）置$x^{(k+1)}=x^{(k)}+p_k$<br>（6）置$k=k+1$，转（2）</p><p>步骤（4）求$p_k$，需要求$H_K^{-1}$，计算比较复杂，所以有其他改进方法。</p><hr><h2 id="三、拟牛顿法"><a href="#三、拟牛顿法" class="headerlink" title="三、拟牛顿法"></a>三、拟牛顿法</h2><p>在牛顿法的迭代中，需要计算海赛矩阵的逆矩阵$H^{-1}$,这一计算比较复杂，考虑用一个正定矩阵$G_k=G(x^{(k)})$来近似代替$H_K^{-1}=H^{-1}(x^{(k)}) $。这就是拟牛顿法的基本想法。</p><h3 id="1-拟牛顿法推导"><a href="#1-拟牛顿法推导" class="headerlink" title="1.拟牛顿法推导"></a>1.拟牛顿法推导</h3><p>先看牛顿迭代中海赛矩阵$H_k$满足的条件。</p><p>首先，$H_k $满足以下关系:</p><p>在$$\nabla f(x)=\nabla f(x^{(k)})+H(x^{(k)})(x-x^{(k)})$$中取$$x=x^{(k+1)}$$</p><p>即得：$$g_{k+1}-g_k=H(x^{(k)})(x^{(k+1)}-x^{(k)}) $$</p><p>记：$g_k=\nabla f(x^{(k)})$；$y_k=g_{k+1}-g_k$；$\delta _k=x^{(k+1)}-x^{(k)} $；$H_k=H(x^{(k)})$；$p_k=-H_k^{-1}g_k$</p><p>则：$$y_k=H_k\delta _k$$<br>或 $$H_k^{-1}y_k=\delta _k$$</p><p>该式称为<strong>拟牛顿条件</strong></p><p>如果$H_k$是正定的（$H_k^{-1}$也是正定的），那么可以保证牛顿法搜索方向$p_k$是下降方向。这是因为搜索方向是$p_k=-H_k^{-1}g_k$</p><p>由式：$$x^{(k+1)}=x^{(k)}-H_k^{-1}g_k $$</p><p>有：$$x=x^{(k)}+ \lambda p_k =x^{(k)} - \lambda H_k^{(-1)}g_k$$</p><p>所以$f(x)$在$x^{(k)}$得泰勒展开式（见上文）可以近似写成：$$f(x)=f(x^{(k)})-\lambda g^T_kH^{-1}_kg_k $$</p><p>因$H^{-1}_k$正定，故有$g_k^TH^{-1}_kg_k&gt;0 $，当$\lambda $为一个充分小得正数时，总有$f(x)&lt;f(x^{(k)})$，也就是说$p_k$是下降方向。</p><p>拟牛顿法将$G_k$作为$H_k^{-1}$的近似，要求矩阵$G_k$满足同样的条件。首先，每次迭代矩阵$G_k$是正定的。同时，$G_k$满足拟牛顿条件：$G_{k+1}y_k=\delta _k$</p><p>按照拟牛顿条件选择$G_k$作为$H_k^{-1}$的近似或选择$B_k$作为$H_k$的近似的算法成为拟牛顿法。</p><p><strong>按照拟牛顿条件选择$G_k$作为$H_k^{-1}$的近似；或者选择$B_k$作为$H_k$的近似的算法称为拟牛顿法。</strong></p><p>按照拟牛顿条件，在每次迭代中可以选择更新矩阵$G_{k+1}$：<br>$$G_{k+1}=G_k+\nabla G_k$$</p><p>这种选择有一定的灵活性，因此有多种具体实现方法。下面介绍Broyden类拟牛顿法。</p><h3 id="2-DFP（Davidon-Fletcher-Powell）算法（DFP-algorithm）"><a href="#2-DFP（Davidon-Fletcher-Powell）算法（DFP-algorithm）" class="headerlink" title="2.DFP（Davidon-Fletcher-Powell）算法（DFP algorithm）"></a>2.DFP（Davidon-Fletcher-Powell）算法（DFP algorithm）</h3><p>DFP算法选择$G_{k+1}$的方法是，假设每一步迭代中矩阵$G_{k+1}$是由$G_k$加上两个附加项构成的，即：<br>$$G_{k+1}=G_k+P_k+Q_k $$<br>其中$P_k，Q_k$是待定矩阵。这时：<br>$$G_{k+1}y_k=G_ky_k+P_ky_k+Q_ky_k $$</p><p>为使$G_{k+1}$满足拟牛顿条件$y_k=H_k\delta _k$，可使$P_k$和$Q_k$满足：<br>$$P_ky_k=\delta _k$$<br>$$Q_ky_k=-G_ky_k$$</p><p>事实上，不难找出这样的$P_k$和$Q_k$，例如取：<br>$$P_k=\frac {\delta _k\delta _k^T}{\delta _k^Ty_k}$$<br>$$Q_k=- \frac {G_ky_ky_k^TG_k}{y^T_kG_ky_k}$$</p><p>这样就可以得到矩阵$G_{k+1}$的迭代公式：<br>$$G_{k+1}=G_k+\frac {\delta _k\delta _k^T}{\delta _k^Ty_k}-\frac {G_ky_ky_k^TG_k}{y^T_kG_ky_k}$$<br>称为<strong>DFP算法</strong>。</p><pre><code>可以证明，如果初始矩阵$G_0$是正定的，则迭代过程中的每个矩阵$G_k$都是正定的。</code></pre><p><strong>DFP算法步骤迭代如下</strong></p><p>输入：目标函数$f(x)$，梯度$g(x)=\nabla f(x)$，精度要求为$\epsilon $<br>输出：$f(x)$的极小点$x^*$。</p><p>（1）选定初始点$x^{(0)}$，取$G_0$为正定对称矩阵，置$k=0$<br>（2）计算$g_k=g(x^{(k)})$。若$||g_k||&lt;\epsilon $，则停止计算，得近似解$x^<em>=x^{(k)}$，否则转（3）<br>（3）置$p_k=-G_kg_k$<code>（这里与BFGS不同）</code><br>（4）一维搜索：求$\lambda _k$使得：$$f(x^{(k)}+\lambda _kp_k)=Min(f(x^{(k)}+\lambda p_k)),{(\lambda ≥0)}$$<br>（5）置$x^{(k+1)}=x^{(k)}+\lambda _kp_k$<br>（6）计算$g_{k+1}=g(x^{(k+1)})$，若$||g_{k+1}||&lt;\epsilon $，则停止计算，得近似解$x^</em>=x^{(k+1)}$；否则，按$G_{k+1}$的迭代公式（上文）计算出$G_{k+1}$<code>（这里与BFGS不同）</code><br>（7）置$k=k+1$，转（3）</p><h3 id="3-BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）"><a href="#3-BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）" class="headerlink" title="3.BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）"></a>3.BFGS算法（Broyden-Fletcher-Goldfarb-Shanno）</h3><p><strong>BFGS算法是最流行的拟牛顿算法。</strong></p><p>考虑用$G_k$逼近海赛矩阵的逆矩阵$H^{-1} $，也可以考虑用$B_k$逼近海赛矩阵。</p><p>这时，相应的拟牛顿条件是：$$B_{k+1}\delta _k=y_k$$</p><p>可以用同样的方法得到另一迭代公式.首先，令：<br>$$B_{k+1}=B_k+P_k+Q_k$$<br>$$B_{k+1}\delta _k=B_k\delta _k+P_k\delta _k+Q_k\delta_k $$</p><p>考虑使$P_k和Q_k$满足：<br>$$P_k\delta_k=y_k$$<br>$$Q_k\delta_k=-B_k\delta_k$$<br>找出适合条件的$P_k和Q_k$，得到<strong>BFGS算法矩阵$B_{k+1}$的迭代公式：</strong><br>$$B_{k+1}=B_k+\frac {y_ky_k^T}{y^T_k\delta_k}-\frac {B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k} $$</p><p>可以证明，如果初始矩阵$B_0$是正定的，则迭代过程中的每个矩阵$B_k$都是正定的。</p><p><strong>BFGS拟牛顿算法的迭代步骤</strong>：</p><p>输入：目标函数$f(x)$，$g(x)=\nabla f(x)$，精度要求$\epsilon$；<br>输出：$f(x)$的极小点$x^*$</p><p>（1）选定初始点$x^{(0)}$，取$B_0$为正定矩阵的对称矩阵，置$k=0$<br>（2）计算$g_k=g(x^{(k)})$。若$||g_k||&lt;\epsilon $，则停止计算，得近似解$x^<em>=x^{(k)}$；否则转（3）<br>（3）由$B_kp_k=-g_k$求出$p_k$ <code>(这里与DFP不同)</code><br>（4）一维搜索：求$\lambda _k$使得：$$f(x^{(k)}+\lambda _kp_k)=Min(f(x^{(k)}+\lambda p_k)),{(\lambda ≥0)}$$<br>（5）置$x^{(k+1)}=x^{(k)}+\lambda _kp_k$<br>（6）计算$g_{k+1}=g(x^{(k+1)})$，若$||g_{k+1}||&lt;\epsilon $，则停止计算，得近似解$x^</em>=x^{(k+1)}$；否则，按$B_{k+1}$的迭代公式（上文）计算出$B_{k+1}$<code>（这里与DFP不同）</code><br>（7）置$k=k+1$，转（3）</p><h3 id="4-Broyden类算法（Broyden’s-algorithm）"><a href="#4-Broyden类算法（Broyden’s-algorithm）" class="headerlink" title="4.Broyden类算法（Broyden’s algorithm）"></a>4.Broyden类算法（Broyden’s algorithm）</h3><p><strong>该算法是由DFP算法和BFGS算法相结合派生出的一类拟牛顿法。</strong></p><p>我们可以从BFDS算法矩阵$B_k$的迭代式（$B_{k+1}=B_k+\frac {y_ky_k^T}{y^T_k\delta_k}-\frac {B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k} $）得到BFGS算法关于$G_k$的迭代公式。</p><p>事实上，若记$G_k=B^{-1}<em>k,G</em>{k+1}=B^{-1}_{k+1} $</p><p>那么对以上BFDS算法矩阵$B_k$的迭代式两次应用Sherman-Morrison公式得：<br>$$G_{k+1}=(I-\frac {\delta _ky_k^T}{\delta_k^Ty_k })G_k(I-\frac {\delta _ky_k^T}{\delta_k^Ty_k })^T+\frac {\delta_k\delta_k^T}{\delta _k^Ty_k} $$<br>称为<strong>BFGS算法关于G_k的迭代公式</strong>。</p><p>将DFP算法的迭代公式：$$G_{k+1}=G_k+\frac {\delta _k\delta _k^T}{\delta _k^Ty_k}-\frac {G_ky_ky_k^TG_k}{y^T_kG_ky_k}$$<br><strong>记作</strong>$G^{DFP}$</p><p>将BFGS算法迭代公式：<br>$$B_{k+1}=B_k+\frac {y_ky_k^T}{y^T_k\delta_k}-\frac {B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k} $$<br><strong>记作</strong>$G^{BFGS}$</p><p>他们都满足方程拟牛顿条件式，所以他们的线性组合<br>$$G_{k+1}=\alpha G^{DFP}+(1-\alpha)G^{BFGS}$$也满足拟牛顿条件式，而且是正定的。</p><p>其中$0≤\alpha≤1$，这样就得到了一类拟牛顿法，称为Broyden类算法。其步骤与上文类似，唯（3）和（6）步有所不同。</p><p>⚠️ Sherman-Morrison公式：假设$A$是$n$阶可逆矩阵，$u,v$是$n$维向量，且$A+uv^T$也是可逆矩阵，则有：$$(A+uv^T)^{-1}=A^{-1}-\frac {A^{-1}uv^TA^{-1}}{1+v^TA^{-1}u} $$</p><h2 id="四、高斯牛顿法"><a href="#四、高斯牛顿法" class="headerlink" title="四、高斯牛顿法"></a>四、高斯牛顿法</h2><p>以后再补充。</p><h2 id="五、共轭梯度法（Conjugate-Gradient）"><a href="#五、共轭梯度法（Conjugate-Gradient）" class="headerlink" title="五、共轭梯度法（Conjugate Gradient）"></a>五、共轭梯度法（Conjugate Gradient）</h2><p>共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p><p>具体的实现步骤请参加<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB" target="_blank" rel="noopener">wiki百科共轭梯度法</a>。</p><p>下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：(绿色为梯度下降法，红色代表共轭梯度法)<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262668307656.jpg" alt=""></p><h2 id="六、其他优化方法"><a href="#六、其他优化方法" class="headerlink" title="六、其他优化方法"></a>六、其他优化方法</h2><h3 id="1、启发式优化方法"><a href="#1、启发式优化方法" class="headerlink" title="1、启发式优化方法"></a>1、启发式优化方法</h3><p>启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。</p><p>还有一种特殊的优化算法被称之多目标优化算法，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有NSGAII算法、MOEA/D算法以及人工免疫算法等。</p><h3 id="2、解决约束优化问题——拉格朗日乘数法"><a href="#2、解决约束优化问题——拉格朗日乘数法" class="headerlink" title="2、解决约束优化问题——拉格朗日乘数法"></a>2、解决约束优化问题——拉格朗日乘数法</h3><p>有关拉格朗日乘数法的介绍请见另一篇博客：<a href="http://www.cnblogs.com/maybe2030/p/4946256.html" target="_blank" rel="noopener">《拉格朗日乘数法》</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;p&gt;线性最小二乘问题，我们可以通过理论推导可以得到其解析解，但是对于非线性最小二乘问题，则需要依赖迭
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="优化算法" scheme="http://www.ihoge.cn/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降、随机梯度下降、批量梯度下降</title>
    <link href="http://www.ihoge.cn/2018/GradientDescent.html"/>
    <id>http://www.ihoge.cn/2018/GradientDescent.html</id>
    <published>2018-05-14T03:20:21.000Z</published>
    <updated>2018-05-28T07:59:23.932Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>李航老师在《统计学习方法》中将机器学习的三要素总结为：模型、策略和算法。其大致含义如下：</p><p><strong>模型</strong>：其实就是机器学习训练的过程中所要学习的条件概率分布或者决策函数。</p><p><strong>策略</strong>：就是使用一种什么样的评价，度量模型训练过程中的学习好坏的方法，同时根据这个方法去实施的调整模型的参数，以期望训练的模型将来对未知的数据具有最好的预测准确度。</p><p><strong>算法</strong>：算法是指模型的具体计算方法。它基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后考虑用什么样的计算方法去求解这个最优模型。</p><p>很多时候机器学习工程师又戏称调参工程师, 由此可见参数调优时作为机器学习工程师必须掌握的一项核心技能。</p><p>这篇文章的目的旨在对常用的参数调优算法进行一次梳理便于随时翻阅。</p><h1 id="1-梯度下降法（Gradient-Descent）"><a href="#1-梯度下降法（Gradient-Descent）" class="headerlink" title="1. 梯度下降法（Gradient Descent）"></a>1. 梯度下降法（Gradient Descent）</h1><h2 id="1-1-一般解释"><a href="#1-1-一般解释" class="headerlink" title="1.1 一般解释"></a>1.1 一般解释</h2><p>$f(x)$在$x_0$的梯度：就是$f(x)$变化最快的方向。梯度下降法是一个最优化算法，通常也称为<em>最速下降法</em>。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262278257042.jpg" alt=""><br>假设$f(x)$是一座山，站在半山腰，往x方向走1米，高度上升0.4米，也就是说x方向上的偏导是 0.4；往y方向走1米，高度上升0.3米，也就是说y方向上的偏导是 0.3；这样梯度方向就是 (0.4 , 0.3)，也就是往这个方向走1米，所上升的高度最高。梯度不仅仅是$f(x)$在某一点变化最快的方向，而且是上升最快的方向；如果想下山，下降最快的方向就是逆着梯度的方向，这就是梯度下降法，又叫最速下降法。</p><h2 id="1-2-梯度下降算法用途"><a href="#1-2-梯度下降算法用途" class="headerlink" title="1.2 梯度下降算法用途"></a>1.2 梯度下降算法用途</h2><p>最速下降法是求解无约束优化问题最简单和最古老的方法之一，虽然现在已经不具有实用性，但是许多有效算法都是以它为基础进行改进和修正而得到的。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。 </p><p>在梯度下降算法中，都是围绕以下这个式子展开：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262224519674.jpg" alt=""><br>其中在上面的式子中$h_θ(x)$代表，输入为x的时候的其当时θ参数下的输出值，与y相减则是一个相对误差，之后再平方乘以1/2，并且其中:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262225198545.jpg" alt=""><br>这里我列举了一个简单的例子，当然实际的x可以有n多个维度。我们知道曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。这个θ的更新过程可以描述为:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15262225480705.jpg" alt=""><br>这里就是根据每一个 x 的分量以及当时的偏差值进行 θ 的更新，其中 α 为步长，这个参数如果设置的太大，那么很容易就在最优值附加徘徊；相反，如果设置的太小，则会导致收敛速度过慢。</p><p>关于步长和学习速率的关系,这里提一下其实这两个是一个概念，叫法不一样，最优化问题中叫步长，但一般在神经网络中也叫学习速率。</p><h2 id="1-3-梯度下降、随机梯度下降、批量梯度下降"><a href="#1-3-梯度下降、随机梯度下降、批量梯度下降" class="headerlink" title="1.3 梯度下降、随机梯度下降、批量梯度下降"></a>1.3 梯度下降、随机梯度下降、批量梯度下降</h2><ul><li><p><strong>梯度下降</strong>：梯度下降就是上面的推导，要留意，在梯度下降中，对于θ的更新，所有的样本都有贡献，也就是参与调整θ.其计算得到的是一个标准梯度。因而理论上来说一次更新的幅度是比较大的。如果样本不多的情况下，当然是这样收敛的速度会更快啦~</p></li><li><p><strong>随机梯度下降</strong>：可以看到多了随机两个字，随机也就是说用样本中的一个例子来近似所有的样本，来调整θ，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，容易陷入到局部最优解中。随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。</p></li><li><p><strong>批量梯度下降</strong>：其实批量的梯度下降就是一种折中的方法，他用了一些小样本来近似全部的，其本质就是随机指定一个例子替代样本不太准，而且批量的话还是非常可以反映样本的一个分布情况的。批量梯度下降最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p></li><li><p><strong>概括</strong>：</p><p>  随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><p>  随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</p></li><li><p><strong>对批量梯度下降法和随机梯度下降法的总结：</strong></p><p>  批量梯度下降—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</p><p>  随机梯度下降—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</p></li></ul><h3 id="梯度下降代码"><a href="#梯度下降代码" class="headerlink" title="梯度下降代码:"></a>梯度下降代码:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="comment">#这里更新权重的时候所有的样本点都用上了</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">        theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>output:iters_count 219iters_count 220iters_count 221iters_count 222iters_count 223iters_count 224iters_count 225theta: [3.0027765778748003, 3.997918297015663]final loss: 9.68238055213e-05iters: 225[Finished in 0.2s]</code></pre><h3 id="随机梯度下降代码"><a href="#随机梯度下降代码" class="headerlink" title="随机梯度下降代码:"></a>随机梯度下降代码:</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每次选取一个值,随机一个点更新 θ</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="comment">#每一次选取随机的一个点进行权重的更新</span></span><br><span class="line">    i = random.randint(<span class="number">0</span>,<span class="number">3</span>)</span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>iters_count 1226iters_count 1227iters_count 1228iters_count 1229iters_count 1230iters_count 1231iters_count 1232theta: [3.002441488688225, 3.9975844154600226]final loss: 9.989420302e-05iters: 1232[Finished in 0.3s]</code></pre><h3 id="批量随机梯度下降代码："><a href="#批量随机梯度下降代码：" class="headerlink" title="批量随机梯度下降代码："></a>批量随机梯度下降代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里用2个样本点</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment">#This is a sample to simulate a function y = theta1*x1 + theta2*x2</span></span><br><span class="line">input_x = [[<span class="number">1</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>], [<span class="number">4</span>,<span class="number">2</span>]]  </span><br><span class="line">y = [<span class="number">19</span>,<span class="number">26</span>,<span class="number">19</span>,<span class="number">20</span>]  </span><br><span class="line">theta = [<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">loss = <span class="number">10</span></span><br><span class="line">step_size = <span class="number">0.001</span></span><br><span class="line">eps =<span class="number">0.0001</span></span><br><span class="line">max_iters = <span class="number">10000</span></span><br><span class="line">error =<span class="number">0</span></span><br><span class="line">iter_count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>( loss &gt; eps <span class="keyword">and</span> iter_count &lt; max_iters):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    i = random.randint(<span class="number">0</span>,<span class="number">3</span>) <span class="comment">#注意这里，我这里批量每次选取的是2个样本点做更新，另一个点是随机点+1的相邻点</span></span><br><span class="line">    j = (i+<span class="number">1</span>)%<span class="number">4</span></span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[i]) * input_x[i][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    pred_y = theta[<span class="number">0</span>]*input_x[j][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[j][<span class="number">1</span>]</span><br><span class="line">    theta[<span class="number">0</span>] = theta[<span class="number">0</span>] - step_size * (pred_y - y[j]) * input_x[j][<span class="number">0</span>]</span><br><span class="line">    theta[<span class="number">1</span>] = theta[<span class="number">1</span>] - step_size * (pred_y - y[j]) * input_x[j][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range (<span class="number">3</span>):</span><br><span class="line">        pred_y = theta[<span class="number">0</span>]*input_x[i][<span class="number">0</span>]+theta[<span class="number">1</span>]*input_x[i][<span class="number">1</span>]</span><br><span class="line">        error = <span class="number">0.5</span>*(pred_y - y[i])**<span class="number">2</span></span><br><span class="line">        loss = loss + error</span><br><span class="line">    iter_count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'iters_count'</span>, iter_count</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">'theta: '</span>,theta </span><br><span class="line"><span class="keyword">print</span> <span class="string">'final loss: '</span>, loss</span><br><span class="line"><span class="keyword">print</span> <span class="string">'iters: '</span>, iter_count</span><br></pre></td></tr></table></figure><pre><code>.....iters_count 543iters_count 544iters_count 545iters_count 546iters_count 547iters_count 548iters_count 549theta: [3.0023012574840764, 3.997553282857357]final loss: 9.81717138358e-05iters: 549</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;李航老师在《统计学习方法》中将机器学习的三要素总结为：模型、策略和算法。其大致含义如下：&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="优化算法" scheme="http://www.ihoge.cn/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>KNN——不需要学习的算法</title>
    <link href="http://www.ihoge.cn/2018/KNN1.html"/>
    <id>http://www.ihoge.cn/2018/KNN1.html</id>
    <published>2018-05-11T15:38:21.000Z</published>
    <updated>2018-05-28T07:53:08.321Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="KNN——不需要学习的算法"><a href="#KNN——不需要学习的算法" class="headerlink" title="KNN——不需要学习的算法"></a>KNN——不需要学习的算法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>KNN（K-Nearest Neighbor）算法是机器学习中算法中最基础和简单的算法之一。它既能用于分类，也能用于回归。本文将在不同的小节详细地介绍 KNN 算法在分类和回归两种任务下的运用原理。</p><p>KNN 算法的思想非常简单：对于任意的 n 维输入向量，其对应于特征空间一个点，输出为该特征向量所对应的类别标签或者预测值。KNN 算法在机器学习算法中有一个十分特别的地方，那就是它没有一个显示的学习过程。它实际上的工作原理是利用训练数据对特征向量空间进行划分，并将其划分的结果作为其最终的算法模型。</p><h2 id="1-KNN分类算法"><a href="#1-KNN分类算法" class="headerlink" title="1. KNN分类算法"></a>1. KNN分类算法</h2><p>KNN 分类算法的分类预测过程十分的简单和容易理解：对于一个需要预测的输入向量 x，我们只需要在训练数据集中寻找 k 个与向量 x 最近的向量的集合，然后把 x 的类标预测为这 k 个样本中类标数最多的那一类。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15260483350913.jpg" alt=""><br>如图所示，$w_1,w_2,w_3$分别代表的是训练机中的三个类别。图中$x_u$最相近的5（k=5）个点如图中箭头所指，很明显与$x_u$最相近的5个点中最多的泪标为$w_1$，因此KNN算法将$x_u$的类别预测为$w_1$。</p><p>基于上述思想给出KNN算法公式原理：</p><h3 id="1-1-KNN算法原理"><a href="#1-1-KNN算法原理" class="headerlink" title="1.1 KNN算法原理"></a>1.1 KNN算法原理</h3><p><strong>输入</strong>：训练数据集</p><p>$T={(x_1,y_1),(x_2,y_2),(x_3,y_3),…,(x_N,y_N)}$</p><p>其中：<br>$x_i\in X⊆R^n$ 为 n 维的实例特征向量。<br>$y_i\in Y={c_1,c_2,…,c_K}$为实例的类别，$i=1,2,3,…,N$，预测实例$x$。</p><p><strong>输出</strong>：预测实例$x$所属类别$y$。</p><p><strong>算法执行步骤</strong>：</p><ol><li>根据给定的距离度量方法（一般情况下使用欧氏距离）在训练集$T$中寻找出与$x$最相近的$k$个样本点，并将这$k$个样本点所表示的集合记为$N_k(x)$；</li><li>根据多数投票原则确定实例$x$所属的类别$y$。</li></ol><p>从上述的 KNN 原理中，我们发现有两个因素必须确定才能是 KNN 分类算法真正能勾运行：（1）算法超参数K；（2）模型向量空间的距离度量。</p><h3 id="1-2-K值的确定"><a href="#1-2-K值的确定" class="headerlink" title="1.2 K值的确定"></a>1.2 K值的确定</h3><p>KNN 算法中只有唯一的一个超参数 K，很明显 K 值的选择对最终算法的预测结果会产生至关重要的影响。接下来就简单的讨论一下 K 值的大小对算法结果的影响以及一般情况下如何选择 K 值。</p><p>如果 K 值选择的比较小，这时候我们就相当于使用较小的领域中的训练样本对实例进行预测。这时候，算法的近似误差（Approximate Error）会减小，因为只有与输入实例相近的训练样本才能才会对预测结果起作用。但是它也会有明显的缺点：算法的估计误差会偏大，预测的结果会对近邻点十分敏感，也就是说如果近邻点是噪声点的话，那么预测就会出错。也就是说，K 值太小会使得 KNN 算法容易过拟合。</p><p>同理，如果 K 值选的比较大的话，这时候距离较远的训练样本都能够对实例的预测结果产生影响。这时候，而模型相对比较鲁棒，不会因个别噪声点对最终的预测产生影响。但是缺点也是十分明显的：算法的近似误差会偏大，距离较远的点（与预测实例不相似）也会同样对预测结果产生作用，使得预测产生较大偏差。此时相当于模型发生欠拟合。</p><p>因此，在实际的工程实践过程中，我们一般<strong>采用交叉验证的方式选取 K 值</strong>。从上面的分析也可以知道，一般 K 值取得比较小。<em>我们会选取 K 值在较小的范围，同时在测试集上准确率最高的那一个确定为最终的算法超参数 K。</em></p><h3 id="1-3-距离的度量"><a href="#1-3-距离的度量" class="headerlink" title="1.3 距离的度量"></a>1.3 距离的度量</h3><p>样本空间中两个点之间的距离度量表示的是两个样本点之间的相似程度：距离越短，表示相似程度越高；相反，距离越大，表示两个样本的相似程度低。</p><p>常用的距离度量方式有：</p><ol><li>闵可夫斯基距离；</li><li>欧氏距离；</li><li>曼哈顿距离；</li><li>切比雪夫距离；</li><li>余弦距离。</li></ol><h4 id="1-3-1-闵可夫斯基距离"><a href="#1-3-1-闵可夫斯基距离" class="headerlink" title="1.3.1 闵可夫斯基距离"></a>1.3.1 闵可夫斯基距离</h4><p>闵可夫斯基距离不是一种距离，而是一类距离的定义。对于 n 维空间中的两个点 $x(x_1,x_2,x_3,…,x_n)$和$y(y_1,y_2,y_3,…,y_n)$，那么$x$和$y$亮点之间的闵可夫斯基距离为：$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{p}}}$$<br>其中p是一个可变参数：</p><ul><li>当p=1时，被称为曼哈顿距离；</li><li>当p=2时，被称为欧式距离；</li><li>当p=$\infty $时，被称为切比雪夫距离。</li></ul><h4 id="1-3-2-欧式距离"><a href="#1-3-2-欧式距离" class="headerlink" title="1.3.2 欧式距离"></a>1.3.2 欧式距离</h4><p>由以上说明可知，欧式距离的计算公式为：<br>$$d_{xy}=\sqrt{\sum_{i=1}^{n}{\left( x_{i}-y_{i} \right)^{2}}}$$</p><p>欧式距离（L2 范数）是最易于理解的一种距离计算方法，源自欧式空间中两点间的距离公式，也是最常用的距离度量方式。</p><h4 id="1-3-3-曼哈顿距离"><a href="#1-3-3-曼哈顿距离" class="headerlink" title="1.3.3 曼哈顿距离"></a>1.3.3 曼哈顿距离</h4><p>由闵可夫斯基距离定义可知，曼哈顿距离的计算公式为：<br>$$d_{xy}=\sum_{i=1}^{n}{\left| x_{i}-y_{i} \right|}$$</p><h2 id="KNN-算法核心：KDTree"><a href="#KNN-算法核心：KDTree" class="headerlink" title="KNN 算法核心：KDTree"></a>KNN 算法核心：KDTree</h2><p>通过以上的分析，我们知道 KNN 分类算法的思想非常简单，它采用的就是 K 最近邻多数投票的思想。所以，算法的关键就是在给定的距离度量下，对预测实例如何准确快速地找到它的最近的 K 个邻居？</p><p>也许绝大多数初学者会说，直接暴力寻找呗，反正 K 一般取值不会特别大。确实，特征空间维度不高并且训练样本容量小的时候确实可行，但是当特征空间维度特别高或者样本容量大时，计算就会非常耗时，因此该方法并不可行。</p><p>因此，为了快速查找到 K 近邻，我们可以考虑使用特殊的数据结构存储训练数据，用来减少搜索次数。其中，KDTree 就是最著名的一种。</p><h3 id="KD-树简介"><a href="#KD-树简介" class="headerlink" title="KD 树简介"></a>KD 树简介</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15260517984690.jpg" alt=""><br>KD 树（K-dimension Tree）是一种对 K 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。KD 树是是一种二叉树，表示对 K 维空间的一个划分，构造 KD 树相当于不断地用垂直于坐标轴的超平面将 K 维空间切分，构成一系列的 K 维超矩形区域。KD 树的每个结点对应于一个 K 维超矩形区域。利用 KD 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><h3 id="KD-树的构造"><a href="#KD-树的构造" class="headerlink" title="KD 树的构造"></a>KD 树的构造</h3><p>KD 树的构造是一个递归的方法：（1）构造根节点，使根节点对应于 K 维空间中包含的所有点的超矩形区域；（2）不断地对 K 维空间进行切分，生成子节点。</p><ul><li><p><strong>构造跟节点</strong></p><p>  首先，在包含所有节点的超矩形区域选择一个坐标轴和在此坐标轴上的一个切分点，确定一个垂直于该坐标轴的超平面，这个超平面将当前区域划分为两个子区域（也即二叉树的两左右孩子节点）。</p></li><li><p><strong>递归构造子节点</strong></p><p>  递归地对两个子区域进行相同的划分，直到子区域内没有实例时终止（此时只有叶子节点）。</p><p>  通常我们循环地选择坐标轴对空间进行划分，当选定一个维度坐标时，切分点我们选择所有训练实例在该坐标轴上的中位数。此时我们来构造的 KD 树是平衡二叉树，但是平衡二叉树在搜索时不一定是最高效的。</p></li></ul><h2 id="KNN-回归算法"><a href="#KNN-回归算法" class="headerlink" title="KNN 回归算法"></a>KNN 回归算法</h2><p>上面我们讲的 KNN 算法主要是用于分类的情况，实际上，KNN 算法也能够用于回归预测。下面介绍一下 KNN 算法如何用于回归的情况。</p><p>众所周知，KNN 算法用于分类的方法如下：首先，对于一个新来的预测实例，我们在训练集上寻找它的最相近的 K 个近邻；然后，采用投票法将它分到这 K 个邻居中的最多的那个类。</p><p>但是，怎么将 KNN 算法用于回归呢？其实大致的步骤是一样的，也是对新来的预测实例寻找 K 近邻，然后对这 K 个样本的目标值去均值即可作为新样本的预测值：</p><p>$$f\left( x \right)=\frac{1}{K}\sum_{i=1}^{K}{x_{i}}$$</p><h2 id="KNN-预测代码演示"><a href="#KNN-预测代码演示" class="headerlink" title="KNN 预测代码演示"></a>KNN 预测代码演示</h2><p>接下来，我们使用 scikit-learn 库中的 KNN 对 iris 数据集分类效果进行预测实战。众所周知，iris 数据集有四个维度的特征，但是为了方便展示效果，我们只使用其中的两个维度。完整的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> neighbors, datasets</span><br><span class="line"></span><br><span class="line">n_neighbors = <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># import some data to play with</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># we only take the first two features. We could avoid this ugly</span></span><br><span class="line"><span class="comment"># slicing by using a two-dim dataset</span></span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create color maps</span></span><br><span class="line">cmap_light = ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">'uniform'</span>, <span class="string">'distance'</span>]:</span><br><span class="line">    <span class="comment"># we create an instance of Neighbours Classifier and fit the data.</span></span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    print(<span class="string">"train_score"</span>, clf.score(X,y))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">1</span>, X[:, <span class="number">0</span>].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">1</span>, X[:, <span class="number">1</span>].max() + <span class="number">1</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, cmap=cmap_bold,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">"3-Class classification (k = %i, weights = '%s')"</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15260528682435.jpg" alt=""></p><ul><li><p><strong>代码解释</strong></p><p>  代码中邻居数使用的是 n_neighbors = 15，只使用 iris 的前两维特征作为分类特征。权重度量采用了两种方式：<strong>均值（uniform）和距离（distance）</strong>。均值代表的是所有的 K 个近邻在分类时重要性选取的是一样的，该参数是默认参数；距离也就是说，分类时 K 个邻居中每个邻居所占的权重与它与预测实例的距离成反比。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;KNN——不需要学习的算法&quot;&gt;&lt;a href=&quot;#KNN——不需要学习的算法&quot; class=&quot;headerlink&quot; title=&quot;KNN——不需要学习的算法&quot;&gt;&lt;/a&gt;KNN——不需要学习的算法&lt;/h1&gt;&lt;h2 id=&quot;概述&quot;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark SQL 基础框架和核心组件</title>
    <link href="http://www.ihoge.cn/2018/SparkSQL1.html"/>
    <id>http://www.ihoge.cn/2018/SparkSQL1.html</id>
    <published>2018-05-09T17:00:21.000Z</published>
    <updated>2018-05-28T07:51:54.349Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark-SQL-基础框架和核心组件"><a href="#Spark-SQL-基础框架和核心组件" class="headerlink" title="Spark SQL 基础框架和核心组件"></a>Spark SQL 基础框架和核心组件</h1><p>今天被问到<code>Spark Sql</code>和<code>Dataframe</code>的关系，当时没有描述好。事后脑海中关于它们的一些细节才一一涌现出来。还是理解的不够深刻，所以这里重新做下相关概念知识的整理也算是一种复习。</p><h2 id="1-Spark-SQL发展史"><a href="#1-Spark-SQL发展史" class="headerlink" title="1. Spark SQL发展史"></a>1. Spark SQL发展史</h2><p>在<code>Spark</code>的早期版本，为什么解决<code>Hive</code>查询在性能上的问题，在<code>Spark</code>生态圈引入了一个名为<code>Shark</code>的项目。（使用<code>Spark</code>的计算引擎而不是<code>MapReduce</code>来执行<code>Hive</code>查询）。<code>Shark</code>是在<code>Hive</code>代码库上构建的，使用<code>Hive</code>查询编译器来解析<code>Hive</code>查询并生成一个抽象语法树，它会转化为一个具有某些基本优化的逻辑计划。<code>Shark</code>应用了额外的优化手段并创建了一个<code>RDD</code>操作额度物理计划，然后在<code>Spark</code>上执行。基于这种对<code>Hive</code>的过分依赖，也让<code>Shark</code>本身具有一些问题：</p><ol><li>只适用于查询<code>Hive</code>表。无法在RDD上进行关系查询。</li><li>在<code>Spark</code>程序中将<code>Hive-QL</code>作为字符串运行容易出错。</li><li>它的<code>Hive</code>优化器是为<code>MapReduce</code>附案例创建的，很难讲<code>Spark</code>扩展到新的数据源和新的处理模型。<br>基于以上原因，<code>Spark</code>团队终止了<code>Shark</code>，原班人马合并全力开发<code>Spark SQL</code>。下面是<code>Spark SQL</code>的发展史。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258813010356.jpg" alt=""><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258818229539.jpg" alt=""><h2 id="2-Spark-SQL的构架"><a href="#2-Spark-SQL的构架" class="headerlink" title="2. Spark SQL的构架"></a>2. Spark SQL的构架</h2><code>Spark SQL</code>是在<code>Spark</code>核心执行引擎上的一个库，它借助JDBC/ODBC公开了SQL接口，用于数据仓库应用程序，或通过命令行进行交互式查询。因此<strong>任何商务智能（BI）工具都可以连接到Spark SQL以内存速度执行分析</strong>。<br>它还提供了<code>Java、Scala、Python、R</code>支持的<code>Dataset API</code>和<code>Dataframe API</code>。（注意：Python和R不支持Dataset）<br><code>Spark SQL</code>用户可以使用<code>Data Sourse API</code>从各种数据源读取和写入数据，从而创建<code>Dataframe</code> 或者 <code>Dataset</code>。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258850302556.jpg" alt=""><br><code>Spark SQL</code>还扩展了用于所有其他<code>Spark</code>库（如 <code>SparkR、SparkStreaming、Structred Streaming、ML、MLlib、GraphX）的DataSet API、Dataframe API和Data Sources API</code>。如下图所示：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258851101623.jpg" alt=""></li></ol><p><code>Spark SQL</code>引入了<code>Catalyst</code>的可扩展优化器，一直吃大多数常见的数据源和算法。<code>Catalyst</code>支持添加新的数据源、优化规则和某些领域（如机器学习）使用的数据类型。</p><h2 id="3-Spark-SQL的四个组件"><a href="#3-Spark-SQL的四个组件" class="headerlink" title="3. Spark SQL的四个组件"></a>3. Spark SQL的四个组件</h2><p>Spark SQL 中的四个组件分别是：<strong>SQL、 Data Sources API、 DataFrame API、 DataSet API</strong><br>下面一一展开：</p><ul><li><p><strong>Spark SQL</strong>可以使用SQL语言向Hive表写入读取数据。</p><p>  SQL通过ODBC/JDBC或命令行选项在Java、Scala、Python、R语言中使用。在编程语言中使用SQL时，结果会转换成DataFrame。<strong>SQL</strong>的优点是可以轻松的处理Hive表，并且可以利用Thrift服务器将BI工具连接到分布式SQL引擎，并利用JDBC/ODBC接口提交SQL或HQL查询。</p></li><li><p><strong>Data Sources API</strong>为使用Spark SQL 读取和写入数据提供了统一的接口。</p><p>  Data Sources API的优点有：</p><pre><code>1. 容易加载/保存Dataframe2. 通过到数据源的*谓词下推*方式进行高效的数据访问，采用这种方式能从数据源读取更少的数据。3. 为任何新的数据源构建库4. 无需包含在Spark代码中5. 很容易与Spark组件包共享新的数据源。--谓词下推的思路是让筛选数据的条件表达式尽可能靠近数据源执行，从而减少数据额移动和传输。谓词就是筛选数据的条件，下推就是让这些筛选动作尽量在靠近数据源的位置执行。</code></pre></li><li><p><strong>DataFrame API</strong>旨在让大数据分析工作更为简单易行。</p><p>  这个API收到了R和Python（Pandas库）中的DataFrame的启发，但他被实际用于大规模数据集的分布式处理一支持大数据分析。<em>DataFrame可以看作是对现有RDD API的扩展，也是RDD纸上的一个抽象</em><br>  DataFrame的优点有：</p><pre><code>1. 易于使用域特定语言开发应用程序2. 在传统RDD基础上的高性能，并且在Scala、Java、Python、R中都具备近似性能3. 在数据源中自动发现模式类型（Schema）和分区4. 支持多种数据源5. 通过Catalyst优化器进行优化和代码生成6. 可以与RDD、DatSet、Pandas和外部数据源（RDBMS关系型数据库、HBase、Cassandra等）互操作</code></pre></li><li><p><strong>DataSet API</strong></p><p>  Spark1.6 版本引入的 DataSet API 结合了 RDD 和 DataFrame的最大优点。 DatSet 会使用编码器将JVM对象转换为用 Spark 的 Tungsten 二进制格式存储的 DatSet 表形式。<br>  它的优点有：</p><pre><code>1. 和 RDD 一样，类型安全性2. 和 DataFrame 一样， 比 RDD 更快3. 与 DataFrame 和 RDD 之间具有互操性4. 缓存的 DatSet 专用的空间比 RDD 更少5. 使用编码器进行序列话比 Java 或 Kyro 序列化更快 </code></pre><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15258847849628.jpg" alt=""></p></li></ul><h2 id="4-Dataframe-和-DataSet-的演变与优势"><a href="#4-Dataframe-和-DataSet-的演变与优势" class="headerlink" title="4. Dataframe 和 DataSet 的演变与优势"></a>4. Dataframe 和 DataSet 的演变与优势</h2><p>Spark SQL 的 DataFrame 相比 R 语言或 Python 的 DataFrame，有更丰富的隐藏后台优化。 条吗可以从文件、Pandas DataFrame、 Hive 中的表、像 MySQL 这样的外部数据库或者 RDD 中创建。 Scala、Java、Python、R 都支持 DataFrame API。 虽然 DataFrame 提供了关系操作和更高的性能，但缺乏类型安全性，这会导致运行时错误。</p><p>在 1.6 版本， DataSet 和 DataFrame 还是单独的类。在 2.0 版本中， DataSet 和 DataFrame API 被统一起来，为开发人员提供单一的 API。 DataFrame 时一个特定的 DataSet[T]，其中 T = Row 的类型， 因此 DataFrame 和 DataSet 共享相同的方法。</p><p>DataSet API 支持 Scala 和 Java，还不支持 Python 和 R。 但是， DataSet API 的许多优点已经天然在 Python 和 R 语言中存在了。 DataFrame 则支持所有这四种语言。</p><h3 id="4-1-为什么使用-DataFrame-和-DataSet"><a href="#4-1-为什么使用-DataFrame-和-DataSet" class="headerlink" title="4.1 为什么使用 DataFrame 和 DataSet"></a>4.1 为什么使用 DataFrame 和 DataSet</h3><p>Spark 要对闭包进行计算、将其序列话，并将他们发送到执行进程。这意味着代码是以原始形式发送的，没有经过优化。 在 RDD 中无法表示结构化数据，对 RDD 进行查询也不可行。使用 RDD 很容易但有时候处理远足会把代码弄乱。与 Hadoop 的世界相比，纯手写的 MapReduce 作业可能比 Hive 和 Pig 作业慢，因为自爱 Hive 和 Pig 下会进行隐含的优化。 DataFrame 也可以用类似的方式看待。</p><p>因此，为什么要使用 DataFrame 和 DataSet，简单的答案是：<strong>速度和易用</strong>！</p><p>DataFrame 提供了优化、速度、自动模式发现、使用多中数据源和多语言的支持。</p><h3 id="4-2-优化"><a href="#4-2-优化" class="headerlink" title="4.2 优化"></a>4.2 优化</h3><p>Catalyst 为 DataFrame 提供了两种优化方式：</p><ol><li>谓词下推到数据源，只读取需要的数据</li><li>创建用于执行的物理计划，并生成比手写代码更优化的 JVM 字节码。</li></ol><p>和 RDD 不同， DataFrame 并不定义 DAG 图。 它会创建抽象语法书， Catalyst 引擎会使用基于规则和机遇开销的优化方法对其进行解析、检查和改进。<br><img src="media/15261969377501.jpg" alt=""></p><h3 id="4-3-速度"><a href="#4-3-速度" class="headerlink" title="4.3 速度"></a>4.3 速度</h3><p><img src="media/15261970734054.jpg" alt=""><br>上图展示了在单个机器上对1000万个证书对的 groupby 聚合操作。 Scala 和 Python 的 DataFrame 操作具有类似的执行时间，因为条吗都被便衣为相同的 JVM 字节码用来执行。</p><p>DataSet 使用优化的编译器把对象进行序列话和反序列化，以便进行处理并通过网络传输。 这些编码器比 Java 或 Kryo 序列话具有更高的性能。</p><p><img src="media/15261972316614.jpg" alt=""></p><h3 id="4-4-自动模式发现"><a href="#4-4-自动模式发现" class="headerlink" title="4.4 自动模式发现"></a>4.4 自动模式发现</h3><p>要从 RDD 创建 DataFrame，必须提供一个模式。而从 JSON、Parquet 和 ORC 文件创建 DataFrame 时，会自动发现一个模式，包括分区的发现。 Data Sources API 框架让这种情况成为可能。</p><h3 id="4-5-多数据源，多编程语言"><a href="#4-5-多数据源，多编程语言" class="headerlink" title="4.5 多数据源，多编程语言"></a>4.5 多数据源，多编程语言</h3><p>DataFrame API 支持按照最常用的格式（JSON、Parquet、ORC 和 Hive 表）进行读写</p><p>DataFrame API 支持本地文件系统、HDFS、S3和采用 JDBC 协议的外部 RDBMS 数据库读取数据</p><p>DataFrame API 支持第三方扩展：Avro、CSV、XML、HBase、ElasticSearch、Cassandra等。（<a href="http://spark-packages.org" target="_blank" rel="noopener">http://spark-packages.org</a>) 提供了第三方组件包的完整列表</p><p>Spark SQL 可以在 Java、Scala、Python、R中实现。利用 Spark SQL 的分布式 SQL 引擎，我们也可以编写纯 SQL 语句。</p><h3 id="4-6-RDD-和其他-API-的互操作性"><a href="#4-6-RDD-和其他-API-的互操作性" class="headerlink" title="4.6 RDD 和其他 API 的互操作性"></a>4.6 RDD 和其他 API 的互操作性</h3><p>Dataframe 可以使用 <strong>.rdd .toDF .toPandas .toDS</strong> 方法转换为 RDD 和 Pandas Dataframe。此外 DataFrame 可以与 Spark Streaming 和机器学习组件库配合使用。</p><h3 id="4-7-仅选择和读取必要的数据"><a href="#4-7-仅选择和读取必要的数据" class="headerlink" title="4.7 仅选择和读取必要的数据"></a>4.7 仅选择和读取必要的数据</h3><p>DataFrame API 、 DataSet API 和 DataSources API 的一个优点是通过将谓词下推到数据源系统，从而提供更丰富的优化手段。 <strong>列修剪、谓词下推、分区修剪</strong>会有这些框架自动完成。 这样，只有需要用到的数据才会读取和处理。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark-SQL-基础框架和核心组件&quot;&gt;&lt;a href=&quot;#Spark-SQL-基础框架和核心组件&quot; class=&quot;headerlink&quot; title=&quot;Spark SQL 基础框架和核心组件&quot;&gt;&lt;/a&gt;Spark SQL 基
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark ML - 聚类算法</title>
    <link href="http://www.ihoge.cn/2018/ML2.html"/>
    <id>http://www.ihoge.cn/2018/ML2.html</id>
    <published>2018-05-05T18:00:21.000Z</published>
    <updated>2018-05-28T07:52:01.154Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="Spark-ML-聚类算法"><a href="#Spark-ML-聚类算法" class="headerlink" title="Spark ML - 聚类算法"></a>Spark ML - 聚类算法</h2><h3 id="1-KMeans快速聚类"><a href="#1-KMeans快速聚类" class="headerlink" title="1.KMeans快速聚类"></a>1.KMeans快速聚类</h3><p>首先到UR需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">KMeans</span>,<span class="type">KMeansModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>​    在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(</span><br><span class="line">    line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib版的教程类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    在得到数据后，我们即可通过ML包的固有流程：创建<code>Estimator</code>并调用其<code>fit()</code>方法来生成相应的<code>Transformer</code>对象，很显然，在这里<code>KMeans</code>类是<code>Estimator</code>，而用于保存训练后模型的<code>KMeansModel</code>类则属于<code>Transformer</code>：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kmeansmodel = <span class="keyword">new</span> <span class="type">KMeans</span>().</span><br><span class="line">      setK(<span class="number">3</span>).</span><br><span class="line">      setFeaturesCol(<span class="string">"features"</span>).</span><br><span class="line">      setPredictionCol(<span class="string">"prediction"</span>).</span><br><span class="line">      fit(df)</span><br></pre></td></tr></table></figure><p>​    与MLlib版本类似，ML包下的KMeans方法也有<code>Seed</code>（随机数种子）、<code>Tol</code>（收敛阈值）、<code>K</code>（簇个数）、<code>MaxIter</code>（最大迭代次数）、<code>initMode</code>（初始化方式）、<code>initStep</code>（KMeans||方法的步数）等参数可供设置，和其他的ML框架算法一样，用户可以通过相应的<code>setXXX()</code>方法来进行设置，或以<code>ParamMap</code>的形式传入参数，这里为了简介期间，使用<code>setXXX()</code>方法设置了参数K，其余参数均采用默认值。</p><p>​    与MLlib中的实现不同，<code>KMeansModel</code>作为一个<code>Transformer</code>，不再提供<code>predict()</code>样式的方法，而是提供了一致性的<code>transform()</code>方法，用于将存储在<code>DataFrame</code>中的给定数据集进行整体处理，生成带有预测簇标签的数据集：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> results = kmeansmodel.transform(df)</span><br></pre></td></tr></table></figure><p>​    为了方便观察，我们可以使用<code>collect()</code>方法，该方法将<code>DataFrame</code>中所有的数据组织成一个<code>Array</code>对象进行返回：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results.collect().foreach(</span><br><span class="line">      row =&gt; &#123;</span><br><span class="line">        println( row(<span class="number">0</span>) + <span class="string">" is predicted as cluster "</span> + row(<span class="number">1</span>))</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>也可以通过<code>KMeansModel</code>类自带的<code>clusterCenters</code>属性获取到模型的所有聚类中心情况：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.clusterCenters.foreach(</span><br><span class="line">      center =&gt; &#123;</span><br><span class="line">        println(<span class="string">"Clustering Center:"</span>+center)</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>​    与MLlib下的实现相同，<code>KMeansModel</code>类也提供了计算 <strong>集合内误差平方和（Within Set Sum of Squared Error, WSSSE)</strong> 的方法来度量聚类的有效性，在真实K值未知的情况下，该值的变化可以作为选取合适K值的一个重要参考：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.computeCost(df)</span><br></pre></td></tr></table></figure><h3 id="2-高斯混合模型-GMM-聚类算法"><a href="#2-高斯混合模型-GMM-聚类算法" class="headerlink" title="2.高斯混合模型(GMM)聚类算法"></a>2.高斯混合模型(GMM)聚类算法</h3><h4 id="2-1-基本原理"><a href="#2-1-基本原理" class="headerlink" title="2.1 基本原理"></a>2.1 基本原理</h4><p>​    <strong>高斯混合模型（Gaussian Mixture Model, GMM）</strong> 是一种概率式的聚类方法，属于生成式模型，它假设所有的数据样本都是由某一个给定参数的 <strong>多元高斯分布</strong> 所生成的。具体地，给定类个数<code>K</code>，对于给定样本空间中的样本 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-137bed5220372f1cad4f3cdf4529593e_l3.svg" alt="img"></p><p>，一个高斯混合模型的概率密度函数可以由K个多元高斯分布组合成的混合分布表示：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1d75e089a9a823703aa88d08ad53936e_l3.svg" alt="img"></p><p>其中，</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-a0f35b7e777b0ecf33b511cfb3174001_l3.svg" alt="img"></p><p>是以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>为均值向量， </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>为协方差矩阵的多元高斯分布的概率密度函数，可以看出，高斯混合模型由K个不同的多元高斯分布共同组成，每一个分布被称为高斯混合模型中的一个 <strong>成分(Component)</strong>， 而</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1a47f13ac9a37fcb6911a1b8e17cbb35_l3.svg" alt="img"></p><p>为第<code>i</code>个多元高斯分布在混合模型中的 <strong>权重</strong> ，且有 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-9312fca34e9b3afde8787a29c58fe192_l3.svg" alt="img"></p><p>。</p><p>假设已有一个存在的高斯混合模型，那么，样本空间中的样本的生成过程即是：以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-af3d8faef7e634b7b15f83cb1606b714_l3.svg" alt="img"></p><p>作为概率（实际上，权重可以直观理解成相应成分产生的样本占总样本的比例），选择出一个混合成分，根据该混合成分的概率密度函数，采样产生出相应的样本。</p><p>那么，利用GMM进行聚类的过程是利用GMM生成数据样本的“逆过程”：给定聚类簇数<code>K</code>，通过给定的数据集，以某一种 <strong>参数估计</strong> 的方法，推导出每一个混合成分的参数（即均值向量 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>、协方差矩阵 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>和权重 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-356e473b3185b432024c4643855f1b9d_l3.svg" alt="img"></p><p>），每一个多元高斯分布成分即对应于聚类后的一个簇。高斯混合模型在训练时使用了极大似然估计法，最大化以下对数似然函数：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-c006bbb984e97b258cf6bcc1d62ee2d7_l3.svg" alt="img"></p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-f57e2abf78c1ba038d4969a8fc513e7a_l3.svg" alt="img"></p><p>显然，该优化式无法直接通过解析方式求得解，故可采用 <strong>期望-最大化(Expectation-Maximization, EM)</strong> 方法求解，具体过程如下（为了简洁，这里省去了具体的数学表达式，详细可见<a href="https://en.wikipedia.org/wiki/Mixture_model" target="_blank" rel="noopener">wikipedia</a>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.根据给定的K值，初始化K个多元高斯分布以及其权重；</span><br><span class="line">2.根据贝叶斯定理，估计每个样本由每个成分生成的后验概率；(EM方法中的E步)</span><br><span class="line">3.根据均值，协方差的定义以及2步求出的后验概率，更新均值向量、协方差矩阵和权重；（EM方法的M步）</span><br><span class="line">重复2~3步，直到似然函数增加值已小于收敛阈值，或达到最大迭代次数</span><br></pre></td></tr></table></figure><p>​    当参数估计过程完成后，对于每一个样本点，根据贝叶斯定理计算出其属于每一个簇的后验概率，并将样本划分到后验概率最大的簇上去。相对于KMeans等直接给出样本点的簇划分的聚类方法，GMM这种给出样本点属于每个簇的概率的聚类方法，被称为 <strong>软聚类(Soft Clustering / Soft Assignment)</strong> 。</p><h4 id="2-2-模型的训练与分析"><a href="#2-2-模型的训练与分析" class="headerlink" title="2.2 模型的训练与分析"></a>2.2 模型的训练与分析</h4><p>​    Spark的ML库提供的高斯混合模型都在<code>org.apache.spark.ml.clustering</code>包下，和其他的聚类方法类似，其具体实现分为两个类：用于抽象GMM的超参数并进行训练的<code>GaussianMixture</code>类（<code>Estimator</code>）和训练后的模型<code>GaussianMixtureModel</code>类（<code>Transformer</code>），在使用前，引入需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">GaussianMixture</span>,<span class="type">GaussianMixtureModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    我们仍采用Iris数据集进行实验。为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib的操作类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    可以通过创建一个<code>GaussianMixture</code>类，设置相应的超参数，并调用<code>fit(..)</code>方法来训练一个GMM模型<code>GaussianMixtureModel</code>，在该方法调用前需要设置一系列超参数，如下表所示：</p><ul><li>K:聚类数目，默认为2 </li><li>maxIter : 最大迭代次数，默认为100 </li><li>seed : 随机数种子，默认为随机Long值 </li><li>Tol : 对数似然函数收敛阈值，默认为0.01 </li></ul><p>其中，每一个超参数均可通过名为<code>setXXX(...)</code>（如maxIterations即为<code>setMaxIterations()</code>）的方法进行设置。这里，我们建立一个简单的<code>GaussianMixture</code>对象，设定其聚类数目为3，其他参数取默认值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> gm = <span class="keyword">new</span> <span class="type">GaussianMixture</span>().setK(<span class="number">3</span>)</span><br><span class="line">               .setPredictionCol(<span class="string">"Prediction"</span>)</span><br><span class="line">               .setProbabilityCol(<span class="string">"Probability"</span>)</span><br><span class="line"><span class="keyword">val</span> gmm = gm.fit(df)</span><br></pre></td></tr></table></figure><p>和<code>KMeans</code>等硬聚类方法不同的是，除了可以得到对样本的聚簇归属预测外，还可以得到样本属于各个聚簇的概率（这里我们存在”Probability”列中）。</p><p>​    调用<code>transform()</code>方法处理数据集之后，打印数据集，可以看到每一个样本的预测簇以及其概率分布向量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = gmm.transform(df)</span><br><span class="line">result.show(<span class="number">150</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>​    得到模型后，即可查看模型的相关参数，与KMeans方法不同，GMM不直接给出聚类中心，而是给出各个混合成分（多元高斯分布）的参数。在ML的实现中，GMM的每一个混合成分都使用一个<code>MultivariateGaussian</code>类（位于<code>org.apache.spark.ml.stat.distribution</code>包）来存储，我们可以使用<code>GaussianMixtureModel</code>类的<code>weights</code>成员获取到各个混合成分的权重，使用<code>gaussians</code>成员来获取到各个混合成分的参数（均值向量和协方差矩阵）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until gmm.getK) &#123;</span><br><span class="line">      println(<span class="string">"Component %d : weight is %f \n mu vector is %s \n sigma matrix is %s"</span> format</span><br><span class="line">      (i, gmm.weights(i), gmm.gaussians(i).mean, gmm.gaussians(i).cov))</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;Spark-ML-聚类算法&quot;&gt;&lt;a href=&quot;#Spark-ML-聚类算法&quot; class=&quot;headerlink&quot; title=&quot;Spark ML - 聚类算法&quot;&gt;&lt;/a&gt;Spark ML - 聚类算法&lt;/h2&gt;&lt;h3 id=
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML - 协同过滤</title>
    <link href="http://www.ihoge.cn/2018/ML1.html"/>
    <id>http://www.ihoge.cn/2018/ML1.html</id>
    <published>2018-05-05T17:59:21.000Z</published>
    <updated>2018-05-28T07:53:22.257Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>​    获取spark自带的MovieLens数据集，其中每行包含一个用户、一个电影、一个该用户对该电影的评分以及时间戳。我们使用默认的<code>ALS.train()</code> 方法，即显性反馈（默认<code>implicitPrefs</code> 为false）来构建推荐模型并根据模型对评分预测的均方根误差来对模型进行评估。</p><h3 id="导入需要的包："><a href="#导入需要的包：" class="headerlink" title="导入需要的包："></a>导入需要的包：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">RegressionEvaluator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.recommendation.<span class="type">ALS</span></span><br></pre></td></tr></table></figure><h3 id="根据数据结构创建读取规范"><a href="#根据数据结构创建读取规范" class="headerlink" title="根据数据结构创建读取规范"></a>根据数据结构创建读取规范</h3><p> 创建一个Rating类型，即[Int, Int, Float, Long];然后建造一个把数据中每一行转化成Rating类的函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Rating</span>(<span class="params">userId: <span class="type">Int</span>, movieId: <span class="type">Int</span>, rating: <span class="type">Float</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">parseRating</span>(<span class="params">str: <span class="type">String</span></span>)</span>: <span class="type">Rating</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> fields = str.split(<span class="string">"::"</span>)</span><br><span class="line">            assert(fields.size == <span class="number">4</span>)</span><br><span class="line">            <span class="type">Rating</span>(fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>).toInt, fields(<span class="number">2</span>).toFloat, fields(<span class="number">3</span>).toLong)</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure><h3 id="读取数据："><a href="#读取数据：" class="headerlink" title="读取数据："></a>读取数据：</h3><p> 导入implicits，读取MovieLens数据集，把数据转化成Rating类型；</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> ratings = spark.sparkContext.textFile(<span class="string">"file:///home/hduser/spark/data/mllib/als/sample_movielens_ratings.txt"</span>).map(parseRating).toDF()</span><br></pre></td></tr></table></figure><p>然后打印数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ratings.show()</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p> 把MovieLens数据集划分训练集和测试集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(training, test) = ratings.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>))</span><br></pre></td></tr></table></figure><p> 使用ALS来建立推荐模型，这里我们构建了两个模型，一个是显性反馈，一个是隐性反馈</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alsExplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setUserCol(<span class="string">"userId"</span>). setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br><span class="line"><span class="keyword">val</span> alsImplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setImplicitPrefs(<span class="literal">true</span>). setUserCol(<span class="string">"userId"</span>).setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br></pre></td></tr></table></figure><p> 在 ML 中的实现有如下的参数:</p><ul><li><code>numBlocks</code> 是用于并行化计算的用户和商品的分块个数 (默认为10)。</li><li><code>rank</code> 是模型中隐语义因子的个数（默认为10）。</li><li><code>maxIter</code> 是迭代的次数（默认为10）。</li><li><code>regParam</code> 是ALS的正则化参数（默认为1.0）。</li><li><code>implicitPrefs</code> 决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本（默认是false，即用显性反馈）。</li><li><code>alpha</code> 是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准（默认为1.0）。</li><li><p><code>nonnegative</code> 决定是否对最小二乘法使用非负的限制（默认为false）。</p><p>可以调整这些参数，不断优化结果，使均方差变小。比如：imaxIter越大，regParam越 小，均方差会越小，推荐结果较优。</p></li></ul><p>接下来，把推荐模型放在训练数据上训练：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> modelExplicit = alsExplicit.fit(training)</span><br><span class="line"><span class="keyword">val</span> modelImplicit = alsImplicit.fit(training)</span><br></pre></td></tr></table></figure><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p> 使用训练好的推荐模型对测试集中的用户商品进行预测评分，得到预测评分的数据集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> predictionsExplicit = modelExplicit.transform(test)</span><br><span class="line"><span class="keyword">val</span> predictionsImplicit = modelImplicit.transform(test)</span><br></pre></td></tr></table></figure><p> 我们把结果输出，对比一下真实结果与预测结果：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictionsExplicit.show()</span><br><span class="line">predictionsImplicit.show()</span><br></pre></td></tr></table></figure><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p> 通过计算模型的均方根误差来对模型进行评估，均方根误差越小，模型越准确：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">RegressionEvaluator</span>().setMetricName(<span class="string">"rmse"</span>).setLabelCol(<span class="string">"rating"</span>). setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line"><span class="keyword">val</span> rmseExplicit = evaluator.evaluate(predictionsExplicit)</span><br><span class="line"><span class="keyword">val</span> rmseImplicit = evaluator.evaluate(predictionsImplicit)</span><br></pre></td></tr></table></figure><p> 打印出两个模型的均方根误差 ：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">s"Explicit:Root-mean-square error = <span class="subst">$rmseExplicit</span>"</span>)</span><br><span class="line">println(<span class="string">s"Implicit:Root-mean-square error = <span class="subst">$rmseImplicit</span>"</span>)</span><br></pre></td></tr></table></figure><p> 可以看到打分的均方差值为1.69和1.80左右。由于本例的数据量很少，预测的结果和实际相比有一定的差距。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;协同过滤算法&quot;&gt;&lt;a href=&quot;#协同过滤算法&quot; class=&quot;headerlink&quot; title=&quot;协同过滤算法&quot;&gt;&lt;/a&gt;协同过滤算法&lt;/h2&gt;&lt;p&gt;​    获取spark自带的MovieLens数据集，其中每行包含一个
      
    
    </summary>
    
      <category term="机器学习" scheme="http://www.ihoge.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理3</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing3.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing3.html</id>
    <published>2018-04-21T19:59:31.000Z</published>
    <updated>2018-04-22T14:55:12.786Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="数据预处理3"><a href="#数据预处理3" class="headerlink" title="数据预处理3"></a>数据预处理3</h1><h2 id="3-1-描述性统计"><a href="#3-1-描述性统计" class="headerlink" title="3.1 描述性统计"></a>3.1 描述性统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    (<span class="string">'custID'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'gender'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'state'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'cardholder'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'balance'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numIntTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'creditLine'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'fraudRisk'</span>, typ.IntegerType()),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([</span><br><span class="line">    typ.StructField(e[<span class="number">0</span>], e[<span class="number">1</span>], <span class="keyword">True</span>) <span class="keyword">for</span> e <span class="keyword">in</span> fields</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: integer (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.groupBy(<span class="string">'gender'</span>).count().show()</span><br></pre></td></tr></table></figure><pre><code>+------+-------+|gender|  count|+------+-------+|     1|6178231||     2|3821769|+------+-------+</code></pre><p>⚠️ 这里男女比例失衡，在实际生产场景中应该重视这个问题</p><h3 id="descrive"><a href="#descrive" class="headerlink" title=".descrive()"></a>.descrive()</h3><p>这里可以选定某些列   <code>Dataframe的.describe()方法属于，RDD不可用</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line">desc = fraud_df.describe(numerical)</span><br><span class="line">desc.show()</span><br></pre></td></tr></table></figure><pre><code>+-------+-----------------+------------------+-----------------+|summary|          balance|          numTrans|      numIntTrans|+-------+-----------------+------------------+-----------------+|  count|         10000000|          10000000|         10000000||   mean|     4109.9199193|        28.9351871|        4.0471899|| stddev|3996.847309737077|26.553781024522852|8.602970115863767||    min|                0|                 0|                0||    max|            41485|               100|               60|+-------+-----------------+------------------+-----------------+</code></pre><ul><li>所有特征成正态分布；最大值是最小值的多倍</li><li>变异系数（均值与标准差之比）非常高（接近或大于1），意味着这是一个广泛的观测数据。</li></ul><p>如何检查偏度？（这里对“平衡”特征进行检查）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.agg(&#123;<span class="string">'balance'</span>:<span class="string">'skewness'</span>&#125;).show()</span><br></pre></td></tr></table></figure><pre><code>+------------------+| skewness(balance)|+------------------+|1.1818315552995033|+------------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.agg(&#123;<span class="string">'balance'</span>:<span class="string">'stddev'</span>&#125;).show()</span><br></pre></td></tr></table></figure><pre><code>+-----------------+|  stddev(balance)|+-----------------+|3996.847309737077|+-----------------+</code></pre><p>列表中的聚合函数包括：</p><ul><li>avg()</li><li>count()</li><li>countDistinct()</li><li>first()</li><li>kurtosis()</li><li>max()</li><li>mean()</li><li>skewness()</li><li>stddev()</li><li>stddev_pop()</li><li>stddev_samp()</li><li>sum()</li><li>sunDistinct()</li><li>var_pop()</li><li>var_samp()</li><li>variance()</li></ul><h2 id="3-2-相关性"><a href="#3-2-相关性" class="headerlink" title="3.2 相关性"></a>3.2 相关性</h2><p>模型应该只包括哪些与目标高度相关的特征，因此检查特征的相关性是非常重要的。其次之间的高度相关特征（即<strong>共线（collinear）性</strong>），可能会导致模型的不可预知性为或者可能进行不必要的复杂化。</p><p>这里的<strong>.corr(…)</strong>方法支持Pearson（皮尔森）相关系数，并且只能计算两两相关性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.corr(<span class="string">'balance'</span>, <span class="string">'numTrans'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.00044523140172659576</code></pre><p>为了创建相关矩阵，可以使用以下脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line"></span><br><span class="line">corr=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(numerical)):</span><br><span class="line">    temp = [<span class="keyword">None</span>] * i</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i, len(numerical)):</span><br><span class="line">        temp.append(fraud_df.corr(numerical[i], numerical[j]))</span><br><span class="line">    corr.append(temp)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 这里令temp = [None] * i是为了填充左下角矩阵</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">corrpd = pd.DataFrame(corr)</span><br><span class="line">corrpd.columns = corrpd.index = numerical</span><br><span class="line">corrpd</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>balance</th><br>      <th>numTrans</th><br>      <th>numIntTrans</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>balance</th><br>      <td>1.0</td><br>      <td>0.000445</td><br>      <td>0.000271</td><br>    </tr><br>    <tr><br>      <th>numTrans</th><br>      <td>NaN</td><br>      <td>1.000000</td><br>      <td>-0.000281</td><br>    </tr><br>    <tr><br>      <th>numIntTrans</th><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>1.000000</td><br>    </tr><br>  </tbody><br></table><br></div><h2 id="3-3-可视化"><a href="#3-3-可视化" class="headerlink" title="3.3 可视化"></a>3.3 可视化</h2><h3 id="3-3-1-直方图"><a href="#3-3-1-直方图" class="headerlink" title="3.3.1 直方图"></a>3.3.1 直方图</h3><p>当数据量非常大时，需要先对数据进行聚合:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hists = fraud_df.select(<span class="string">'balance'</span>).rdd.flatMap(<span class="keyword">lambda</span> x: x).histogram(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'bins'</span>:hists[<span class="number">0</span>][:<span class="number">-1</span>],</span><br><span class="line">    <span class="string">'freq'</span>:hists[<span class="number">1</span>]</span><br><span class="line">&#125;</span><br><span class="line">plt.bar(data[<span class="string">'bins'</span>], data[<span class="string">'freq'</span>], width=<span class="number">2000</span>)</span><br><span class="line">plt.title(<span class="string">'Histogram of \'balance\''</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqkvrtx0f5j31000g2q41.jpg" alt=""></p><h3 id="3-3-2-散点图"><a href="#3-3-2-散点图" class="headerlink" title="3.3.2 散点图"></a>3.3.2 散点图</h3><p>因为Pyspark不支持在服务器端的任何可视化模块，并且视图同时绘制数十亿的观测数据是非常不切实际的。在这个例子中，把欺诈数据集作为一个阶层抽取0.02%大约2000个观测数据。</p><p>使用<strong>df.sampleBy(…)</strong>方法随机抽取样本集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line"></span><br><span class="line">data_sample = fraud_df.sampleBy(<span class="string">'gender'</span>, &#123;<span class="number">1</span>: <span class="number">0.001</span>, <span class="number">2</span>: <span class="number">0.001</span>&#125;).select(numerical)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_sample.describe().show()</span><br></pre></td></tr></table></figure><pre><code>+-------+------------------+------------------+-----------------+|summary|           balance|          numTrans|      numIntTrans|+-------+------------------+------------------+-----------------+|  count|              2032|              2032|             2032||   mean| 3962.410433070866|28.546751968503937|3.989665354330709|| stddev|3876.8504670568145|25.611704520599815|8.235480127222067||    min|                 0|                 0|                0||    max|             28000|               100|               60|+-------+------------------+------------------+-----------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data_sample.rdd.map(<span class="keyword">lambda</span> x: [x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]]).collect()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Scatter3D</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># data = [d1,d3,d2]</span></span><br><span class="line">range_color = [</span><br><span class="line">    <span class="string">'#313695'</span>, <span class="string">'#4575b4'</span>, <span class="string">'#74add1'</span>, <span class="string">'#abd9e9'</span>, <span class="string">'#e0f3f8'</span>, <span class="string">'#ffffbf'</span>,</span><br><span class="line">    <span class="string">'#fee090'</span>, <span class="string">'#fdae61'</span>, <span class="string">'#f46d43'</span>, <span class="string">'#d73027'</span>, <span class="string">'#a50026'</span>]</span><br><span class="line">scatter3D = Scatter3D(<span class="string">"3D 散点图示例"</span>, width=<span class="number">1200</span>, height=<span class="number">800</span>)</span><br><span class="line">scatter3D.add(<span class="string">"3D"</span>, </span><br><span class="line">              data, is_visualmap=<span class="keyword">True</span>, </span><br><span class="line">              visual_range_color=range_color, </span><br><span class="line">              grid3d_opacity=<span class="number">0.5</span>,</span><br><span class="line">              is_grid3d_rotate = <span class="keyword">True</span>,</span><br><span class="line">              xaxis3d_name=<span class="string">'balance'</span>,</span><br><span class="line">              yaxis3d_name=<span class="string">'numTrans'</span>,</span><br><span class="line">              zaxis3d_name=<span class="string">'numIntTrans'</span></span><br><span class="line">             )</span><br><span class="line">scatter3D.render()</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqkvm4tn42j31kw0szk4z.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;数据预处理3&quot;&gt;&lt;a href=&quot;#数据预处理3&quot; class=&quot;headerlink&quot; title=&quot;数据预处理3&quot;&gt;&lt;/a&gt;数据预处理3&lt;/h1&gt;&lt;h2 id=&quot;3-1-描述性统计&quot;&gt;&lt;a href=&quot;#3-1-描述性统计&quot;
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理2</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing2.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing2.html</id>
    <published>2018-04-21T19:59:21.000Z</published>
    <updated>2018-05-17T12:15:10.227Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="数据预处理2"><a href="#数据预处理2" class="headerlink" title="数据预处理2"></a>数据预处理2</h1><h2 id="2-1-RDD读取方式"><a href="#2-1-RDD读取方式" class="headerlink" title="2.1 RDD读取方式"></a>2.1 RDD读取方式</h2><p><strong>1、读取文件为RDD</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fileData = sc.textFile(<span class="string">'/pydata/ccFraud.gz'</span>)</span><br><span class="line">header = fileData.first()</span><br><span class="line"></span><br><span class="line">rddData = fileData \</span><br><span class="line">    .filter(<span class="keyword">lambda</span> row: row != header) \</span><br><span class="line">    .map(<span class="keyword">lambda</span> row: [int(elem) <span class="keyword">for</span> elem <span class="keyword">in</span> row.split(<span class="string">','</span>)])</span><br></pre></td></tr></table></figure><p><strong>2、创建schema</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [typ.StructField(h[:], typ.IntegerType(), <span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> header.split(<span class="string">','</span>)]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType(fields)</span><br></pre></td></tr></table></figure><p><strong>3、RDD创建DataFrame</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfData = spark.createDataFrame(fraud, schema)</span><br><span class="line">dfData.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- &quot;custID&quot;: integer (nullable = true) |-- &quot;gender&quot;: integer (nullable = true) |-- &quot;state&quot;: integer (nullable = true) |-- &quot;cardholder&quot;: integer (nullable = true) |-- &quot;balance&quot;: integer (nullable = true) |-- &quot;numTrans&quot;: integer (nullable = true) |-- &quot;numIntlTrans&quot;: integer (nullable = true) |-- &quot;creditLine&quot;: integer (nullable = true) |-- &quot;fraudRisk&quot;: integer (nullable = true)</code></pre><p><strong>4、创建视图</strong></p><p>这里视图sql查询失败，而在spark-shell中没问题，是什么原因？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfData.createOrReplaceTempView(<span class="string">"DataViw"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"select gender,count(*) from DataViw group by gender"</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+-------+|count()|+-------+|      0|+-------+</code></pre><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fqkmuptfisj30qo0f0gnx.jpg" alt=""></p><h2 id="2-2-DataFrame读取模式"><a href="#2-2-DataFrame读取模式" class="headerlink" title="2.2 DataFrame读取模式"></a>2.2 DataFrame读取模式</h2><p><strong>创建类型方式一</strong>：全部为IntegerType</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    <span class="string">'custID'</span>,</span><br><span class="line">    <span class="string">'gender'</span>,</span><br><span class="line">    <span class="string">'state'</span>,</span><br><span class="line">    <span class="string">'cardholder'</span>,</span><br><span class="line">    <span class="string">'balance'</span>,</span><br><span class="line">    <span class="string">'numTrans'</span>,</span><br><span class="line">    <span class="string">'numIntlTrans'</span>,</span><br><span class="line">    <span class="string">'creditLine'</span>,</span><br><span class="line">    <span class="string">'fraudRisk'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([typ.StructField(f, typ.IntegerType(), <span class="keyword">True</span>) <span class="keyword">for</span> f <span class="keyword">in</span> fields])</span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: integer (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntlTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><p><strong>创建类型方式二</strong>: 指定每一列的类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    (<span class="string">'custID'</span>, typ.StringType()),</span><br><span class="line">    (<span class="string">'gender'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'state'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'cardholder'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'balance'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numIntlTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'creditLine'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'fraudRisk'</span>, typ.IntegerType()),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([</span><br><span class="line">    typ.StructField(e[<span class="number">0</span>], e[<span class="number">1</span>], <span class="keyword">True</span>) <span class="keyword">for</span> e <span class="keyword">in</span> fields</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: string (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntlTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>+------+------+-----+----------+-------+--------+------------+----------+---------+|custID|gender|state|cardholder|balance|numTrans|numIntlTrans|creditLine|fraudRisk|+------+------+-----+----------+-------+--------+------------+----------+---------+|     1|     1|   35|         1|   3000|       4|          14|         2|        0||     2|     2|    2|         1|      0|       9|           0|        18|        0||     3|     2|    2|         1|      0|      27|           9|        16|        0||     4|     1|   15|         1|      0|      12|           0|         5|        0||     5|     1|   46|         1|      0|      11|          16|         7|        0||     6|     2|   44|         2|   5546|      21|           0|        13|        0||     7|     1|    3|         1|   2000|      41|           0|         1|        0||     8|     1|   10|         1|   6016|      20|           3|         6|        0||     9|     2|   32|         1|   2428|       4|          10|        22|        0||    10|     1|   23|         1|      0|      18|          56|         5|        0|+------+------+-----+----------+-------+--------+------------+----------+---------+only showing top 10 rows</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;数据预处理2&quot;&gt;&lt;a href=&quot;#数据预处理2&quot; class=&quot;headerlink&quot; title=&quot;数据预处理2&quot;&gt;&lt;/a&gt;数据预处理2&lt;/h1&gt;&lt;h2 id=&quot;2-1-RDD读取方式&quot;&gt;&lt;a href=&quot;#2-1-RDD读
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理1</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing1.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing1.html</id>
    <published>2018-04-21T19:59:00.000Z</published>
    <updated>2018-04-22T14:55:13.475Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Pyspark数据预处理1"><a href="#Pyspark数据预处理1" class="headerlink" title="Pyspark数据预处理1"></a>Pyspark数据预处理1</h1><h2 id="1-1-重复值"><a href="#1-1-重复值" class="headerlink" title="1.1 重复值"></a>1.1 重复值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">'M'</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>, <span class="string">'gender'</span>])</span><br></pre></td></tr></table></figure><h3 id="1-1-1-查重"><a href="#1-1-1-查重" class="headerlink" title="1.1.1 查重"></a>1.1.1 <strong>查重</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"总数据量：&#123;&#125;"</span>.format(df.count()))</span><br><span class="line">print(<span class="string">"重复数据量：&#123;&#125;"</span>.format(df.count() - df.distinct().count()))</span><br></pre></td></tr></table></figure><pre><code>总数据量：7重复数据量：1</code></pre><h3 id="1-1-2-去重"><a href="#1-1-2-去重" class="headerlink" title="1.1.2  去重"></a>1.1.2  <strong>去重</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = df.dropDuplicates()</span><br><span class="line">df1.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+| id|weight|height|age|gender|+---+------+------+---+------+|  5| 133.2|   5.7| 54|     F||  5| 129.2|   5.3| 42|     M||  1| 144.5|   5.9| 33|     M||  4| 144.5|   5.9| 33|     M||  2| 167.2|   5.4| 45|     M||  3| 124.1|   5.2| 23|     F|+---+------+------+---+------+</code></pre><h3 id="1-1-3-查询排除ID列后的重复值"><a href="#1-1-3-查询排除ID列后的重复值" class="headerlink" title="1.1.3  查询排除ID列后的重复值"></a>1.1.3  <strong>查询排除ID列后的重复值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"df1中样本量：&#123;&#125;"</span>.format(df1.count()))</span><br><span class="line">print(<span class="string">"df1去重后样本量：&#123;&#125;"</span>\</span><br><span class="line">      .format(</span><br><span class="line">      df1.select([c <span class="keyword">for</span> c <span class="keyword">in</span> df1.columns <span class="keyword">if</span> c != <span class="string">'id'</span>])</span><br><span class="line">      .distinct()</span><br><span class="line">      .count()))</span><br></pre></td></tr></table></figure><pre><code>df1中样本量：6df1去重后样本量：5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2 = df1.dropDuplicates(subset=[c <span class="keyword">for</span> c <span class="keyword">in</span> df.columns <span class="keyword">if</span> c != <span class="string">'id'</span>])</span><br><span class="line">df2.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+| id|weight|height|age|gender|+---+------+------+---+------+|  5| 133.2|   5.7| 54|     F||  4| 144.5|   5.9| 33|     M||  2| 167.2|   5.4| 45|     M||  3| 124.1|   5.2| 23|     F||  5| 129.2|   5.3| 42|     M|+---+------+------+---+------+</code></pre><h2 id="1-1-4-查看是否有重复ID"><a href="#1-1-4-查看是否有重复ID" class="headerlink" title="1.1.4 查看是否有重复ID"></a>1.1.4 查看是否有重复ID</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line">df2.agg(</span><br><span class="line">    fn.count(<span class="string">'id'</span>).alias(<span class="string">'count'</span>),</span><br><span class="line">    fn.countDistinct(<span class="string">'id'</span>).alias(<span class="string">'distinct'</span>))\</span><br><span class="line">    .show()</span><br></pre></td></tr></table></figure><pre><code>+-----+--------+|count|distinct|+-----+--------+|    5|       4|+-----+--------+</code></pre><p>这里需要给每行数据赋唯一ID。通过<code>fn.monotonically_increasing_id()</code>方法给每一条记录提供一个唯一并且递增的ID，当数据放置在大约不到10亿个分区中，每个分区的记录少于8亿条，ID就能被保证时唯一的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df3 = df2.withColumn(<span class="string">'new_id'</span>, fn.monotonically_increasing_id())</span><br><span class="line">df3.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+-------------+| id|weight|height|age|gender|       new_id|+---+------+------+---+------+-------------+|  5| 133.2|   5.7| 54|     F|  25769803776||  1| 144.5|   5.9| 33|     M| 171798691840||  2| 167.2|   5.4| 45|     M| 592705486848||  3| 124.1|   5.2| 23|     F|1236950581248||  5| 129.2|   5.3| 42|     M|1365799600128|+---+------+------+---+------+-------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df3.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- id: long (nullable = true) |-- weight: double (nullable = true) |-- height: double (nullable = true) |-- age: long (nullable = true) |-- gender: string (nullable = true) |-- new_id: long (nullable = false)</code></pre><h2 id="1-2-空缺值"><a href="#1-2-空缺值" class="headerlink" title="1.2. 空缺值"></a>1.2. 空缺值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_miss = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.6</span>, <span class="number">28</span>,   <span class="string">'M'</span>,  <span class="number">100000</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>,   <span class="string">'M'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="keyword">None</span> , <span class="number">5.2</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>,   <span class="string">'M'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>,   <span class="string">'F'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="keyword">None</span>, <span class="string">'F'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>,   <span class="string">'M'</span>,  <span class="number">76000</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>, <span class="string">'gender'</span>, <span class="string">'income'</span>])</span><br></pre></td></tr></table></figure><h3 id="1-2-1-查找缺失值"><a href="#1-2-1-查找缺失值" class="headerlink" title="1.2.1 查找缺失值"></a>1.2.1 查找缺失值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_miss.rdd.map(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x[<span class="string">'id'</span>], sum([c <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">for</span> c <span class="keyword">in</span> x]))</span><br><span class="line">).collect()</span><br></pre></td></tr></table></figure><pre><code>[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_miss.where(<span class="string">'id == 3'</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+------+| id|weight|height| age|gender|income|+---+------+------+----+------+------+|  3|  null|   5.2|null|  null|  null|+---+------+------+----+------+------+</code></pre><h3 id="1-2-2-检查每一列缺失值数据的百分比："><a href="#1-2-2-检查每一列缺失值数据的百分比：" class="headerlink" title="1.2.2 检查每一列缺失值数据的百分比："></a>1.2.2 检查每一列缺失值数据的百分比：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_miss.agg(*[</span><br><span class="line">    (<span class="number">1</span> - (fn.count(c) / fn.count(<span class="string">'*'</span>))).alias(c + <span class="string">'_missing'</span>)</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns</span><br><span class="line">]).show()</span><br></pre></td></tr></table></figure><pre><code>+----------+------------------+--------------+------------------+------------------+------------------+|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|+----------+------------------+--------------+------------------+------------------+------------------+|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|+----------+------------------+--------------+------------------+------------------+------------------+</code></pre><h3 id="1-2-3-移除缺失严重的特征"><a href="#1-2-3-移除缺失严重的特征" class="headerlink" title="1.2.3 移除缺失严重的特征"></a>1.2.3 移除缺失严重的特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一：</span></span><br><span class="line">df_miss1 = df_miss.select([</span><br><span class="line">    c <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns <span class="keyword">if</span> c != <span class="string">'income'</span></span><br><span class="line">])</span><br><span class="line">df_miss1.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+| id|weight|height| age|gender|+---+------+------+----+------+|  1| 143.5|   5.6|  28|     M||  2| 167.2|   5.4|  45|     M||  3|  null|   5.2|null|  null||  4| 144.5|   5.9|  33|     M||  5| 133.2|   5.7|  54|     F||  6| 124.1|   5.2|null|     F||  7| 129.2|   5.3|  42|     M|+---+------+------+----+------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式二： 设定阀值,阀值越大过滤越严格</span></span><br><span class="line">df_miss2 = df_miss.dropna(thresh=<span class="number">3</span>)</span><br><span class="line">df_miss2.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+------+| id|weight|height| age|gender|income|+---+------+------+----+------+------+|  1| 143.5|   5.6|  28|     M|100000||  2| 167.2|   5.4|  45|     M|  null||  4| 144.5|   5.9|  33|     M|  null||  5| 133.2|   5.7|  54|     F|  null||  6| 124.1|   5.2|null|     F|  null||  7| 129.2|   5.3|  42|     M| 76000|+---+------+------+----+------+------+</code></pre><h3 id="1-2-4-填充空缺值-很好用的方法"><a href="#1-2-4-填充空缺值-很好用的方法" class="headerlink" title="1.2.4 填充空缺值(很好用的方法)"></a>1.2.4 填充空缺值(很好用的方法)</h3><p>使用df.fillna()方法并传给它一个字典效率高</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充平均值，先创建平均值字典</span></span><br><span class="line">means = df_miss1.agg(</span><br><span class="line">        *[fn.mean(c).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss1.columns <span class="keyword">if</span> c != <span class="string">'gender'</span>])\</span><br><span class="line">        .toPandas().to_dict(<span class="string">'records'</span>)[<span class="number">0</span>]</span><br><span class="line">means[<span class="string">'gender'</span>] = <span class="string">'missing'</span></span><br><span class="line"></span><br><span class="line">df_miss1.fillna(means).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------------------+------+---+-------+| id|            weight|height|age| gender|+---+------------------+------+---+-------+|  1|             143.5|   5.6| 28|      M||  2|             167.2|   5.4| 45|      M||  3|140.28333333333333|   5.2| 40|missing||  4|             144.5|   5.9| 33|      M||  5|             133.2|   5.7| 54|      F||  6|             124.1|   5.2| 40|      F||  7|             129.2|   5.3| 42|      M|+---+------------------+------+---+-------+</code></pre><h2 id="1-3-离群值"><a href="#1-3-离群值" class="headerlink" title="1.3. 离群值"></a>1.3. 离群值</h2><h3 id="1-3-1-定义离群区间"><a href="#1-3-1-定义离群区间" class="headerlink" title="1.3.1 定义离群区间"></a>1.3.1 定义离群区间</h3><p>IQR：定义为上分位和下分位之差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_outliers = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.3</span>, <span class="number">28</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">154.2</span>, <span class="number">5.5</span>, <span class="number">45</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">342.3</span>, <span class="number">5.1</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.5</span>, <span class="number">33</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.4</span>, <span class="number">54</span>),</span><br><span class="line">        (<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.1</span>, <span class="number">21</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>])</span><br></pre></td></tr></table></figure><p>计算每个特征的上下截断点：<code>.approxQuantitle(...)方法</code></p><ul><li>第一个参数指定列名</li><li>第二个参数可以时0～1之间的任意数（0.5为指定中位数）</li><li>第三个参数指定对每个度量的容忍度（如果为1，就会计算一个度量的准确值）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cols = [<span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>]</span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    quantiles = df_outliers.approxQuantile(col, [<span class="number">0.25</span>, <span class="number">0.75</span>], <span class="number">0.01</span>)</span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [quantiles[<span class="number">0</span>] - <span class="number">0.4</span> * IQR, quantiles[<span class="number">1</span>] + <span class="number">0.4</span> * IQR]</span><br><span class="line">    </span><br><span class="line">bounds</span><br><span class="line"><span class="comment"># 根据实际业务情况进行调整</span></span><br></pre></td></tr></table></figure><pre><code>{&apos;age&apos;: [17.6, 64.4], &apos;height&apos;: [4.9399999999999995, 5.66], &apos;weight&apos;: [119.19999999999999, 164.2]}</code></pre><h3 id="1-3-2-标记离群值"><a href="#1-3-2-标记离群值" class="headerlink" title="1.3.2 标记离群值"></a>1.3.2 标记离群值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">outliers = df_outliers\</span><br><span class="line">            .select(*[<span class="string">'id'</span>]</span><br><span class="line">            + [(</span><br><span class="line">                (df_outliers[c] &lt; bounds[c][<span class="number">0</span>])|</span><br><span class="line">                (df_outliers[c] &gt; bounds[c][<span class="number">1</span>])</span><br><span class="line">            ).alias(c + <span class="string">'_o'</span>) <span class="keyword">for</span> c <span class="keyword">in</span> cols ])</span><br><span class="line">outliers.show()</span><br></pre></td></tr></table></figure><pre><code>+---+--------+--------+-----+| id|weight_o|height_o|age_o|+---+--------+--------+-----+|  7|   false|   false|false||  6|   false|   false|false||  5|   false|   false|false||  1|   false|   false|false||  3|    true|   false| true||  2|   false|   false|false||  4|   false|   false|false|+---+--------+--------+-----+</code></pre><h3 id="1-3-3-查看离群值"><a href="#1-3-3-查看离群值" class="headerlink" title="1.3.3 查看离群值"></a>1.3.3 查看离群值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bad_outlier = df_outliers.join(outliers, on=<span class="string">'id'</span>)</span><br><span class="line">bad_outlier.filter(<span class="string">'weight_o'</span>).select(<span class="string">'id'</span>, <span class="string">'weight'</span>).show()</span><br><span class="line">bad_outlier.filter(<span class="string">'age_o'</span>).select(<span class="string">'id'</span>, <span class="string">'age'</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+| id|weight|+---+------+|  3| 342.3|+---+------++---+---+| id|age|+---+---+|  3| 99|+---+---+</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Pyspark数据预处理1&quot;&gt;&lt;a href=&quot;#Pyspark数据预处理1&quot; class=&quot;headerlink&quot; title=&quot;Pyspark数据预处理1&quot;&gt;&lt;/a&gt;Pyspark数据预处理1&lt;/h1&gt;&lt;h2 id=&quot;1-1
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>流式处理框架 &amp; Spark Streaming &amp; Flume &amp; Kafka</title>
    <link href="http://www.ihoge.cn/2018/strame.html"/>
    <id>http://www.ihoge.cn/2018/strame.html</id>
    <published>2018-04-19T02:59:21.000Z</published>
    <updated>2018-05-17T12:12:42.787Z</updated>
    
    <content type="html"><![CDATA[<h1 id="流式处理框架-Spark-Streaming"><a href="#流式处理框架-Spark-Streaming" class="headerlink" title="流式处理框架 Spark Streaming"></a>流式处理框架 Spark Streaming</h1><ul><li>很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。</li><li>对于技术人员来说，可以利用数据挖掘和OLAP（On-Line Analytical Processing）分析工具从中找到对企业有价值的信息，这也是离线批处理的一般过程。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241530856455.jpg" alt=""></li></ul><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>流式处理框架基础</li><li>Flume</li><li>Kafka</li></ol><h2 id="1-流式处理框架基础"><a href="#1-流式处理框架基础" class="headerlink" title="1. 流式处理框架基础"></a>1. 流式处理框架基础</h2><h3 id="1-1-流数据一般特征"><a href="#1-1-流数据一般特征" class="headerlink" title="1.1 流数据一般特征"></a>1.1 流数据一般特征</h3><p><strong>流数据具有如下特征：</strong></p><ul><li>数据快速持续到达，潜在大小也许是无穷无尽的；<ul><li>数据来源众多，格式复杂；</li><li>数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储；</li><li>注重数据的整体价值，不过分关注个别数据；</li><li>数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序；</li></ul></li></ul><p>因此，用于静态数据存储的存储工具和批处理计算框架便无法满足流式计算、实时分析的应用需求。</p><h3 id="1-2-实时计算的核心框架"><a href="#1-2-实时计算的核心框架" class="headerlink" title="1.2 实时计算的核心框架"></a>1.2 实时计算的核心框架</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241530202093.jpg" alt=""></p><h3 id="1-3-数据源"><a href="#1-3-数据源" class="headerlink" title="1.3 数据源"></a>1.3 数据源</h3><p><strong>为何数据源在流式计算中显得异常重要</strong></p><p>所谓数据源，是指为了满足不同需求而不断输出数据的框架。不同于针对静态数据的批处理，要进行实时计算，就必须有足够“实时”且能够稳定运行的消息“投递者”，它必须能够高效的整合更低一级数据源发送的实时数据，并根据处理框架的API对数据进行规范化处理和转存，同时，对于流式处理系统而言，很多时候经过流式处理框架处理后的数据也是流数据，也可视作一种数据源。</p><p><strong>数据源的高低级之分</strong></p><p>对于流式处理框架而言，数据源的高低级之分，其实就是数据“输出”框架的高低级之分。任何数据源，要进行收集、转存和传递等工作，就需要有与之对应的处理框架，其框架越完善、功能越强大、且能够提供更便于实时处理的数据，我们就称其高级数据源，反之就称为低级数据源。</p><p><strong>数据源本身的“处理框架”</strong></p><p>这其实并不是规范的说法，通常来说，低级数据源我们会直接称呼其数据流，如文件流、套接字流等，而高级数据源，我们更多的会根据其作用，称其为日志收集系统（如Flume），或分布式消息队列（如Kafka）。对于初学者而言，我们可以将这些复杂的框架名称统一想象成一个数据的中转站，这些框架的核心作用都是收集、暂存然后投递数据。</p><p><strong>大数据分析师的知识边界</strong></p><p>一方面，我们要进行流式计算，就必须了解数据源对计算框架投递消息的方法，如此才能根据数据源设计算法，或者根据计算需求调整数据源；另一方面，我们也要区别大数据分析师与ETL工程师的区别，我们只需要了解与计算框架最近一层的数据源是如何与计算框架相互嵌套的既可，而无需再深究该数据源更低一层数据源、甚至是数据产生第一层数据源是如何工作的。</p><h2 id="2-Flume"><a href="#2-Flume" class="headerlink" title="2. Flume"></a>2. Flume</h2><h3 id="2-1-Flume介绍"><a href="#2-1-Flume介绍" class="headerlink" title="2.1 Flume介绍"></a>2.1 Flume介绍</h3><p>Flume是Cloudera公司开发的分布式、高可用的日志收集系统。现已开源给Apache。<br>目前比较有代表性的日志收集系统，除了Flume之外，还有FaceBook的Scribe。</p><p>Flume原始版本为OG，后经过整体架构的重新设计，以改名为Flume-NG。Flume发展至今，已不限于日志收集，还可以通过简单的配置，收集不同数据源的海量数据并将数据准确高效的传输到不同的处理框架或数据存储系统。<br>目前Flume可对接的主流大数据框架有Hadoop、Kafka、Hive、HBase、Spark等。</p><h3 id="2-2-Flume基本架构"><a href="#2-2-Flume基本架构" class="headerlink" title="2.2 Flume基本架构"></a>2.2 Flume基本架构</h3><p>Flume-NG采用三层架构设计，分别对应其三个核心组件：Source、Channel和Sink，其中，Source主要功能为收集上一层数据源传输过来的数据，Channel用于数据的暂时存储，Sink则用于数据的处理，即数据的传输对象和传输方式。其基本架构如下图所示：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241531282473.jpg" alt=""></p><p>其中，一个Agent代表一个Java进程，上图表示一个Event（数据）在一个Agent的传输流程。</p><ul><li><code>Event</code>：一条消息或者一个数据，具有可选的头信息，在头信息中可以设置时间戳、主机名等信息；</li><li><code>Source</code>：数据源，接收或者收集不同形式的数据源；</li><li><code>Channel</code>：Event的临时缓冲区，Source接收的Event会先发送到Channel进行缓存，在缓冲区内等待Sink的消费；</li><li><code>Sink</code>：用于处理Channel中缓存的数据，并发送至下一层（处理框架、存储中心或者下一个Agent）；</li><li><code>Agent</code>：包含了Source、Channel、Sink等组件的Flume进程；</li><li><code>Interceptor</code>：Event拦截器，在数据进入Channel之前根据配置要求，对数据进行头信息（Header）编辑，添加时间戳、主机名称等；</li><li><code>Selector</code>：Event的选择器，即决定Event流入Channel的方式，主要有<code>复制</code>（Replicating）和<code>复用</code>（Multiplexing）两种选方式；</li><li><code>Sink Processor</code>：Event Sink处理器，Flume提供了故障转移处理器和负载均衡处理器两种。</li></ul><h3 id="2-3-Flume核心组件-Source"><a href="#2-3-Flume核心组件-Source" class="headerlink" title="2.3 Flume核心组件:Source"></a>2.3 Flume核心组件:Source</h3><p><strong>Source核心功能</strong></p><ul><li>用于对接各种数据源，并将收集到的Event发送至用于临时存储的Channel中。Flume中每个Agent对应一个Source，而每个Agent的Source在启动前必须通过修改配置变量来设置其接受消息种类，此即称为Source种类</li></ul><p><strong>常用Scoure种类</strong>(依据数据类型分类)</p><ul><li><p><code>Avro Source</code>：Avro是Doug Cutting牵头开发的一个数据序列化系统，设计用于支持大批量数据交换的应用。Avro Source支持Avro协议，接收RPC请求，<code>Avro Source</code>通过监听Avro端口接收外部Avro客户端流事件，在Flume的多层架构中经常被使用接收上游Sink发送的event。</p></li><li><p><code>Kafka Source</code>：对接分布式消息队列Kafka，作为Kafka的消费者持续从Kafka中拉取数据，如果多个kafka source同时消费Kafka中同一个Topic，则其会被设置为同组id，从而保证多个Kafka Source之间不会重复拉取数据。</p></li><li><p><code>Exec Source</code>：支持Linux命令，收集标准输出数据或者通过tail -f file的方式监听指定文件。Exce Source可以实现实时的消息传输，但却不会记录已读取的文件的位置，不支持断点续传，如果Exce Source重启或者挂掉都会造成后续增加的消息无法接收，建议只在测试环境中使用。</p></li><li><p><code>Spooling Directory Source</code>：用于监听一个文件夹，收集文件夹中新文件数据，手机玩新文件数据会将文件名称的后缀改为.completed，缺点是不支持老文件（已经completed的文件）中新增数据集的收集，并且不能够对嵌套文件夹进行递归监听。</p></li><li><p><code>Taildir Source</code>：监听一个文件或文件夹，通过正则表达式匹配需要监听的数据源文件，支持文件夹嵌套递归监听，Taildir Source通过将监听文件的位置写入到文件中，从而实现断点续传，并且能够保证没有重复的数据读取。</p></li></ul><h3 id="2-4-Flume核心组件-Channel"><a href="#2-4-Flume核心组件-Channel" class="headerlink" title="2.4 Flume核心组件:Channel"></a>2.4 Flume核心组件:Channel</h3><p><strong>Channel核心功能</strong></p><ul><li>Channel是Event的<code>临时缓冲区</code>，存储Source收集但尚未被Sink读取的Event，其目标是为平衡Source收集速度和Sink读取速度，可视为Flume内部的消息队列。Channel线程的安全性较高且具有事务性，支持Source写失败重复写和Sink读失败重复读等操作。同时，我们根据Channel存储方式划分Channel种类。</li></ul><p><strong>常用Channel种类</strong> (依据存储方式分类)</p><ul><li><p>Memory Channel：<code>缓冲区所有数据都存于内存</code>，Memory Channel读写速度快，但存储量受内存限制，且当Flume进程挂掉、服务器宕机或重启时都会造成数据丢失。建议在服务器内存充足，且不关心数据丢失的场景下使用。</p></li><li><p>File Channel：缓冲区所有数据写入磁盘，File Channel存储容量大，无数据丢失风险。File Channel数据存储路径可以配置多个磁盘文件路径，通过磁盘并行写入提高其性能，在写入磁盘时是顺序写入，且单个数据文件大小可通过配置文件中maxFileSize参数进行调整，当被写入文件大小超过上限时，Flume会自动创建新文件用来存储后续Event。但数据文件数量不会无限增长，一旦旧文件被Sink读取完成，就将被删除。Flume通过设置检查点和备份检查点实现Agent重启之后快速将File Channel中的数据按顺序回放到内存中，以保证Agent在失败重启后仍能快速安全地提供服务。</p></li><li><p><code>Kafka Channel</code>：值得一提的是，Kafka可作为Flume中Channel存储方式，Kafka是分布式、可扩展、高容错、高吞吐的分布式系统，Kafka通过其优秀的架构设计充分利用磁盘顺序读写特性，在廉价的硬件条件下就能完成高效的消息发布和订阅，对比其他两种Channel，Kafka Channel在读取速度、存储量和容错性上完美的弥补了二者短板，若能合理利用Kafka性能，能够达到事半功倍的效果。</p></li></ul><h3 id="2-5-Flume核心组件-Sink"><a href="#2-5-Flume核心组件-Sink" class="headerlink" title="2.5 Flume核心组件:Sink"></a>2.5 Flume核心组件:Sink</h3><p><strong>Sink核心功能</strong></p><ul><li>用于处理Channel中缓存的数据，当数据经过Sink Processor处理后由Sink进行后续处理，Sink可将数据传输至静态数据存储中心进行数据保存，也可以将数据传输至实时处理框架进行数据处理，当然，也可以将数据传输至下一层数据源，进行进一步数据聚合、整理或推送。</li></ul><p><strong>常用Sink种类</strong> (依据投递接收方划分)</p><ul><li><p>Avro Sink：Avro Sink常用于对接下一层Arvo Source，通过发送RPC请求将Event发送到下一层的Avro Source，同时，Avro Sink提供了端到端的批量压缩数据传输，从而解决RPC传输过程中占用大量网络资源以及产生大量Socket连接等问题。</p></li><li><p>HDFS Sink：HDFS作为Hadoop生态中的最常用文件系统，具有高容错、可扩展、高性能、低成本等特点，HDFS Sink通过将Event写入HDFS进行数据存储，能够有效、长期存储大量数据。</p></li><li><p>Kafka Sink：在消息传递过程中采用Kafka框架能够从很大程度上降低系统耦合度，从而增加系统系统稳定性和容错机制，Flume可通过Kafka Sink将Event写入Kafka的Topic，其他应用通过Kafka获得数据。Flume从1.7.0开始支持Kafka 0.9及以上版本。</p></li></ul><h3 id="Interceptor（拦截器）-与选择器配合使用"><a href="#Interceptor（拦截器）-与选择器配合使用" class="headerlink" title="Interceptor（拦截器） (与选择器配合使用)"></a>Interceptor（拦截器） (与选择器配合使用)</h3><p><strong>Interceptor（拦截器）功能介绍</strong></p><ul><li>Source将Event写入Channel之前，可以用拦截器对Event进行添加头信息等简单处理，Source和Channel之间可设多个拦截器，不同拦截器可根据自身规则对Event进行简单处理，注意，拦截器属于轻量级插件，无法应对复杂数据处理工作。</li></ul><p><strong>常用Interceptor</strong></p><ul><li><p>主机拦截器（Host Interceptor）：Flume通过主机拦截器在Event头信息中添加主机名称或者IP。通过主机拦截器，Channel可以根据不同的主机信息分区存储Event，后续Sink也可根据不同主机信息对Event进行分别处理。</p></li><li><p>静态拦截器（Static Interceptor）：主要用于修改、过滤Event在此被拦截之前所设置的信息。</p></li></ul><h3 id="Selector（选择器）-与拦截器配合使用"><a href="#Selector（选择器）-与拦截器配合使用" class="headerlink" title="Selector（选择器） (与拦截器配合使用)"></a>Selector（选择器） (与拦截器配合使用)</h3><p><strong>功能介绍</strong></p><ul><li>Source发送的Event通过Channel选择器来决定以何种方式写入Channel，Flume提供了三种常用的选择器，分别是复制Channel选择器（Replicating Channel Selector）、复用Channel选择器（Multiplexing Channel Selector）和自定义选择器。</li></ul><p><strong>常用选择器</strong></p><ul><li><p>复制选择器（ Replicating Channel Selector ）：是Flume选择器的默认模式，即不对选择器进行设置时采用的模式，此时Source将以复制的方式将一个Event写入多个Channel中，不同的Sink可从不同的的Channel中获得相同的数据。复制选择器用途较多，当一个Event要做多个用途时，可考虑用复制选择器。</p></li><li><p>复用Channel选择器（Multiplexing Channel Selector）：复用选择器需要配合拦截器共同使用，复用选择器会根据Event的头信息来判断每个Event应该写入哪个Channel中。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241532271911.jpg" alt=""></p></li></ul><p>💡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关于负载均衡和故障排除的相关说明:</span><br><span class="line">Flume为了提高整体容错能力和稳定性，提供了负载均衡故障转移功能</span><br><span class="line">这两项功能配置较为简单，只要合理配置Sink组，并且在每组Sink中设置多个</span><br><span class="line">子Sink，就能够自动进行负载均衡和故障转移。</span><br></pre></td></tr></table></figure><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p>Kafka是由LinkedIn公司开源的分布式消息队列。现已加入到Apache软件基金会，并且凭借其高吞吐、可扩展、高可用、部署简单、开发接口丰富等特性，已在各大互联网公司的实际生产环境中广泛使用。<br>同时，大多数分布式处理系统都支持使用kafka，如Spark、Storm、Druid、Flume等<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241532641098.jpg" alt=""></p><h3 id="Kafka基本特点"><a href="#Kafka基本特点" class="headerlink" title="Kafka基本特点"></a>Kafka基本特点</h3><ul><li>Kafka其实就是消息“中转站”</li><li>Kafka本身不产生消息，也不对消息进行永久存储，它只是消息的“中转站”。</li></ul><ul><li>数据“中转站”至关重要</li><li>在实际工作中，我们广泛需要消息“中转站”的原因，是数据采集端种类各异，消息格式数据格式种类繁多，同时数据应用情景也非常复杂，提取数据可能为了实时计算，可能为了存储一段时间后进行批处理，也可能直接推送至数据产品前段，也可能中间夹杂各种其他过程以及各过程之间的复用。因此，我们需要“中转站”对数据进行暂存、简单处理、再次投递。</li></ul><ul><li>Kafka作为最优秀的消息“中转站”之一，并被广泛认可，完全得益于其优秀的架构。</li></ul><h3 id="Kafka构架特点"><a href="#Kafka构架特点" class="headerlink" title="Kafka构架特点"></a>Kafka构架特点</h3><ol><li>生产者和消费者不需要彼此了解</li><li>消费者的性能问题不会影响生产者</li><li>消费者受到保护，免受生产者的伤害</li><li>在处理负载方面有很大的灵活性</li><li>消息可供任何人使用 - </li><li>大量的新用例，监控，审计和故障排除</li></ol><ul><li>高吞吐率</li></ul><ol><li>Kafka利用顺序读写磁盘的设计，提供了可以和内存随机读写相匹敌的读写速度；</li><li>其灵活的客户端API设计，利用Linux操作系统提供的“零拷贝”特性，减少了消息网络传输的时间；</li><li>同时提供端到端的消息压缩传输，对同一主题下的消息采用分布式存储；</li></ol><ul><li>高容错、高可用</li></ul><p>Kafka在集群模式下允许用户对每个分区（partition）配置多个副本，且将副本均匀的分到各个节点（broker）内存储，保证同一个分区的副本不会再同一台机器上存储。同时，多副本之间采用Leader-Follower机制同步消息，只有Leader对外提供读写服务，当Leader意外失败、Broker进程关闭、服务器宕机等情况导致数据不可用时，kafka会从Follower中选择一个成为Leader继续提供读写服务。</p><ul><li>可拓展</li></ul><p>理论上Kafka的性能随着Broker的增加而增加，增加一个Broker只需为新增Broker设置一个唯一编号，配置文件编写完成后，Kafka就能通过ZooKeeper发现新的Broker，并投入使用。</p><ul><li>接藕</li></ul><p>Kafka内部能够将消息生产阶段和处理阶段分开，两个阶段互相独立，且各自能实现自己的处理逻辑，通过Kafka提供的消息写入和消费接口实现对消息的连接处理。两个阶段相互独立，不仅降低了自身复杂度，同时也实现了对外部框架提供部分服务的功能（如Flume中嵌入Kafka）。</p><ul><li><strong>峰值处理</strong></li></ul><p>实际工作中，经常会遇见数据在某个时间点爆发式增长（如双11），此时如果后台处理系统无法及时处理峰值需求，就会导致数据积压，严重时会导致系统崩溃。而若能合理使用Kafka进行数据中转，就相当于给系统接入了一个巨大的缓冲区，其既能接收持续暴增的请求，也能根据后台需求提供服务，进而提高了系统数据处理能力。</p><h3 id="Kafka基本架构"><a href="#Kafka基本架构" class="headerlink" title="Kafka基本架构"></a>Kafka基本架构</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241533218101.jpg" alt=""></p><ul><li>Producer代表消息生产者</li><li>Consumer代表消息消费者</li><li>Broker代表Kafka集群中各节点</li><li>Partition表示消息的一个分区</li><li>ZooKeeper为Kafka集群提供资源调度服务</li></ul><h3 id="Kafka基本概念"><a href="#Kafka基本概念" class="headerlink" title="Kafka基本概念"></a>Kafka基本概念</h3><ul><li><p><strong>Broker</strong>：一个Kafka的实例就是一个Broker，相当于Flume中的Agent；</p></li><li><p><strong>Topic</strong>：主题，Kafka中同一类型数据集的名称，相当于数据库中的表，Producer将同一类型数据写入同一个Topic中，同一个Consumer或Consumer Group从同一个Topic中消费数据，同时，一个Topic在物理上会被分成多分存储到不同的物理机上；</p></li><li><p><strong>Partition</strong>：分区，一个Topic可设置多个分区，相当于把一个数据集分成多分，存储到不同分区中进行存储（类似于HDFS），分区命名规则为topicname-index；</p></li><li><p><strong>Segment</strong>：段文件，Kafka中最小存储单位，一个topic包含多个Partition，一个Partition又包含多个Segment，Segment以其内部消息的起始偏移量进行索引；</p></li><li><p><strong>Offset</strong>：消息的起始偏移量，可作为Segment的索引；</p></li><li><p>Replication：副本，一个Partition可有一个或多个副本，创建Topic时可设置副本数量；</p></li><li><p>Producer：消息生产者，负责向Kafka集群中发布消息；</p></li><li><p>Consumer Group：消费者组，一个Consumer Group可包含一个或多个Consumer，当一个topic被一个消费者组消费的时候，一条消息只能由其中一位消费者消费，不会出现多位消费者消费同一条信息的情况；</p></li><li><p>Consumer：消息消费者，可从指定topic中拉取消息；</p></li><li><p>ZooKeeper：kafka需要ZooKeeper对其进行协调管理，安装Kafka过程将自带一个ZooKeeper。</p></li></ul><h2 id="实操流程"><a href="#实操流程" class="headerlink" title="实操流程"></a>实操流程</h2><h3 id="单节点演示"><a href="#单节点演示" class="headerlink" title="单节点演示"></a>单节点演示</h3><ol><li><p>启动zoopeeper服务<br> <code>bin/zookeeper-server-start.sh config/zookeeper.properties</code></p></li><li><p>启动Kafka服务<br> <code>bin/kafka-server-start.sh config/server.properties</code></p></li><li><p>创建topic取名cdatest<br> <code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic cdatest</code></p></li><li><p>查看topic<br> <code>bin/kafka-topics.sh --list --zookeeper localhost:2181</code></p></li><li><p>启动生产者指定topic，并在终端输入测试数据<br> <code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic cdatest</code></p></li><li><p>启动消费者指定topic<br> <code>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic cdatest --from-beginning</code></p></li></ol><h3 id="集群模式测试"><a href="#集群模式测试" class="headerlink" title="集群模式测试"></a>集群模式测试</h3><ol><li><p>修改参数server.properties</p></li><li><p>启动zookeeper集群服务</p></li><li><p>启动kafka集群（分别在各节点执行）<br><code>./bin/kafka-server-start.sh config/server.properties</code></p></li><li><p>任意一个节点创建topic<br><code>./bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 3 --topic kktest</code></p></li><li><p>查看topic状态<br><code>./bin/kafka-topics.sh --describe --zookeeper master:2181,slave1:2181,slave2:2181 --topic kktest</code></p></li><li><p>创建producer，手动输入测试数据（以master为例）<br><code>./bin/kafka-console-producer.sh --broker-list master:9092 --topic kktest</code></p></li><li><p>以slave1为例创建consumer消费数据<br><code>./bin/kafka-console-consumer.sh --bootstrap-server slave1:9092 --from-beginning --topic kktest</code></p></li></ol><p><strong>可以在不同节点创建多个生产者和消费者</strong></p><h2 id="使用外部数据源进行数据传输"><a href="#使用外部数据源进行数据传输" class="headerlink" title="使用外部数据源进行数据传输"></a>使用外部数据源进行数据传输</h2><ol><li><p>启动Kafka，启动prodecer和consumer</p></li><li><p>在Kafka安装路径内创建test.txt然后执行:q</p></li><li>~/kafka/bin/connect-standalone.sh ~/kafka/config/connect-standalone.properties ~/kafka/config/connect-file-source.properties ~/kafka/config/connect-file-sink.properties</li></ol><p><strong>自定义模式读取外部文件时，有两个主要的配置文件</strong><br>connect-file-source.properties<br>connect-file-sink.properties</p><p>根据需要修改文件读取模式、文件路径、Topic等</p><h2 id="Kafka于Flume联合部署"><a href="#Kafka于Flume联合部署" class="headerlink" title="Kafka于Flume联合部署"></a>Kafka于Flume联合部署</h2><p>创建flume配置文件：如kafkaSource.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent    a1.sources = r1    a1.sinks = k1    a1.channels = c1   # Describe/configure the source    a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource    a1.sources.r1.kafka.topics = kafkaTopic  a1.sources.r1.kafka.bootstrap.servers = master:9092,slave1:9092,slave2:9092  # Describe the sink    a1.sinks.k1.type = logger # Use a channel which buffers events in memory    a1.channels.c1.type = memory    a1.channels.c1.capacity = 1000    a1.channels.c1.transactionCapacity = 100   # Bind the source and sink to the channel    a1.sources.r1.channels = c1    a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>启动Flume：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/flume/bin/flume-ng agent --conf ~/flume/conf --conf-file ~/flume/conf/kafkaSource.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>创建kafkaTopic</p><p><code>~/kafka/bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 3 --topic kafkaTopic</code></p><p>创建producer，输入测试信息<br><code>~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sparkMLLib</code></p><p><strong>顺序出错的分析</strong></p><ol><li>Kafka传出来的数据顺序就已经被打乱</li><li>计算资源（虽然传输的数据很微小，但是集群的启动成本高昂）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;流式处理框架-Spark-Streaming&quot;&gt;&lt;a href=&quot;#流式处理框架-Spark-Streaming&quot; class=&quot;headerlink&quot; title=&quot;流式处理框架 Spark Streaming&quot;&gt;&lt;/a&gt;流式处理框架 Spark Streami
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Strutured Streaming(Spark 2.3)</title>
    <link href="http://www.ihoge.cn/2018/StructuredStreaming.html"/>
    <id>http://www.ihoge.cn/2018/StructuredStreaming.html</id>
    <published>2018-04-19T02:59:21.000Z</published>
    <updated>2018-05-17T12:12:27.498Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark-2-3-Strutured-Streaming"><a href="#Spark-2-3-Strutured-Streaming" class="headerlink" title="Spark 2.3 Strutured Streaming"></a>Spark 2.3 Strutured Streaming</h1><p>为了继续实现 Spark 更快，更轻松，更智能的目标，Spark 2.3 在许多模块都做了重要的更新，比如 Structured Streaming 引入了低延迟的连续处理（continuous processing）；支持 stream-to-stream joins；通过改善 pandas UDFs 的性能来提升 PySpark；支持第四种调度引擎 Kubernetes clusters（其他三种分别是自带的独立模式Standalone，YARN、Mesos）。除了这些比较具有里程碑的重要功能外，Spark  2.3 还有以下几个重要的更新：<br>引入 DataSource v2 APIs<br>矢量化(Vectorized)的 ORC reader<br>Spark History Server v2 with K-V store<br>基于 Structured Streaming 的机器学习管道API模型<br>MLlib 增强<br>Spark SQL 增强<br>这篇文章将简单地介绍上面一些高级功能和改进，更多的特性请参见 <a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark 2.3 release notes</a></p><p>这里主要针对Spark2.3 对Structured Streaming的新特性进行描述。</p><p>The simplest way to perform streaming analytics is not having to reason about streaming.</p><h2 id="毫秒延迟的连续流处理"><a href="#毫秒延迟的连续流处理" class="headerlink" title="毫秒延迟的连续流处理"></a>毫秒延迟的连续流处理</h2><p>Apache Spark 2.0 的 Structured Streaming 将微批次处理(micro-batch processing)从它的高级 APIs 中解耦出去，原因有两个：首先，开发人员更容易学习这些 API，不需要考虑这些 APIs 的微批次处理情况;其次，它允许开发人员将一个流视为一个无限表，他们查询流的数据，就像他们查询静态表一样简便。<br>但是，为了给开发人员提供不同的流处理模式，社区引入了一种新的毫秒级低延迟(millisecond low-latency连续模式(continuous mode)。<br>在内部，结构化的流引擎逐步执行微批中的查询计算，执行周期由触发器间隔决定，这个延迟对大多数真实世界的流应用程序来说是可以容忍的。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241174964831.jpg" alt=""></p><h3 id="微批处理（micro-batch）"><a href="#微批处理（micro-batch）" class="headerlink" title="微批处理（micro-batch）"></a>微批处理（micro-batch）</h3><p>Structured Streaming默认使用微量批处理执行模型。这意味着Spark流引擎会定期检查流源，并对自上次批量结束后到达的新数据运行批量查询。At a high-level, it looks like this:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241491220257.jpg" alt=""></p><p>在微批处理体系结构中，驱动程序通过将记录偏移量保存到<strong>预写的日志中（write-ahead-log）</strong>来检查进度，然后用它来重新启动查询。请注意，为了获得确定性的重新执行和端对端语义，在微批处理启动之前，要在下一个微批处理中处理的范围偏移保存到日志中。因此，数据源中可用的记录可能<strong>不得不等待当前的微批次在其偏移记录之前</strong>完成，并且在下一个微批中处理它。过程看起来像这样。</p><p>In this architecture, the driver checkpoints the progress by saving the records offsets to a write-ahead-log, which may be then used to restart the query. Note that the range offsets to be processed in the next micro-batch is saved to the log before the micro-batch has started in order to get deterministic re-executions and end-to-end semantics. As a result, a record that is available at the source may have to wait for the current micro-batch to be completed before its offset is logged and the next micro-batch processes it. At the record level, the timeline looks like this.<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241495217055.jpg" alt=""><br>这会导致更多的延迟时间。</p><h3 id="连续处理（Continuous-Processing）"><a href="#连续处理（Continuous-Processing）" class="headerlink" title="连续处理（Continuous Processing）"></a>连续处理（Continuous Processing）</h3><p>而对于连续模式，流读取器连续拉取源数据并处理数据，而不是按指定的触发时间间隔读取一批数据。通过不断地查询源数据和处理数据，新的记录在到达时立即被处理，将等待时间缩短到毫秒，满足低延迟的应用程序的需求，具体如下面图所示：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241175216530.jpg" alt=""></p><p>目前连续模式支持 map-like Dataset 操作，包括投影(projections)、selections以及其他 SQL 函数，但是不支持 current_timestamp(), current_date() 以及聚合函数。它还支持将 Kafka 作为数据源和数据存储目的地(sink)，也支持 console 和 memory sink。<br>现在，开发人员可以根据延迟要求选择模式连续或微量批处理，来构建大规模实时流式传输应用程序，同时这些系统还能够享受到 Structured Streaming 提供的 fault-tolerance 和 reliability guarantees 特性。<br>简单来说，Spark 2.3 中的连续模式是实验性的，它提供了以下特性：<br>端到端的毫秒级延迟<br>至少一次语义保证<br>支持 map-like 的 Dataset 操作</p><h2 id="流与流进行Join"><a href="#流与流进行Join" class="headerlink" title="流与流进行Join"></a>流与流进行Join</h2><p>Spark 2.0 版本的 Structured Streaming 支持流 DataFrame/Dataset 和静态数据集之间的 join，但是 Spark 2.3 带来了期待已久的流和流的 Join 操作。支持内连接和外连接，可用在大量的实时场景中。<br>广告收益是流与流进行Join的典型用例。例如，展示广告流和广告点击流共享您希望进行流式分析的公共关键字(如adId)和相关数据，根据这些数据你可以分析出哪些广告更容易被点击。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241175635666.jpg" alt=""></p><p>这个例子看起来很简单，但是实现流和流的Join需要解决很多技术难题，如下：</p><ul><li><strong>缓存处理延迟数据：</strong>需要缓存延迟的数据，直到从其他流中找到匹配的事件;</li><li><strong>先置缓冲区大小：</strong>限制流式连接缓冲区大小的唯一方法是将延迟超过某个阈值的数据丢弃。此最大延迟阈值应由用户根据业务需求和系统资源限制之间的平衡进行配置。</li><li><strong>明确定义的语义</strong>：在静态连接和流式连接之间保持一致的SQL连接语义，具有或不具有上述阈值。</li></ul><p>幸运的事Spark2.3解决了所有这些问题，因此我们使用SQL连接的清晰语义来表达计算，并控制相关事件之间的延迟容忍。<br>假设有两个不同的kafka流，我们要通过两个流中的共有的<code>adID</code>属性进行连接。让我们来看看代码是怎么实现的：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241453518177.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241453673148.jpg" alt=""></p><p>与所有的结构化数据流的查询相同，代码是完全一样的。将kafka流数据当作静态的Dataframe一样去定义。当执行此查询时，结构化流式处理引擎会根据需要将点击和展示作为流状态进行缓冲。对于特定广告，只要接收到两个相关事件（即收到第二个事件后），即会生成联合输出。随着数据到达，连接的输出将逐步生成并写入查询接收器（例如，另一个Kafka）。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241457085725.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark-2-3-Strutured-Streaming&quot;&gt;&lt;a href=&quot;#Spark-2-3-Strutured-Streaming&quot; class=&quot;headerlink&quot; title=&quot;Spark 2.3 Strutu
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark的基本架构</title>
    <link href="http://www.ihoge.cn/2018/IntroductionToSpark.html"/>
    <id>http://www.ihoge.cn/2018/IntroductionToSpark.html</id>
    <published>2018-04-18T02:59:21.000Z</published>
    <updated>2018-05-17T12:15:24.772Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark的基本架构"><a href="#Spark的基本架构" class="headerlink" title="Spark的基本架构"></a>Spark的基本架构</h1><p>当单机没有足够的能力和资源来执行大量信息的计算（或者低延迟计算），这时就需要一个集群或一组机器将许多机器的资源集中在一起，使我们可以使用全部累积的在一起的计算和存储资源。现在只有一组机器不够强大，你需要一个框架来协调他们之间的工作。 Spark是一种工具，可以管理和协调跨计算机集群执行数据任务。<br>Spark用于执行任务的机器集群可以由Spark的Standalone，YARN或Mesos等集群管理器进行管理。然后，我们向这些集群管理器提交Spark应用程序，这些集群管理器将资源授予我们的应用程序，以便我们完成我们的工作。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240180152481.jpg" alt=""></p><h3 id="1-Spark-Application"><a href="#1-Spark-Application" class="headerlink" title="1. Spark Application"></a>1. Spark Application</h3><p>Spark应用程序由一个驱动程序进程和一组执行程序进程组成。Driver进程运行main（）函数，位于集群中的一个节点上，它负责三件事：维护Spark应用程序的相关信息;回应用户的程序或输入;分配和安排Executors之间的工作。驱动程序过程是绝对必要的 - 它是Spark应用程序的核心，并在应用程序的生命周期中保留所有相关信息。<br>Executor负责实际执行Driver分配给他们的工作。这意味着，每个Executor只有两个任务：执行由驱动程序分配给它的代码，并将该执行程序的计算状态报告给驱动程序节点。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239867627806.jpg" alt=""></p><p>群集管理器控制物理机器并为Spark应用程序分配资源。这可以是几个核心集群管理员之一：Spark的Standalone，YARN或Mesos。这意味着可以同时在群集上运行多个Spark应用程序。<br>在前面的插图中，左侧是我们的driver，右侧是四个executors。在该图中，我们删除了群集节点的概念。用户可以通过配置指定有多少执行者应该落在每个节点上。</p><ul><li>Spark有一些集群管理器，负责调度可用资源。</li><li>驱动程序进程负责执行执行程序中的驱动程序命令，以完成我们的任务。</li></ul><h3 id="2-Spark’s-Languge-APIs"><a href="#2-Spark’s-Languge-APIs" class="headerlink" title="2. Spark’s Languge APIs"></a>2. Spark’s Languge APIs</h3><p>尽管我们的executor大多会一直运行Spark代码。但我们仍然可以通过Spark的语言API用多种不同语言运行Spark代码。大多数情况下，Spark会在每种语言中提供一些核心“concepts”，并将不同语言的代码译成运行在机器集群上的Spark代码。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239873536392.jpg" alt=""></p><p> <code>Spark有两套基本的API：低级非结构化(Unstructured)API和更高级别的结构化(Structured)API。</code></p><h3 id="3-SparkSession"><a href="#3-SparkSession" class="headerlink" title="3. SparkSession"></a>3. SparkSession</h3><p>我们通过驱动程序来控制Spark应用程序。该驱动程序进程将自身作为名为SparkSession并作为唯一的接口API对象向用户开放。 SparkSession实例是Spark在群集中执行用户定义操作的方式。 SparkSession和Spark应用程序之间有一对一的对应关系。在Scala和Python中，变量在启动控制台时可用作spark。让我们看下简单的Scala和/或Python中的SparkSession。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239862105558.jpg" alt=""></p><h3 id="4-Dataframe"><a href="#4-Dataframe" class="headerlink" title="4. Dataframe"></a>4. Dataframe</h3><p>DataFrame是最常见的<code>Structured API</code>（结构化API），只是表示有类型的<code>包含行和列的数据表</code>。一个简单的比喻就是一个带有命名列的电子表格。其根本区别在于，当电子表格位于一台计算机上某个特定位置时，Spark DataFrame可以跨越数千台计算机。将数据放在多台计算机上的原因无非有两种：数据太大而无法放在一台计算机上，或者在一台计算机上执行计算所需的时间太长。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15239862709297.jpg" alt=""></p><p>DataFrame概念并不是Spark独有的。 R和Python都有相似的概念。但是，Python / R DataFrame（有一些例外）存在于一台机器上，而不是多台机器上。这限制了您可以对python和R中给定的DataFrame执行的操作与该特定机器上存在的资源进行对比。但是，由于Spark具有适用于Python和R的<code>Spark’s Language APIs</code>，因此将Pandas（Python）DataFrame转换为Spark DataFrame和R DataFrame转换为Spark DataFrame（R）非常容易。</p><p><strong>注意</strong><br>Spark有几个核心抽象：Datasets，Dadaframes，SQL Table和弹性分布式数据集（RDD）。这些抽象都表示分布式数据集合，但它们有不同的接口来处理这些数据。最简单和最有效的是DataFrames，它可以用于所有语言。<strong>以下概念适用于所有的核心抽象。</strong></p><h3 id="5-Partitions"><a href="#5-Partitions" class="headerlink" title="5. Partitions"></a>5. Partitions</h3><p>为了允许每个执行者并行执行工作，Spark将数据分解成称为分区的块。分区是位于集群中的一台物理机上的一组行。 DataFrame的分区表示数据在执行过程中如何在整个机器群中物理分布。如果你有一个分区，即使你有数千个执行者，Spark也只会有一个分区。如果有多个分区，但只有一个执行程序Spark仍然只有一个并行性，因为只有一个计算资源。<br>值得注意的是，使用DataFrames，我们不会（大部分）操作 手动分区（基于个人）。我们只需指定物理分区中数据的高级转换，并且Spark确定此工作将如何在集群上实际执行。较低级别的API确实存在（通过弹性分布式数据集接口）。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240195737335.jpg" alt=""></p><h3 id="6-Transformations"><a href="#6-Transformations" class="headerlink" title="6. Transformations"></a>6. Transformations</h3><p>在Spark中，核心数据结构是不可改变的，这意味着一旦创建它们就不能更改。起初，这可能看起来像一个奇怪的概念，如果你不能改变它，你应该如何使用它？为了“更改”DataFrame，您必须指示Spark如何修改您所需的DataFrame。这些说明被称为<code>转换</code>。<br>转换操作没有返回输出，这是因为我们只指定了一个抽象转换，并且Spark不会在转换之前采取行动，直到我们执行一个动作。Transformations是如何使用Spark来表达业务逻辑的核心。Spark有两种类型的Transformations，一种是窄依赖转换关系，一种是宽依赖转换关系。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239881561168.jpg" alt=""><br>宽依赖指输入分区对多输出分区起作用（多个孩子）。这被称为shuffle，Spark将在群集之间交换分区。对于窄依赖转换，Spark将自动执行称为流水线的操作，这意味着如果我们在DataFrame上指定了多个过滤器，它们将全部在内存中执行。当我们执行shuffle时，Spark会将结果写入磁盘。</p><h3 id="7-Lazy-Evaluation"><a href="#7-Lazy-Evaluation" class="headerlink" title="7. Lazy Evaluation"></a>7. Lazy Evaluation</h3><p>Lazy Evaluation意味着Spark将等到执行计算指令图的最后时刻。在Spark中，我们不是在表达某些操作时立即修改数据，而是建立起来应用于源数据的转换计划。Spark将把原始DataFrame转换计划编译为一个高效的物理计划，该计划将在群集中尽可能高效地运行。这为最终用户带来了巨大的好处，因为Spark可以优化整个数据流从端到端。这方面的一个例子就是所谓的“predicate pushdown” DataFrames。如果我们构建一个大的Spark作业，但在最后指定了一个过滤器，只需要我们从源数据中获取一行，则执行此操作的最有效方法就是访问我们需要的单个记录。 Spark实际上会通过自动推低滤波器来优化这一点。</p><h3 id="8-Actions"><a href="#8-Actions" class="headerlink" title="8. Actions"></a>8. Actions</h3><p>转换使我们能够建立我们的逻辑计划。为了触发计算，我们需要一个动作操作。一个动作指示Spark计算一系列转换的结果。<br>在指定我们的操作时，我们开始了一个Spark作业，它运行我们的过滤器转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区的基础上执行计数，然后一个collect将我们的结果带到各自语言的本地对象。我们可以通过检查Spark UI（<a href="http://localhost:4040）来看到所有这些，Spark" target="_blank" rel="noopener">http://localhost:4040）来看到所有这些，Spark</a> UI是一个包含在Spark中的工具，它允许我们监视集群上运行的Spark作业。</p><h3 id="9-Dataframe-amp-SQL"><a href="#9-Dataframe-amp-SQL" class="headerlink" title="9. Dataframe &amp; SQL"></a>9. Dataframe &amp; SQL</h3><p>Spark SQL是Spark为结构化和半结构化数据处理设计的最受欢迎的模块之一。 Spark SQL允许用户使用SQL或可在Java，Scala，Python和R中使用的DataFrame和Dataset API来查询Spark程序中的structured data。由于DataFrame API提供了一种统一的方法来访问各种的数据源（包括Hive datasets，Avro，Parquet，ORC，JSON和JDBC），用户能够以相同方式连接到任何数据源，并将这些多个数据源连接在一起。 Spark SQL使用Hive meta store为用户提供了与现有Hive数据，查询和UDF完全兼容的功能。用户可以无缝地 在Spark上无需修改即可运行其当前的Hive工作负载。<br>Spark SQL也可以通过spark-sql shell来访问，现有的业务工具可以通过标准的JDBC和ODBC接口进行连接。</p><p>现在我们通过一个示例并在DataFrame和SQL中进行跟踪。不管语言如何，以完全相同的方式启动相同的转换。您可以在SQL或DataFrames（R，Python，Scala或Java）中表达业务逻辑，并且在实际执行代码之前，Spark会将该逻辑编译计划优化并最终生成最优的物理计划。 Spark SQL允许您作为用户将任何DataFrame注册为表或视图（临时表），并使用纯SQL查询它。编写SQL查询或编写DataFrame代码之间没有性能差异 都“编译”到我们在DataFrame代码中指定的相同底层计划。<br>通过一个简单的方法调用就可以将任何DataFrame制作成表格或视图。</p><p><strong>With SQl</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162004942.jpg" alt=""><br><strong>With DataFrame</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162492089.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162738329.jpg" alt=""></p><p>现在有7个步骤将我们带回源数据。您可以在这些DataFrame的解释计划中看到这一点。以上图解说明了我们在“代码”中执行的一系列步骤。真正的执行计划（解释中可见的执行计划）将与下面的执行计划有所不同，因为在物理执行方面进行了优化，然而，该执行计划与任何计划一样都是起点。这个执行计划是一个有向无环图（DAG）的转换，每个转换产生一个新的不可变DataFrame，我们在这个DataFrame上调用一个动作来产生一个结果。</p><ol><li>第一步是读取数据。但是Spark实际上并没有读取它（Lazy Evaluation）</li><li>第二步是我们的分组，在技术上，当我们调用groupBy时，我们最终得到了一个RelationalGroupedDataset，它是DataFrame的一个奇特名称，该DataFrame具有指定的分组，但需要用户在可以进一步查询之前<strong>指定聚合</strong>。</li><li>因此第三步是指定聚合。我们使用总和聚合方法。这需要输入一列 表达式或简单的列名称。 sum方法调用的结果是一个新的dataFrame。你会看到它有一个新的模式，但它知道每个列的类型。（再次强调！）这里没有执行计算是非常重要的。这只是我们表达的另一种转换，Spark仅仅能够跟踪我们提供的类型信息。</li><li>第四步是简化语言，我们使用withColumnRename给原始列重新定义新名称。当然，这不会执行计算 - 这只是另一种转换！</li><li>第五步导入一个函数对数据进行排序，即desc函数。从destination_total列中找到的最大值。</li><li>第六步，我们将指定一个限制。这只是说明我们只需要五个值。这就像一个过滤器，只是它按位置而不是按值过滤。可以肯定地说，它基本上只是指定了一定大小的DataFrame。</li><li>最后一步是我们的行动！现在我们实际上开始收集上面的DataFrame结果的过程，Spark将以我们正在执行的语言返回一个列表或数组。现在我们看下它的解释计划。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240170262028.jpg" alt=""><br>虽然这个解释计划与我们确切的“概念计划”不符，但所有的部分都在那里。可以看到limit语句以及orderBy（在第一行）。你也可以看到我们的聚合是如何在partial_sum调用中的两个阶段发生的。这是因为数字列表是可交换的，并且Spark可以执行sum()并按分区进行划分。当然，我们也可以看到我们如何在DataFrame中读取数据。同时我们也可以将它写出到Spark支持的任何数据源中。例如，假设我们想要将这些信息存储在PostgreSQL等数据库中，或者将它们写入另一个文件。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark的基本架构&quot;&gt;&lt;a href=&quot;#Spark的基本架构&quot; class=&quot;headerlink&quot; title=&quot;Spark的基本架构&quot;&gt;&lt;/a&gt;Spark的基本架构&lt;/h1&gt;&lt;p&gt;当单机没有足够的能力和资源来执行大量信息
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
  </entry>
  
</feed>
