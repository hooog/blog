<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hoooge&#39;s Blog</title>
  
  <subtitle>Life is short</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.ihoge.cn/"/>
  <updated>2018-05-05T17:23:04.407Z</updated>
  <id>http://www.ihoge.cn/</id>
  
  <author>
    <name>刘知行</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark ML - 聚类算法</title>
    <link href="http://www.ihoge.cn/2018/ML2.html"/>
    <id>http://www.ihoge.cn/2018/ML2.html</id>
    <published>2018-05-05T18:00:21.000Z</published>
    <updated>2018-05-05T17:23:04.407Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="Spark-ML-聚类算法"><a href="#Spark-ML-聚类算法" class="headerlink" title="Spark ML - 聚类算法"></a>Spark ML - 聚类算法</h2><h3 id="1-KMeans快速聚类"><a href="#1-KMeans快速聚类" class="headerlink" title="1.KMeans快速聚类"></a>1.KMeans快速聚类</h3><p>首先到UR需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">KMeans</span>,<span class="type">KMeansModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vectors</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>​    在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(</span><br><span class="line">    line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib版的教程类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    在得到数据后，我们即可通过ML包的固有流程：创建<code>Estimator</code>并调用其<code>fit()</code>方法来生成相应的<code>Transformer</code>对象，很显然，在这里<code>KMeans</code>类是<code>Estimator</code>，而用于保存训练后模型的<code>KMeansModel</code>类则属于<code>Transformer</code>：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> kmeansmodel = <span class="keyword">new</span> <span class="type">KMeans</span>().</span><br><span class="line">      setK(<span class="number">3</span>).</span><br><span class="line">      setFeaturesCol(<span class="string">"features"</span>).</span><br><span class="line">      setPredictionCol(<span class="string">"prediction"</span>).</span><br><span class="line">      fit(df)</span><br></pre></td></tr></table></figure><p>​    与MLlib版本类似，ML包下的KMeans方法也有<code>Seed</code>（随机数种子）、<code>Tol</code>（收敛阈值）、<code>K</code>（簇个数）、<code>MaxIter</code>（最大迭代次数）、<code>initMode</code>（初始化方式）、<code>initStep</code>（KMeans||方法的步数）等参数可供设置，和其他的ML框架算法一样，用户可以通过相应的<code>setXXX()</code>方法来进行设置，或以<code>ParamMap</code>的形式传入参数，这里为了简介期间，使用<code>setXXX()</code>方法设置了参数K，其余参数均采用默认值。</p><p>​    与MLlib中的实现不同，<code>KMeansModel</code>作为一个<code>Transformer</code>，不再提供<code>predict()</code>样式的方法，而是提供了一致性的<code>transform()</code>方法，用于将存储在<code>DataFrame</code>中的给定数据集进行整体处理，生成带有预测簇标签的数据集：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> results = kmeansmodel.transform(df)</span><br></pre></td></tr></table></figure><p>​    为了方便观察，我们可以使用<code>collect()</code>方法，该方法将<code>DataFrame</code>中所有的数据组织成一个<code>Array</code>对象进行返回：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results.collect().foreach(</span><br><span class="line">      row =&gt; &#123;</span><br><span class="line">        println( row(<span class="number">0</span>) + <span class="string">" is predicted as cluster "</span> + row(<span class="number">1</span>))</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>也可以通过<code>KMeansModel</code>类自带的<code>clusterCenters</code>属性获取到模型的所有聚类中心情况：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.clusterCenters.foreach(</span><br><span class="line">      center =&gt; &#123;</span><br><span class="line">        println(<span class="string">"Clustering Center:"</span>+center)</span><br><span class="line">      &#125;)</span><br></pre></td></tr></table></figure><p>​    与MLlib下的实现相同，<code>KMeansModel</code>类也提供了计算 <strong>集合内误差平方和（Within Set Sum of Squared Error, WSSSE)</strong> 的方法来度量聚类的有效性，在真实K值未知的情况下，该值的变化可以作为选取合适K值的一个重要参考：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kmeansmodel.computeCost(df)</span><br></pre></td></tr></table></figure><h3 id="2-高斯混合模型-GMM-聚类算法"><a href="#2-高斯混合模型-GMM-聚类算法" class="headerlink" title="2.高斯混合模型(GMM)聚类算法"></a>2.高斯混合模型(GMM)聚类算法</h3><h4 id="2-1-基本原理"><a href="#2-1-基本原理" class="headerlink" title="2.1 基本原理"></a>2.1 基本原理</h4><p>​    <strong>高斯混合模型（Gaussian Mixture Model, GMM）</strong> 是一种概率式的聚类方法，属于生成式模型，它假设所有的数据样本都是由某一个给定参数的 <strong>多元高斯分布</strong> 所生成的。具体地，给定类个数<code>K</code>，对于给定样本空间中的样本 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-137bed5220372f1cad4f3cdf4529593e_l3.svg" alt="img"></p><p>，一个高斯混合模型的概率密度函数可以由K个多元高斯分布组合成的混合分布表示：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1d75e089a9a823703aa88d08ad53936e_l3.svg" alt="img"></p><p>其中，</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-a0f35b7e777b0ecf33b511cfb3174001_l3.svg" alt="img"></p><p>是以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>为均值向量， </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>为协方差矩阵的多元高斯分布的概率密度函数，可以看出，高斯混合模型由K个不同的多元高斯分布共同组成，每一个分布被称为高斯混合模型中的一个 <strong>成分(Component)</strong>， 而</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-1a47f13ac9a37fcb6911a1b8e17cbb35_l3.svg" alt="img"></p><p>为第<code>i</code>个多元高斯分布在混合模型中的 <strong>权重</strong> ，且有 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-9312fca34e9b3afde8787a29c58fe192_l3.svg" alt="img"></p><p>。</p><p>假设已有一个存在的高斯混合模型，那么，样本空间中的样本的生成过程即是：以 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-af3d8faef7e634b7b15f83cb1606b714_l3.svg" alt="img"></p><p>作为概率（实际上，权重可以直观理解成相应成分产生的样本占总样本的比例），选择出一个混合成分，根据该混合成分的概率密度函数，采样产生出相应的样本。</p><p>那么，利用GMM进行聚类的过程是利用GMM生成数据样本的“逆过程”：给定聚类簇数<code>K</code>，通过给定的数据集，以某一种 <strong>参数估计</strong> 的方法，推导出每一个混合成分的参数（即均值向量 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-81c6fc10ad791a9237b3a37caf7047a3_l3.svg" alt="img"></p><p>、协方差矩阵 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-3665f1bb0e135d4c56400c158883b6f8_l3.svg" alt="img"></p><p>和权重 </p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-356e473b3185b432024c4643855f1b9d_l3.svg" alt="img"></p><p>），每一个多元高斯分布成分即对应于聚类后的一个簇。高斯混合模型在训练时使用了极大似然估计法，最大化以下对数似然函数：</p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-c006bbb984e97b258cf6bcc1d62ee2d7_l3.svg" alt="img"></p><p><img src="http://dblab.xmu.edu.cn/blog/wp-content/ql-cache/quicklatex.com-f57e2abf78c1ba038d4969a8fc513e7a_l3.svg" alt="img"></p><p>显然，该优化式无法直接通过解析方式求得解，故可采用 <strong>期望-最大化(Expectation-Maximization, EM)</strong> 方法求解，具体过程如下（为了简洁，这里省去了具体的数学表达式，详细可见<a href="https://en.wikipedia.org/wiki/Mixture_model" target="_blank" rel="noopener">wikipedia</a>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.根据给定的K值，初始化K个多元高斯分布以及其权重；</span><br><span class="line">2.根据贝叶斯定理，估计每个样本由每个成分生成的后验概率；(EM方法中的E步)</span><br><span class="line">3.根据均值，协方差的定义以及2步求出的后验概率，更新均值向量、协方差矩阵和权重；（EM方法的M步）</span><br><span class="line">重复2~3步，直到似然函数增加值已小于收敛阈值，或达到最大迭代次数</span><br></pre></td></tr></table></figure><p>​    当参数估计过程完成后，对于每一个样本点，根据贝叶斯定理计算出其属于每一个簇的后验概率，并将样本划分到后验概率最大的簇上去。相对于KMeans等直接给出样本点的簇划分的聚类方法，GMM这种给出样本点属于每个簇的概率的聚类方法，被称为 <strong>软聚类(Soft Clustering / Soft Assignment)</strong> 。</p><h4 id="2-2-模型的训练与分析"><a href="#2-2-模型的训练与分析" class="headerlink" title="2.2 模型的训练与分析"></a>2.2 模型的训练与分析</h4><p>​    Spark的ML库提供的高斯混合模型都在<code>org.apache.spark.ml.clustering</code>包下，和其他的聚类方法类似，其具体实现分为两个类：用于抽象GMM的超参数并进行训练的<code>GaussianMixture</code>类（<code>Estimator</code>）和训练后的模型<code>GaussianMixtureModel</code>类（<code>Transformer</code>），在使用前，引入需要的包：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.&#123;<span class="type">GaussianMixture</span>,<span class="type">GaussianMixtureModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></span><br></pre></td></tr></table></figure><p>开启<code>RDD</code>的隐式转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure><p>​    我们仍采用Iris数据集进行实验。为了便于生成相应的<code>DataFrame</code>，这里定义一个名为<code>model_instance</code>的<code>case class</code>作为<code>DataFrame</code>每一行（一个数据样本）的数据类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">model_instance</span> (<span class="params">features: org.apache.spark.ml.linalg.<span class="type">Vector</span></span>)</span></span><br></pre></td></tr></table></figure><p>在定义数据类型完成后，即可将数据读入<code>RDD[model_instance]</code>的结构中，并通过<code>RDD</code>的隐式转换<code>.toDF()</code>方法完成<code>RDD</code>到<code>DataFrame</code>的转换：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rawData = sc.textFile(<span class="string">"file:///home/hduser/iris.data"</span>)</span><br><span class="line"><span class="keyword">val</span> df = rawData.map(line =&gt;</span><br><span class="line">      &#123; model_instance( <span class="type">Vectors</span>.dense(line.split(<span class="string">","</span>).filter(p =&gt; p.matches(<span class="string">"\\d*(\\.?)\\d*"</span>))</span><br><span class="line">      .map(_.toDouble)) )&#125;).toDF()</span><br></pre></td></tr></table></figure><p>​    与MLlib的操作类似，我们使用了filter算子，过滤掉类标签，正则表达式<code>\\d*(\\.?)\\d*</code>可以用于匹配实数类型的数字，<code>\\d*</code>使用了<code>*</code>限定符，表示匹配0次或多次的数字字符，<code>\\.?</code>使用了<code>?</code>限定符，表示匹配0次或1次的小数点。</p><p>​    可以通过创建一个<code>GaussianMixture</code>类，设置相应的超参数，并调用<code>fit(..)</code>方法来训练一个GMM模型<code>GaussianMixtureModel</code>，在该方法调用前需要设置一系列超参数，如下表所示：</p><ul><li>K:聚类数目，默认为2 </li><li>maxIter : 最大迭代次数，默认为100 </li><li>seed : 随机数种子，默认为随机Long值 </li><li>Tol : 对数似然函数收敛阈值，默认为0.01 </li></ul><p>其中，每一个超参数均可通过名为<code>setXXX(...)</code>（如maxIterations即为<code>setMaxIterations()</code>）的方法进行设置。这里，我们建立一个简单的<code>GaussianMixture</code>对象，设定其聚类数目为3，其他参数取默认值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> gm = <span class="keyword">new</span> <span class="type">GaussianMixture</span>().setK(<span class="number">3</span>)</span><br><span class="line">               .setPredictionCol(<span class="string">"Prediction"</span>)</span><br><span class="line">               .setProbabilityCol(<span class="string">"Probability"</span>)</span><br><span class="line"><span class="keyword">val</span> gmm = gm.fit(df)</span><br></pre></td></tr></table></figure><p>和<code>KMeans</code>等硬聚类方法不同的是，除了可以得到对样本的聚簇归属预测外，还可以得到样本属于各个聚簇的概率（这里我们存在”Probability”列中）。</p><p>​    调用<code>transform()</code>方法处理数据集之后，打印数据集，可以看到每一个样本的预测簇以及其概率分布向量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = gmm.transform(df)</span><br><span class="line">result.show(<span class="number">150</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>​    得到模型后，即可查看模型的相关参数，与KMeans方法不同，GMM不直接给出聚类中心，而是给出各个混合成分（多元高斯分布）的参数。在ML的实现中，GMM的每一个混合成分都使用一个<code>MultivariateGaussian</code>类（位于<code>org.apache.spark.ml.stat.distribution</code>包）来存储，我们可以使用<code>GaussianMixtureModel</code>类的<code>weights</code>成员获取到各个混合成分的权重，使用<code>gaussians</code>成员来获取到各个混合成分的参数（均值向量和协方差矩阵）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until gmm.getK) &#123;</span><br><span class="line">      println(<span class="string">"Component %d : weight is %f \n mu vector is %s \n sigma matrix is %s"</span> format</span><br><span class="line">      (i, gmm.weights(i), gmm.gaussians(i).mean, gmm.gaussians(i).cov))</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;Spark-ML-聚类算法&quot;&gt;&lt;a href=&quot;#Spark-ML-聚类算法&quot; class=&quot;headerlink&quot; title=&quot;Spark ML - 聚类算法&quot;&gt;&lt;/a&gt;Spark ML - 聚类算法&lt;/h2&gt;&lt;h3 id=
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Spark ML - 协同过滤</title>
    <link href="http://www.ihoge.cn/2018/ML1.html"/>
    <id>http://www.ihoge.cn/2018/ML1.html</id>
    <published>2018-05-05T17:59:21.000Z</published>
    <updated>2018-05-05T17:22:29.171Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>​    获取spark自带的MovieLens数据集，其中每行包含一个用户、一个电影、一个该用户对该电影的评分以及时间戳。我们使用默认的<code>ALS.train()</code> 方法，即显性反馈（默认<code>implicitPrefs</code> 为false）来构建推荐模型并根据模型对评分预测的均方根误差来对模型进行评估。</p><h3 id="导入需要的包："><a href="#导入需要的包：" class="headerlink" title="导入需要的包："></a>导入需要的包：</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">RegressionEvaluator</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.recommendation.<span class="type">ALS</span></span><br></pre></td></tr></table></figure><h3 id="根据数据结构创建读取规范"><a href="#根据数据结构创建读取规范" class="headerlink" title="根据数据结构创建读取规范"></a>根据数据结构创建读取规范</h3><p> 创建一个Rating类型，即[Int, Int, Float, Long];然后建造一个把数据中每一行转化成Rating类的函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Rating</span>(<span class="params">userId: <span class="type">Int</span>, movieId: <span class="type">Int</span>, rating: <span class="type">Float</span>, timestamp: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">parseRating</span>(<span class="params">str: <span class="type">String</span></span>)</span>: <span class="type">Rating</span> = &#123;</span><br><span class="line">            <span class="keyword">val</span> fields = str.split(<span class="string">"::"</span>)</span><br><span class="line">            assert(fields.size == <span class="number">4</span>)</span><br><span class="line">            <span class="type">Rating</span>(fields(<span class="number">0</span>).toInt, fields(<span class="number">1</span>).toInt, fields(<span class="number">2</span>).toFloat, fields(<span class="number">3</span>).toLong)</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure><h3 id="读取数据："><a href="#读取数据：" class="headerlink" title="读取数据："></a>读取数据：</h3><p> 导入implicits，读取MovieLens数据集，把数据转化成Rating类型；</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> ratings = spark.sparkContext.textFile(<span class="string">"file:///home/hduser/spark/data/mllib/als/sample_movielens_ratings.txt"</span>).map(parseRating).toDF()</span><br></pre></td></tr></table></figure><p>然后打印数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ratings.show()</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p> 把MovieLens数据集划分训练集和测试集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(training, test) = ratings.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>))</span><br></pre></td></tr></table></figure><p> 使用ALS来建立推荐模型，这里我们构建了两个模型，一个是显性反馈，一个是隐性反馈</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> alsExplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setUserCol(<span class="string">"userId"</span>). setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br><span class="line"><span class="keyword">val</span> alsImplicit = <span class="keyword">new</span> <span class="type">ALS</span>().setMaxIter(<span class="number">5</span>).setRegParam(<span class="number">0.01</span>).setImplicitPrefs(<span class="literal">true</span>). setUserCol(<span class="string">"userId"</span>).setItemCol(<span class="string">"movieId"</span>).setRatingCol(<span class="string">"rating"</span>)</span><br></pre></td></tr></table></figure><p> 在 ML 中的实现有如下的参数:</p><ul><li><code>numBlocks</code> 是用于并行化计算的用户和商品的分块个数 (默认为10)。</li><li><code>rank</code> 是模型中隐语义因子的个数（默认为10）。</li><li><code>maxIter</code> 是迭代的次数（默认为10）。</li><li><code>regParam</code> 是ALS的正则化参数（默认为1.0）。</li><li><code>implicitPrefs</code> 决定了是用显性反馈ALS的版本还是用适用隐性反馈数据集的版本（默认是false，即用显性反馈）。</li><li><code>alpha</code> 是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准（默认为1.0）。</li><li><p><code>nonnegative</code> 决定是否对最小二乘法使用非负的限制（默认为false）。</p><p>可以调整这些参数，不断优化结果，使均方差变小。比如：imaxIter越大，regParam越 小，均方差会越小，推荐结果较优。</p></li></ul><p>接下来，把推荐模型放在训练数据上训练：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> modelExplicit = alsExplicit.fit(training)</span><br><span class="line"><span class="keyword">val</span> modelImplicit = alsImplicit.fit(training)</span><br></pre></td></tr></table></figure><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p> 使用训练好的推荐模型对测试集中的用户商品进行预测评分，得到预测评分的数据集</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> predictionsExplicit = modelExplicit.transform(test)</span><br><span class="line"><span class="keyword">val</span> predictionsImplicit = modelImplicit.transform(test)</span><br></pre></td></tr></table></figure><p> 我们把结果输出，对比一下真实结果与预测结果：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictionsExplicit.show()</span><br><span class="line">predictionsImplicit.show()</span><br></pre></td></tr></table></figure><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p> 通过计算模型的均方根误差来对模型进行评估，均方根误差越小，模型越准确：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">RegressionEvaluator</span>().setMetricName(<span class="string">"rmse"</span>).setLabelCol(<span class="string">"rating"</span>). setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line"><span class="keyword">val</span> rmseExplicit = evaluator.evaluate(predictionsExplicit)</span><br><span class="line"><span class="keyword">val</span> rmseImplicit = evaluator.evaluate(predictionsImplicit)</span><br></pre></td></tr></table></figure><p> 打印出两个模型的均方根误差 ：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">println(<span class="string">s"Explicit:Root-mean-square error = <span class="subst">$rmseExplicit</span>"</span>)</span><br><span class="line">println(<span class="string">s"Implicit:Root-mean-square error = <span class="subst">$rmseImplicit</span>"</span>)</span><br></pre></td></tr></table></figure><p> 可以看到打分的均方差值为1.69和1.80左右。由于本例的数据量很少，预测的结果和实际相比有一定的差距。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;协同过滤算法&quot;&gt;&lt;a href=&quot;#协同过滤算法&quot; class=&quot;headerlink&quot; title=&quot;协同过滤算法&quot;&gt;&lt;/a&gt;协同过滤算法&lt;/h2&gt;&lt;p&gt;​    获取spark自带的MovieLens数据集，其中每行包含一个
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="ML" scheme="http://www.ihoge.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理3</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing3.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing3.html</id>
    <published>2018-04-21T19:59:31.000Z</published>
    <updated>2018-04-22T14:55:12.786Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="数据预处理3"><a href="#数据预处理3" class="headerlink" title="数据预处理3"></a>数据预处理3</h1><h2 id="3-1-描述性统计"><a href="#3-1-描述性统计" class="headerlink" title="3.1 描述性统计"></a>3.1 描述性统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    (<span class="string">'custID'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'gender'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'state'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'cardholder'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'balance'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numIntTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'creditLine'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'fraudRisk'</span>, typ.IntegerType()),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([</span><br><span class="line">    typ.StructField(e[<span class="number">0</span>], e[<span class="number">1</span>], <span class="keyword">True</span>) <span class="keyword">for</span> e <span class="keyword">in</span> fields</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: integer (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.groupBy(<span class="string">'gender'</span>).count().show()</span><br></pre></td></tr></table></figure><pre><code>+------+-------+|gender|  count|+------+-------+|     1|6178231||     2|3821769|+------+-------+</code></pre><p>⚠️ 这里男女比例失衡，在实际生产场景中应该重视这个问题</p><h3 id="descrive"><a href="#descrive" class="headerlink" title=".descrive()"></a>.descrive()</h3><p>这里可以选定某些列   <code>Dataframe的.describe()方法属于，RDD不可用</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line">desc = fraud_df.describe(numerical)</span><br><span class="line">desc.show()</span><br></pre></td></tr></table></figure><pre><code>+-------+-----------------+------------------+-----------------+|summary|          balance|          numTrans|      numIntTrans|+-------+-----------------+------------------+-----------------+|  count|         10000000|          10000000|         10000000||   mean|     4109.9199193|        28.9351871|        4.0471899|| stddev|3996.847309737077|26.553781024522852|8.602970115863767||    min|                0|                 0|                0||    max|            41485|               100|               60|+-------+-----------------+------------------+-----------------+</code></pre><ul><li>所有特征成正态分布；最大值是最小值的多倍</li><li>变异系数（均值与标准差之比）非常高（接近或大于1），意味着这是一个广泛的观测数据。</li></ul><p>如何检查偏度？（这里对“平衡”特征进行检查）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.agg(&#123;<span class="string">'balance'</span>:<span class="string">'skewness'</span>&#125;).show()</span><br></pre></td></tr></table></figure><pre><code>+------------------+| skewness(balance)|+------------------+|1.1818315552995033|+------------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.agg(&#123;<span class="string">'balance'</span>:<span class="string">'stddev'</span>&#125;).show()</span><br></pre></td></tr></table></figure><pre><code>+-----------------+|  stddev(balance)|+-----------------+|3996.847309737077|+-----------------+</code></pre><p>列表中的聚合函数包括：</p><ul><li>avg()</li><li>count()</li><li>countDistinct()</li><li>first()</li><li>kurtosis()</li><li>max()</li><li>mean()</li><li>skewness()</li><li>stddev()</li><li>stddev_pop()</li><li>stddev_samp()</li><li>sum()</li><li>sunDistinct()</li><li>var_pop()</li><li>var_samp()</li><li>variance()</li></ul><h2 id="3-2-相关性"><a href="#3-2-相关性" class="headerlink" title="3.2 相关性"></a>3.2 相关性</h2><p>模型应该只包括哪些与目标高度相关的特征，因此检查特征的相关性是非常重要的。其次之间的高度相关特征（即<strong>共线（collinear）性</strong>），可能会导致模型的不可预知性为或者可能进行不必要的复杂化。</p><p>这里的<strong>.corr(…)</strong>方法支持Pearson（皮尔森）相关系数，并且只能计算两两相关性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.corr(<span class="string">'balance'</span>, <span class="string">'numTrans'</span>)</span><br></pre></td></tr></table></figure><pre><code>0.00044523140172659576</code></pre><p>为了创建相关矩阵，可以使用以下脚本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line"></span><br><span class="line">corr=[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(numerical)):</span><br><span class="line">    temp = [<span class="keyword">None</span>] * i</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i, len(numerical)):</span><br><span class="line">        temp.append(fraud_df.corr(numerical[i], numerical[j]))</span><br><span class="line">    corr.append(temp)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 这里令temp = [None] * i是为了填充左下角矩阵</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">corrpd = pd.DataFrame(corr)</span><br><span class="line">corrpd.columns = corrpd.index = numerical</span><br><span class="line">corrpd</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>balance</th><br>      <th>numTrans</th><br>      <th>numIntTrans</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>balance</th><br>      <td>1.0</td><br>      <td>0.000445</td><br>      <td>0.000271</td><br>    </tr><br>    <tr><br>      <th>numTrans</th><br>      <td>NaN</td><br>      <td>1.000000</td><br>      <td>-0.000281</td><br>    </tr><br>    <tr><br>      <th>numIntTrans</th><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>1.000000</td><br>    </tr><br>  </tbody><br></table><br></div><h2 id="3-3-可视化"><a href="#3-3-可视化" class="headerlink" title="3.3 可视化"></a>3.3 可视化</h2><h3 id="3-3-1-直方图"><a href="#3-3-1-直方图" class="headerlink" title="3.3.1 直方图"></a>3.3.1 直方图</h3><p>当数据量非常大时，需要先对数据进行聚合:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hists = fraud_df.select(<span class="string">'balance'</span>).rdd.flatMap(<span class="keyword">lambda</span> x: x).histogram(<span class="number">20</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'bins'</span>:hists[<span class="number">0</span>][:<span class="number">-1</span>],</span><br><span class="line">    <span class="string">'freq'</span>:hists[<span class="number">1</span>]</span><br><span class="line">&#125;</span><br><span class="line">plt.bar(data[<span class="string">'bins'</span>], data[<span class="string">'freq'</span>], width=<span class="number">2000</span>)</span><br><span class="line">plt.title(<span class="string">'Histogram of \'balance\''</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqkvrtx0f5j31000g2q41.jpg" alt=""></p><h3 id="3-3-2-散点图"><a href="#3-3-2-散点图" class="headerlink" title="3.3.2 散点图"></a>3.3.2 散点图</h3><p>因为Pyspark不支持在服务器端的任何可视化模块，并且视图同时绘制数十亿的观测数据是非常不切实际的。在这个例子中，把欺诈数据集作为一个阶层抽取0.02%大约2000个观测数据。</p><p>使用<strong>df.sampleBy(…)</strong>方法随机抽取样本集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">numerical = [<span class="string">'balance'</span>, <span class="string">'numTrans'</span>, <span class="string">'numIntTrans'</span>]</span><br><span class="line"></span><br><span class="line">data_sample = fraud_df.sampleBy(<span class="string">'gender'</span>, &#123;<span class="number">1</span>: <span class="number">0.001</span>, <span class="number">2</span>: <span class="number">0.001</span>&#125;).select(numerical)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_sample.describe().show()</span><br></pre></td></tr></table></figure><pre><code>+-------+------------------+------------------+-----------------+|summary|           balance|          numTrans|      numIntTrans|+-------+------------------+------------------+-----------------+|  count|              2032|              2032|             2032||   mean| 3962.410433070866|28.546751968503937|3.989665354330709|| stddev|3876.8504670568145|25.611704520599815|8.235480127222067||    min|                 0|                 0|                0||    max|             28000|               100|               60|+-------+------------------+------------------+-----------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data_sample.rdd.map(<span class="keyword">lambda</span> x: [x[<span class="number">0</span>],x[<span class="number">1</span>],x[<span class="number">2</span>]]).collect()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> Scatter3D</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="comment"># data = [d1,d3,d2]</span></span><br><span class="line">range_color = [</span><br><span class="line">    <span class="string">'#313695'</span>, <span class="string">'#4575b4'</span>, <span class="string">'#74add1'</span>, <span class="string">'#abd9e9'</span>, <span class="string">'#e0f3f8'</span>, <span class="string">'#ffffbf'</span>,</span><br><span class="line">    <span class="string">'#fee090'</span>, <span class="string">'#fdae61'</span>, <span class="string">'#f46d43'</span>, <span class="string">'#d73027'</span>, <span class="string">'#a50026'</span>]</span><br><span class="line">scatter3D = Scatter3D(<span class="string">"3D 散点图示例"</span>, width=<span class="number">1200</span>, height=<span class="number">800</span>)</span><br><span class="line">scatter3D.add(<span class="string">"3D"</span>, </span><br><span class="line">              data, is_visualmap=<span class="keyword">True</span>, </span><br><span class="line">              visual_range_color=range_color, </span><br><span class="line">              grid3d_opacity=<span class="number">0.5</span>,</span><br><span class="line">              is_grid3d_rotate = <span class="keyword">True</span>,</span><br><span class="line">              xaxis3d_name=<span class="string">'balance'</span>,</span><br><span class="line">              yaxis3d_name=<span class="string">'numTrans'</span>,</span><br><span class="line">              zaxis3d_name=<span class="string">'numIntTrans'</span></span><br><span class="line">             )</span><br><span class="line">scatter3D.render()</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqkvm4tn42j31kw0szk4z.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;数据预处理3&quot;&gt;&lt;a href=&quot;#数据预处理3&quot; class=&quot;headerlink&quot; title=&quot;数据预处理3&quot;&gt;&lt;/a&gt;数据预处理3&lt;/h1&gt;&lt;h2 id=&quot;3-1-描述性统计&quot;&gt;&lt;a href=&quot;#3-1-描述性统计&quot;
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理2</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing2.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing2.html</id>
    <published>2018-04-21T19:59:21.000Z</published>
    <updated>2018-04-22T14:55:12.053Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="数据预处理2"><a href="#数据预处理2" class="headerlink" title="数据预处理2"></a>数据预处理2</h1><h2 id="2-1-RDD读取方式"><a href="#2-1-RDD读取方式" class="headerlink" title="2.1 RDD读取方式"></a>2.1 RDD读取方式</h2><p><strong>1、读取文件为RDD</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fileData = sc.textFile(<span class="string">'/pydata/ccFraud.gz'</span>)</span><br><span class="line">header = fileData.first()</span><br><span class="line"></span><br><span class="line">rddData = fileData \</span><br><span class="line">    .filter(<span class="keyword">lambda</span> row: row != header) \</span><br><span class="line">    .map(<span class="keyword">lambda</span> row: [int(elem) <span class="keyword">for</span> elem <span class="keyword">in</span> row.split(<span class="string">','</span>)])</span><br></pre></td></tr></table></figure><p><strong>2、创建schema</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [typ.StructField(h[:], typ.IntegerType(), <span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> header.split(<span class="string">','</span>)]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType(fields)</span><br></pre></td></tr></table></figure><p><strong>3、RDD创建DataFrame</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfData = spark.createDataFrame(fraud, schema)</span><br><span class="line">dfData.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- &quot;custID&quot;: integer (nullable = true) |-- &quot;gender&quot;: integer (nullable = true) |-- &quot;state&quot;: integer (nullable = true) |-- &quot;cardholder&quot;: integer (nullable = true) |-- &quot;balance&quot;: integer (nullable = true) |-- &quot;numTrans&quot;: integer (nullable = true) |-- &quot;numIntlTrans&quot;: integer (nullable = true) |-- &quot;creditLine&quot;: integer (nullable = true) |-- &quot;fraudRisk&quot;: integer (nullable = true)</code></pre><p><strong>4、创建视图</strong></p><p>这里视图sql查询失败，而在spark-shell中没问题，是什么原因？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfData.createOrReplaceTempView(<span class="string">"DataViw"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"select gender,count(*) from DataViw group by gender"</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+-------+|count()|+-------+|      0|+-------+</code></pre><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fqkmuptfisj30qo0f0gnx.jpg" alt=""></p><h2 id="2-2-DataFrame读取模式"><a href="#2-2-DataFrame读取模式" class="headerlink" title="2.2 DataFrame读取模式"></a>2.2 DataFrame读取模式</h2><p><strong>创建类型方式一</strong>：全部为IntegerType</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    <span class="string">'custID'</span>,</span><br><span class="line">    <span class="string">'gender'</span>,</span><br><span class="line">    <span class="string">'state'</span>,</span><br><span class="line">    <span class="string">'cardholder'</span>,</span><br><span class="line">    <span class="string">'balance'</span>,</span><br><span class="line">    <span class="string">'numTrans'</span>,</span><br><span class="line">    <span class="string">'numIntlTrans'</span>,</span><br><span class="line">    <span class="string">'creditLine'</span>,</span><br><span class="line">    <span class="string">'fraudRisk'</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([typ.StructField(f, typ.IntegerType(), <span class="keyword">True</span>) <span class="keyword">for</span> f <span class="keyword">in</span> fields])</span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: integer (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntlTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><p><strong>创建类型方式二</strong>: 指定每一列的类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.types <span class="keyword">as</span> typ</span><br><span class="line"></span><br><span class="line">fields = [</span><br><span class="line">    (<span class="string">'custID'</span>, typ.StringType()),</span><br><span class="line">    (<span class="string">'gender'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'state'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'cardholder'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'balance'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'numIntlTrans'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'creditLine'</span>, typ.IntegerType()),</span><br><span class="line">    (<span class="string">'fraudRisk'</span>, typ.IntegerType()),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">schema = typ.StructType([</span><br><span class="line">    typ.StructField(e[<span class="number">0</span>], e[<span class="number">1</span>], <span class="keyword">True</span>) <span class="keyword">for</span> e <span class="keyword">in</span> fields</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">fraud_df = spark.read.csv(<span class="string">"/pydata/ccFraud.gz"</span>, header=<span class="string">'true'</span>, schema = schema, sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- custID: string (nullable = true) |-- gender: integer (nullable = true) |-- state: integer (nullable = true) |-- cardholder: integer (nullable = true) |-- balance: integer (nullable = true) |-- numTrans: integer (nullable = true) |-- numIntlTrans: integer (nullable = true) |-- creditLine: integer (nullable = true) |-- fraudRisk: integer (nullable = true)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fraud_df.show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>+------+------+-----+----------+-------+--------+------------+----------+---------+|custID|gender|state|cardholder|balance|numTrans|numIntlTrans|creditLine|fraudRisk|+------+------+-----+----------+-------+--------+------------+----------+---------+|     1|     1|   35|         1|   3000|       4|          14|         2|        0||     2|     2|    2|         1|      0|       9|           0|        18|        0||     3|     2|    2|         1|      0|      27|           9|        16|        0||     4|     1|   15|         1|      0|      12|           0|         5|        0||     5|     1|   46|         1|      0|      11|          16|         7|        0||     6|     2|   44|         2|   5546|      21|           0|        13|        0||     7|     1|    3|         1|   2000|      41|           0|         1|        0||     8|     1|   10|         1|   6016|      20|           3|         6|        0||     9|     2|   32|         1|   2428|       4|          10|        22|        0||    10|     1|   23|         1|      0|      18|          56|         5|        0|+------+------+-----+----------+-------+--------+------------+----------+---------+only showing top 10 rows</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;数据预处理2&quot;&gt;&lt;a href=&quot;#数据预处理2&quot; class=&quot;headerlink&quot; title=&quot;数据预处理2&quot;&gt;&lt;/a&gt;数据预处理2&lt;/h1&gt;&lt;h2 id=&quot;2-1-RDD读取方式&quot;&gt;&lt;a href=&quot;#2-1-RDD读
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>spark数据预处理1</title>
    <link href="http://www.ihoge.cn/2018/DataPreprocessing1.html"/>
    <id>http://www.ihoge.cn/2018/DataPreprocessing1.html</id>
    <published>2018-04-21T19:59:00.000Z</published>
    <updated>2018-04-22T14:55:13.475Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Pyspark数据预处理1"><a href="#Pyspark数据预处理1" class="headerlink" title="Pyspark数据预处理1"></a>Pyspark数据预处理1</h1><h2 id="1-1-重复值"><a href="#1-1-重复值" class="headerlink" title="1.1 重复值"></a>1.1 重复值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">'M'</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">'F'</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">'M'</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>, <span class="string">'gender'</span>])</span><br></pre></td></tr></table></figure><h3 id="1-1-1-查重"><a href="#1-1-1-查重" class="headerlink" title="1.1.1 查重"></a>1.1.1 <strong>查重</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"总数据量：&#123;&#125;"</span>.format(df.count()))</span><br><span class="line">print(<span class="string">"重复数据量：&#123;&#125;"</span>.format(df.count() - df.distinct().count()))</span><br></pre></td></tr></table></figure><pre><code>总数据量：7重复数据量：1</code></pre><h3 id="1-1-2-去重"><a href="#1-1-2-去重" class="headerlink" title="1.1.2  去重"></a>1.1.2  <strong>去重</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df1 = df.dropDuplicates()</span><br><span class="line">df1.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+| id|weight|height|age|gender|+---+------+------+---+------+|  5| 133.2|   5.7| 54|     F||  5| 129.2|   5.3| 42|     M||  1| 144.5|   5.9| 33|     M||  4| 144.5|   5.9| 33|     M||  2| 167.2|   5.4| 45|     M||  3| 124.1|   5.2| 23|     F|+---+------+------+---+------+</code></pre><h3 id="1-1-3-查询排除ID列后的重复值"><a href="#1-1-3-查询排除ID列后的重复值" class="headerlink" title="1.1.3  查询排除ID列后的重复值"></a>1.1.3  <strong>查询排除ID列后的重复值</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"df1中样本量：&#123;&#125;"</span>.format(df1.count()))</span><br><span class="line">print(<span class="string">"df1去重后样本量：&#123;&#125;"</span>\</span><br><span class="line">      .format(</span><br><span class="line">      df1.select([c <span class="keyword">for</span> c <span class="keyword">in</span> df1.columns <span class="keyword">if</span> c != <span class="string">'id'</span>])</span><br><span class="line">      .distinct()</span><br><span class="line">      .count()))</span><br></pre></td></tr></table></figure><pre><code>df1中样本量：6df1去重后样本量：5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df2 = df1.dropDuplicates(subset=[c <span class="keyword">for</span> c <span class="keyword">in</span> df.columns <span class="keyword">if</span> c != <span class="string">'id'</span>])</span><br><span class="line">df2.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+| id|weight|height|age|gender|+---+------+------+---+------+|  5| 133.2|   5.7| 54|     F||  4| 144.5|   5.9| 33|     M||  2| 167.2|   5.4| 45|     M||  3| 124.1|   5.2| 23|     F||  5| 129.2|   5.3| 42|     M|+---+------+------+---+------+</code></pre><h2 id="1-1-4-查看是否有重复ID"><a href="#1-1-4-查看是否有重复ID" class="headerlink" title="1.1.4 查看是否有重复ID"></a>1.1.4 查看是否有重复ID</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line">df2.agg(</span><br><span class="line">    fn.count(<span class="string">'id'</span>).alias(<span class="string">'count'</span>),</span><br><span class="line">    fn.countDistinct(<span class="string">'id'</span>).alias(<span class="string">'distinct'</span>))\</span><br><span class="line">    .show()</span><br></pre></td></tr></table></figure><pre><code>+-----+--------+|count|distinct|+-----+--------+|    5|       4|+-----+--------+</code></pre><p>这里需要给每行数据赋唯一ID。通过<code>fn.monotonically_increasing_id()</code>方法给每一条记录提供一个唯一并且递增的ID，当数据放置在大约不到10亿个分区中，每个分区的记录少于8亿条，ID就能被保证时唯一的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df3 = df2.withColumn(<span class="string">'new_id'</span>, fn.monotonically_increasing_id())</span><br><span class="line">df3.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+---+------+-------------+| id|weight|height|age|gender|       new_id|+---+------+------+---+------+-------------+|  5| 133.2|   5.7| 54|     F|  25769803776||  1| 144.5|   5.9| 33|     M| 171798691840||  2| 167.2|   5.4| 45|     M| 592705486848||  3| 124.1|   5.2| 23|     F|1236950581248||  5| 129.2|   5.3| 42|     M|1365799600128|+---+------+------+---+------+-------------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df3.printSchema()</span><br></pre></td></tr></table></figure><pre><code>root |-- id: long (nullable = true) |-- weight: double (nullable = true) |-- height: double (nullable = true) |-- age: long (nullable = true) |-- gender: string (nullable = true) |-- new_id: long (nullable = false)</code></pre><h2 id="1-2-空缺值"><a href="#1-2-空缺值" class="headerlink" title="1.2. 空缺值"></a>1.2. 空缺值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_miss = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.6</span>, <span class="number">28</span>,   <span class="string">'M'</span>,  <span class="number">100000</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>,   <span class="string">'M'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="keyword">None</span> , <span class="number">5.2</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>,   <span class="string">'M'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>,   <span class="string">'F'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="keyword">None</span>, <span class="string">'F'</span>,  <span class="keyword">None</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>,   <span class="string">'M'</span>,  <span class="number">76000</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>, <span class="string">'gender'</span>, <span class="string">'income'</span>])</span><br></pre></td></tr></table></figure><h3 id="1-2-1-查找缺失值"><a href="#1-2-1-查找缺失值" class="headerlink" title="1.2.1 查找缺失值"></a>1.2.1 查找缺失值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_miss.rdd.map(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x[<span class="string">'id'</span>], sum([c <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">for</span> c <span class="keyword">in</span> x]))</span><br><span class="line">).collect()</span><br></pre></td></tr></table></figure><pre><code>[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_miss.where(<span class="string">'id == 3'</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+------+| id|weight|height| age|gender|income|+---+------+------+----+------+------+|  3|  null|   5.2|null|  null|  null|+---+------+------+----+------+------+</code></pre><h3 id="1-2-2-检查每一列缺失值数据的百分比："><a href="#1-2-2-检查每一列缺失值数据的百分比：" class="headerlink" title="1.2.2 检查每一列缺失值数据的百分比："></a>1.2.2 检查每一列缺失值数据的百分比：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_miss.agg(*[</span><br><span class="line">    (<span class="number">1</span> - (fn.count(c) / fn.count(<span class="string">'*'</span>))).alias(c + <span class="string">'_missing'</span>)</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns</span><br><span class="line">]).show()</span><br></pre></td></tr></table></figure><pre><code>+----------+------------------+--------------+------------------+------------------+------------------+|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|+----------+------------------+--------------+------------------+------------------+------------------+|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|+----------+------------------+--------------+------------------+------------------+------------------+</code></pre><h3 id="1-2-3-移除缺失严重的特征"><a href="#1-2-3-移除缺失严重的特征" class="headerlink" title="1.2.3 移除缺失严重的特征"></a>1.2.3 移除缺失严重的特征</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式一：</span></span><br><span class="line">df_miss1 = df_miss.select([</span><br><span class="line">    c <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns <span class="keyword">if</span> c != <span class="string">'income'</span></span><br><span class="line">])</span><br><span class="line">df_miss1.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+| id|weight|height| age|gender|+---+------+------+----+------+|  1| 143.5|   5.6|  28|     M||  2| 167.2|   5.4|  45|     M||  3|  null|   5.2|null|  null||  4| 144.5|   5.9|  33|     M||  5| 133.2|   5.7|  54|     F||  6| 124.1|   5.2|null|     F||  7| 129.2|   5.3|  42|     M|+---+------+------+----+------+</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式二： 设定阀值,阀值越大过滤越严格</span></span><br><span class="line">df_miss2 = df_miss.dropna(thresh=<span class="number">3</span>)</span><br><span class="line">df_miss2.show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+------+----+------+------+| id|weight|height| age|gender|income|+---+------+------+----+------+------+|  1| 143.5|   5.6|  28|     M|100000||  2| 167.2|   5.4|  45|     M|  null||  4| 144.5|   5.9|  33|     M|  null||  5| 133.2|   5.7|  54|     F|  null||  6| 124.1|   5.2|null|     F|  null||  7| 129.2|   5.3|  42|     M| 76000|+---+------+------+----+------+------+</code></pre><h3 id="1-2-4-填充空缺值-很好用的方法"><a href="#1-2-4-填充空缺值-很好用的方法" class="headerlink" title="1.2.4 填充空缺值(很好用的方法)"></a>1.2.4 填充空缺值(很好用的方法)</h3><p>使用df.fillna()方法并传给它一个字典效率高</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充平均值，先创建平均值字典</span></span><br><span class="line">means = df_miss1.agg(</span><br><span class="line">        *[fn.mean(c).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss1.columns <span class="keyword">if</span> c != <span class="string">'gender'</span>])\</span><br><span class="line">        .toPandas().to_dict(<span class="string">'records'</span>)[<span class="number">0</span>]</span><br><span class="line">means[<span class="string">'gender'</span>] = <span class="string">'missing'</span></span><br><span class="line"></span><br><span class="line">df_miss1.fillna(means).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------------------+------+---+-------+| id|            weight|height|age| gender|+---+------------------+------+---+-------+|  1|             143.5|   5.6| 28|      M||  2|             167.2|   5.4| 45|      M||  3|140.28333333333333|   5.2| 40|missing||  4|             144.5|   5.9| 33|      M||  5|             133.2|   5.7| 54|      F||  6|             124.1|   5.2| 40|      F||  7|             129.2|   5.3| 42|      M|+---+------------------+------+---+-------+</code></pre><h2 id="1-3-离群值"><a href="#1-3-离群值" class="headerlink" title="1.3. 离群值"></a>1.3. 离群值</h2><h3 id="1-3-1-定义离群区间"><a href="#1-3-1-定义离群区间" class="headerlink" title="1.3.1 定义离群区间"></a>1.3.1 定义离群区间</h3><p>IQR：定义为上分位和下分位之差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">df_outliers = spark.createDataFrame([</span><br><span class="line">        (<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.3</span>, <span class="number">28</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">154.2</span>, <span class="number">5.5</span>, <span class="number">45</span>),</span><br><span class="line">        (<span class="number">3</span>, <span class="number">342.3</span>, <span class="number">5.1</span>, <span class="number">99</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.5</span>, <span class="number">33</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.4</span>, <span class="number">54</span>),</span><br><span class="line">        (<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.1</span>, <span class="number">21</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>),</span><br><span class="line">    ], [<span class="string">'id'</span>, <span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>])</span><br></pre></td></tr></table></figure><p>计算每个特征的上下截断点：<code>.approxQuantitle(...)方法</code></p><ul><li>第一个参数指定列名</li><li>第二个参数可以时0～1之间的任意数（0.5为指定中位数）</li><li>第三个参数指定对每个度量的容忍度（如果为1，就会计算一个度量的准确值）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cols = [<span class="string">'weight'</span>, <span class="string">'height'</span>, <span class="string">'age'</span>]</span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    quantiles = df_outliers.approxQuantile(col, [<span class="number">0.25</span>, <span class="number">0.75</span>], <span class="number">0.01</span>)</span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [quantiles[<span class="number">0</span>] - <span class="number">0.4</span> * IQR, quantiles[<span class="number">1</span>] + <span class="number">0.4</span> * IQR]</span><br><span class="line">    </span><br><span class="line">bounds</span><br><span class="line"><span class="comment"># 根据实际业务情况进行调整</span></span><br></pre></td></tr></table></figure><pre><code>{&apos;age&apos;: [17.6, 64.4], &apos;height&apos;: [4.9399999999999995, 5.66], &apos;weight&apos;: [119.19999999999999, 164.2]}</code></pre><h3 id="1-3-2-标记离群值"><a href="#1-3-2-标记离群值" class="headerlink" title="1.3.2 标记离群值"></a>1.3.2 标记离群值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">outliers = df_outliers\</span><br><span class="line">            .select(*[<span class="string">'id'</span>]</span><br><span class="line">            + [(</span><br><span class="line">                (df_outliers[c] &lt; bounds[c][<span class="number">0</span>])|</span><br><span class="line">                (df_outliers[c] &gt; bounds[c][<span class="number">1</span>])</span><br><span class="line">            ).alias(c + <span class="string">'_o'</span>) <span class="keyword">for</span> c <span class="keyword">in</span> cols ])</span><br><span class="line">outliers.show()</span><br></pre></td></tr></table></figure><pre><code>+---+--------+--------+-----+| id|weight_o|height_o|age_o|+---+--------+--------+-----+|  7|   false|   false|false||  6|   false|   false|false||  5|   false|   false|false||  1|   false|   false|false||  3|    true|   false| true||  2|   false|   false|false||  4|   false|   false|false|+---+--------+--------+-----+</code></pre><h3 id="1-3-3-查看离群值"><a href="#1-3-3-查看离群值" class="headerlink" title="1.3.3 查看离群值"></a>1.3.3 查看离群值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bad_outlier = df_outliers.join(outliers, on=<span class="string">'id'</span>)</span><br><span class="line">bad_outlier.filter(<span class="string">'weight_o'</span>).select(<span class="string">'id'</span>, <span class="string">'weight'</span>).show()</span><br><span class="line">bad_outlier.filter(<span class="string">'age_o'</span>).select(<span class="string">'id'</span>, <span class="string">'age'</span>).show()</span><br></pre></td></tr></table></figure><pre><code>+---+------+| id|weight|+---+------+|  3| 342.3|+---+------++---+---+| id|age|+---+---+|  3| 99|+---+---+</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Pyspark数据预处理1&quot;&gt;&lt;a href=&quot;#Pyspark数据预处理1&quot; class=&quot;headerlink&quot; title=&quot;Pyspark数据预处理1&quot;&gt;&lt;/a&gt;Pyspark数据预处理1&lt;/h1&gt;&lt;h2 id=&quot;1-1
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Strutured Streaming(Spark 2.3)</title>
    <link href="http://www.ihoge.cn/2018/StructuredStreaming.html"/>
    <id>http://www.ihoge.cn/2018/StructuredStreaming.html</id>
    <published>2018-04-19T02:59:21.000Z</published>
    <updated>2018-04-22T14:55:14.155Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark-2-3-Strutured-Streaming"><a href="#Spark-2-3-Strutured-Streaming" class="headerlink" title="Spark 2.3 Strutured Streaming"></a>Spark 2.3 Strutured Streaming</h1><p>为了继续实现 Spark 更快，更轻松，更智能的目标，Spark 2.3 在许多模块都做了重要的更新，比如 Structured Streaming 引入了低延迟的连续处理（continuous processing）；支持 stream-to-stream joins；通过改善 pandas UDFs 的性能来提升 PySpark；支持第四种调度引擎 Kubernetes clusters（其他三种分别是自带的独立模式Standalone，YARN、Mesos）。除了这些比较具有里程碑的重要功能外，Spark  2.3 还有以下几个重要的更新：<br>引入 DataSource v2 APIs<br>矢量化(Vectorized)的 ORC reader<br>Spark History Server v2 with K-V store<br>基于 Structured Streaming 的机器学习管道API模型<br>MLlib 增强<br>Spark SQL 增强<br>这篇文章将简单地介绍上面一些高级功能和改进，更多的特性请参见 <a href="https://spark.apache.org/releases/spark-release-2-3-0.html" target="_blank" rel="noopener">Spark 2.3 release notes</a></p><p>这里主要针对Spark2.3 对Structured Streaming的新特性进行描述。</p><p>The simplest way to perform streaming analytics is not having to reason about streaming.</p><h2 id="毫秒延迟的连续流处理"><a href="#毫秒延迟的连续流处理" class="headerlink" title="毫秒延迟的连续流处理"></a>毫秒延迟的连续流处理</h2><p>Apache Spark 2.0 的 Structured Streaming 将微批次处理(micro-batch processing)从它的高级 APIs 中解耦出去，原因有两个：首先，开发人员更容易学习这些 API，不需要考虑这些 APIs 的微批次处理情况;其次，它允许开发人员将一个流视为一个无限表，他们查询流的数据，就像他们查询静态表一样简便。<br>但是，为了给开发人员提供不同的流处理模式，社区引入了一种新的毫秒级低延迟(millisecond low-latency连续模式(continuous mode)。<br>在内部，结构化的流引擎逐步执行微批中的查询计算，执行周期由触发器间隔决定，这个延迟对大多数真实世界的流应用程序来说是可以容忍的。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241174964831.jpg" alt=""></p><h3 id="微批处理（micro-batch）"><a href="#微批处理（micro-batch）" class="headerlink" title="微批处理（micro-batch）"></a>微批处理（micro-batch）</h3><p>Structured Streaming默认使用微量批处理执行模型。这意味着Spark流引擎会定期检查流源，并对自上次批量结束后到达的新数据运行批量查询。At a high-level, it looks like this:<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241491220257.jpg" alt=""></p><p>在微批处理体系结构中，驱动程序通过将记录偏移量保存到<strong>预写的日志中（write-ahead-log）</strong>来检查进度，然后用它来重新启动查询。请注意，为了获得确定性的重新执行和端对端语义，在微批处理启动之前，要在下一个微批处理中处理的范围偏移保存到日志中。因此，数据源中可用的记录可能<strong>不得不等待当前的微批次在其偏移记录之前</strong>完成，并且在下一个微批中处理它。过程看起来像这样。</p><p>In this architecture, the driver checkpoints the progress by saving the records offsets to a write-ahead-log, which may be then used to restart the query. Note that the range offsets to be processed in the next micro-batch is saved to the log before the micro-batch has started in order to get deterministic re-executions and end-to-end semantics. As a result, a record that is available at the source may have to wait for the current micro-batch to be completed before its offset is logged and the next micro-batch processes it. At the record level, the timeline looks like this.<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241495217055.jpg" alt=""><br>这会导致更多的延迟时间。</p><h3 id="连续处理（Continuous-Processing）"><a href="#连续处理（Continuous-Processing）" class="headerlink" title="连续处理（Continuous Processing）"></a>连续处理（Continuous Processing）</h3><p>而对于连续模式，流读取器连续拉取源数据并处理数据，而不是按指定的触发时间间隔读取一批数据。通过不断地查询源数据和处理数据，新的记录在到达时立即被处理，将等待时间缩短到毫秒，满足低延迟的应用程序的需求，具体如下面图所示：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241175216530.jpg" alt=""></p><p>目前连续模式支持 map-like Dataset 操作，包括投影(projections)、selections以及其他 SQL 函数，但是不支持 current_timestamp(), current_date() 以及聚合函数。它还支持将 Kafka 作为数据源和数据存储目的地(sink)，也支持 console 和 memory sink。<br>现在，开发人员可以根据延迟要求选择模式连续或微量批处理，来构建大规模实时流式传输应用程序，同时这些系统还能够享受到 Structured Streaming 提供的 fault-tolerance 和 reliability guarantees 特性。<br>简单来说，Spark 2.3 中的连续模式是实验性的，它提供了以下特性：<br>端到端的毫秒级延迟<br>至少一次语义保证<br>支持 map-like 的 Dataset 操作</p><h2 id="流与流进行Join"><a href="#流与流进行Join" class="headerlink" title="流与流进行Join"></a>流与流进行Join</h2><p>Spark 2.0 版本的 Structured Streaming 支持流 DataFrame/Dataset 和静态数据集之间的 join，但是 Spark 2.3 带来了期待已久的流和流的 Join 操作。支持内连接和外连接，可用在大量的实时场景中。<br>广告收益是流与流进行Join的典型用例。例如，展示广告流和广告点击流共享您希望进行流式分析的公共关键字(如adId)和相关数据，根据这些数据你可以分析出哪些广告更容易被点击。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241175635666.jpg" alt=""></p><p>这个例子看起来很简单，但是实现流和流的Join需要解决很多技术难题，如下：</p><ul><li><strong>缓存处理延迟数据：</strong>需要缓存延迟的数据，直到从其他流中找到匹配的事件;</li><li><strong>先置缓冲区大小：</strong>限制流式连接缓冲区大小的唯一方法是将延迟超过某个阈值的数据丢弃。此最大延迟阈值应由用户根据业务需求和系统资源限制之间的平衡进行配置。</li><li><strong>明确定义的语义</strong>：在静态连接和流式连接之间保持一致的SQL连接语义，具有或不具有上述阈值。</li></ul><p>幸运的事Spark2.3解决了所有这些问题，因此我们使用SQL连接的清晰语义来表达计算，并控制相关事件之间的延迟容忍。<br>假设有两个不同的kafka流，我们要通过两个流中的共有的<code>adID</code>属性进行连接。让我们来看看代码是怎么实现的：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241453518177.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241453673148.jpg" alt=""></p><p>与所有的结构化数据流的查询相同，代码是完全一样的。将kafka流数据当作静态的Dataframe一样去定义。当执行此查询时，结构化流式处理引擎会根据需要将点击和展示作为流状态进行缓冲。对于特定广告，只要接收到两个相关事件（即收到第二个事件后），即会生成联合输出。随着数据到达，连接的输出将逐步生成并写入查询接收器（例如，另一个Kafka）。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241457085725.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark-2-3-Strutured-Streaming&quot;&gt;&lt;a href=&quot;#Spark-2-3-Strutured-Streaming&quot; class=&quot;headerlink&quot; title=&quot;Spark 2.3 Strutu
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="strutured Streaming" scheme="http://www.ihoge.cn/tags/strutured-Streaming/"/>
    
  </entry>
  
  <entry>
    <title>流式处理框架 &amp; Spark Streaming &amp; Flume &amp; Kafka</title>
    <link href="http://www.ihoge.cn/2018/strame.html"/>
    <id>http://www.ihoge.cn/2018/strame.html</id>
    <published>2018-04-19T02:59:21.000Z</published>
    <updated>2018-04-22T14:55:14.911Z</updated>
    
    <content type="html"><![CDATA[<h1 id="流式处理框架-Spark-Streaming"><a href="#流式处理框架-Spark-Streaming" class="headerlink" title="流式处理框架 Spark Streaming"></a>流式处理框架 Spark Streaming</h1><ul><li>很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。</li><li>对于技术人员来说，可以利用数据挖掘和OLAP（On-Line Analytical Processing）分析工具从中找到对企业有价值的信息，这也是离线批处理的一般过程。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241530856455.jpg" alt=""></li></ul><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol><li>流式处理框架基础</li><li>Flume</li><li>Kafka</li></ol><h2 id="1-流式处理框架基础"><a href="#1-流式处理框架基础" class="headerlink" title="1. 流式处理框架基础"></a>1. 流式处理框架基础</h2><h3 id="1-1-流数据一般特征"><a href="#1-1-流数据一般特征" class="headerlink" title="1.1 流数据一般特征"></a>1.1 流数据一般特征</h3><p><strong>流数据具有如下特征：</strong></p><ul><li>数据快速持续到达，潜在大小也许是无穷无尽的；<ul><li>数据来源众多，格式复杂；</li><li>数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储；</li><li>注重数据的整体价值，不过分关注个别数据；</li><li>数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序；</li></ul></li></ul><p>因此，用于静态数据存储的存储工具和批处理计算框架便无法满足流式计算、实时分析的应用需求。</p><h3 id="1-2-实时计算的核心框架"><a href="#1-2-实时计算的核心框架" class="headerlink" title="1.2 实时计算的核心框架"></a>1.2 实时计算的核心框架</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241530202093.jpg" alt=""></p><h3 id="1-3-数据源"><a href="#1-3-数据源" class="headerlink" title="1.3 数据源"></a>1.3 数据源</h3><p><strong>为何数据源在流式计算中显得异常重要</strong></p><p>所谓数据源，是指为了满足不同需求而不断输出数据的框架。不同于针对静态数据的批处理，要进行实时计算，就必须有足够“实时”且能够稳定运行的消息“投递者”，它必须能够高效的整合更低一级数据源发送的实时数据，并根据处理框架的API对数据进行规范化处理和转存，同时，对于流式处理系统而言，很多时候经过流式处理框架处理后的数据也是流数据，也可视作一种数据源。</p><p><strong>数据源的高低级之分</strong></p><p>对于流式处理框架而言，数据源的高低级之分，其实就是数据“输出”框架的高低级之分。任何数据源，要进行收集、转存和传递等工作，就需要有与之对应的处理框架，其框架越完善、功能越强大、且能够提供更便于实时处理的数据，我们就称其高级数据源，反之就称为低级数据源。</p><p><strong>数据源本身的“处理框架”</strong></p><p>这其实并不是规范的说法，通常来说，低级数据源我们会直接称呼其数据流，如文件流、套接字流等，而高级数据源，我们更多的会根据其作用，称其为日志收集系统（如Flume），或分布式消息队列（如Kafka）。对于初学者而言，我们可以将这些复杂的框架名称统一想象成一个数据的中转站，这些框架的核心作用都是收集、暂存然后投递数据。</p><p><strong>大数据分析师的知识边界</strong></p><p>一方面，我们要进行流式计算，就必须了解数据源对计算框架投递消息的方法，如此才能根据数据源设计算法，或者根据计算需求调整数据源；另一方面，我们也要区别大数据分析师与ETL工程师的区别，我们只需要了解与计算框架最近一层的数据源是如何与计算框架相互嵌套的既可，而无需再深究该数据源更低一层数据源、甚至是数据产生第一层数据源是如何工作的。</p><h2 id="2-Flume"><a href="#2-Flume" class="headerlink" title="2. Flume"></a>2. Flume</h2><h3 id="2-1-Flume介绍"><a href="#2-1-Flume介绍" class="headerlink" title="2.1 Flume介绍"></a>2.1 Flume介绍</h3><p>Flume是Cloudera公司开发的分布式、高可用的日志收集系统。现已开源给Apache。<br>目前比较有代表性的日志收集系统，除了Flume之外，还有FaceBook的Scribe。</p><p>Flume原始版本为OG，后经过整体架构的重新设计，以改名为Flume-NG。Flume发展至今，已不限于日志收集，还可以通过简单的配置，收集不同数据源的海量数据并将数据准确高效的传输到不同的处理框架或数据存储系统。<br>目前Flume可对接的主流大数据框架有Hadoop、Kafka、Hive、HBase、Spark等。</p><h3 id="2-2-Flume基本架构"><a href="#2-2-Flume基本架构" class="headerlink" title="2.2 Flume基本架构"></a>2.2 Flume基本架构</h3><p>Flume-NG采用三层架构设计，分别对应其三个核心组件：Source、Channel和Sink，其中，Source主要功能为收集上一层数据源传输过来的数据，Channel用于数据的暂时存储，Sink则用于数据的处理，即数据的传输对象和传输方式。其基本架构如下图所示：<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241531282473.jpg" alt=""></p><p>其中，一个Agent代表一个Java进程，上图表示一个Event（数据）在一个Agent的传输流程。</p><ul><li><code>Event</code>：一条消息或者一个数据，具有可选的头信息，在头信息中可以设置时间戳、主机名等信息；</li><li><code>Source</code>：数据源，接收或者收集不同形式的数据源；</li><li><code>Channel</code>：Event的临时缓冲区，Source接收的Event会先发送到Channel进行缓存，在缓冲区内等待Sink的消费；</li><li><code>Sink</code>：用于处理Channel中缓存的数据，并发送至下一层（处理框架、存储中心或者下一个Agent）；</li><li><code>Agent</code>：包含了Source、Channel、Sink等组件的Flume进程；</li><li><code>Interceptor</code>：Event拦截器，在数据进入Channel之前根据配置要求，对数据进行头信息（Header）编辑，添加时间戳、主机名称等；</li><li><code>Selector</code>：Event的选择器，即决定Event流入Channel的方式，主要有<code>复制</code>（Replicating）和<code>复用</code>（Multiplexing）两种选方式；</li><li><code>Sink Processor</code>：Event Sink处理器，Flume提供了故障转移处理器和负载均衡处理器两种。</li></ul><h3 id="2-3-Flume核心组件-Source"><a href="#2-3-Flume核心组件-Source" class="headerlink" title="2.3 Flume核心组件:Source"></a>2.3 Flume核心组件:Source</h3><p><strong>Source核心功能</strong></p><ul><li>用于对接各种数据源，并将收集到的Event发送至用于临时存储的Channel中。Flume中每个Agent对应一个Source，而每个Agent的Source在启动前必须通过修改配置变量来设置其接受消息种类，此即称为Source种类</li></ul><p><strong>常用Scoure种类</strong>(依据数据类型分类)</p><ul><li><p><code>Avro Source</code>：Avro是Doug Cutting牵头开发的一个数据序列化系统，设计用于支持大批量数据交换的应用。Avro Source支持Avro协议，接收RPC请求，<code>Avro Source</code>通过监听Avro端口接收外部Avro客户端流事件，在Flume的多层架构中经常被使用接收上游Sink发送的event。</p></li><li><p><code>Kafka Source</code>：对接分布式消息队列Kafka，作为Kafka的消费者持续从Kafka中拉取数据，如果多个kafka source同时消费Kafka中同一个Topic，则其会被设置为同组id，从而保证多个Kafka Source之间不会重复拉取数据。</p></li><li><p><code>Exec Source</code>：支持Linux命令，收集标准输出数据或者通过tail -f file的方式监听指定文件。Exce Source可以实现实时的消息传输，但却不会记录已读取的文件的位置，不支持断点续传，如果Exce Source重启或者挂掉都会造成后续增加的消息无法接收，建议只在测试环境中使用。</p></li><li><p><code>Spooling Directory Source</code>：用于监听一个文件夹，收集文件夹中新文件数据，手机玩新文件数据会将文件名称的后缀改为.completed，缺点是不支持老文件（已经completed的文件）中新增数据集的收集，并且不能够对嵌套文件夹进行递归监听。</p></li><li><p><code>Taildir Source</code>：监听一个文件或文件夹，通过正则表达式匹配需要监听的数据源文件，支持文件夹嵌套递归监听，Taildir Source通过将监听文件的位置写入到文件中，从而实现断点续传，并且能够保证没有重复的数据读取。</p></li></ul><h3 id="2-4-Flume核心组件-Channel"><a href="#2-4-Flume核心组件-Channel" class="headerlink" title="2.4 Flume核心组件:Channel"></a>2.4 Flume核心组件:Channel</h3><p><strong>Channel核心功能</strong></p><ul><li>Channel是Event的<code>临时缓冲区</code>，存储Source收集但尚未被Sink读取的Event，其目标是为平衡Source收集速度和Sink读取速度，可视为Flume内部的消息队列。Channel线程的安全性较高且具有事务性，支持Source写失败重复写和Sink读失败重复读等操作。同时，我们根据Channel存储方式划分Channel种类。</li></ul><p><strong>常用Channel种类</strong> (依据存储方式分类)</p><ul><li><p>Memory Channel：<code>缓冲区所有数据都存于内存</code>，Memory Channel读写速度快，但存储量受内存限制，且当Flume进程挂掉、服务器宕机或重启时都会造成数据丢失。建议在服务器内存充足，且不关心数据丢失的场景下使用。</p></li><li><p>File Channel：缓冲区所有数据写入磁盘，File Channel存储容量大，无数据丢失风险。File Channel数据存储路径可以配置多个磁盘文件路径，通过磁盘并行写入提高其性能，在写入磁盘时是顺序写入，且单个数据文件大小可通过配置文件中maxFileSize参数进行调整，当被写入文件大小超过上限时，Flume会自动创建新文件用来存储后续Event。但数据文件数量不会无限增长，一旦旧文件被Sink读取完成，就将被删除。Flume通过设置检查点和备份检查点实现Agent重启之后快速将File Channel中的数据按顺序回放到内存中，以保证Agent在失败重启后仍能快速安全地提供服务。</p></li><li><p><code>Kafka Channel</code>：值得一提的是，Kafka可作为Flume中Channel存储方式，Kafka是分布式、可扩展、高容错、高吞吐的分布式系统，Kafka通过其优秀的架构设计充分利用磁盘顺序读写特性，在廉价的硬件条件下就能完成高效的消息发布和订阅，对比其他两种Channel，Kafka Channel在读取速度、存储量和容错性上完美的弥补了二者短板，若能合理利用Kafka性能，能够达到事半功倍的效果。</p></li></ul><h3 id="2-5-Flume核心组件-Sink"><a href="#2-5-Flume核心组件-Sink" class="headerlink" title="2.5 Flume核心组件:Sink"></a>2.5 Flume核心组件:Sink</h3><p><strong>Sink核心功能</strong></p><ul><li>用于处理Channel中缓存的数据，当数据经过Sink Processor处理后由Sink进行后续处理，Sink可将数据传输至静态数据存储中心进行数据保存，也可以将数据传输至实时处理框架进行数据处理，当然，也可以将数据传输至下一层数据源，进行进一步数据聚合、整理或推送。</li></ul><p><strong>常用Sink种类</strong> (依据投递接收方划分)</p><ul><li><p>Avro Sink：Avro Sink常用于对接下一层Arvo Source，通过发送RPC请求将Event发送到下一层的Avro Source，同时，Avro Sink提供了端到端的批量压缩数据传输，从而解决RPC传输过程中占用大量网络资源以及产生大量Socket连接等问题。</p></li><li><p>HDFS Sink：HDFS作为Hadoop生态中的最常用文件系统，具有高容错、可扩展、高性能、低成本等特点，HDFS Sink通过将Event写入HDFS进行数据存储，能够有效、长期存储大量数据。</p></li><li><p>Kafka Sink：在消息传递过程中采用Kafka框架能够从很大程度上降低系统耦合度，从而增加系统系统稳定性和容错机制，Flume可通过Kafka Sink将Event写入Kafka的Topic，其他应用通过Kafka获得数据。Flume从1.7.0开始支持Kafka 0.9及以上版本。</p></li></ul><h3 id="Interceptor（拦截器）-与选择器配合使用"><a href="#Interceptor（拦截器）-与选择器配合使用" class="headerlink" title="Interceptor（拦截器） (与选择器配合使用)"></a>Interceptor（拦截器） (与选择器配合使用)</h3><p><strong>Interceptor（拦截器）功能介绍</strong></p><ul><li>Source将Event写入Channel之前，可以用拦截器对Event进行添加头信息等简单处理，Source和Channel之间可设多个拦截器，不同拦截器可根据自身规则对Event进行简单处理，注意，拦截器属于轻量级插件，无法应对复杂数据处理工作。</li></ul><p><strong>常用Interceptor</strong></p><ul><li><p>主机拦截器（Host Interceptor）：Flume通过主机拦截器在Event头信息中添加主机名称或者IP。通过主机拦截器，Channel可以根据不同的主机信息分区存储Event，后续Sink也可根据不同主机信息对Event进行分别处理。</p></li><li><p>静态拦截器（Static Interceptor）：主要用于修改、过滤Event在此被拦截之前所设置的信息。</p></li></ul><h3 id="Selector（选择器）-与拦截器配合使用"><a href="#Selector（选择器）-与拦截器配合使用" class="headerlink" title="Selector（选择器） (与拦截器配合使用)"></a>Selector（选择器） (与拦截器配合使用)</h3><p><strong>功能介绍</strong></p><ul><li>Source发送的Event通过Channel选择器来决定以何种方式写入Channel，Flume提供了三种常用的选择器，分别是复制Channel选择器（Replicating Channel Selector）、复用Channel选择器（Multiplexing Channel Selector）和自定义选择器。</li></ul><p><strong>常用选择器</strong></p><ul><li><p>复制选择器（ Replicating Channel Selector ）：是Flume选择器的默认模式，即不对选择器进行设置时采用的模式，此时Source将以复制的方式将一个Event写入多个Channel中，不同的Sink可从不同的的Channel中获得相同的数据。复制选择器用途较多，当一个Event要做多个用途时，可考虑用复制选择器。</p></li><li><p>复用Channel选择器（Multiplexing Channel Selector）：复用选择器需要配合拦截器共同使用，复用选择器会根据Event的头信息来判断每个Event应该写入哪个Channel中。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241532271911.jpg" alt=""></p></li></ul><p>💡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">关于负载均衡和故障排除的相关说明:</span><br><span class="line">Flume为了提高整体容错能力和稳定性，提供了负载均衡故障转移功能</span><br><span class="line">这两项功能配置较为简单，只要合理配置Sink组，并且在每组Sink中设置多个</span><br><span class="line">子Sink，就能够自动进行负载均衡和故障转移。</span><br></pre></td></tr></table></figure><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p>Kafka是由LinkedIn公司开源的分布式消息队列。现已加入到Apache软件基金会，并且凭借其高吞吐、可扩展、高可用、部署简单、开发接口丰富等特性，已在各大互联网公司的实际生产环境中广泛使用。<br>同时，大多数分布式处理系统都支持使用kafka，如Spark、Storm、Druid、Flume等<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15241532641098.jpg" alt=""></p><h3 id="Kafka基本特点"><a href="#Kafka基本特点" class="headerlink" title="Kafka基本特点"></a>Kafka基本特点</h3><ul><li>Kafka其实就是消息“中转站”</li><li>Kafka本身不产生消息，也不对消息进行永久存储，它只是消息的“中转站”。</li></ul><ul><li>数据“中转站”至关重要</li><li>在实际工作中，我们广泛需要消息“中转站”的原因，是数据采集端种类各异，消息格式数据格式种类繁多，同时数据应用情景也非常复杂，提取数据可能为了实时计算，可能为了存储一段时间后进行批处理，也可能直接推送至数据产品前段，也可能中间夹杂各种其他过程以及各过程之间的复用。因此，我们需要“中转站”对数据进行暂存、简单处理、再次投递。</li></ul><ul><li>Kafka作为最优秀的消息“中转站”之一，并被广泛认可，完全得益于其优秀的架构。</li></ul><h3 id="Kafka构架特点"><a href="#Kafka构架特点" class="headerlink" title="Kafka构架特点"></a>Kafka构架特点</h3><ol><li>生产者和消费者不需要彼此了解</li><li>消费者的性能问题不会影响生产者</li><li>消费者受到保护，免受生产者的伤害</li><li>在处理负载方面有很大的灵活性</li><li>消息可供任何人使用 - </li><li>大量的新用例，监控，审计和故障排除</li></ol><ul><li>高吞吐率</li></ul><ol><li>Kafka利用顺序读写磁盘的设计，提供了可以和内存随机读写相匹敌的读写速度；</li><li>其灵活的客户端API设计，利用Linux操作系统提供的“零拷贝”特性，减少了消息网络传输的时间；</li><li>同时提供端到端的消息压缩传输，对同一主题下的消息采用分布式存储；</li></ol><ul><li>高容错、高可用</li></ul><p>Kafka在集群模式下允许用户对每个分区（partition）配置多个副本，且将副本均匀的分到各个节点（broker）内存储，保证同一个分区的副本不会再同一台机器上存储。同时，多副本之间采用Leader-Follower机制同步消息，只有Leader对外提供读写服务，当Leader意外失败、Broker进程关闭、服务器宕机等情况导致数据不可用时，kafka会从Follower中选择一个成为Leader继续提供读写服务。</p><ul><li>可拓展</li></ul><p>理论上Kafka的性能随着Broker的增加而增加，增加一个Broker只需为新增Broker设置一个唯一编号，配置文件编写完成后，Kafka就能通过ZooKeeper发现新的Broker，并投入使用。</p><ul><li>接藕</li></ul><p>Kafka内部能够将消息生产阶段和处理阶段分开，两个阶段互相独立，且各自能实现自己的处理逻辑，通过Kafka提供的消息写入和消费接口实现对消息的连接处理。两个阶段相互独立，不仅降低了自身复杂度，同时也实现了对外部框架提供部分服务的功能（如Flume中嵌入Kafka）。</p><ul><li><strong>峰值处理</strong></li></ul><p>实际工作中，经常会遇见数据在某个时间点爆发式增长（如双11），此时如果后台处理系统无法及时处理峰值需求，就会导致数据积压，严重时会导致系统崩溃。而若能合理使用Kafka进行数据中转，就相当于给系统接入了一个巨大的缓冲区，其既能接收持续暴增的请求，也能根据后台需求提供服务，进而提高了系统数据处理能力。</p><h3 id="Kafka基本架构"><a href="#Kafka基本架构" class="headerlink" title="Kafka基本架构"></a>Kafka基本架构</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15241533218101.jpg" alt=""></p><ul><li>Producer代表消息生产者</li><li>Consumer代表消息消费者</li><li>Broker代表Kafka集群中各节点</li><li>Partition表示消息的一个分区</li><li>ZooKeeper为Kafka集群提供资源调度服务</li></ul><h3 id="Kafka基本概念"><a href="#Kafka基本概念" class="headerlink" title="Kafka基本概念"></a>Kafka基本概念</h3><ul><li><p><strong>Broker</strong>：一个Kafka的实例就是一个Broker，相当于Flume中的Agent；</p></li><li><p><strong>Topic</strong>：主题，Kafka中同一类型数据集的名称，相当于数据库中的表，Producer将同一类型数据写入同一个Topic中，同一个Consumer或Consumer Group从同一个Topic中消费数据，同时，一个Topic在物理上会被分成多分存储到不同的物理机上；</p></li><li><p><strong>Partition</strong>：分区，一个Topic可设置多个分区，相当于把一个数据集分成多分，存储到不同分区中进行存储（类似于HDFS），分区命名规则为topicname-index；</p></li><li><p><strong>Segment</strong>：段文件，Kafka中最小存储单位，一个topic包含多个Partition，一个Partition又包含多个Segment，Segment以其内部消息的起始偏移量进行索引；</p></li><li><p><strong>Offset</strong>：消息的起始偏移量，可作为Segment的索引；</p></li><li><p>Replication：副本，一个Partition可有一个或多个副本，创建Topic时可设置副本数量；</p></li><li><p>Producer：消息生产者，负责向Kafka集群中发布消息；</p></li><li><p>Consumer Group：消费者组，一个Consumer Group可包含一个或多个Consumer，当一个topic被一个消费者组消费的时候，一条消息只能由其中一位消费者消费，不会出现多位消费者消费同一条信息的情况；</p></li><li><p>Consumer：消息消费者，可从指定topic中拉取消息；</p></li><li><p>ZooKeeper：kafka需要ZooKeeper对其进行协调管理，安装Kafka过程将自带一个ZooKeeper。</p></li></ul><h2 id="实操流程"><a href="#实操流程" class="headerlink" title="实操流程"></a>实操流程</h2><h3 id="单节点演示"><a href="#单节点演示" class="headerlink" title="单节点演示"></a>单节点演示</h3><ol><li><p>启动zoopeeper服务<br> <code>bin/zookeeper-server-start.sh config/zookeeper.properties</code></p></li><li><p>启动Kafka服务<br> <code>bin/kafka-server-start.sh config/server.properties</code></p></li><li><p>创建topic取名cdatest<br> <code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic cdatest</code></p></li><li><p>查看topic<br> <code>bin/kafka-topics.sh --list --zookeeper localhost:2181</code></p></li><li><p>启动生产者指定topic，并在终端输入测试数据<br> <code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic cdatest</code></p></li><li><p>启动消费者指定topic<br> <code>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic cdatest --from-beginning</code></p></li></ol><h3 id="集群模式测试"><a href="#集群模式测试" class="headerlink" title="集群模式测试"></a>集群模式测试</h3><ol><li><p>修改参数server.properties</p></li><li><p>启动zookeeper集群服务</p></li><li><p>启动kafka集群（分别在各节点执行）<br><code>./bin/kafka-server-start.sh config/server.properties</code></p></li><li><p>任意一个节点创建topic<br><code>./bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 3 --topic kktest</code></p></li><li><p>查看topic状态<br><code>./bin/kafka-topics.sh --describe --zookeeper master:2181,slave1:2181,slave2:2181 --topic kktest</code></p></li><li><p>创建producer，手动输入测试数据（以master为例）<br><code>./bin/kafka-console-producer.sh --broker-list master:9092 --topic kktest</code></p></li><li><p>以slave1为例创建consumer消费数据<br><code>./bin/kafka-console-consumer.sh --bootstrap-server slave1:9092 --from-beginning --topic kktest</code></p></li></ol><p><strong>可以在不同节点创建多个生产者和消费者</strong></p><h2 id="使用外部数据源进行数据传输"><a href="#使用外部数据源进行数据传输" class="headerlink" title="使用外部数据源进行数据传输"></a>使用外部数据源进行数据传输</h2><ol><li><p>启动Kafka，启动prodecer和consumer</p></li><li><p>在Kafka安装路径内创建test.txt然后执行:q</p></li><li>~/kafka/bin/connect-standalone.sh ~/kafka/config/connect-standalone.properties ~/kafka/config/connect-file-source.properties ~/kafka/config/connect-file-sink.properties</li></ol><p><strong>自定义模式读取外部文件时，有两个主要的配置文件</strong><br>connect-file-source.properties<br>connect-file-sink.properties</p><p>根据需要修改文件读取模式、文件路径、Topic等</p><h2 id="Kafka于Flume联合部署"><a href="#Kafka于Flume联合部署" class="headerlink" title="Kafka于Flume联合部署"></a>Kafka于Flume联合部署</h2><p>创建flume配置文件：如kafkaSource.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent    a1.sources = r1    a1.sinks = k1    a1.channels = c1   # Describe/configure the source    a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource    a1.sources.r1.kafka.topics = kafkaTopic  a1.sources.r1.kafka.bootstrap.servers = master:9092,slave1:9092,slave2:9092  # Describe the sink    a1.sinks.k1.type = logger # Use a channel which buffers events in memory    a1.channels.c1.type = memory    a1.channels.c1.capacity = 1000    a1.channels.c1.transactionCapacity = 100   # Bind the source and sink to the channel    a1.sources.r1.channels = c1    a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p>启动Flume：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/flume/bin/flume-ng agent --conf ~/flume/conf --conf-file ~/flume/conf/kafkaSource.conf --name a1 -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>创建kafkaTopic</p><p><code>~/kafka/bin/kafka-topics.sh --create --zookeeper master:2181,slave1:2181,slave2:2181 --replication-factor 2 --partitions 3 --topic kafkaTopic</code></p><p>创建producer，输入测试信息<br><code>~/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sparkMLLib</code></p><p><strong>顺序出错的分析</strong></p><ol><li>Kafka传出来的数据顺序就已经被打乱</li><li>计算资源（虽然传输的数据很微小，但是集群的启动成本高昂）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;流式处理框架-Spark-Streaming&quot;&gt;&lt;a href=&quot;#流式处理框架-Spark-Streaming&quot; class=&quot;headerlink&quot; title=&quot;流式处理框架 Spark Streaming&quot;&gt;&lt;/a&gt;流式处理框架 Spark Streami
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://www.ihoge.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark的基本架构</title>
    <link href="http://www.ihoge.cn/2018/IntroductionToSpark.html"/>
    <id>http://www.ihoge.cn/2018/IntroductionToSpark.html</id>
    <published>2018-04-18T02:59:21.000Z</published>
    <updated>2018-04-22T14:55:15.713Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="Spark的基本架构"><a href="#Spark的基本架构" class="headerlink" title="Spark的基本架构"></a>Spark的基本架构</h1><p>当单机没有足够的能力和资源来执行大量信息的计算（或者低延迟计算），这时就需要一个集群或一组机器将许多机器的资源集中在一起，使我们可以使用全部累积的在一起的计算和存储资源。现在只有一组机器不够强大，你需要一个框架来协调他们之间的工作。 Spark是一种工具，可以管理和协调跨计算机集群执行数据任务。<br>Spark用于执行任务的机器集群可以由Spark的Standalone，YARN或Mesos等集群管理器进行管理。然后，我们向这些集群管理器提交Spark应用程序，这些集群管理器将资源授予我们的应用程序，以便我们完成我们的工作。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240180152481.jpg" alt=""></p><h3 id="1-Spark-Application"><a href="#1-Spark-Application" class="headerlink" title="1. Spark Application"></a>1. Spark Application</h3><p>Spark应用程序由一个驱动程序进程和一组执行程序进程组成。Driver进程运行main（）函数，位于集群中的一个节点上，它负责三件事：维护Spark应用程序的相关信息;回应用户的程序或输入;分配和安排Executors之间的工作。驱动程序过程是绝对必要的 - 它是Spark应用程序的核心，并在应用程序的生命周期中保留所有相关信息。<br>Executor负责实际执行Driver分配给他们的工作。这意味着，每个Executor只有两个任务：执行由驱动程序分配给它的代码，并将该执行程序的计算状态报告给驱动程序节点。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239867627806.jpg" alt=""></p><p>群集管理器控制物理机器并为Spark应用程序分配资源。这可以是几个核心集群管理员之一：Spark的Standalone，YARN或Mesos。这意味着可以同时在群集上运行多个Spark应用程序。<br>在前面的插图中，左侧是我们的driver，右侧是四个executors。在该图中，我们删除了群集节点的概念。用户可以通过配置指定有多少执行者应该落在每个节点上。</p><ul><li>Spark有一些集群管理器，负责调度可用资源。</li><li>驱动程序进程负责执行执行程序中的驱动程序命令，以完成我们的任务。</li></ul><h3 id="2-Spark’s-Languge-APIs"><a href="#2-Spark’s-Languge-APIs" class="headerlink" title="2. Spark’s Languge APIs"></a>2. Spark’s Languge APIs</h3><p>尽管我们的executor大多会一直运行Spark代码。但我们仍然可以通过Spark的语言API用多种不同语言运行Spark代码。大多数情况下，Spark会在每种语言中提供一些核心“concepts”，并将不同语言的代码译成运行在机器集群上的Spark代码。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239873536392.jpg" alt=""></p><p> <code>Spark有两套基本的API：低级非结构化(Unstructured)API和更高级别的结构化(Structured)API。</code></p><h3 id="3-SparkSession"><a href="#3-SparkSession" class="headerlink" title="3. SparkSession"></a>3. SparkSession</h3><p>我们通过驱动程序来控制Spark应用程序。该驱动程序进程将自身作为名为SparkSession并作为唯一的接口API对象向用户开放。 SparkSession实例是Spark在群集中执行用户定义操作的方式。 SparkSession和Spark应用程序之间有一对一的对应关系。在Scala和Python中，变量在启动控制台时可用作spark。让我们看下简单的Scala和/或Python中的SparkSession。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239862105558.jpg" alt=""></p><h3 id="4-Dataframe"><a href="#4-Dataframe" class="headerlink" title="4. Dataframe"></a>4. Dataframe</h3><p>DataFrame是最常见的<code>Structured API</code>（结构化API），只是表示有类型的<code>包含行和列的数据表</code>。一个简单的比喻就是一个带有命名列的电子表格。其根本区别在于，当电子表格位于一台计算机上某个特定位置时，Spark DataFrame可以跨越数千台计算机。将数据放在多台计算机上的原因无非有两种：数据太大而无法放在一台计算机上，或者在一台计算机上执行计算所需的时间太长。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15239862709297.jpg" alt=""></p><p>DataFrame概念并不是Spark独有的。 R和Python都有相似的概念。但是，Python / R DataFrame（有一些例外）存在于一台机器上，而不是多台机器上。这限制了您可以对python和R中给定的DataFrame执行的操作与该特定机器上存在的资源进行对比。但是，由于Spark具有适用于Python和R的<code>Spark’s Language APIs</code>，因此将Pandas（Python）DataFrame转换为Spark DataFrame和R DataFrame转换为Spark DataFrame（R）非常容易。</p><p><strong>注意</strong><br>Spark有几个核心抽象：Datasets，Dadaframes，SQL Table和弹性分布式数据集（RDD）。这些抽象都表示分布式数据集合，但它们有不同的接口来处理这些数据。最简单和最有效的是DataFrames，它可以用于所有语言。<strong>以下概念适用于所有的核心抽象。</strong></p><h3 id="5-Partitions"><a href="#5-Partitions" class="headerlink" title="5. Partitions"></a>5. Partitions</h3><p>为了允许每个执行者并行执行工作，Spark将数据分解成称为分区的块。分区是位于集群中的一台物理机上的一组行。 DataFrame的分区表示数据在执行过程中如何在整个机器群中物理分布。如果你有一个分区，即使你有数千个执行者，Spark也只会有一个分区。如果有多个分区，但只有一个执行程序Spark仍然只有一个并行性，因为只有一个计算资源。<br>值得注意的是，使用DataFrames，我们不会（大部分）操作 手动分区（基于个人）。我们只需指定物理分区中数据的高级转换，并且Spark确定此工作将如何在集群上实际执行。较低级别的API确实存在（通过弹性分布式数据集接口）。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240195737335.jpg" alt=""></p><h3 id="6-Transformations"><a href="#6-Transformations" class="headerlink" title="6. Transformations"></a>6. Transformations</h3><p>在Spark中，核心数据结构是不可改变的，这意味着一旦创建它们就不能更改。起初，这可能看起来像一个奇怪的概念，如果你不能改变它，你应该如何使用它？为了“更改”DataFrame，您必须指示Spark如何修改您所需的DataFrame。这些说明被称为<code>转换</code>。<br>转换操作没有返回输出，这是因为我们只指定了一个抽象转换，并且Spark不会在转换之前采取行动，直到我们执行一个动作。Transformations是如何使用Spark来表达业务逻辑的核心。Spark有两种类型的Transformations，一种是窄依赖转换关系，一种是宽依赖转换关系。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15239881561168.jpg" alt=""><br>宽依赖指输入分区对多输出分区起作用（多个孩子）。这被称为shuffle，Spark将在群集之间交换分区。对于窄依赖转换，Spark将自动执行称为流水线的操作，这意味着如果我们在DataFrame上指定了多个过滤器，它们将全部在内存中执行。当我们执行shuffle时，Spark会将结果写入磁盘。</p><h3 id="7-Lazy-Evaluation"><a href="#7-Lazy-Evaluation" class="headerlink" title="7. Lazy Evaluation"></a>7. Lazy Evaluation</h3><p>Lazy Evaluation意味着Spark将等到执行计算指令图的最后时刻。在Spark中，我们不是在表达某些操作时立即修改数据，而是建立起来应用于源数据的转换计划。Spark将把原始DataFrame转换计划编译为一个高效的物理计划，该计划将在群集中尽可能高效地运行。这为最终用户带来了巨大的好处，因为Spark可以优化整个数据流从端到端。这方面的一个例子就是所谓的“predicate pushdown” DataFrames。如果我们构建一个大的Spark作业，但在最后指定了一个过滤器，只需要我们从源数据中获取一行，则执行此操作的最有效方法就是访问我们需要的单个记录。 Spark实际上会通过自动推低滤波器来优化这一点。</p><h3 id="8-Actions"><a href="#8-Actions" class="headerlink" title="8. Actions"></a>8. Actions</h3><p>转换使我们能够建立我们的逻辑计划。为了触发计算，我们需要一个动作操作。一个动作指示Spark计算一系列转换的结果。<br>在指定我们的操作时，我们开始了一个Spark作业，它运行我们的过滤器转换（一个窄依赖转换），然后是一个聚合（一个宽依赖转换），它在每个分区的基础上执行计数，然后一个collect将我们的结果带到各自语言的本地对象。我们可以通过检查Spark UI（<a href="http://localhost:4040）来看到所有这些，Spark" target="_blank" rel="noopener">http://localhost:4040）来看到所有这些，Spark</a> UI是一个包含在Spark中的工具，它允许我们监视集群上运行的Spark作业。</p><h3 id="9-Dataframe-amp-SQL"><a href="#9-Dataframe-amp-SQL" class="headerlink" title="9. Dataframe &amp; SQL"></a>9. Dataframe &amp; SQL</h3><p>Spark SQL是Spark为结构化和半结构化数据处理设计的最受欢迎的模块之一。 Spark SQL允许用户使用SQL或可在Java，Scala，Python和R中使用的DataFrame和Dataset API来查询Spark程序中的structured data。由于DataFrame API提供了一种统一的方法来访问各种的数据源（包括Hive datasets，Avro，Parquet，ORC，JSON和JDBC），用户能够以相同方式连接到任何数据源，并将这些多个数据源连接在一起。 Spark SQL使用Hive meta store为用户提供了与现有Hive数据，查询和UDF完全兼容的功能。用户可以无缝地 在Spark上无需修改即可运行其当前的Hive工作负载。<br>Spark SQL也可以通过spark-sql shell来访问，现有的业务工具可以通过标准的JDBC和ODBC接口进行连接。</p><p>现在我们通过一个示例并在DataFrame和SQL中进行跟踪。不管语言如何，以完全相同的方式启动相同的转换。您可以在SQL或DataFrames（R，Python，Scala或Java）中表达业务逻辑，并且在实际执行代码之前，Spark会将该逻辑编译计划优化并最终生成最优的物理计划。 Spark SQL允许您作为用户将任何DataFrame注册为表或视图（临时表），并使用纯SQL查询它。编写SQL查询或编写DataFrame代码之间没有性能差异 都“编译”到我们在DataFrame代码中指定的相同底层计划。<br>通过一个简单的方法调用就可以将任何DataFrame制作成表格或视图。</p><p><strong>With SQl</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162004942.jpg" alt=""><br><strong>With DataFrame</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162492089.jpg" alt=""></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15240162738329.jpg" alt=""></p><p>现在有7个步骤将我们带回源数据。您可以在这些DataFrame的解释计划中看到这一点。以上图解说明了我们在“代码”中执行的一系列步骤。真正的执行计划（解释中可见的执行计划）将与下面的执行计划有所不同，因为在物理执行方面进行了优化，然而，该执行计划与任何计划一样都是起点。这个执行计划是一个有向无环图（DAG）的转换，每个转换产生一个新的不可变DataFrame，我们在这个DataFrame上调用一个动作来产生一个结果。</p><ol><li>第一步是读取数据。但是Spark实际上并没有读取它（Lazy Evaluation）</li><li>第二步是我们的分组，在技术上，当我们调用groupBy时，我们最终得到了一个RelationalGroupedDataset，它是DataFrame的一个奇特名称，该DataFrame具有指定的分组，但需要用户在可以进一步查询之前<strong>指定聚合</strong>。</li><li>因此第三步是指定聚合。我们使用总和聚合方法。这需要输入一列 表达式或简单的列名称。 sum方法调用的结果是一个新的dataFrame。你会看到它有一个新的模式，但它知道每个列的类型。（再次强调！）这里没有执行计算是非常重要的。这只是我们表达的另一种转换，Spark仅仅能够跟踪我们提供的类型信息。</li><li>第四步是简化语言，我们使用withColumnRename给原始列重新定义新名称。当然，这不会执行计算 - 这只是另一种转换！</li><li>第五步导入一个函数对数据进行排序，即desc函数。从destination_total列中找到的最大值。</li><li>第六步，我们将指定一个限制。这只是说明我们只需要五个值。这就像一个过滤器，只是它按位置而不是按值过滤。可以肯定地说，它基本上只是指定了一定大小的DataFrame。</li><li>最后一步是我们的行动！现在我们实际上开始收集上面的DataFrame结果的过程，Spark将以我们正在执行的语言返回一个列表或数组。现在我们看下它的解释计划。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15240170262028.jpg" alt=""><br>虽然这个解释计划与我们确切的“概念计划”不符，但所有的部分都在那里。可以看到limit语句以及orderBy（在第一行）。你也可以看到我们的聚合是如何在partial_sum调用中的两个阶段发生的。这是因为数字列表是可交换的，并且Spark可以执行sum()并按分区进行划分。当然，我们也可以看到我们如何在DataFrame中读取数据。同时我们也可以将它写出到Spark支持的任何数据源中。例如，假设我们想要将这些信息存储在PostgreSQL等数据库中，或者将它们写入另一个文件。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;Spark的基本架构&quot;&gt;&lt;a href=&quot;#Spark的基本架构&quot; class=&quot;headerlink&quot; title=&quot;Spark的基本架构&quot;&gt;&lt;/a&gt;Spark的基本架构&lt;/h1&gt;&lt;p&gt;当单机没有足够的能力和资源来执行大量信息
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://www.ihoge.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>分布式数据库HBase原理</title>
    <link href="http://www.ihoge.cn/2018/hbase.html"/>
    <id>http://www.ihoge.cn/2018/hbase.html</id>
    <published>2018-04-16T05:59:21.000Z</published>
    <updated>2018-04-22T14:55:17.503Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="目录："><a href="#目录：" class="headerlink" title="目录："></a>目录：</h2><ol><li>概述</li><li>HBase访问借口</li><li>HBase数据模型</li><li>HBase实现原理</li><li>HBase运行机制</li><li>HBase应用方案</li></ol><hr><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>HBase是一个高可靠、高性能、面向咧、可伸缩的分布式数据库。是谷歌<code>BigTable</code>的开源实现，主要用来存储<code>非结构化</code>和<code>半结构化</code>的松散数据。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238450244324.jpg" alt=""><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238450357302.jpg" alt=""></p><h3 id="为什么要HBase？"><a href="#为什么要HBase？" class="headerlink" title="为什么要HBase？"></a>为什么要HBase？</h3><ol><li>受限于Hadoop MR编程框架的高延迟数据处理机制，<code>无法满足大规模数据实时处理的需求</code>。</li><li>HDFS不能随机访问</li><li>传统关系型数据无法应对剧增的海量数据</li><li>传统关系型数据库在数据结构变化时需要停机维护；孔裂浪费存储空间</li></ol><p>因此，业界出现了一类面相半结构化数据存储和处理的高可扩展、低写入、查询延迟的系统。如键值数据库、文档数据库和<code>列族</code>数据库（如BitTable和HBase）。如今HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中。</p><h3 id="HBase与传统关系数据库的对比"><a href="#HBase与传统关系数据库的对比" class="headerlink" title="HBase与传统关系数据库的对比"></a>HBase与传统关系数据库的对比</h3><table><thead><tr><th>特性</th><th>传统关系数据库</th><th>HBase</th></tr></thead><tbody><tr><td>数据类型</td><td>关系模式，具有丰富的数据类型后和存储方式</td><td>采用更简洁的数据模型，吧数据存储为未经解释的字符串</td></tr><tr><td>数据操作</td><td>包含丰富的操作，涉及复杂的夺标链接</td><td>不存在复杂的表于表之间的关系，只有简单的增、删、查、清空等，避免表于表间复杂关系</td></tr><tr><td>存储模式</td><td>基于行存储</td><td>基于列存储，每个列族有几个文件保存，不同列族的文件是分离的</td></tr><tr><td>数据索引</td><td>通过针对不同列构建复杂多个索引以提高访问性能</td><td>只有一个索引（行键），访问方法为或行键访问或行键扫面，通过巧妙的设计，速度不会慢下来</td></tr><tr><td>数据维护</td><td>更新操作用最新的数据覆盖旧的</td><td>更新操作生成一个新的版本，久的版本仍然保留</td></tr><tr><td>可伸缩性</td><td>很难实现横向拓展，纵向拓展空间也有限</td><td>可以轻易通过在集群中增加或减少硬件数量实现性能伸缩</td></tr></tbody></table><h3 id="HBase访问接口"><a href="#HBase访问接口" class="headerlink" title="HBase访问接口"></a>HBase访问接口</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238467033431.jpg" alt=""></p><h2 id="HBase数据模型"><a href="#HBase数据模型" class="headerlink" title="HBase数据模型"></a>HBase数据模型</h2><ol><li>数据模型概述</li><li>数据模型相关概念</li><li>概念视图</li><li>物理视图</li><li>面向列的存储</li></ol><h3 id="数据模型概述"><a href="#数据模型概述" class="headerlink" title="数据模型概述"></a>数据模型概述</h3><ul><li>HBase是一个<code>稀疏</code>、<code>多维度</code>、排序的<code>映射表</code>，这张表的索引时<code>行键</code>、<code>列族</code>、<code>列限定符</code>、、<code>时间戳</code>。</li><li>每一个值是一个未经解释的字符串，没有数据类型。</li><li>每一行都有一个可排序的行键和任意多的列</li><li>表在水平方向由一个或者多个列族组成，一个列族可以包含任意多个列，同一个列族里面的数据存储在一起</li><li>列族支持动态扩展，轻松的添加列族或列，无预先定义列的数量和类型。所有列均以字符串形式存储，用户需自行进行数据类型转换。</li><li>HBase中执行更新操作时，生成新版本，保留旧版本，查询时默认返回最新版本。创建时可以设置最多保留版本数量。<h3 id="数据模型相关概念"><a href="#数据模型相关概念" class="headerlink" title="数据模型相关概念"></a>数据模型相关概念</h3></li><li>表：HBase采用表来组织数据，表由行和列组成，列换分为若干个列族，避免夺标链接操作，追求分析效率。</li><li>行：每个HBAse表由若干行组成，每个行由行键（Row Key）来标示。不给行键所在的列进行命名，让其拥有纵向可拓展性。</li><li>列族：一个HBase表被分组成许多“列族”的集合，它是基本的访问控制单元，也是基本存储单元。</li><li>列限定符：相当于列名</li><li>单元格：在HBaae表中，通过行、列族和列限定符确定一个“单元格”，（时间戳定义其版本）单元格中存储的数据没有数据类型，总被视为字节数组byte[]</li><li>HBase中需要根据行键、列族、列限定符和时间戳确定一个单元格，因此是一个“四维”坐标。<br>💡：上文中提到HBase数据只有一个索引（行键），</li></ul><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238479970872.jpg" alt=""></p><h3 id="概念视图"><a href="#概念视图" class="headerlink" title="概念视图"></a>概念视图</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238492197984.jpg" alt=""></p><h3 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238492407274.jpg" alt=""></p><h3 id="面向列的存储"><a href="#面向列的存储" class="headerlink" title="面向列的存储"></a>面向列的存储</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238493181117.jpg" alt=""></p><p>💡<strong>不同存储模型优劣对比</strong></p><ul><li><p>面向行存储的数据库主要采用NSM（N-ary Storage Model）存储模型，即一个元组（行）会被连续存储在磁盘页中，数据是一行行进行存储，读取也是一行行进行读取。当要选取某属性进行分析时，也需要首先扫面完整元组内容。</p><ul><li>优点：适用于联机事务性数据处理，即将分布于不同地理位置的数据利用网络进行连接，进而进行统一的存储和管理。</li><li>缺点：鄙视和分析性操作</li></ul></li><li><p>面向列存储的数据库主要采用DSM（Decompostion Storage Model）存储模型，该模型会对关系进行垂直分解，并为每个属性分配一个子关系，每个子关系单独存储。</p><ul><li>优点：在批处理和即兴查询等分析操作中能够直接定位目标列，能够有有效I/O开销；同一列数据类型相同，存储过程能够拥有很高的数据压缩旅，从而节省存储空间。</li><li>缺点：执行连续操作时要付出昂贵的元组重构代价。</li></ul></li></ul><p>💡 <strong>总结：NSM存储模型更加适合事务型应用，DSM存储模型更加适合分析性应用</strong></p><h2 id="HBase实现原理"><a href="#HBase实现原理" class="headerlink" title="HBase实现原理"></a>HBase实现原理</h2><h3 id="HBase功能组件"><a href="#HBase功能组件" class="headerlink" title="HBase功能组件"></a>HBase功能组件</h3><ul><li>库函数</li><li>一个Master主服务器</li><li>许多个Region服务器</li></ul><p>Master负责管理和维护Hbase表的分区信息，维护Region服务器列表，分配Region，负责均衡，和Namenode功能类似。</p><p>Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求，和Datanode功能类似。</p><p>客户端并不是直接从Master主服务器读取数据，而是在获得Region的存储位置后，直接从Region服务器上读取数据。</p><p>客户端并不依赖Master，而是通过<code>Zookeeper</code>来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计是的Master负载很小。</p><h3 id="表和Region"><a href="#表和Region" class="headerlink" title="表和Region"></a>表和Region</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238507508277.jpg" alt=""></p><ul><li>一个表包含多个Region</li><li>开始只有一个Region，后来不断分裂</li><li><p>Regin拆分操作非常快（开始只是修改文件指向），接近瞬间。因为拆分之后的Region读取的仍然是原存储文件，知道“合并”过程吧存储文件异步写到独立的文件之后，才会读取新文件。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238509042129.jpg" alt=""></p></li><li><p>每个Region默认大小是100MB到200MB（2006之前）</p><ul><li>每个Region的最佳大小取决于丹台服曲奇的有效处理能力</li><li>目前每个Region的最佳大小建议1G～2G（2013以后）</li></ul></li><li>每个Region不会被分拆到多个Region服务器（Region最小不可分）</li><li>每个Region服务器存储10～1000个Region<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238514793939.jpg" alt=""></li></ul><h3 id="Region的定位"><a href="#Region的定位" class="headerlink" title="Region的定位"></a>Region的定位</h3><ul><li>元数据表，又名<code>META</code>表，存储了<code>Region</code>和<code>Region</code>服务器的映射关系。</li><li>当HBase表很大时，<code>META</code>表也会被分裂成多个<code>Region</code></li><li>根数据表，有明<code>ROOT</code>表，记录所有元数据的具体位置</li><li><code>ROOT</code>表只有唯一一个<code>Region</code>，名字是在程序中被写死的</li><li><p><code>Zookeeper</code>文件记录了<code>ROOT</code>表的位置<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238589518486.jpg" alt=""></p></li><li><p>为了加快访问速度，<code>META</code>表的全部<code>Region</code>都会被保存在内存中</p></li><li>假设<code>META</code>表的每行（一个映射条目）在内存中大约占用1KB，每个<code>Region</code>限制为128MB，那么上面三层结构可以曹村的用户数据表的<code>Region</code>数目的计算方法是：<ul><li><code>ROOT</code>表能够寻址<code>META</code>表的<code>Region</code>个数 X 每个<code>META</code>表能寻址的个数</li></ul></li><li>一个<code>ROOT</code>表最多只能有一个<code>Region</code>大小，也就是最多只能有<code>128MB</code>，按照每行占用1KB内存计算，128MB空间可以容纳$128MB ÷ 1kb = 2^{17}$。也就是一个ROOT可以寻址$2^{17}$个.META表的Region。</li><li>同理每个<code>META</code>表的Region可以存之的用户数据表的Region个数也是$2^{17}$个</li></ul><p>客户访问数据时的“三级寻址”：</p><ul><li>为了加速寻址，客户端会缓存位置信息，同时需要解决缓存失效的问题</li><li>寻址过程客户端需要询问<code>Zookeeper</code>服务器，不需要链接<code>Master</code>服务器。</li></ul><h2 id="HBase运行机制"><a href="#HBase运行机制" class="headerlink" title="HBase运行机制"></a>HBase运行机制</h2><ol><li>HBase系统架构</li><li>Region服务器工作原理</li><li>Store工作原理</li><li>HLog工作原理</li></ol><h3 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title="HBase系统架构"></a>HBase系统架构</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238605952550.jpg" alt=""></p><ul><li>客户端<ul><li>客户端包含访问HBase的接口,同时在缓存中维护者已经好访问过的Region位置信息,用来加快后续数据访问的过程</li></ul></li><li>Zookeeper服务器<ul><li>Zookeeper可以帮助选出一个Master作为集群的总管,并保证在任何时刻总有唯一一个Master在运行,这就避免了Master“单点失效”问题.</li></ul></li></ul><p>💡<strong>Zookeeper是一个很好的集群管理工具,被大量用于分布式计算,提供配置维护、域名服务、分布式同步、组服务等</strong><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238608443919.jpg" alt=""></p><ul><li>Master服务器: 主服务器主要负责和Region的管理工作<ul><li>管理用户对表的增删改查</li><li>实现不同Region服务器之间的负载均衡</li><li>在Region分裂或合并后,负责重新调整Region的分布</li><li>对发生故障时晓得Region服务器上的Region进行迁移</li></ul></li><li>Region服务器:<ul><li>Region服务器时HBase最核心的模块,负责维护分配给自己的Region,并响应用户的读写请求</li></ul></li></ul><h3 id="Region服务器的工作原理"><a href="#Region服务器的工作原理" class="headerlink" title="Region服务器的工作原理"></a>Region服务器的工作原理</h3><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238616055354.jpg" alt=""></p><p><strong>1.用户读写数据过程</strong></p><ul><li>用户写入数据时，被分配到相应Region服务器去执行</li><li>用户数据首先被写入到MemStore和Hlog中</li><li>只有当操作写入Hlog之后，commit()调用才会将其返回给客户端</li><li>当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找</li></ul><p><strong>2.缓存的刷新</strong></p><ul><li>系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记</li><li><p>每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件</p></li><li><p>每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务</p></li></ul><p><strong>3.StoreFile的合并</strong></p><ul><li>每次刷写都生成一个新的StoreFile，数量太多，影响查找速度</li><li>调用Store.compact()把多个合并成一个</li><li>合并操作比较耗费资源，只有数量达到一个阈值才启动合并</li></ul><p>💡<code>这么做是为了尽量一次性刷到磁盘,以此提高速度.但是如果StoreFile数量太多影响查找速度</code></p><h3 id="Store工作原理"><a href="#Store工作原理" class="headerlink" title="Store工作原理"></a>Store工作原理</h3><ul><li>Store是Region服务器的核心</li><li>多个StoreFile合并成一个</li><li>单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238621060069.jpg" alt=""></li></ul><h3 id="HLog工作原理"><a href="#HLog工作原理" class="headerlink" title="HLog工作原理"></a>HLog工作原理</h3><ul><li>分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复</li><li>HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）</li><li>用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘</li><li>Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master</li><li>Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录</li><li>系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器</li><li>Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复</li><li>共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志</li></ul><h2 id="应用方案"><a href="#应用方案" class="headerlink" title="应用方案"></a>应用方案</h2><ol><li>HBase实际应用中的性能优化方法</li><li>HBase性能监视</li><li>在HBase之上构建SQL引擎</li><li>构建HBase二级索引</li></ol><h3 id="HBase实际应用中的性能优化方法"><a href="#HBase实际应用中的性能优化方法" class="headerlink" title="HBase实际应用中的性能优化方法"></a>HBase实际应用中的性能优化方法</h3><p><strong>行键</strong><br>行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。<br>举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。<br><strong>InMemory</strong><br>创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。<br><strong>Max Version</strong><br>创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。<br><strong>Time to Live</strong><br>创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 <em> 24 </em> 60 * 60)。</p><h3 id="HBase性能监视"><a href="#HBase性能监视" class="headerlink" title="HBase性能监视"></a>HBase性能监视</h3><ol><li>Master-status(自带)</li><li>Ganglia</li><li>OpenTSDB</li><li>Ambari</li></ol><p><strong>Master-status</strong><br>HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030.如果master运行在名为master.foo.com的主机中，mater的主页地址就是<a href="http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面" target="_blank" rel="noopener">http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看该页面</a><br>可以查看HBase集群的当前状态<br><strong>Ganglia</strong><br>Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能<br><strong>OpenTSDB</strong><br>OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等<br><strong>Ambari</strong><br>Ambari 的作用就是创建、管理、监视 Hadoop 的集群 (推荐HDP CDH)<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238656031566.jpg" alt=""></p><h3 id="在HBase之上构建SQL引擎"><a href="#在HBase之上构建SQL引擎" class="headerlink" title="在HBase之上构建SQL引擎"></a>在HBase之上构建SQL引擎</h3><p>NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因：</p><p>　　1.易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。<br>　　2.减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。　</p><p>方案：<br>1.Hive整合HBase<br>2.Phoenix</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;目录：&quot;&gt;&lt;a href=&quot;#目录：&quot; class=&quot;headerlink&quot; title=&quot;目录：&quot;&gt;&lt;/a&gt;目录：&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;概述&lt;/li&gt;
&lt;li&gt;HBase访问借口&lt;/li&gt;
&lt;li&gt;HBase数据模型&lt;/
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="hbase" scheme="http://www.ihoge.cn/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>免安装免配置 还免费的Spark 集群 --Databrickes Spark Clould</title>
    <link href="http://www.ihoge.cn/2018/Databrickes.html"/>
    <id>http://www.ihoge.cn/2018/Databrickes.html</id>
    <published>2018-04-15T19:59:21.000Z</published>
    <updated>2018-04-22T14:55:19.382Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>摘要：本文带你畅游<code>Databrickes Spark Clould</code>云服务。小白迅速上手大数据Spark开发环境，从此告别集群<code>Bug</code>的烦恼，彻底解放物理机负担让你随时随地想<code>Run</code>就<code>Run</code>😎。</p><h2 id="目录："><a href="#目录：" class="headerlink" title="目录："></a>目录：</h2><ol><li>Databrickes是个啥？</li><li>Databrickes注册</li><li>Databrickes快速入门</li><li>其他进阶神操作</li></ol><hr><h2 id="Databrickes是个啥？"><a href="#Databrickes是个啥？" class="headerlink" title="Databrickes是个啥？"></a>Databrickes是个啥？</h2><p><a href="https://databricks.com" target="_blank" rel="noopener">YouTube简介，自备云梯</a><br>Databricks 属于 Spark 的商业化公司，由美国伯克利大学 AMP 实验室著名的 Spark 大数据处理系统多位创始人联合创立。Databricks 致力于提供基于 Spark 的云服务，可用于数据集成，数据管道等任务。<br>Databricks 公司的云解决方案由三部分组成：Databricks 平台、Spark 和 Databricks 工作区。该产品背后的理念是提供处理数据的单独空间，不受托管环境和 Hadoop 集群管理的影响，整个过程在云中完成。该产品有几个核心概念：由 Notebooks 提供一种与数据交互并构建图形的方法，当用户了解了显示数据的方式时，就可以开始构建主控面板以监视某些类型的数据。最后，用户可以通过该平台的任务启动器来规划 Apache Spark 的运行时间。</p><p>说白了就是<code>Databricks</code>捞钱的，老是开源spark 也不收你钱，大家也要吃饭不是，干脆搞个 spark on cloud，服务懒人或小公司，你既不用搭建集群也不用维护，交点儿钱直接用他们提供的在线集群。当然还有一个作用，推广普及spark ，所以也就有了<code>Community</code>版，即免费版</p><p><code>Databricks</code>相当于给你了一个在线spark 集群，即：<code>Spark online</code>（听着怎么这么像网游 (⊙﹏⊙)b），我等尚处spark初级阶段的小白们刚好可以用他们提供的免费版来学习，省去了搭建和维护配置的过程，用来学习或温习Spark API真是完美！<br>（笔者比较懒，下面<code>Databricks Spark Cloud</code>都用<code>DSC</code>代替😂）<br>话不多说，开搞！！！</p><h2 id="注册Databricks"><a href="#注册Databricks" class="headerlink" title="注册Databricks"></a>注册Databricks</h2><p><a href="https://databricks.com" target="_blank" rel="noopener">官网直达</a></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqdwayuscdj31kw0pg1kx.jpg" alt=""><br><code>Do you want to TRY DATACRICKS FREE?</code><br><code>Yeah, click here!</code></p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238167919311.jpg" alt=""></p><p>下面就是注册页面，不再啰嗦。注意的是左下角要进行人际身份验证，网络不好的童鞋请自备云梯😎。然后验证邮箱，点击链接到登录界面：</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238177684915.jpg" alt=""></p><p>💡<code>If you don&#39;t want to pay for money, pelese click here!</code></p><h2 id="搭建云上Spark集群环境"><a href="#搭建云上Spark集群环境" class="headerlink" title="搭建云上Spark集群环境"></a>搭建云上Spark集群环境</h2><p><a href="https://docs.databricks.com/getting-started/index.html" target="_blank" rel="noopener">入门指南</a><br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238199155320.jpg" alt=""></p><p>这货长这样～界面清爽颜值高，功能齐全效率高。</p><h2 id="基本使用流程："><a href="#基本使用流程：" class="headerlink" title="基本使用流程："></a>基本使用流程：</h2><ol><li>创建集群<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238206713516.jpg" alt=""></li></ol><p>没办法，笔者已经有一个集群在运行了，作为免费用户只能拥有一个活动集群，或者升级<code>高级用户</code>😎</p><p>2.创建nootebook<br>选择集群，选择语言，新建项目，凯撸～<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238208963616.jpg" alt=""><br>创建笔记、加载依赖资源、创建文件夹</p><p>3.创建Table<br>上传本地数据，速度有点慢，耐心等待一会～😅<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238209363742.jpg" alt=""></p><h2 id="任务流程可视化、数据可视化"><a href="#任务流程可视化、数据可视化" class="headerlink" title="任务流程可视化、数据可视化"></a>任务流程可视化、数据可视化</h2><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238214224410.jpg" alt=""><br>自动生成任务阶段流程，生成DAG。</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238214940209.jpg" alt=""><br>丰富的画图工具，只需点点点即可完成高颜值图表。</p><h2 id="其他操作："><a href="#其他操作：" class="headerlink" title="其他操作："></a>其他操作：</h2><p>下面是重点！！<br>下面是重点！！<br>下面是重点！！</p><p><img src="http://p6rvh6ej2.bkt.clouddn.com/15238216319562.jpg" alt=""><br>这里提供了丰富的适合不同场景的训练数据、源代码及简介。<br>只需找到<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15238217238654.jpg" alt=""><br>即可快速导入到工作台。只可惜自己英文太烂，啃的很慢😂😂😂</p><p><code>DSC</code>不仅提供了方便高可用的云集群服务，而且提供了丰富的学习资料，对像我等来说简直是完美的产品。感谢各位大佬给我们提供了如此便捷的学习环境和资源，<code>Life is short</code>，只争朝夕！<br><a href="https://youtu.be/fn3WeMZZcCk?t=17m19s" target="_blank" rel="noopener">油管视频自备云梯</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;摘要：本文带你畅游&lt;code&gt;Databrickes Spark Clould&lt;/code&gt;云服务。小白迅速上手大数据Spark开发环境，从此告别集群&lt;code&gt;Bug&lt;/code&gt;的烦恼，彻底解放物理机负担让你随时随地想&lt;code&gt;Ru
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="databrickes" scheme="http://www.ihoge.cn/tags/databrickes/"/>
    
  </entry>
  
  <entry>
    <title>弹性式分布数据集RDD——Pyspark基础 （二）</title>
    <link href="http://www.ihoge.cn/2018/pyspark.html"/>
    <id>http://www.ihoge.cn/2018/pyspark.html</id>
    <published>2018-04-15T09:59:21.000Z</published>
    <updated>2018-04-22T14:55:20.485Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RDD的内部运行方式"><a href="#RDD的内部运行方式" class="headerlink" title="RDD的内部运行方式"></a>RDD的内部运行方式</h2><p>RDD不仅是一组不可变的JVM（Java虚拟机）对象的分布集，而且是Spark的核心，可以让任务执行高速运算。</p><p>RDD将<code>跟踪（计入日记）应用于每个快的所有转换</code>，以加速计算速度，并在发生错误和部分数据丢失时提供回退（容错机制）。</p><p>RDD采用并行的运行方式，也就是每个转换操作并行执行，从而提高速度。<br>RDD有两种并行操作：</p><ul><li>转换操作（返回指向新的RDD的指针）</li><li>动作操作（在运行计算后向驱动程序返回值）</li></ul><p>数据集的转换通常是<code>惰性</code>的，这也意味着任何转换操作仅在调用数据集上的操作时才执行。该延迟执行会产生风多的精细查询：针对性能进行优化查询。这种优化始于Spark的DAGScheduler——面向阶段的调度器。DAGScheduler负责Stage级的调度详见:<a href="http://ihoge.cn/2018/Spark%20Scheduler.html" target="_blank" rel="noopener">Spark运行原理剖析</a></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqdbpax6jbj31kw0hntga.jpg" alt=""></p><p><strong>由于具有单独的RDD转换和动作，DAGScheduler可以在查询中执行优化。包括但不限于避免shuffle数据（最耗费资源的任务）</strong></p><h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><p>方式一： 用<code>.parallelize(...)</code>集合（元素list或array）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = sc.parallelize([(<span class="string">'a'</span>,<span class="number">1</span>),(<span class="string">'b'</span>,<span class="number">2</span>),(<span class="string">'c'</span>,<span class="number">3</span>),(<span class="string">'d'</span>,<span class="number">5</span>),(<span class="string">'e'</span>,<span class="number">5</span>)])</span><br></pre></td></tr></table></figure></p><p>方式二： 读入外部文件</p><ul><li>支持多文件系统中读取：如NTFS、FAT、HFS+（Mac OS Extended），或者如HDFS、S3、Cassandra这类的分布式文件系统，还有其他类文件系统。</li><li>指出多种数据格式：如文本、parquet、JSON、Hive tables（Hive表）以及使用JDBC驱动程序可读取的关系数据库中的数据。（注意：Spark可以自动处理压缩数据集）</li></ul><p>💡<strong>Tip1：</strong>读取的方式不同，持有对象表达方式也不同。从文件中读取的数据表示为<code>MapPartitionsRDD</code>；使用集合方法的数据表示为<code>ParallelCollectionRDD</code></p><p>💡<strong>Tip2：</strong>RDD是无schema的数据结构（和DataFrame不同），所以我们几乎可以混用任何数据结构：tuple、dict、list和spark等都能支持。如果对数据集使用<code>.collect()</code>方法，将把RDD对所有元素返回给驱动程序，驱动程序将其序列化成了一个列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_from_file = sc.textFile(<span class="string">"hdfs://master:9000/pydata/VS14MORT.txt.gz"</span>,<span class="number">4</span>) <span class="comment"># 这里表示4个分区</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractInformation</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">    selected_indices = [</span><br><span class="line">         <span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">18</span>,</span><br><span class="line">         <span class="number">19</span>,<span class="number">21</span>,<span class="number">22</span>,<span class="number">23</span>,<span class="number">24</span>,<span class="number">25</span>,<span class="number">27</span>,<span class="number">28</span>,<span class="number">29</span>,<span class="number">30</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">34</span>,</span><br><span class="line">         <span class="number">36</span>,<span class="number">37</span>,<span class="number">38</span>,<span class="number">39</span>,<span class="number">40</span>,<span class="number">41</span>,<span class="number">42</span>,<span class="number">43</span>,<span class="number">44</span>,<span class="number">45</span>,<span class="number">46</span>,<span class="number">47</span>,<span class="number">48</span>,</span><br><span class="line">         <span class="number">49</span>,<span class="number">50</span>,<span class="number">51</span>,<span class="number">52</span>,<span class="number">53</span>,<span class="number">54</span>,<span class="number">55</span>,<span class="number">56</span>,<span class="number">58</span>,<span class="number">60</span>,<span class="number">61</span>,<span class="number">62</span>,<span class="number">63</span>,</span><br><span class="line">         <span class="number">64</span>,<span class="number">65</span>,<span class="number">66</span>,<span class="number">67</span>,<span class="number">68</span>,<span class="number">69</span>,<span class="number">70</span>,<span class="number">71</span>,<span class="number">72</span>,<span class="number">73</span>,<span class="number">74</span>,<span class="number">75</span>,<span class="number">76</span>,</span><br><span class="line">         <span class="number">77</span>,<span class="number">78</span>,<span class="number">79</span>,<span class="number">81</span>,<span class="number">82</span>,<span class="number">83</span>,<span class="number">84</span>,<span class="number">85</span>,<span class="number">87</span>,<span class="number">89</span></span><br><span class="line">    ]</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Input record schema</span></span><br><span class="line"><span class="string">        schema: n-m (o) -- xxx</span></span><br><span class="line"><span class="string">            n - position from</span></span><br><span class="line"><span class="string">            m - position to</span></span><br><span class="line"><span class="string">            o - number of characters</span></span><br><span class="line"><span class="string">            xxx - description</span></span><br><span class="line"><span class="string">        1. 1-19 (19) -- reserved positions</span></span><br><span class="line"><span class="string">        2. 20 (1) -- resident status</span></span><br><span class="line"><span class="string">        3. 21-60 (40) -- reserved positions</span></span><br><span class="line"><span class="string">        4. 61-62 (2) -- education code (1989 revision)</span></span><br><span class="line"><span class="string">        5. 63 (1) -- education code (2003 revision)</span></span><br><span class="line"><span class="string">        6. 64 (1) -- education reporting flag</span></span><br><span class="line"><span class="string">        7. 65-66 (2) -- month of death</span></span><br><span class="line"><span class="string">        8. 67-68 (2) -- reserved positions</span></span><br><span class="line"><span class="string">        9. 69 (1) -- sex</span></span><br><span class="line"><span class="string">        10. 70 (1) -- age: 1-years, 2-months, 4-days, 5-hours, 6-minutes, 9-not stated</span></span><br><span class="line"><span class="string">        11. 71-73 (3) -- number of units (years, months etc)</span></span><br><span class="line"><span class="string">        12. 74 (1) -- age substitution flag (if the age reported in positions 70-74 is calculated using dates of birth and death)</span></span><br><span class="line"><span class="string">        13. 75-76 (2) -- age recoded into 52 categories</span></span><br><span class="line"><span class="string">        14. 77-78 (2) -- age recoded into 27 categories</span></span><br><span class="line"><span class="string">        15. 79-80 (2) -- age recoded into 12 categories</span></span><br><span class="line"><span class="string">        16. 81-82 (2) -- infant age recoded into 22 categories</span></span><br><span class="line"><span class="string">        17. 83 (1) -- place of death</span></span><br><span class="line"><span class="string">        18. 84 (1) -- marital status</span></span><br><span class="line"><span class="string">        19. 85 (1) -- day of the week of death</span></span><br><span class="line"><span class="string">        20. 86-101 (16) -- reserved positions</span></span><br><span class="line"><span class="string">        21. 102-105 (4) -- current year</span></span><br><span class="line"><span class="string">        22. 106 (1) -- injury at work</span></span><br><span class="line"><span class="string">        23. 107 (1) -- manner of death</span></span><br><span class="line"><span class="string">        24. 108 (1) -- manner of disposition</span></span><br><span class="line"><span class="string">        25. 109 (1) -- autopsy</span></span><br><span class="line"><span class="string">        26. 110-143 (34) -- reserved positions</span></span><br><span class="line"><span class="string">        27. 144 (1) -- activity code</span></span><br><span class="line"><span class="string">        28. 145 (1) -- place of injury</span></span><br><span class="line"><span class="string">        29. 146-149 (4) -- ICD code</span></span><br><span class="line"><span class="string">        30. 150-152 (3) -- 358 cause recode</span></span><br><span class="line"><span class="string">        31. 153 (1) -- reserved position</span></span><br><span class="line"><span class="string">        32. 154-156 (3) -- 113 cause recode</span></span><br><span class="line"><span class="string">        33. 157-159 (3) -- 130 infant cause recode</span></span><br><span class="line"><span class="string">        34. 160-161 (2) -- 39 cause recode</span></span><br><span class="line"><span class="string">        35. 162 (1) -- reserved position</span></span><br><span class="line"><span class="string">        36. 163-164 (2) -- number of entity-axis conditions</span></span><br><span class="line"><span class="string">        37-56. 165-304 (140) -- list of up to 20 conditions</span></span><br><span class="line"><span class="string">        57. 305-340 (36) -- reserved positions</span></span><br><span class="line"><span class="string">        58. 341-342 (2) -- number of record axis conditions</span></span><br><span class="line"><span class="string">        59. 343 (1) -- reserved position</span></span><br><span class="line"><span class="string">        60-79. 344-443 (100) -- record axis conditions</span></span><br><span class="line"><span class="string">        80. 444 (1) -- reserve position</span></span><br><span class="line"><span class="string">        81. 445-446 (2) -- race</span></span><br><span class="line"><span class="string">        82. 447 (1) -- bridged race flag</span></span><br><span class="line"><span class="string">        83. 448 (1) -- race imputation flag</span></span><br><span class="line"><span class="string">        84. 449 (1) -- race recode (3 categories)</span></span><br><span class="line"><span class="string">        85. 450 (1) -- race recode (5 categories)</span></span><br><span class="line"><span class="string">        86. 461-483 (33) -- reserved positions</span></span><br><span class="line"><span class="string">        87. 484-486 (3) -- Hispanic origin</span></span><br><span class="line"><span class="string">        88. 487 (1) -- reserved</span></span><br><span class="line"><span class="string">        89. 488 (1) -- Hispanic origin/race recode</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">    record_split = re\</span><br><span class="line">        .compile(</span><br><span class="line">            <span class="string">r'([\s]&#123;19&#125;)([0-9]&#123;1&#125;)([\s]&#123;40&#125;)([0-9\s]&#123;2&#125;)([0-9\s]&#123;1&#125;)([0-9]&#123;1&#125;)([0-9]&#123;2&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([\s]&#123;2&#125;)([FM]&#123;1&#125;)([0-9]&#123;1&#125;)([0-9]&#123;3&#125;)([0-9\s]&#123;1&#125;)([0-9]&#123;2&#125;)([0-9]&#123;2&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([0-9]&#123;2&#125;)([0-9\s]&#123;2&#125;)([0-9]&#123;1&#125;)([SMWDU]&#123;1&#125;)([0-9]&#123;1&#125;)([\s]&#123;16&#125;)([0-9]&#123;4&#125;)'</span> +</span><br><span class="line">            <span class="string">r'([YNU]&#123;1&#125;)([0-9\s]&#123;1&#125;)([BCOU]&#123;1&#125;)([YNU]&#123;1&#125;)([\s]&#123;34&#125;)([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)'</span> +</span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;4&#125;)([0-9]&#123;3&#125;)([\s]&#123;1&#125;)([0-9\s]&#123;3&#125;)([0-9\s]&#123;3&#125;)([0-9\s]&#123;2&#125;)([\s]&#123;1&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([0-9\s]&#123;2&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)([A-Z0-9\s]&#123;7&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;7&#125;)([\s]&#123;36&#125;)([A-Z0-9\s]&#123;2&#125;)([\s]&#123;1&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([A-Z0-9\s]&#123;5&#125;)([\s]&#123;1&#125;)([0-9\s]&#123;2&#125;)([0-9\s]&#123;1&#125;)'</span> + </span><br><span class="line">            <span class="string">r'([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)([\s]&#123;33&#125;)([0-9\s]&#123;3&#125;)([0-9\s]&#123;1&#125;)([0-9\s]&#123;1&#125;)'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        rs = np.array(record_split.split(row))[selected_indices]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        rs = np.array([<span class="string">'-99'</span>] * len(selected_indices))</span><br><span class="line">    <span class="keyword">return</span> rs</span><br><span class="line"></span><br><span class="line">data_file = data_from_file.map(extractInformation)</span><br><span class="line">data_file.map(<span class="keyword">lambda</span> row: row).take(<span class="number">1</span>)</span><br><span class="line">data_file.cache()</span><br><span class="line">data_file.is_cached</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><h2 id="全局作用域和局部作用域"><a href="#全局作用域和局部作用域" class="headerlink" title="全局作用域和局部作用域"></a>全局作用域和局部作用域</h2><p>Spark可以在两种模式下运行：本地和集群。本地运行Spark代码时和目前使用的python没有说明不同。然而他如果将相同的代码部署到集群，便可能会导致大量的困扰，这就需要了解Spark是怎么在集群上执行工作的。这里有一篇文章介绍的很详细。<a href="http://ihoge.cn/2018/Spark%20Scheduler.html" target="_blank" rel="noopener">参考：Spark运行原理详解</a></p><p>在集群模式下，提交任务时任务发送给了Master节点。该驱动程序节点为任务创建DAG，并且决定哪一个执行者（Worker）节点运行特定的任务。然后该驱动程序知识工作者执行它们的任务，并且在结束时将结果返回给驱动程序。然而在这之前，驱动程序为每一个任务的终止做准备：驱动程序中有一组变量和方法，以变工作者在RDD上执行任务。</p><p>这组变量和方法在执行者的上下问本质上是静态的，每个执行器从驱动程序中获取的一份变量和方法的<strong>副本</strong>。这意味着运行任务时，如果执行者改变这些变量或覆盖这些方法，它不影响任何其他执行者的副本或者驱动程序的变量和方法。这可能会导致一些意想不到的行为和运行错误，这些行为和错误通常都很难被追踪到。</p><h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>转换操作可以调整数据集。包括映射、筛选、链接、转换数据集中的值。</p><h3 id="map-转换"><a href="#map-转换" class="headerlink" title=".map()转换"></a>.map()转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_2014 = data_file.map(<span class="keyword">lambda</span> x: x[<span class="number">16</span>])</span><br><span class="line">data_2014.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>[&apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;2014&apos;, &apos;-99&apos;]</code></pre><h3 id="filter-转换"><a href="#filter-转换" class="headerlink" title=".filter()转换"></a>.filter()转换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_filter = data_file.filter(<span class="keyword">lambda</span> x: x[<span class="number">16</span>] == <span class="string">'2014'</span> <span class="keyword">and</span> x[<span class="number">21</span>] == <span class="string">'0'</span>)</span><br><span class="line">print(data_filter.count())</span><br><span class="line">data_file.take(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>22[array([&apos;1&apos;, &apos;  &apos;, &apos;2&apos;, &apos;1&apos;, &apos;01&apos;, &apos;M&apos;, &apos;1&apos;, &apos;087&apos;, &apos; &apos;, &apos;43&apos;, &apos;23&apos;, &apos;11&apos;,        &apos;  &apos;, &apos;4&apos;, &apos;M&apos;, &apos;4&apos;, &apos;2014&apos;, &apos;U&apos;, &apos;7&apos;, &apos;C&apos;, &apos;N&apos;, &apos; &apos;, &apos; &apos;, &apos;I64 &apos;,        &apos;238&apos;, &apos;070&apos;, &apos;   &apos;, &apos;24&apos;, &apos;01&apos;, &apos;11I64  &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;01&apos;,        &apos;I64  &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;01&apos;, &apos; &apos;,        &apos; &apos;, &apos;1&apos;, &apos;1&apos;, &apos;100&apos;, &apos;6&apos;], dtype=&apos;&lt;U40&apos;), array([&apos;1&apos;, &apos;  &apos;, &apos;2&apos;, &apos;1&apos;, &apos;01&apos;, &apos;M&apos;, &apos;1&apos;, &apos;058&apos;, &apos; &apos;, &apos;37&apos;, &apos;17&apos;, &apos;08&apos;,        &apos;  &apos;, &apos;4&apos;, &apos;D&apos;, &apos;3&apos;, &apos;2014&apos;, &apos;U&apos;, &apos;7&apos;, &apos;C&apos;, &apos;N&apos;, &apos; &apos;, &apos; &apos;, &apos;I250&apos;,        &apos;214&apos;, &apos;062&apos;, &apos;   &apos;, &apos;21&apos;, &apos;03&apos;, &apos;11I250 &apos;, &apos;61I272 &apos;, &apos;62E669 &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;,        &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;       &apos;, &apos;03&apos;,        &apos;I250 &apos;, &apos;E669 &apos;, &apos;I272 &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;,        &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;     &apos;, &apos;01&apos;, &apos; &apos;,        &apos; &apos;, &apos;1&apos;, &apos;1&apos;, &apos;100&apos;, &apos;6&apos;], dtype=&apos;&lt;U40&apos;)]</code></pre><h3 id="flatMap-转换"><a href="#flatMap-转换" class="headerlink" title=".flatMap()转换"></a>.flatMap()转换</h3><p><code>.flatMap()</code>方法和.map()工作类似，不同的是flatMap()返回一个扁平的结果而不是一个列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_flat = data_file.flatMap(<span class="keyword">lambda</span> x: (x[<span class="number">16</span>], int(x[<span class="number">16</span>])+<span class="number">1</span>))</span><br><span class="line">data_flat.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>[&apos;2014&apos;, 2015, &apos;2014&apos;, 2015, &apos;2014&apos;, 2015, &apos;2014&apos;, 2015, &apos;2014&apos;, 2015]</code></pre><p>.flatMap()可以用于过滤一些格式不正确的记录。在这个机制下，.flatMap()方法吧每一行看作一个列表对待，然后将所有记录简单的加入到一起，通过传递一个空列表可以丢弃格式不正确的记录。</p><h3 id="distinct-转换"><a href="#distinct-转换" class="headerlink" title=".distinct()转换"></a>.distinct()转换</h3><p>这里用该方法检查性别列表是否只包含了男性和女性验证我们是否准确解释了数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">distinct_gender = data_file.map(<span class="keyword">lambda</span> x: x[<span class="number">5</span>]).distinct()</span><br><span class="line">distinct_gender.collect()</span><br></pre></td></tr></table></figure><pre><code>[&apos;M&apos;, &apos;F&apos;, &apos;-99&apos;]</code></pre><h3 id="sample-转换"><a href="#sample-转换" class="headerlink" title=".sample() 转换"></a>.sample() 转换</h3><p>该方法返回数据集的随机样本。第一个参数<code>withReplacement</code>指定采样是否应该替换，第二个参数<code>fraction</code>定义返回数据量的百分比，第三个参数是伪随机数产生器的种子<code>seed</code>。</p><p>为了节省运算时间，这里选取愿数据千分之一的随机数据作为下面的练习数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_sample = data_file.sample(<span class="keyword">False</span>, <span class="number">0.001</span>, <span class="number">666</span>)</span><br><span class="line">data_sample.cache()</span><br></pre></td></tr></table></figure><pre><code>PythonRDD[25] at RDD at PythonRDD.scala:48</code></pre><h3 id="leftOuterJoin-转换"><a href="#leftOuterJoin-转换" class="headerlink" title=".leftOuterJoin()转换"></a>.leftOuterJoin()转换</h3><ul><li>.leftOuterJoin()： 根据两个数据集中都有得值来连接两个RDD，并返回<code>左侧</code>的RDD记录，而右边的记录副加载两个RDD匹配的地方。</li><li>.join() ：只返回两个RDD之间的关联数值</li><li>.intersection()：返回两个RDD中相等的记录</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1 = sc.parallelize([(<span class="string">'a'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">4</span>), (<span class="string">'c'</span>,<span class="number">10</span>)])</span><br><span class="line">rdd2 = sc.parallelize([(<span class="string">'a'</span>,<span class="number">4</span>), (<span class="string">'a'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">6</span>), (<span class="string">'d'</span>,<span class="number">15</span>)])</span><br><span class="line">print(<span class="string">"leftOuterJoin: "</span>,rdd1.leftOuterJoin(rdd2).collect())</span><br><span class="line">print(<span class="string">"Join: "</span>,rdd1.join(rdd2).collect())</span><br><span class="line">print(<span class="string">"intersection: "</span>, rdd1.intersection(rdd2).collect())</span><br></pre></td></tr></table></figure><pre><code>leftOuterJoin:  [(&apos;c&apos;, (10, None)), (&apos;b&apos;, (4, 6)), (&apos;a&apos;, (1, 1)), (&apos;a&apos;, (1, 4))]Join:  [(&apos;b&apos;, (4, 6)), (&apos;a&apos;, (1, 1)), (&apos;a&apos;, (1, 4))]intersection:  [(&apos;a&apos;, 1)]</code></pre><h3 id="repartition-转换"><a href="#repartition-转换" class="headerlink" title=".repartition()转换"></a>.repartition()转换</h3><p>重新对数据集进行分区，改变数据集分赛区的数量。此功能应该谨慎并且仅当真正需要的时候使用，因为它会充足数据，导致性能产生巨大的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(len(rdd2.glom().collect()))</span><br><span class="line">rdd2 = rdd2.repartition(<span class="number">4</span>)</span><br><span class="line">print(len(rdd2.glom().collect()))</span><br></pre></td></tr></table></figure><pre><code>34</code></pre><h2 id="动作"><a href="#动作" class="headerlink" title="动作"></a>动作</h2><h3 id="collect-动作"><a href="#collect-动作" class="headerlink" title=".collect() 动作"></a>.collect() 动作</h3><p>返回所有RDD的元素给驱动程序</p><p>💡同时常用的还有: <code>.collectAsMap()</code>方法</p><h3 id="take-动作"><a href="#take-动作" class="headerlink" title=".take() 动作"></a>.take() 动作</h3><p>可以说这事最有用的方法，返回<strong>单个数据分区</strong>的前n行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd.take(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#等同于:</span></span><br><span class="line">rdd.first()</span><br></pre></td></tr></table></figure></p><h3 id="reduce-动作"><a href="#reduce-动作" class="headerlink" title=".reduce() 动作"></a>.reduce() 动作</h3><p>该方法使用指定的方法减少RDD中的元素。可以用该方法计算RDD总的元素之和：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd1.map(<span class="keyword">lambda</span> x: x[<span class="number">1</span>]).reduce(<span class="keyword">lambda</span> x, y: x + y)</span><br></pre></td></tr></table></figure></p><p>在每一个分区里，reduce()方法运行求和方法，将改总和返回给最终聚合所在的程序节点。</p><p>⚠️<strong>警告：</strong><br>要谨慎注意的是，reduce传递的函数需要时关联的，既满足元素顺序改变结果不变，操作符顺序改变结果不变。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([1, 2, 0.5, 0.1],1)</span><br><span class="line">rdd.reduce(lambda x, y: x / y)</span><br><span class="line"></span><br><span class="line">out: 10.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize([1, 2, 0.5, 0.1],2)</span><br><span class="line">rdd.reduce(lambda x, y: x / y)</span><br><span class="line"></span><br><span class="line">out: 0.1</span><br></pre></td></tr></table></figure><p>这里我们希望输出结果是10.0，第一个只把RDD放在一个分区，输出结果符合预期。但是在第二个例子中，分了2个区，结果就不对了。因为该方法是在每个分区并行计算的。</p><h3 id="reduceByKey-动作"><a href="#reduceByKey-动作" class="headerlink" title=".reduceByKey()  动作"></a>.reduceByKey()  动作</h3><p>该方法和.reduce()方法类似，但是实在key-key基础上运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_key = sc.parallelize([(<span class="string">'a'</span>,<span class="number">3</span>), (<span class="string">'a'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">6</span>), (<span class="string">'d'</span>,<span class="number">1</span>), (<span class="string">'b'</span>,<span class="number">6</span>), (<span class="string">'d'</span>,<span class="number">15</span>), (<span class="string">'d'</span>,<span class="number">3</span>), (<span class="string">'a'</span>,<span class="number">7</span>), (<span class="string">'b'</span>, <span class="number">8</span>)],<span class="number">4</span>)</span><br><span class="line">data_key.reduceByKey(<span class="keyword">lambda</span> x, y: x+y).collect()</span><br></pre></td></tr></table></figure><pre><code>[(&apos;b&apos;, 20), (&apos;a&apos;, 11), (&apos;d&apos;, 19)]</code></pre><h3 id="count-动作"><a href="#count-动作" class="headerlink" title=".count()  动作"></a>.count()  动作</h3><p>.count() 方法统计出了RDD里所有的元素数量。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.count()</span><br></pre></td></tr></table></figure></p><p>.count() 方法产生入戏方法同样的结果，但不需要把整个数据集移动到驱动程序：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(rdd.collect()). # ⚠️警告：不要这样做！！</span><br></pre></td></tr></table></figure></p><h3 id="countByKey-动作"><a href="#countByKey-动作" class="headerlink" title=".countByKey() 动作"></a>.countByKey() 动作</h3><p>如果数据集是Ket-Value形式，可以使用.countByKey()方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_key.countByKey().items()</span><br></pre></td></tr></table></figure><pre><code>dict_items([(&apos;a&apos;, 3), (&apos;b&apos;, 3), (&apos;d&apos;, 3)])</code></pre><h3 id="saveAsTextFile-动作"><a href="#saveAsTextFile-动作" class="headerlink" title=".saveAsTextFile()  动作"></a>.saveAsTextFile()  动作</h3><p>该方法将RDD保存为文本文件：每个文件一个分区</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_key.saveAsTextFile(<span class="string">'hdfs://master:9000/out/data_key.txt'</span>)</span><br></pre></td></tr></table></figure><p>要读取它的时候需要解析，因为所有行都被视为字符串：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseInput</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    pattern = re.compile(<span class="string">r"\(\'([a-z]+)\',.([0-9]+)\)"</span>) <span class="comment"># 这里“+”号代表匹配一个或多个匹配字符，否则针对双位数动作操作会报错</span></span><br><span class="line">    row_split = pattern.split(row)</span><br><span class="line">    <span class="keyword">return</span> (row_split[<span class="number">1</span>], row_split[<span class="number">2</span>])</span><br><span class="line">data_key_read = sc.textFile(<span class="string">'hdfs://master:9000/out/data_key.txt'</span>)</span><br><span class="line">data_key_read.map(parseInput).collect()</span><br></pre></td></tr></table></figure><pre><code>[(&apos;a&apos;, &apos;3&apos;), (&apos;a&apos;, &apos;1&apos;), (&apos;b&apos;, &apos;6&apos;), (&apos;d&apos;, &apos;1&apos;), (&apos;b&apos;, &apos;6&apos;), (&apos;d&apos;, &apos;15&apos;), (&apos;d&apos;, &apos;3&apos;), (&apos;a&apos;, &apos;7&apos;), (&apos;b&apos;, &apos;8&apos;)]</code></pre><p>💡同时还有：</p><ul><li>rdd.saveAsHadoopDataset</li><li>rdd.saveAsSequenceFile</li><li>…<br>等方法</li></ul><h3 id="foreach-动作"><a href="#foreach-动作" class="headerlink" title=".foreach()  动作"></a>.foreach()  动作</h3><p>这个方法对RDD里的每个元素，用迭代方法应用相同的函数；和<code>.map()</code>相比，<code>.foreach()</code>方法按照一个接一个的方式，对每一条记录应用一个定义好的函数。当希望将数据曹村道PySpark本身不支持的数据库是，该方法很有用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    print(x)</span><br><span class="line"></span><br><span class="line">rdd.foreach(f)</span><br></pre></td></tr></table></figure></p><h2 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h2><ul><li><code>RDD是Spark的核心</code>；这些无<code>schema</code>数据结构早Spark中处理的最基本的数据结构。</li><li>RDD的两种创建方式： parallelize 和 文件读取</li><li>Spark中的转化是惰性的，只在操作被调用时应用。</li><li>Scala 和 Python RDD之间一个主要的区别是速度： Python RDD 比 Scala 慢很多！</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;RDD的内部运行方式&quot;&gt;&lt;a href=&quot;#RDD的内部运行方式&quot; class=&quot;headerlink&quot; title=&quot;RDD的内部运行方式&quot;&gt;&lt;/a&gt;RDD的内部运行方式&lt;/h2&gt;&lt;p&gt;RDD不仅是一组不可变的JVM（Java虚拟机）对象的分布集，而且是Spar
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="pyspark" scheme="http://www.ihoge.cn/tags/pyspark/"/>
    
  </entry>
  
  <entry>
    <title>Spark运行原理剖析</title>
    <link href="http://www.ihoge.cn/2018/Spark%20Scheduler.html"/>
    <id>http://www.ihoge.cn/2018/Spark Scheduler.html</id>
    <published>2018-04-15T09:59:21.000Z</published>
    <updated>2018-05-09T12:38:03.223Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>Apache Spark是一个开源的，强大的分布式查询和处理引擎。它提供MapReduce的灵活性和可扩展性，但速度明显更高。<br><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqd8rm8qdtj30er07a74i.jpg" alt=""></p><p>Spark的核心是根据RDD来实现的，Spark Scheduler则为Spark核心实现的重要一环，其作用就是任务调度。Spark的任务调度就是如何组织任务去处理RDD中每个分区的数据，根据RDD的依赖关系构建DAG，基于DAG划分Stage，将每个Stage中的任务发到指定节点运行。基于Spark的任务调度原理，我们可以合理规划资源利用，做到尽可能用最少的资源高效地完成任务计算。</p><h2 id="分布式运行框架"><a href="#分布式运行框架" class="headerlink" title="分布式运行框架"></a>分布式运行框架</h2><p>Spark可以部署在多种资源管理平台，例如Yarn、Mesos等，Spark本身也实现了一个简易的资源管理机制，称之为Standalone模式。以下所述均表示Spark-on-Yarn。Spark部署在Yarn上有两种运行模式，分别为yarn-client和yarn-cluster模式，它们的区别仅仅在于Spark Driver是运行在Client端还是ApplicationMater端。如下图所示为Spark部署在Yarn上，以yarn-cluster模式运行的分布式计算框架。<br><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqd9qs2x4ej31920n4n11.jpg" alt=""></p><p>其中蓝色部分是Spark里的概念，包括Client、ApplicationMaster、Driver和Executor，其中Client和ApplicationMaster主要是负责与Yarn进行交互；Driver作为Spark应用程序的总控，负责分发任务以及监控任务运行状态；Executor负责执行任务，并上报状态信息给Driver，从逻辑上来看Executor是进程，运行在其中的任务是线程，所以说Spark的任务是线程级别的。通过下面的时序图可以更清晰地理解一个Spark应用程序从提交到运行的完整流程。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqd9s947htj31am11mafh.jpg" alt=""></p><p><strong>Client</strong><br>提交一个Spark应用程序，首先通过Client向ResourceManager请求启动一个Application，同时检查是否有足够的资源满足Application的需求，如果资源条件满足，则准备ApplicationMaster的启动上下文，交给ResourceManager，并循环监控Application状态。</p><p><strong>ResourceManager</strong><br>当提交的资源队列中有资源时，ResourceManager会在某个NodeManager上启动ApplicationMaster进程，ApplicationMaster会单独启动Driver后台线程，当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。</p><p><strong>Driver</strong><br>Driver线程主要是初始化SparkContext对象，准备运行所需的上下文，然后一方面保持与ApplicationMaster的RPC连接，通过ApplicationMaster申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲Executor上。</p><p><strong>Executor</strong><br>当ResourceManager向ApplicationMaster返回Container资源时，ApplicationMaster就尝试在对应的Container上启动Executor进程，Executor进程起来后，会向Driver注册，注册成功后保持与Driver的心跳，同时等待Driver分发任务，当分发的任务执行完毕后，将任务状态上报给Driver。</p><blockquote><p>Driver把资源申请的逻辑给抽象出来，以适配不同的资源管理系统，所以才间接地通过ApplicationMaster去和Yarn打交道。</p></blockquote><p>从上述时序图可知，<strong>Client只管提交Application并监控Application的状态</strong>。对于Spark的任务调度主要是集中在两个方面: <code>资源申请</code>和<code>任务分发</code>，其主要是通过<code>ApplicationMaster</code>、<code>Driver</code>以及<code>Executor</code>之间来完成，下面详细剖析Spark任务调度每个细节。</p><h2 id="Spark任务调度总览"><a href="#Spark任务调度总览" class="headerlink" title="Spark任务调度总览"></a>Spark任务调度总览</h2><p>当Driver起来后，Driver则会根据用户程序逻辑准备任务，并根据Executor资源情况逐步分发任务。在详细阐述任务调度前，首先说明下Spark里的几个概念。一个Spark应用程序包括Job、Stage以及Task三个概念：</p><ul><li>Job是以Action方法为界，遇到一个Action方法则触发一个Job；</li><li>Stage是Job的子集，以RDD宽依赖(即Shuffle)为界，遇到Shuffle做一次划分；</li><li>Task是Stage的子集，以并行度(分区数)来衡量，分区数是多少，则有多少个task。</li></ul><p>Spark的任务调度总体来说分两路进行，一路是Stage级的调度，一路是Task级的调度，总体调度流程如下图所示。<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqda2pc5qcj31dw12sn79.jpg" alt=""></p><p>Spark RDD通过其Transactions操作，形成了RDD血缘关系图，即DAG，最后通过Action的调用，触发Job并调度执行。DAGScheduler负责Stage级的调度，主要是将DAG切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度。TaskScheduler负责Task级的调度，将DAGScheduler给过来的TaskSet按照指定的调度策略分发到Executor上执行，调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多种实现，分别对接不同的资源管理系统。有了上述感性的认识后，下面这张图描述了Spark-On-Yarn模式下在任务调度期间，ApplicationMaster、Driver以及Executor内部模块的交互过程。<br><img src="http://p6rvh6ej2.bkt.clouddn.com/15258693615222.jpg" alt=""><br>Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并启动SchedulerBackend以及HeartbeatReceiver。SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。下面着重剖析DAGScheduler负责的Stage调度以及TaskScheduler负责的Task调度。</p><h3 id="Stage级的调度"><a href="#Stage级的调度" class="headerlink" title="Stage级的调度"></a>Stage级的调度</h3><p>Spark的任务调度是从DAG切割开始，主要是由DAGScheduler来完成。当遇到一个Action操作后就会触发一个Job的计算，并交给DAGScheduler来提交，下图是涉及到Job提交的相关方法调用流程图。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqda8al64mj31kw0hdq7n.jpg" alt=""></p><p>Job由最终的RDD和Action方法封装而成，SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，具体划分策略是，由最终的RDD不断通过依赖回溯判断父依赖是否是款依赖，即以Shuffle为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算，如上图紫色流程部分。划分的Stages分两类，一类叫做ResultStage，为DAG最下游的Stage，由Action方法决定，另一类叫做ShuffleMapStage，为下游Stage准备数据，下面看一个简单的例子WordCount。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqda8um68bj31cw0huahy.jpg" alt=""></p><p>Job由saveAsTextFile触发，该Job由RDD-3和saveAsTextFile方法组成，根据RDD之间的依赖关系从RDD-3开始回溯搜索，直到没有依赖的RDD-0，在回溯搜索过程中，RDD-3依赖RDD-2，并且是宽依赖，所以在RDD-2和RDD-3之间划分Stage，RDD-3被划到最后一个Stage，即ResultStage中，RDD-2依赖RDD-1，RDD-1依赖RDD-0，这些依赖都是窄依赖，所以将RDD-0、RDD-1和RDD-2划分到同一个Stage，即ShuffleMapStage中，实际执行的时候，数据记录会一气呵成地执行RDD-0到RDD-2的转化。不难看出，其本质上是一个深度优先搜索算法。</p><p>一个Stage是否被提交，需要判断它的父Stage是否执行，只有在父Stage执行完毕才能提交当前Stage，如果一个Stage没有父Stage，那么从该Stage开始提交。Stage提交时会将Task信息（分区信息以及方法等）序列化并被打包成TaskSet交给TaskScheduler，一个Partition对应一个Task，另一方面监控Stage的运行状态，只有Executor丢失或者Task由于Fetch失败才需要重新提交失败的Stage以调度运行失败的任务，其他类型的Task失败会在TaskScheduler的调度过程中重试。</p><p>相对来说DAGScheduler做的事情较为简单，仅仅是在Stage层面上划分DAG，提交Stage并监控相关状态信息。TaskScheduler则相对较为复杂，下面详细阐述其细节。</p><h3 id="Task级的调度"><a href="#Task级的调度" class="headerlink" title="Task级的调度"></a>Task级的调度</h3><p>Spark Task的调度是由TaskScheduler来完成，由前文可知，DAGScheduler将Stage打包到TaskSet交给TaskScheduler，TaskScheduler会将其封装为TaskSetManager加入到调度队列中，TaskSetManager负责监控管理同一个Stage中的Tasks，TaskScheduler就是以TaskSetManager为单元来调度任务。前面也提到，TaskScheduler初始化后会启动SchedulerBackend，它负责跟外界打交道，接收Executor的注册信息，并维护Executor的状态，所以说SchedulerBackend是管“粮食”的，同时它在启动后会定期地去“询问”TaskScheduler有没有任务要运行，也就是说，它会定期地“问”TaskScheduler“我有这么余量，你要不要啊”，TaskScheduler在SchedulerBackend“问”它的时候，会从调度队列中按照指定的调度策略选择TaskSetManager去调度运行，大致方法调用流程如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqda9kfl3dj31kw0lqq8i.jpg" alt=""></p><h4 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h4><p>前面讲到，TaskScheduler会先把DAGScheduler给过来的TaskSet封装成TaskSetManager扔到任务队列里，然后再从任务队列里按照一定的规则把它们取出来在SchedulerBackend给过来的Executor上运行。这个调度过程实际上还是比较粗粒度的，是面向TaskSetManager的。</p><p>TaskScheduler是以树的方式来管理任务队列，树中的节点类型为Schdulable，叶子节点为TaskSetManager，非叶子节点为Pool，下图是它们之间的继承关系。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdaa1sdk7j313o0qc0uz.jpg" alt=""></p><p>TaskScheduler支持两种调度策略，一种是FIFO，也是默认的调度策略，另一种是FAIR。在TaskScheduler初始化过程中会实例化rootPool，表示树的根节点，是Pool类型。如果是采用FIFO调度策略，则直接简单地将TaskSetManager按照先来先到的方式入队，出队时直接拿出最先进队的TaskSetManager，其树结构大致如下图所示，TaskSetManager保存在一个FIFO队列中。<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqdaa8cgqwj316u0j2q6a.jpg" alt=""></p><p>在阐述FAIR调度策略前，先贴一段使用FAIR调度策略的应用程序代码，后面针对该代码逻辑来详细阐述FAIR调度的实现细节。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultiJobTest</span> </span>&#123;</span><br><span class="line">  <span class="comment">// spark.scheduler.mode=FAIR</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = spark.sparkContext.textFile(...)</span><br><span class="line">      .map(_.split(<span class="string">"\\s+"</span>))</span><br><span class="line">      .map(x =&gt; (x(<span class="number">0</span>), x(<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobExecutor = <span class="type">Executors</span>.newFixedThreadPool(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    jobExecutor.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        spark.sparkContext.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="string">"count-pool"</span>)</span><br><span class="line">        <span class="keyword">val</span> cnt = rdd.groupByKey().count()</span><br><span class="line">        println(<span class="string">s"Count: <span class="subst">$cnt</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    jobExecutor.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        spark.sparkContext.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="string">"take-pool"</span>)</span><br><span class="line">        <span class="keyword">val</span> data = rdd.sortByKey().take(<span class="number">10</span>)</span><br><span class="line">        println(<span class="string">s"Data Samples: "</span>)</span><br><span class="line">        data.foreach &#123; x =&gt; println(x.mkString(<span class="string">", "</span>)) &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    jobExecutor.shutdown()</span><br><span class="line">    <span class="keyword">while</span> (!jobExecutor.isTerminated) &#123;&#125;</span><br><span class="line">    println(<span class="string">"Done!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述应用程序中使用两个线程分别调用了Action方法，即有两个Job会并发提交，但是不管怎样，这两个Job被切分成若干TaskSet后终究会被交到TaskScheduler这里统一管理，其调度树大致如下图所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqdac0cfroj31kw0pf0zt.jpg" alt=""></p><p>在出队时，则会对所有TaskSetManager排序，具体排序过程是从根节点rootPool开始，递归地去排序子节点，最后合并到一个ArrayBuffer里，代码逻辑如下。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSortedTaskSetQueue</span></span>: <span class="type">ArrayBuffer</span>[<span class="type">TaskSetManager</span>] = &#123;</span><br><span class="line">    <span class="keyword">var</span> sortedTaskSetQueue = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">TaskSetManager</span>]</span><br><span class="line">    <span class="keyword">val</span> sortedSchedulableQueue = schedulableQueue.asScala.toSeq.sortWith(taskSetSchedulingAlgorithm.comparator)</span><br><span class="line">    <span class="keyword">for</span> (schedulable &lt;- sortedSchedulableQueue) &#123;</span><br><span class="line">      sortedTaskSetQueue ++= schedulable.getSortedTaskSetQueue</span><br><span class="line">    &#125;</span><br><span class="line">    sortedTaskSetQueue</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>使用FAIR调度策略时，上面代码中的taskSetSchedulingAlgorithm的类型为FairSchedulingAlgorithm，排序过程的比较是基于Fair-share来比较的，每个要排序的对象包含三个属性: runningTasks值（正在运行的Task数）、minShare值、weight值，比较时会综合考量runningTasks值，minShare以及weight值。如果A对象的runningTasks大于它的minShare，B对象的runningTasks小于它的minShare，那么B排在A前面；如果A、B对象的runningTasks都小于它们的minShare，那么就比较runningTasks与minShare的比值，谁小谁排前面；如果A、B对象的runningTasks都大于它们的minShare，那么就比较runningTasks与weight的比值，谁小谁排前面。整体上来说就是通过minShare和weight这两个参数控制比较过程，可以做到不让资源被某些长时间Task给一直占了。</p><p>从调度队列中拿到TaskSetManager后，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。前面也提到，TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task。</p><h4 id="本地化调度"><a href="#本地化调度" class="headerlink" title="本地化调度"></a>本地化调度</h4><p>从调度队列中拿到TaskSetManager后，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。前面也提到，TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task。<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqdadrcr1pj31ai068q4f.jpg" alt=""></p><p>在TaskSetManager初始化过程中，会对Tasks按照Locality级别进行分类，Task的Locality有五种，优先级由高到低顺序：PROCESS_LOCAL(指定的Executor)，NODE_LOCAL(指定的主机节点)，NO_PREF(无所谓)，RACK_LOCAL(指定的机架)，ANY(满足不了Task的Locality就随便调度)。这五种Locality级别存在包含关系，RACK_LOCAL包含NODE_LOCAL，NODE_LOCAL包含PROCESS_LOCAL，然而ANY包含其他所有四种。初始化阶段在对Task分类时，根据Task的preferredLocations判断它属于哪个Locality级别，属于PROCESS_LOCAL的Task同时也会被加入到NODE_LOCAL、RACK_LOCAL类别中，比如，一个Task的preferredLocations指定了在Executor-2上执行，那么它属于Executor-2对应的PROCESS_LOCAL类别，同时也把他加入到Executor-2所在的主机对应的NODE_LOCAL类别，Executor-2所在的主机的机架对应的RACK_LOCAL类别中，以及ANY类别，这样在调度执行时，满足不了PROCESS_LOCAL，就逐步退化到NODE_LOCAL，RACK_LOCAL，ANY。</p><p>TaskSetManager在决定调度哪些Task时，是通过上面流程图中的resourceOffer方法来实现，为了尽可能地将Task调度到它的preferredLocations上，它采用一种延迟调度算法。resourceOffer方法原型如下，参数包括要调度任务的Executor Id、主机地址以及最大可容忍的Locality级别。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resourceOffer</span></span>(</span><br><span class="line">      execId: <span class="type">String</span>,</span><br><span class="line">      host: <span class="type">String</span>,</span><br><span class="line">      maxLocality: <span class="type">TaskLocality</span>.<span class="type">TaskLocality</span>)</span><br><span class="line">    : <span class="type">Option</span>[<span class="type">TaskDescription</span>]</span><br></pre></td></tr></table></figure><p>延迟调度算法的大致流程如下图所示。<br><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqdaela7nrj31kw0pvgr7.jpg" alt=""></p><p>首先看是否存在execId对应的PROCESS_LOCAL类别的任务，如果存在，取出来调度，否则根据当前时间，判断是否超过了PROCESS_LOCAL类别最大容忍的延迟，如果超过，则退化到下一个级别NODE_LOCAL，否则等待不调度。退化到下一个级别NODE_LOCAL后调度流程也类似，看是否存在host对应的NODE_LOCAL类别的任务，如果存在，取出来调度，否则根据当前时间，判断是否超过了NODE_LOCAL类别最大容忍的延迟，如果超过，则退化到下一个级别RACK_LOCAL，否则等待不调度，以此类推…..。当不满足Locatity类别会选择等待，直到下一轮调度重复上述流程，如果你比较激进，可以调大每个类别的最大容忍延迟时间，如果不满足Locatity时就会等待多个调度周期，直到满足或者超过延迟时间退化到下一个级别为止。</p><h4 id="失败重试与黑名单机制"><a href="#失败重试与黑名单机制" class="headerlink" title="失败重试与黑名单机制"></a>失败重试与黑名单机制</h4><p>除了选择合适的Task调度运行外，还需要监控Task的执行状态，前面也提到，与外部打交道的是SchedulerBackend，Task被提交到Executor启动执行后，Executor会将执行状态上报给SchedulerBackend，SchedulerBackend则告诉TaskScheduler，TaskScheduler找到该Task对应的TaskSetManager，并通知到该TaskSetManager，这样TaskSetManager就知道Task的失败与成功状态，对于失败的Task，会记录它失败的次数，如果失败次数还没有超过最大重试次数，那么就把它放回待调度的Task池子中，否则整个Application失败。</p><p>在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“黑暗”时间，“黑暗”时间是指这段时间内不要再往这个节点上调度这个Task了。</p><h4 id="推测式执行"><a href="#推测式执行" class="headerlink" title="推测式执行"></a>推测式执行</h4><p>TaskScheduler在启动SchedulerBackend后，还会启动一个后台线程专门负责推测任务的调度，推测任务是指对一个Task在不同的Executor上启动多个实例，如果有Task实例运行成功，则会干掉其他Executor上运行的实例。推测调度线程会每隔固定时间检查是否有Task需要推测执行，如果有，则会调用SchedulerBackend的reviveOffers去尝试拿资源运行推测任务。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdah2m113j31ho0gstau.jpg" alt=""></p><p>检查是否有Task需要推测执行的逻辑最后会交到TaskSetManager，TaskSetManager采用基于统计的算法，检查Task是否需要推测执行，算法流程大致如下图所示。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdaj1pwd0j31au11oq9z.jpg" alt=""><br>TaskSetManager首先会统计成功的Task数，当成功的Task数超过75%(可通过参数spark.speculation.quantile控制)时，再统计所有成功的Tasks的运行时间，得到一个中位数，用这个中位数乘以1.5(可通过参数spark.speculation.multiplier控制)得到运行时间门限，如果在运行的Tasks的运行时间超过这个门限，则对它启用推测。算法逻辑较为简单，其实就是对那些拖慢整体进度的Tasks启用推测，以加速整个TaskSet即Stage的运行。</p><h2 id="资源申请机制"><a href="#资源申请机制" class="headerlink" title="资源申请机制"></a>资源申请机制</h2><p>在前文已经提过，ApplicationMaster和SchedulerBackend起来后，SchedulerBackend通过ApplicationMaster申请资源，ApplicationMaster就是用来专门适配YARN申请Container资源的，当申请到Container，会在相应Container上启动Executor进程，其他事情就交给SchedulerBackend。Spark早期版本只支持静态资源申请，即一开始就指定用多少资源，在整个Spark应用程序运行过程中资源都不能改变，后来支持动态Executor申请，用户不需要指定确切的Executor数量，Spark会动态调整Executor的数量以达到资源利用的最大化。</p><h3 id="静态资源申请"><a href="#静态资源申请" class="headerlink" title="静态资源申请"></a>静态资源申请</h3><p>静态资源申请是用户在提交Spark应用程序时，就要提前估计应用程序需要使用的资源，包括Executor数(num_executor)、每个Executor上的core数(executor_cores)、每个Executor的内存(executor_memory)以及Driver的内存(driver_memory)。</p><p>在估计资源使用时，应当首先了解这些资源是怎么用的。任务的并行度由分区数(Partitions)决定，一个Stage有多少分区，就会有多少Task。每个Task默认占用一个Core，一个Executor上的所有core共享Executor上的内存，一次并行运行的Task数等于num_executor*executor_cores，如果分区数超过该值，则需要运行多个轮次，一般来说建议运行3～5轮较为合适，否则考虑增加num_executor或executor_cores。由于一个Executor的所有tasks会共享内存executor_memory，所以建议executor_cores不宜过大。executor_memory的设置则需要综合每个分区的数据量以及是否有缓存等逻辑。下图描绘了一个应用程序内部资源利用情况。<br><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fqdak8sm5bj31kw0uj16a.jpg" alt=""></p><h3 id="动态资源申请"><a href="#动态资源申请" class="headerlink" title="动态资源申请"></a>动态资源申请</h3><p>动态资源申请目前只支持到Executor，即可以不用指定num_executor，通过参数spark.dynamicAllocation.enabled来控制。由于许多Spark应用程序一开始可能不需要那么多Executor或者其本身就不需要太多Executor，所以不必一次性申请那么多Executor，根据具体的任务数动态调整Executor的数量，尽可能做到资源的不浪费。由于动态Executor的调整会导致Executor动态的添加与删除，如果删除Executor，其上面的中间Shuffle结果可能会丢失，这就需要借助第三方的ShuffleService了，如果Spark是部署在Yarn上，则可以在Yarn上配置Spark的ShuffleService，具体操作仅需做两点:</p><p>1.首先在yarn-site.xml中加上如下配置：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle,spark_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.spark_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.spark.network.yarn.YarnShuffleService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.shuffle.service.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>7337<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>2.将Spark ShuffleService jar包<code>$SPARK_HOME/lib/spark-*-yarn-shuffle.jar</code>拷贝到每台NodeManager的<code>$HADOOP_HOME/share/hadoop/yarn/lib/</code>下，并重启所有的NodeManager。</p><p>当启用动态Executor申请时，在SparkContext初始化过程中会实例化ExecutorAllocationManager，它是被用来专门控制动态Executor申请逻辑的，动态Executor申请是一种基于当前Task负载压力实现动态增删Executor的机制。一开始会按照参数spark.dynamicAllocation.initialExecutors设置的初始Executor数申请，然后根据当前积压的Task数量，逐步增长申请的Executor数，如果当前有积压的Task，那么取积压的Task数和spark.dynamicAllocation.maxExecutors中的最小值作为Executor数上限，每次新增加申请的Executor为2的次方，即第一次增加1，第二次增加2，第三次增加4，…。另一方面，如果一个Executor在一段时间内都没有Task运行，则将其回收，但是在Remove Executor时，要保证最少的Executor数，该值通过参数spark.dynamicAllocation.minExecutors来控制，如果Executor上有Cache的数据，则永远不会被Remove，以保证中间数据不丢失。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本文详细阐述了Spark的任务调度，着重讨论Spark on Yarn的部署调度，剖析了从应用程序提交到运行的全过程。Spark Schedule算是Spark中的一个大模块，它负责任务下发与监控等，基本上扮演了Spark大脑的角色。了解Spark Schedule有助于帮助我们清楚地认识Spark应用程序的运行轨迹，同时在我们实现其他系统时，也可以借鉴Spark的实现。</p><p>转载请注明出处，本文永久链接：<a href="http://sharkdtu.com/posts/spark-scheduler.html" target="_blank" rel="noopener">http://sharkdtu.com/posts/spark-scheduler.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;Apache Spark是一个开源的，强大的分布式查询和处理引擎。它提供MapReduce的灵活性和可扩展性，但速度明显更高。&lt;br&gt;&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNc79ly1fq
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="spark" scheme="http://www.ihoge.cn/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkSQL DataFrame进阶篇</title>
    <link href="http://www.ihoge.cn/2018/Sparksql2.html"/>
    <id>http://www.ihoge.cn/2018/Sparksql2.html</id>
    <published>2018-04-14T09:59:21.000Z</published>
    <updated>2018-04-22T14:54:42.223Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="1-创建SparkSession【2-0】和-SQLContext实例【1-x】"><a href="#1-创建SparkSession【2-0】和-SQLContext实例【1-x】" class="headerlink" title="1.创建SparkSession【2.0】和 SQLContext实例【1.x】"></a>1.创建SparkSession【2.0】和 SQLContext实例【1.x】</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>创建<span class="type">SparkSession</span>【<span class="number">2.0</span>】</span><br><span class="line"><span class="comment">///spark2.0后，用sparksession代替sparkcontext和sqlcontext的创建</span></span><br><span class="line"><span class="keyword">val</span> spark= <span class="type">SparkSession</span>.builder().appName(<span class="string">"SparkSQLTest"</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> numbers=spark.range(<span class="number">1</span> ,<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">numbers.show()</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>创建<span class="type">SQLContext</span>实例【<span class="number">1.</span>x】</span><br><span class="line"><span class="keyword">val</span> conf= <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SQL_Advanced_case"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"><span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext=<span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">import</span>.sqlContext.implicits._</span><br></pre></td></tr></table></figure><h1 id="2-创建DataFrame"><a href="#2-创建DataFrame" class="headerlink" title="2.创建DataFrame"></a>2.创建DataFrame</h1><h2 id="方式1-不创建RDD"><a href="#方式1-不创建RDD" class="headerlink" title="方式1:不创建RDD"></a>方式1:不创建RDD</h2><p>使用createDataFram方法，直接基于列表List创建DataFrame.缺点是创建出来的DataFrame没有列名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val customerData=List((&quot;Alex&quot;,&quot;浙江&quot;,39,230.00), (&quot;Bob&quot;,&quot;北京&quot;, 18, 170.00), (&quot;Chris&quot;, &quot;江苏&quot;, 45, 529.95), (&quot;Dave&quot;, &quot;北京&quot;, 25, 99.99), (&quot;Ellie&quot;, &quot;浙江&quot;, 23, 1299.95), (&quot;Fred&quot;, &quot;北京&quot;, 21, 1099.00))</span><br><span class="line">val customerDF1=sqlContext.createDataFrame(customerData)</span><br><span class="line">customerDF1.printSchema</span><br></pre></td></tr></table></figure></p><h2 id="方式2-不创建RDD"><a href="#方式2-不创建RDD" class="headerlink" title="方式2:不创建RDD"></a>方式2:不创建RDD</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用createDataFram方法，直接基于列表<span class="type">List</span>创建<span class="type">DataFrame</span>.即便定义了样例类，但基于createDataFrame创建出来的<span class="type">DataFrame</span>缺点是创建出来的<span class="type">DataFrame</span>没有列名</span><br><span class="line"><span class="keyword">val</span> customerData=<span class="type">List</span>((<span class="string">"Alex"</span>,<span class="string">"浙江"</span>,<span class="number">39</span>,<span class="number">230.00</span>), (<span class="string">"Bob"</span>,<span class="string">"北京"</span>, <span class="number">18</span>, <span class="number">170.00</span>), (<span class="string">"Chris"</span>, <span class="string">"江苏"</span>, <span class="number">45</span>, <span class="number">529.95</span>), (<span class="string">"Dave"</span>, <span class="string">"北京"</span>, <span class="number">25</span>, <span class="number">99.99</span>), (<span class="string">"Ellie"</span>, <span class="string">"浙江"</span>, <span class="number">23</span>, <span class="number">1299.95</span>), (<span class="string">"Fred"</span>, <span class="string">"北京"</span>, <span class="number">21</span>, <span class="number">1099.00</span>))</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerInfo</span>(<span class="params">customer:<span class="type">String</span>, province:<span class="type">String</span>, age: <span class="type">Int</span>, total:<span class="type">Double</span> </span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">cdf=cDataRDD</span>.<span class="title">map</span>(<span class="params">x=&gt;x.split(","</span>)).<span class="title">map</span>(<span class="params">x=&gt;<span class="type">CustomerInfo</span>(x(0</span>),<span class="title">x</span>(<span class="params">1</span>),<span class="title">x</span>(<span class="params">2</span>).<span class="title">toInt</span>,<span class="title">x</span>(<span class="params">3</span>).<span class="title">toDouble</span>)).<span class="title">toDF</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">customerDF1=sqlContext</span>.<span class="title">createDataFrame</span>(<span class="params">customerData</span>)</span></span><br><span class="line"><span class="class"><span class="title">customerDF1</span>.<span class="title">printSchema</span></span></span><br></pre></td></tr></table></figure><h2 id="方式3：传统方式"><a href="#方式3：传统方式" class="headerlink" title="方式3：传统方式"></a>方式3：传统方式</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">先创建<span class="type">RDD</span>和样例类，然后通过toDF创建<span class="type">DataFrame</span>。此时<span class="type">DataFrame</span>中包含列信息。</span><br><span class="line"><span class="keyword">val</span> customerData=<span class="type">Array</span>(<span class="string">"Alex,浙江,39,230.00"</span>,<span class="string">"Bob,北京,18,170.00"</span>,<span class="string">"Chris,江苏,45,529.95"</span>,<span class="string">"Dave,北京,25,99.99"</span>,<span class="string">"Ellie,浙江,23,1299.95"</span>,<span class="string">"Fred,北京,21,1099.00"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerInfo</span>(<span class="params">customer:<span class="type">String</span>, province:<span class="type">String</span>, age: <span class="type">Int</span>, total:<span class="type">Double</span> </span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">customerRDD=sc</span>.<span class="title">makeRDD</span>(<span class="params">customerData</span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">customerDF1=customerRDD</span>.<span class="title">map</span>(<span class="params">x=&gt;x.split(","</span>)).<span class="title">map</span>(<span class="params">x=&gt;<span class="type">CustomerInfo</span>(x(0</span>),<span class="title">x</span>(<span class="params">1</span>),<span class="title">x</span>(<span class="params">2</span>).<span class="title">toInt</span>,<span class="title">x</span>(<span class="params">3</span>).<span class="title">toDouble</span>)).<span class="title">toDF</span></span></span><br><span class="line"><span class="class"><span class="title">customerDF1</span>.<span class="title">printSchema</span></span></span><br></pre></td></tr></table></figure><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="修改列名"><a href="#修改列名" class="headerlink" title="修改列名"></a>修改列名</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> customerDF=customerDF1.withColumnRenamed(<span class="string">"_1"</span>,<span class="string">"customer"</span>).withColumnRenamed(<span class="string">"_2"</span>,<span class="string">"province"</span>).withColumnRenamed(<span class="string">"_3"</span>, <span class="string">"age"</span>).withColumnRenamed(<span class="string">"_4"</span>, <span class="string">"total"</span>)</span><br></pre></td></tr></table></figure><h3 id="查看表模式"><a href="#查看表模式" class="headerlink" title="查看表模式"></a>查看表模式</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.printSchema</span><br></pre></td></tr></table></figure><h3 id="数值型数据基本的统计与分析"><a href="#数值型数据基本的统计与分析" class="headerlink" title="数值型数据基本的统计与分析"></a>数值型数据基本的统计与分析</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.describe().show</span><br></pre></td></tr></table></figure><h1 id="3-DataFrame方法"><a href="#3-DataFrame方法" class="headerlink" title="3.DataFrame方法"></a>3.DataFrame方法</h1><h2 id="select"><a href="#select" class="headerlink" title="select"></a>select</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">customerDF.select(customerDF.col(<span class="string">"customer"</span>)).show</span><br><span class="line">customerDF.select(customerDF(<span class="string">"customer"</span>)).show</span><br><span class="line">customerDF.select(<span class="string">"customer"</span>, <span class="string">"province"</span>).show</span><br><span class="line">customerDF.select($<span class="string">"customer"</span>, $<span class="string">"province"</span>).show</span><br><span class="line">customerDF.select(col(<span class="string">"customer"</span>), col(<span class="string">"province"</span>)).show</span><br><span class="line">customerDF.select(customerDF(<span class="string">"customer"</span>), col(<span class="string">"province"</span>)).show</span><br><span class="line">customerDF.select(<span class="string">"customer"</span>, $<span class="string">"province"</span>).show<span class="comment">//错误,字符串与$不能混用。</span></span><br><span class="line">customerDF.select(col(<span class="string">"customer"</span>), $<span class="string">"province"</span>).show<span class="comment">//是否正确？正确！！</span></span><br></pre></td></tr></table></figure><h3 id="使用表达式"><a href="#使用表达式" class="headerlink" title="使用表达式"></a>使用表达式</h3><p>（Column对象中的方法）<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF. select($<span class="string">"customer"</span>,( $<span class="string">"age"</span>*<span class="number">2</span>)+<span class="number">10</span>, $<span class="string">"province"</span>===<span class="string">"浙江"</span>).show <span class="comment">//计算$"province"==="浙江"这一关系表达式的值</span></span><br></pre></td></tr></table></figure></p><h3 id="as、alias列的重命名"><a href="#as、alias列的重命名" class="headerlink" title="as、alias列的重命名"></a>as、alias列的重命名</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF. select($<span class="string">"customer"</span> as <span class="string">"name"</span>,( $<span class="string">"age"</span>*<span class="number">2</span>)+<span class="number">10</span> alias <span class="string">"newAge"</span>, $<span class="string">"province"</span>===<span class="string">"浙江"</span> as <span class="string">"isZJ"</span>).show</span><br></pre></td></tr></table></figure><h3 id="lit添加列"><a href="#lit添加列" class="headerlink" title="lit添加列"></a>lit添加列</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.spark.sql.functions中的方法lit</span><br><span class="line"><span class="keyword">val</span> cdf1=customerDF.select($<span class="string">"customer"</span>, $<span class="string">"age"</span>, when($<span class="string">"age"</span>&lt;<span class="number">20</span>,<span class="number">1</span>).when($<span class="string">"age"</span>&lt;<span class="number">30</span>, <span class="number">2</span>).otherwise(<span class="number">3</span>) as <span class="string">"ageGroup"</span>, lit(<span class="literal">false</span>) as <span class="string">"trusted"</span>)</span><br><span class="line">cdf1.show</span><br><span class="line">cdf1.printSchema</span><br></pre></td></tr></table></figure><h3 id="drop"><a href="#drop" class="headerlink" title="drop"></a>drop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> cdf2=cdf1.drop(<span class="string">"trusted"</span>)</span><br><span class="line">cdf2.show</span><br><span class="line">cdf2.printSchema</span><br></pre></td></tr></table></figure><h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerDF.select($<span class="string">"province"</span>).distinct.show</span><br></pre></td></tr></table></figure><h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">customerDF.filter($<span class="string">"age"</span>&gt;<span class="number">30</span>).show</span><br><span class="line">customerDF.filter(<span class="string">"age&gt;30"</span>).show</span><br><span class="line">customerDF.filter($<span class="string">"age"</span>&lt;=<span class="number">30</span> and $<span class="string">"province"</span>===<span class="string">"浙江"</span> )</span><br><span class="line">customerDF.filter(<span class="string">"age&lt;=30 and province =’浙江’"</span>)</span><br></pre></td></tr></table></figure><h2 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h2><h3 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h3><p>向已有的DataFrame添加一个新列，不删除之前的列<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> customerAgeGroupDF=customerDF.withColumn(<span class="string">"agegroup"</span>, when($<span class="string">"age"</span>&lt;<span class="number">20</span>, <span class="number">1</span>).when($<span class="string">"age"</span>&lt;<span class="number">30</span>, <span class="number">2</span>).otherwise(<span class="number">3</span>))</span><br><span class="line">customerAgeGroupDF.show</span><br></pre></td></tr></table></figure></p><h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><p>操作返回GroupedData对象【2.0中为RelationalGroupedDataSet】<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//其中封装了大量聚合方法。</span></span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>).max().show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>,<span class="string">"province"</span>).count().show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>).min(<span class="string">"age"</span>, <span class="string">"total"</span>).show()</span><br></pre></td></tr></table></figure></p><h3 id="agg"><a href="#agg" class="headerlink" title="agg"></a>agg</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>).agg(sum($<span class="string">"total"</span>), min($<span class="string">"total"</span>)).show()</span><br></pre></td></tr></table></figure><h3 id="pivot"><a href="#pivot" class="headerlink" title="pivot"></a>pivot</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">customerAgeGroupDF.groupBy(<span class="string">"province"</span>).pivot(<span class="string">"agegroup"</span>).sum(<span class="string">"total"</span>).show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"province"</span>).pivot(<span class="string">"agegroup"</span>,<span class="type">Seq</span>(<span class="number">1</span>,<span class="number">2</span>)). agg(<span class="string">"total"</span>).show()</span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"province"</span>).pivot(<span class="string">"agegroup"</span>,<span class="type">Seq</span>(<span class="number">2</span>,<span class="number">3</span>)).agg(sum($<span class="string">"total"</span>), min($<span class="string">"total"</span>)).filter($<span class="string">"provice"</span>=!=<span class="string">"北京"</span>).show</span><br></pre></td></tr></table></figure><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">customerDF.orderBy(<span class="string">"age"</span>).show</span><br><span class="line">customerDF.orderBy($<span class="string">"age"</span>).show</span><br><span class="line">customerDF.orderBy(desc(<span class="string">"age"</span>)).show()</span><br><span class="line"><span class="comment">/*此处orderBy方法一定要用在所有聚合函数之后，因为groupBy方法返回的是GroupedData类型数据,</span></span><br><span class="line"><span class="comment">该类型数据中的聚合方法返回DateFrame类型对象，而orderBy是DataFrame中的方法，所以用在groupBy</span></span><br><span class="line"><span class="comment">之后会提示错误：orderBy不是GroupedData的成员方法。*/</span></span><br><span class="line">customerAgeGroupDF.groupBy(<span class="string">"agegroup"</span>,<span class="string">"province"</span>).count().orderBy($<span class="string">"agegroup"</span>.desc).show()</span><br><span class="line">students.sort($<span class="string">"age"</span>.asc).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;1-创建SparkSession【2-0】和-SQLContext实例【1-x】&quot;&gt;&lt;a href=&quot;#1-创建SparkSession【2-0】和-SQLContext实例【1-x】&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="sparksql" scheme="http://www.ihoge.cn/tags/sparksql/"/>
    
  </entry>
  
  <entry>
    <title>SparkSQL DataFrame基础篇</title>
    <link href="http://www.ihoge.cn/2018/SparkSQl.html"/>
    <id>http://www.ihoge.cn/2018/SparkSQl.html</id>
    <published>2018-04-14T05:59:21.000Z</published>
    <updated>2018-04-22T14:55:25.956Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>#SparkSQL DataFrame基础篇</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Spark</span> <span class="number">2.2</span>及以后的<span class="type">SparkSession</span>替换了<span class="type">Spark</span>以前版本中的<span class="type">SparkContext</span>和<span class="type">SQLContext</span>，为<span class="type">Spark</span>集群提供了唯一的入口点。</span><br><span class="line"><span class="keyword">val</span> spark =<span class="type">SparkSession</span>.builder().</span><br><span class="line">appName(“<span class="type">SparkExample</span>”).</span><br><span class="line">getOrCreate()</span><br><span class="line">为了向后兼容，<span class="type">SparkSession</span>对象包含<span class="type">SparkContext</span>和<span class="type">SQLContext</span>对象。当使用交互式<span class="type">Spark</span> shell时，创建一个<span class="type">SparkSession</span>类型对象名为spark。</span><br><span class="line"></span><br><span class="line">因此该文档里所有的<span class="type">SQLContext</span>在spark2<span class="number">.2</span>+中都可以替换成spark</span><br></pre></td></tr></table></figure><h2 id="基于反射机制创建DataFrame"><a href="#基于反射机制创建DataFrame" class="headerlink" title="基于反射机制创建DataFrame"></a>基于反射机制创建DataFrame</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//students.txt</span><br><span class="line">160201,Michael,17</span><br><span class="line">160101,Andy,23</span><br><span class="line">160301,justin,23</span><br><span class="line">160202,John,22</span><br><span class="line">160102,Herry,17</span><br><span class="line">160203,Brewster,18</span><br><span class="line">160302,Brice,20</span><br><span class="line">160303,Justin,25</span><br><span class="line">160103,Jerry,22</span><br><span class="line">160304,Tom,24</span><br></pre></td></tr></table></figure><p>不结合hive，使用spark实例(适用于spark1+)<br>val spark=new org.apache.spark.sql.spark(sc)<br>import spark.implicits._</p><p>结合hive后，访问hive时使用HiveContext实例<br>val hiveContext=new org.apache.spark.sql.hive.HiveContext(sc)<br>import hiveContext.implicits._</p><p><code>这里与Spark2.2+有所不同</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br><span class="line">students.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line"><span class="keyword">val</span> youngstudents=spark.sql(<span class="string">"SELECT name FROM tb_students WHERE age&gt;=19 AND age&lt;=22"</span>)</span><br><span class="line">youngstudents.show</span><br></pre></td></tr></table></figure><h2 id="基于编程创建DataFrame"><a href="#基于编程创建DataFrame" class="headerlink" title="基于编程创建DataFrame"></a>基于编程创建DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._   </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._         </span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> students=sc.textFile(<span class="string">"hdfs://master:9000/sqldata/students.txt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schemaString=<span class="string">"id name age"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> schema=<span class="type">StructType</span>(schemaString.split(<span class="string">" "</span>).map(fieldname=&gt; <span class="type">StructField</span>(fieldname, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rowRDD=students.map(_.split(<span class="string">","</span>)).map(p=&gt;<span class="type">Row</span>(p(<span class="number">0</span>), p(<span class="number">1</span>), p(<span class="number">2</span>).trim))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> studentsDataFrame=spark.createDataFrame(rowRDD, schema) </span><br><span class="line"></span><br><span class="line">studentsDataFrame.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> names=spark.sql(<span class="string">"SELECT name FROM tb_students"</span>)</span><br><span class="line"></span><br><span class="line">names.show</span><br></pre></td></tr></table></figure><h2 id="基于DataFrame创建Json文件"><a href="#基于DataFrame创建Json文件" class="headerlink" title="基于DataFrame创建Json文件"></a>基于DataFrame创建Json文件</h2><h3 id="1-创建DataFrame"><a href="#1-创建DataFrame" class="headerlink" title="1.创建DataFrame"></a>1.创建DataFrame</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br></pre></td></tr></table></figure><h3 id="2-另存为json类型文件"><a href="#2-另存为json类型文件" class="headerlink" title="2.另存为json类型文件"></a>2.另存为json类型文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//students.save("hdfs://master:9000/sqldata/students.json")//无法执行</span></span><br><span class="line"></span><br><span class="line">spark1<span class="number">.4</span>及以后，dataframe中的save方法不建议使用，有的直接被弃用，</span><br><span class="line">使用<span class="type">DataFrame</span>中的write方法返回一个<span class="type">DataFrameWriter</span>类型对象，再使用里面的save方法、format().save()、parquet()等方法</span><br><span class="line"></span><br><span class="line">students.write.save(<span class="string">"hdfs://master:9000/sqldata/students.json"</span>)</span><br><span class="line">students.write.format(<span class="string">"json"</span>).save(<span class="string">"hdfs://master:9000/sqldata/s1.json"</span>)<span class="comment">//Spark 1.4 DataFrameWriter中方法format、save</span></span><br><span class="line">students.write.json(<span class="string">"hdfs://master:9000/sqldata/s2.json"</span>)<span class="comment">//Spark 1.4 DataFrameWriter中方法json</span></span><br></pre></td></tr></table></figure><h2 id="基于DataFrame创建Parquet文件"><a href="#基于DataFrame创建Parquet文件" class="headerlink" title="基于DataFrame创建Parquet文件"></a>基于DataFrame创建Parquet文件</h2><h3 id="1-创建DataFrame-1"><a href="#1-创建DataFrame-1" class="headerlink" title="1.创建DataFrame"></a>1.创建DataFrame</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br></pre></td></tr></table></figure><h3 id="2-另存为parquet类型文件"><a href="#2-另存为parquet类型文件" class="headerlink" title="2.另存为parquet类型文件"></a>2.另存为parquet类型文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//students.save("hdfs://master:9000/sqldata/students.parquet")//无法执行</span></span><br><span class="line"></span><br><span class="line">spark1<span class="number">.4</span>及以后，dataframe中的save方法不建议使用，有的直接被弃用，</span><br><span class="line">使用<span class="type">DataFrame</span>中的write方法返回一个<span class="type">DataFrameWriter</span>类型对象，再使用里面的save方法、format().save()、parquet()等方法</span><br><span class="line"></span><br><span class="line">students.write.save(<span class="string">"hdfs://master:9000/sqldata/students.parquet"</span>)</span><br><span class="line">students.write.format(<span class="string">"parquet"</span>).save(<span class="string">"hdfs://master:9000/sqldata/s1.parquet"</span>)</span><br><span class="line">students.write.parquet(<span class="string">"hdfs://master:9000/sqldata/s2.parquet"</span>)</span><br></pre></td></tr></table></figure><h2 id="基于Json创建DataFrame"><a href="#基于Json创建DataFrame" class="headerlink" title="基于Json创建DataFrame"></a>基于Json创建DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//read方法是spark中的方法，返回值类型是DataFrameReader,以往的jsonFile方法已经不建议使用 </span></span><br><span class="line"><span class="keyword">val</span> students=spark.read.json(<span class="string">"hdfs://master:9000/sqldata/s2.json"</span>)<span class="comment">//read.json()是最通用的一种方法s1.json,s2.json都可读     </span></span><br><span class="line">students.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from tb_students"</span>).show</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">val students=spark.read.load("hdfs://master:9000/sqldata/students.json")//可执行，但换做读s1.json,s2.json不可读</span></span><br><span class="line"><span class="comment">val students=spark.read.load("hdfs://master:9000/sqldata/s1.json") //不可执行，load默认读取parquet类型文件</span></span><br><span class="line"><span class="comment">val students=spark.jsonFile("hdfs://master:9000/sqldata/students.json") //jsonFile不可用</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="基于Parquet创建DataFrame"><a href="#基于Parquet创建DataFrame" class="headerlink" title="基于Parquet创建DataFrame"></a>基于Parquet创建DataFrame</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//read方法是spark中的方法，返回值类型是DataFrameReader,以往的parquetFile方法已经不建议使用 </span></span><br><span class="line"><span class="keyword">val</span> students=spark.read.parquet(<span class="string">"hdfs://master:9000/sqldata/students.parquet"</span>) <span class="comment">//最通用的一种方法,students.parquet,s1.parquet,s2.parquet都可读</span></span><br><span class="line">students.registerTempTable(<span class="string">"tb_students"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from tb_students"</span>).show</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">val students=spark.read.load("hdfs://master:9000/sqldata/students.parquet")</span></span><br><span class="line"><span class="comment">val students=spark.load.parquet("hdfs://master:9000/sqldata/s1.parquet")//此处不可执行因为load非SparkSession中方法，Spark1.X中load方法可执行，默认读取parquet文件</span></span><br><span class="line"><span class="comment">val students=spark.parquetFile("hdfs://master:9000/sqldata/students.parquet")//parquetFile不可用</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="DataFrame的其他操作"><a href="#DataFrame的其他操作" class="headerlink" title="DataFrame的其他操作"></a>DataFrame的其他操作</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">students.head    </span><br><span class="line"></span><br><span class="line">students.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">students.show(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">students.columns</span><br><span class="line"></span><br><span class="line">students.dtypes</span><br><span class="line"></span><br><span class="line">students.printSchema</span><br><span class="line"><span class="comment">//更详细的Schema信息</span></span><br></pre></td></tr></table></figure><h3 id="withColumn"><a href="#withColumn" class="headerlink" title="withColumn"></a>withColumn</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.withColumn(<span class="string">"bonus"</span>, students(<span class="string">"age"</span>)*<span class="number">50</span>).show</span><br><span class="line">students.withColumn(<span class="string">"bonus"</span>, $<span class="string">"age"</span>*<span class="number">50</span>).show<span class="comment">//可以运行</span></span><br><span class="line">students.withColumn(<span class="string">"bonus"</span>, <span class="string">"age"</span>*<span class="number">50</span>).show<span class="comment">//不可运行</span></span><br></pre></td></tr></table></figure><h3 id="withColumnRenamed"><a href="#withColumnRenamed" class="headerlink" title="withColumnRenamed"></a>withColumnRenamed</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newstudents=students.withColumnRenamed(<span class="string">"age"</span>, <span class="string">"newage"</span>)</span><br><span class="line"></span><br><span class="line">newstudents.printSchema</span><br></pre></td></tr></table></figure><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span>(<span class="params">id: <span class="type">String</span>, name : <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/students.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Student</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">students</span> </span>= rdd.toDF()</span><br><span class="line"></span><br><span class="line">students.select(<span class="string">"age"</span>, <span class="string">"name"</span>).show</span><br><span class="line">students.select( $<span class="string">"age"</span>, $<span class="string">"name"</span>).show</span><br><span class="line">students.selectExpr(<span class="string">"age+1"</span>, <span class="string">"name"</span>, <span class="string">"abs(age)"</span>).show</span><br><span class="line">students.selectExpr(<span class="string">"age+1 as newage1"</span>, <span class="string">"name as newName"</span>, <span class="string">"sqrt(age) as newage2"</span>).show</span><br></pre></td></tr></table></figure><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">students.filter(<span class="string">"age&gt;20"</span>).show</span><br><span class="line">students.filter($<span class="string">"age"</span>&gt;<span class="number">20</span>).show</span><br><span class="line">students.filter($<span class="string">"age"</span>&gt;<span class="number">23</span> &amp;&amp; $<span class="string">"name"</span>===<span class="string">"Justin"</span>).show  <span class="comment">//可以执行</span></span><br><span class="line">students.filter(<span class="symbol">'age</span>&gt;<span class="number">20</span>).show <span class="comment">//可以执行</span></span><br><span class="line">students.filter(<span class="string">"age&gt;23 &amp;&amp; name===Justin"</span>).show        <span class="comment">//不可以执行，字符串中不能用逻辑运算符</span></span><br></pre></td></tr></table></figure><h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">students.where(<span class="symbol">'age</span>&gt;<span class="number">20</span>).show   </span><br><span class="line">students.where($<span class="string">"age"</span>&gt;<span class="number">20</span>).show </span><br><span class="line">students.where($<span class="string">"age"</span>&gt;<span class="number">23</span> &amp;&amp; $<span class="string">"name"</span>===<span class="string">"Justin"</span>).show</span><br><span class="line">students.where(<span class="symbol">'age</span>&gt;<span class="number">23</span> &amp;&amp; <span class="symbol">'name</span>===<span class="string">"Justin"</span>).show</span><br><span class="line">students.where(<span class="string">"age&gt;23 &amp;&amp; name===Justin"</span>).show      <span class="comment">//不可以执行,字符串中不能用逻辑运算符</span></span><br></pre></td></tr></table></figure><h3 id="orderBy"><a href="#orderBy" class="headerlink" title="orderBy"></a>orderBy</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.orderBy(<span class="string">"age"</span>, <span class="string">"id"</span>).show(<span class="number">5</span>)</span><br><span class="line">students.orderBy(students(<span class="string">"age"</span>)).show(<span class="number">5</span>)</span><br><span class="line">students.orderBy($<span class="string">"age"</span>).show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">newstudents.groupBy(&apos;course_id).mean(&quot;score&quot;).orderBy(&apos;course_id).show</span><br><span class="line"></span><br><span class="line">max(&quot;col&quot;)</span><br><span class="line">min(&quot;col&quot;)</span><br><span class="line">mean(&quot;col&quot;)</span><br><span class="line">sum(&quot;col&quot;)</span><br><span class="line">该四种方法只适用于数值型的GroupedData对象</span><br></pre></td></tr></table></figure><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.sort(<span class="string">"age"</span>).show(<span class="number">5</span>)</span><br><span class="line">students.sort($<span class="string">"age"</span>.desc).show(<span class="number">5</span>)</span><br><span class="line">students.sort(<span class="string">"age"</span>.desc).show(<span class="number">5</span>)<span class="comment">//不可执行</span></span><br></pre></td></tr></table></figure><h3 id="toDF"><a href="#toDF" class="headerlink" title="toDF"></a>toDF</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> newstudents=students.toDF(<span class="string">"newid"</span>, <span class="string">"newage"</span>, <span class="string">"newname"</span>)</span><br><span class="line">newstudents.printSchema</span><br></pre></td></tr></table></figure><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Score</span>(<span class="params">id:<span class="type">String</span>,course_id:<span class="type">String</span>,score:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">scores_rdd=sc</span>.<span class="title">textFile</span>(<span class="params">"hdfs://master:9000/sqldata/scores.txt"</span>).<span class="title">map</span>(<span class="params">_.split(","</span>)).<span class="title">map</span>(<span class="params">p =&gt; <span class="type">Score</span>(p(0</span>), <span class="title">p</span>(<span class="params">1</span>), <span class="title">p</span>(<span class="params">2</span>).<span class="title">trim</span>.<span class="title">toInt</span>))</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">scores</span> </span>= scores_rdd.toDF()</span><br><span class="line">students.join(scores, students(<span class="string">"id"</span> )===scores(<span class="string">"id"</span>), <span class="string">"outer"</span>).show</span><br></pre></td></tr></table></figure><h2 id="案例练习"><a href="#案例练习" class="headerlink" title="案例练习"></a>案例练习</h2><h3 id="Spark-SQL基本案例1"><a href="#Spark-SQL基本案例1" class="headerlink" title="Spark SQL基本案例1"></a>Spark SQL基本案例1</h3><p><strong>scores.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">scores 学号、课程编号、成绩</span><br><span class="line">160201,1,60</span><br><span class="line">160101,2,90</span><br><span class="line">160301,1,70</span><br><span class="line">160202,3,70</span><br><span class="line">160102,3,50</span><br><span class="line">160102,4,95</span><br><span class="line">160302,1,98</span><br><span class="line">160303,2,57</span><br><span class="line">160103,3,64</span><br><span class="line">160304,3,50</span><br><span class="line">160201,2,77</span><br><span class="line">160101,3,57</span><br><span class="line">160301,3,72</span><br><span class="line">160202,2,80</span><br><span class="line">160102,2,58</span><br><span class="line">160102,3,97</span><br><span class="line">160302,4,91</span><br><span class="line">160303,1,67</span><br><span class="line">160103,2,62</span><br><span class="line">160304,4,71</span><br></pre></td></tr></table></figure></p><p><strong>students.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">160201,Michael,17</span><br><span class="line">160101,Andy,23</span><br><span class="line">160301,justin,23</span><br><span class="line">160202,John,22</span><br><span class="line">160102,Herry,17</span><br><span class="line">160203,Brewster,18</span><br><span class="line">160302,Brice,20</span><br><span class="line">160303,Justin,25</span><br><span class="line">160103,Jerry,22</span><br><span class="line">160304,Tom,24</span><br></pre></td></tr></table></figure></p><p>1.利用反射机制创建students.txt对应的DataFrame，其中包含id、name、age三个字段。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd=sc.textFile(&quot;hdfs://master:9000/sqldata/students.txt&quot;).map(_.split(&quot;,&quot;)).map(p=&gt;Student(p(0),p(1),p(2).toInt))</span><br><span class="line">students = rdd.toDF</span><br><span class="line">students.show</span><br></pre></td></tr></table></figure></p><p>2.查看所有学生的姓名。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">students.select(&apos;name).show</span><br></pre></td></tr></table></figure></p><p>3.查询所有学生的年龄，并按照年龄降序排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">students.select(&apos;age).show</span><br><span class="line">students.sort(&quot;age&quot;.desc).show()</span><br></pre></td></tr></table></figure></p><p>4.查询年龄小于19或年龄大于21的所有学生。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">students.where($&quot;age&quot;&gt;21 || $&quot;age&quot;&lt;19).show</span><br></pre></td></tr></table></figure></p><p>5.添加scholarship字段，每个学生的scholarship项是其龄*20。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val newstudents = students.withColumn(&quot;scholarship&quot;,$&quot;age&quot;*20)</span><br><span class="line">newstudents.show(5)</span><br></pre></td></tr></table></figure></p><p>6.将添加scholarship字段后的DataFrame分别以Parquet和JSON格式保存至HDFS上。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">newstudents.write.format(&quot;json&quot;).save(&quot;hdfs://master:9000/sqldata/newstudents.json&quot;)</span><br><span class="line">newstudents.write.format(&quot;parquet&quot;).save(&quot;hdfs://master:9000/sqldata/newstudents.parquet&quot;)</span><br></pre></td></tr></table></figure></p><p>7.利用SQL语句实现案例1中2-4。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">students.registerTempTable(&quot;tb_students&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;SELECT name FROM tb_students WHERE age&gt;=19 AND age&lt;=21&quot;)</span><br></pre></td></tr></table></figure></p><p>8.利用自定义接口创建scores.txt对应的DataFrame，其中包含id、course_id、score三个字段。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val scores_rdd=sc.textFile(&quot;hdfs://master:9000/sqldata/scores.txt&quot;).map(_.split(&quot;,&quot;)).map(p =&gt; Score(p(0), p(1), p(2).trim.toInt))</span><br><span class="line"></span><br><span class="line">val scores = scores.toDF()</span><br><span class="line">val newstudents = students.join(scores, students(&quot;id&quot; )==scores(&quot;id&quot;), &quot;outer&quot;)</span><br><span class="line">newstudents.show</span><br></pre></td></tr></table></figure></p><p>9.按照课程编号分组，查看每门课的平均成绩，并按课程编号升序排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">newstudents.groupBy(&apos;course_id).mean(&quot;score&quot;).orderBy(&apos;course_id).show</span><br></pre></td></tr></table></figure></p><p>10.按照学生编号分组，查看个学生的姓名和其所有课程的平均成绩，并在统计结果中筛选出平均成绩大于72的同学。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val newstudents = students.join(scores, students(&quot;id&quot; )==scores(&quot;id&quot;), &quot;outer&quot;)</span><br><span class="line">newstudents.groupBy($&quot;name&quot;).mean(&quot;score&quot;).where($&quot;avg(score)&quot;&gt;72).orderBy(&quot;name&quot;).show</span><br></pre></td></tr></table></figure></p><h3 id="Spark-SQL基本案例2"><a href="#Spark-SQL基本案例2" class="headerlink" title="Spark SQL基本案例2"></a>Spark SQL基本案例2</h3><p><strong>students.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">学号/姓名/性别/年龄/学年/系别/</span><br><span class="line">160201,Michael,male,37,2012,2,2</span><br><span class="line">160101,Rose,female,33,2011,1,1</span><br><span class="line">160301,justin,male,23,2013,3,3</span><br><span class="line">160202,John,male,22,2012,2,2</span><br><span class="line">160102,Lucy,female,27,2011,2,1</span><br><span class="line">160203,Brewster,male,37,2012,1,2</span><br><span class="line">160302,Susan,female,30,2013,1,3</span><br><span class="line">160303,Justin,male,23,2013,3,3</span><br><span class="line">160103,John,male,22,2011,2,1</span><br><span class="line">160304,Lucy,female,27,2013,3,3</span><br></pre></td></tr></table></figure></p><p><strong>departments.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Computer,1</span><br><span class="line">Math,2</span><br><span class="line">Art,3</span><br></pre></td></tr></table></figure></p><p><strong>projects.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">学号/创新项目编号/学年/</span><br><span class="line">160201,XC2014001,2014,16,64,2</span><br><span class="line">160101,XC201213,2012,32,48,3</span><br><span class="line">160301,RW201103,2011,32,48,3</span><br><span class="line">160202,XC2014002,2014,16,64,1</span><br><span class="line">160102,XC2013002,2013,16,64,2</span><br><span class="line">160102,XC2012011,2012,16,48,2</span><br><span class="line">160302,RW201401,2014,32,32,2</span><br><span class="line">160304,RW201503,2015,16,32,1</span><br></pre></td></tr></table></figure></p><p><strong>scores.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">160201,90</span><br><span class="line">160101,83</span><br><span class="line">160301,80</span><br><span class="line">160202,70</span><br><span class="line">160102,67</span><br><span class="line">160102,89</span><br><span class="line">160302,91</span><br><span class="line">160303,58</span><br><span class="line">160103,64</span><br></pre></td></tr></table></figure></p><p><strong>scholarship.txt</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">160201,2013,2000</span><br><span class="line">160201,2014,3000</span><br><span class="line">160102,2013,2000</span><br><span class="line">160101,2013,2000</span><br><span class="line">160301,2014,2000</span><br><span class="line">160302,2014,2000</span><br><span class="line">160302,2015,3000</span><br><span class="line">160302,2016,4000</span><br></pre></td></tr></table></figure></p><p>1.查询创建的五个表的概要信息。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>2.查询各院系学生总数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>3.查询各院系学生奖学金的总和并排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>4.查询各院系学生的平均学分绩值并排序。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>5.统计各学院每年学生参与创新项目所获得的创新学分数总数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;#SparkSQL DataFrame基础篇&lt;/p&gt;
&lt;figure class=&quot;highlight scala&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/spa
      
    
    </summary>
    
      <category term="Spark" scheme="http://www.ihoge.cn/categories/Spark/"/>
    
    
      <category term="sparksql" scheme="http://www.ihoge.cn/tags/sparksql/"/>
    
  </entry>
  
  <entry>
    <title>使用aconda3-5.1.0（Python3.6.4） 搭建pyspark远程部署</title>
    <link href="http://www.ihoge.cn/2018/anacondaPyspark.html"/>
    <id>http://www.ihoge.cn/2018/anacondaPyspark.html</id>
    <published>2018-04-13T17:59:21.000Z</published>
    <updated>2018-04-22T14:55:33.612Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>首次安装的环境搭配是这样的：<br> jdk8<br> hadoop2.6.5<br> spark2.1<br> scala2.12.4<br> Anaconda3-5.1.0<br>一连串的报错让人惊喜无限，尽管反复调整配置始终无法解决。<br><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqavez8o4aj310q094jsn.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fqavfnihv5j316g0katdf.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fqavgahjl2j315y0e00vm.jpg" alt=""></p><p>坑了一整天后最后最终发现是<strong>版本不兼容！！</strong>再次提醒自己一定要重视各组件版本的问题。这里最主要的是spark和Anaconda版本的兼容问题，为了兼容python3尽量用新版的spark。最终解决方案的版本搭配如下：<br> jdk8<br> hadoop2.7.5<br> spark2.3.0<br> scala2.11.12<br> Anaconda3-5.1.0</p><h3 id="一、VM安装Ubuntu16-04虚拟机"><a href="#一、VM安装Ubuntu16-04虚拟机" class="headerlink" title="一、VM安装Ubuntu16.04虚拟机"></a>一、VM安装Ubuntu16.04虚拟机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vim</span><br><span class="line">sudo apt-get install openssh-server</span><br><span class="line"></span><br><span class="line"># 配置ssh免密登陆</span><br><span class="line">ssh localhost</span><br><span class="line">ssh-keygen -t rsa //一路回车</span><br><span class="line">cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line"></span><br><span class="line">sudo vi /etc/hosts //添加各个节点ip</span><br><span class="line">192.168.221.132 master</span><br><span class="line">192.168.221.133 slave1</span><br><span class="line">192.168.221.134 slave2</span><br><span class="line"></span><br><span class="line"># sudo vi /etc/hostname</span><br><span class="line">master</span><br></pre></td></tr></table></figure><h3 id="二、配置profile环境变量"><a href="#二、配置profile环境变量" class="headerlink" title="二、配置profile环境变量"></a>二、配置profile环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#Java</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_161</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jar</span><br><span class="line">#Hadoop</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">#Scala</span><br><span class="line">export SCALA_HOME=/home/hadoop/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br><span class="line">#Anaconda</span><br><span class="line">export PATH=/home/hadoop/anaconda3/bin:$PATH</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=/home/hadoop/anaconda3/bin/jupyter</span><br><span class="line">export PYSPARK_DRIVER_PYTHON_OPTS=&quot;notebook&quot;</span><br><span class="line">export PYSPARK_PYTHON=/home/hadoop/anaconda3/bin/python</span><br><span class="line">#Spark</span><br><span class="line">export SPARK_HOME=/home/hadoop/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><h3 id="三、hadoop-六个配置文件"><a href="#三、hadoop-六个配置文件" class="headerlink" title="三、hadoop 六个配置文件"></a>三、hadoop 六个配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/home/hadoop/hadoop/jdk1.8.0_161</span><br><span class="line"></span><br><span class="line"># core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:/home/hadoop/hadoop/tmp&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:50090&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/name&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;file:/home/hadoop/hadoop/tmp/dfs/data&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:10020&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master:19888&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># yarn-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line"># slaves</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="三、spark两个配置文件"><a href="#三、spark两个配置文件" class="headerlink" title="三、spark两个配置文件"></a>三、spark两个配置文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># spark-env.sh</span><br><span class="line">#java</span><br><span class="line">export JAVA_HOME=/home/hadoop/jdk1.8.0_161</span><br><span class="line">#scala</span><br><span class="line">export SCALA_HOME=/home/hadoop/scala</span><br><span class="line">#hadoop</span><br><span class="line">export HADOOP_HOME=/home/hadoop/hadoop</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=/home/hadoop/hadoop/etc/hadoop</span><br><span class="line">#spark</span><br><span class="line">export SPARK_HOME=/home/hadoop/spark</span><br><span class="line">export SPARK_LOCAL_DIRS=/home/hadoop/spark</span><br><span class="line">export SPARK_DIST_CLASSPATH=$(/home/hadoop/hadoop/bin/hadoop classpath)</span><br><span class="line">export SPARK_WORKER_CORES=1</span><br><span class="line">export SPARK_WORKER_INSTANCES=1</span><br><span class="line">export SPARK_WORKER_MEMORY=1g</span><br><span class="line">export SPARK_MASTER_IP=master</span><br><span class="line">export SPARK_LIBRARY_PATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$HADOOP_HOME/lib/native</span><br><span class="line"></span><br><span class="line"># slaves</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><h3 id="四、解压缩文件"><a href="#四、解压缩文件" class="headerlink" title="四、解压缩文件"></a>四、解压缩文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scp jdk-8u161-linux-x64.tar hadoop@master:~</span><br><span class="line">scp Anaconda3-5.1.0-Linux-x86_64.sh hadoop@master:~</span><br><span class="line">scp -r hadoop/ hadoop@master:~</span><br><span class="line">scp -r scala/ hadoop@master:~</span><br><span class="line">scp -r spark/ hadoop@master:~</span><br><span class="line"></span><br><span class="line">tar -xvf jdk-8u161-linux-x64.tar -C ./</span><br><span class="line"></span><br><span class="line">source ~/.profile</span><br><span class="line">分别查看jdk版本、hadoop版本、scala版本</span><br><span class="line"></span><br><span class="line"># 集群模式启动spark查看jps</span><br><span class="line">spark-shell --master spark://master:7077 --executor-memory 512m --total-executor-cores 2</span><br></pre></td></tr></table></figure><h3 id="五、安装Anaconda"><a href="#五、安装Anaconda" class="headerlink" title="五、安装Anaconda"></a>五、安装Anaconda</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-5.1.0-Linux-x86_64.sh -b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建配置jupyter_notebook_config.py</span><br><span class="line">jupyter notebook --generate-config</span><br><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br><span class="line"></span><br><span class="line">c = get_config()</span><br><span class="line">c.IPKernelApp.pylab = &apos;inline&apos;</span><br><span class="line">c.NotebookApp.ip = &apos;*&apos; </span><br><span class="line">c.NotebookApp.open.browser = False</span><br><span class="line">c.NotebookApp.password = u&apos;&apos;</span><br><span class="line">c.NotebookApp.port = 8888</span><br></pre></td></tr></table></figure><h3 id="六、关机后克隆出两个新节点并配置相关内容"><a href="#六、关机后克隆出两个新节点并配置相关内容" class="headerlink" title="六、关机后克隆出两个新节点并配置相关内容"></a>六、关机后克隆出两个新节点并配置相关内容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/hostname</span><br><span class="line"></span><br><span class="line">sudo vi /etc/hosts</span><br></pre></td></tr></table></figure><h3 id="七、远程测试pyspark集群"><a href="#七、远程测试pyspark集群" class="headerlink" title="七、远程测试pyspark集群"></a>七、远程测试pyspark集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 服务器端启动集群</span><br><span class="line">start-all.sh</span><br><span class="line">spark/sbin/start-all.sh</span><br><span class="line"></span><br><span class="line"># hadoop和spark的进程都显示正常后开始启动pyspark</span><br><span class="line">1、local模式运行</span><br><span class="line">pyspark</span><br><span class="line"></span><br><span class="line">2、Stand Alone运行模式</span><br><span class="line">MASTER=spark://master:7077 pyspark --num-executors 1 --total-executor-cores 3 --executor-memory 512m</span><br><span class="line"></span><br><span class="line">3、Hadoop YARN-client 模式</span><br><span class="line">HADOOP_CONF_DIR=/home/hadoop/hadoop/etc/hadoop pyspark --master yarn --deploy-mode client</span><br></pre></td></tr></table></figure><p>然后在远程Web端输入192.168.221.132:8888<br>页面打开后需要输入验证信息（第一次验证即可）：<br>输入上图<code>token</code>后面的字符串和用户密码<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqbhup77mqj31fu0j0wu2.jpg" alt=""><br>输入<code>sc</code>测试<br><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fqbhwgb3wdj310m0dowfj.jpg" alt=""></p><p>至此，aconda3-5.1.0（Python3.6.4） 搭建pyspark远程服务器部署成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;首次安装的环境搭配是这样的：&lt;br&gt; jdk8&lt;br&gt; hadoop2.6.5&lt;br&gt; spark2.1
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="environment" scheme="http://www.ihoge.cn/tags/environment/"/>
    
  </entry>
  
  <entry>
    <title>Hive集成Spark+Sql</title>
    <link href="http://www.ihoge.cn/2018/HiveSpark.html"/>
    <id>http://www.ihoge.cn/2018/HiveSpark.html</id>
    <published>2018-04-13T03:59:21.000Z</published>
    <updated>2018-04-22T14:55:36.971Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="一、Hive安装"><a href="#一、Hive安装" class="headerlink" title="一、Hive安装"></a>一、Hive安装</h2><h3 id="1-Hive简介"><a href="#1-Hive简介" class="headerlink" title="1.Hive简介"></a>1.Hive简介</h3><p>​    Hive是Facebook开发的构建于Hadoop集群之上的数据仓库应用，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行。</p><p>​    Hive是一个可以提供有效的、合理的且直观的组织和使用数据的模型，即使对于经验丰富的Java开发工程师来说，将这些常见的数据运算对应到底层的MapReduce Java API也是令人敬畏的。Hive可以帮用户做这些工作，用户就可以集中精力关注查询本身了。Hive可以将大多数的查询转换为MapReduce任务。Hive最适合于数据仓库应用程序，使用该应用程序进行相关的静态数据分析，不需要快速响应给出结果，而且数据本身也不会频繁变化。</p><p>​    Hive不是一个完整的数据库。Hadoop以及HDFS的设计本身约束和局限性限制了Hive所能胜任的工作。最大的限制就是Hive不支持记录级别的更新、插入或者删除。用户可以通过查询生成新表或将查询结果导入到文件中去。因为，Hadoop是一个面向批处理的系统，而MapReduce启动任务启动过程需要消耗很长时间，所以Hive延时也比较长。Hive还不支持事务。因此，Hive不支持联机事务处理（OLTP），更接近于一个联机分析技术（OLAP）工具，但是，目前还没有满足“联机”部分。</p><p>​    Hive提供了一系列的工具，可以用来进行数据提取转化加载(ETL)，其中，ETL是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。因此，Hive是最适合数据仓库应用程序的，它可以维护海量数据，而且可以对数据进行挖掘，然后形成意见和报告等。</p><p>​    因为大多数的数据仓库应用程序是基于SQL的关系数据库现实的，所以，Hive降低了将这些应用程序移植到Hadoop上的障碍。如果用户懂得SQL，那么学习使用Hive会很容易。因为Hive定义了简单的类SQL 查询语言——HiveQL，这里值得一提的是，与SQLServer、Oracle相比，HiveQL和MySQL提供的SQL语言更接近。同样的，相对于其他的Hadoop语言和工具来说，Hive也使得开发者将基于SQL的应用程序移植到Hadoop变得更加容易。</p><h3 id="2-Hive安装"><a href="#2-Hive安装" class="headerlink" title="2.Hive安装"></a>2.Hive安装</h3><p>​    接下来，开始Hive的安装，安装Hive之前，首先需要装好Hadoop和Spark。在<strong><a href="https://hive.apache.org/" target="_blank" rel="noopener">Hive官网</a></strong>可下载最新版本Hive，并且能够查阅版本改动说明，本次课程采用1.2.2版本进行安装。可以采用WinSCP传输apache-hive-1.2.2-bin.tar至虚拟机“下载”文件夹中，再进行后续安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载                                              # 进入下载文件夹</span><br><span class="line">sudo tar -zxf apache-hive-1.2.2-bin.tar.gz -C /usr/local    # 安装至/usr/local文件夹内</span><br><span class="line">cd /usr/local                                         # 进入/usr/local文件夹</span><br><span class="line">sudo mv ./apache-hive-1.2.2-bin/ ./hive               # 更名为hive</span><br><span class="line">sudo chown -R hadoop ./hive                           # 修改hive权限</span><br><span class="line">mkdir -p /usr/local/hive/warehouse                    # 创建元数据存储文件夹</span><br><span class="line">sudo chmod a+rwx /usr/local/hive/warehouse            # 修改文件权限</span><br></pre></td></tr></table></figure><p>然后添加Hive安装路径至系统环境变量</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure><p>添加下述路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Hive</span></span><br><span class="line">export HIVE_HOME=/usr/local/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure><p>并使之生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure><p>修改hive读取spark的jar包地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/bin</span><br><span class="line">vim hive</span><br></pre></td></tr></table></figure><p>修改为</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add Spark assembly jar to the classpath</span></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$SPARK_HOME</span>"</span> ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  sparkAssemblyPath=`ls <span class="variable">$&#123;SPARK_HOME&#125;</span>/jars/*.jar`</span><br><span class="line">  CLASSPATH=<span class="string">"<span class="variable">$&#123;CLASSPATH&#125;</span>:<span class="variable">$&#123;sparkAssemblyPath&#125;</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>然后采用hive默认配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hive/conf</span><br><span class="line">cp hive-default.xml.template hive-default.xml</span><br></pre></td></tr></table></figure><p>尝试启动Hive，此时启动是以本地模式进行启动，能正常启动则说明安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>​    若出现jline等jar包错误，则需要进入到hadoop安装目录下的share/hadoop/yarn/lib下删除jline-0.9.94.jar文件，再启动hive即可（因为高版本的Hadoop对Hive有捆绑）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/share/hadoop/yarn/lib</span><br><span class="line">rm -rf jline-0.9.94.jar</span><br></pre></td></tr></table></figure><p>###3. Hive的基本配置</p><p>​    在安装Hive时，默认情况下，元数据存储在Derby数据库中。Derby是一个完全用Java编写的数据库，所以可以跨平台，但需要在JVM中运行 。因为多用户和系统可能需要并发访问元数据存储，所以默认的内置数据库并不适用于生产环境。任何一个适用于JDBC进行连接的数据库都可用作元数据库存储，这里我们把MySQL作为存储元数据的数据库。接下来，我们分别对这两种方式进行介绍，即使用Derby数据库的方式和使用MySQL数据库的方式。</p><h4 id="3-1-使用Derby作为元数据库"><a href="#3-1-使用Derby作为元数据库" class="headerlink" title="3.1 使用Derby作为元数据库"></a>3.1 使用Derby作为元数据库</h4><p>​    本地模式中，用户的“表”等元数据信息，都默认存储在file://user/hive/warehouse，对于其他模式默认存储路径是hdfs://namenode_server/user/hive/warehouse。使用如下命令编辑hive-site.xml文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>在hive-site.xml文件添加以下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:derby:;databaseName=/usr/local/hive/metastore_db;create=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                           </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​    若要以伪分布式模式和分布式模式配置Hive，只需根据Hadoop配置文件core-site.xml中fs.defaultFS的值对hive.metastore.warehouse.dir 进行相应修改即可。配置完成之后即可启动Hive，然后尝试使用HiveQL命令创建表。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> derby;</span><br><span class="line"><span class="keyword">use</span> derby;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> x(a <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> x;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> x;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure><h4 id="3-2-使用MySQL作为元数据库"><a href="#3-2-使用MySQL作为元数据库" class="headerlink" title="3.2 使用MySQL作为元数据库"></a>3.2 使用MySQL作为元数据库</h4><h5 id="3-2-1-安装MySQL"><a href="#3-2-1-安装MySQL" class="headerlink" title="3.2.1 安装MySQL"></a>3.2.1 安装MySQL</h5><p>首先，查看并卸载系统自带的MySQL相关安装包（或之前安装过MySQL），命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install rpm</span><br><span class="line">rpm -qa | grep mysql</span><br></pre></td></tr></table></figure><p>若没有安装rpm工具，系统会有提示，按照提示安装即可。接下来查看是否有系统自带的MySQL相关安装包，若有，按下面命令删除：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm -e --nodeps mysql-libs-xxxxxx</span><br></pre></td></tr></table></figure><p>注：xxxxx是已经安装的mysql的版本号，然后进行MySQL的安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install mysql-server</span><br></pre></td></tr></table></figure><p>安装完成后，启动设置MySQL服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql start</span><br><span class="line">mysql -u root -p</span><br></pre></td></tr></table></figure><p>当然，还可使用下列命令进行额外设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chkconfig mysql on                              # 设置开机自动启动</span><br><span class="line">sudo /usr/bin/mysqladmin -u root password '123'      # 设置root用户密码</span><br></pre></td></tr></table></figure><p>接下来，创建hive用户及其数据库等，用于存放Hive的元数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/mysql/my.cnf</span><br><span class="line">注释掉：bind-address            = 127.0.0.1</span><br><span class="line"></span><br><span class="line">create database hive;</span><br><span class="line">grant all on *.* to hive@localhost identified by &apos;hive&apos;; </span><br><span class="line">flush privileges;</span><br><span class="line">exit;</span><br></pre></td></tr></table></figure><p>切换hive用户登陆</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -u hive -p hive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure><p>若能看到hive数据库存在，则说明创建成功。</p><h5 id="3-2-2-修改Hive配置"><a href="#3-2-2-修改Hive配置" class="headerlink" title="3.2.2 修改Hive配置"></a>3.2.2 修改Hive配置</h5><p>接下来，修改hive-site.xml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>输入下列信息</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>或者指定元数据文件夹</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>location of default database for the warehouse<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost:3306/hive;createDatebaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span>                           </span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword <span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后将JDBC文件放到hive的lib文件夹内，JDBC包的下载参考前述部分</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载</span><br><span class="line">cp mysql-connector-java-5.1.26-bin.jar /usr/local/hive/lib</span><br><span class="line">mkdir -p /usr/local/hive/tmp</span><br><span class="line">sudo chmod a+rwx /usr/local/hive/tmp</span><br></pre></td></tr></table></figure><p>也可从官网直接下载最新版jdbc</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz</span><br></pre></td></tr></table></figure><p>然后进行解压安装。当然，如果之前删除了jline-0.9.94.jar，此时需要把hive对应的jar包放进去</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/hive/lib/jline-2.12.jar  /usr/local/hadoop/share/hadoop/yarn/lib</span><br></pre></td></tr></table></figure><p>然后尝试启动hive</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">schematool -dbType mysql -initSchema</span><br><span class="line">start-all.sh</span><br><span class="line">hive</span><br></pre></td></tr></table></figure><p>成启动后，即可输入hive –help查看hive常用命令。</p><h2 id="二、Hive使用"><a href="#二、Hive使用" class="headerlink" title="二、Hive使用"></a>二、Hive使用</h2><h3 id="1-Hive基本数据类型"><a href="#1-Hive基本数据类型" class="headerlink" title="1.Hive基本数据类型"></a>1.Hive基本数据类型</h3><p>首先，我们简单叙述一下HiveQL的基本数据类型。</p><p>Hive支持基本数据类型和复杂类型, 基本数据类型主要有数值类型(INT、FLOAT、DOUBLE ) 、布尔型和字符串, 复杂类型有三种:ARRAY、MAP 和 STRUCT。</p><h4 id="1-1-基本数据类型"><a href="#1-1-基本数据类型" class="headerlink" title="1.1 基本数据类型"></a>1.1 基本数据类型</h4><ul><li>TINYINT: 1个字节</li><li>SMALLINT: 2个字节</li><li>INT: 4个字节</li><li>BIGINT: 8个字节</li><li>BOOLEAN: TRUE/FALSE </li><li>FLOAT: 4个字节，单精度浮点型</li><li>DOUBLE: 8个字节，双精度浮点型STRING 字符串</li></ul><h4 id="1-2-复杂数据类型"><a href="#1-2-复杂数据类型" class="headerlink" title="1. 2 复杂数据类型"></a>1. 2 复杂数据类型</h4><ul><li>ARRAY: 有序字段</li><li>MAP: 无序字段</li><li>STRUCT: 一组命名的字段</li></ul><h3 id="2-常用的HiveQL操作命令"><a href="#2-常用的HiveQL操作命令" class="headerlink" title="2.常用的HiveQL操作命令"></a>2.常用的HiveQL操作命令</h3><p>​    Hive常用的HiveQL操作命令主要包括：数据定义、数据操作。接下来详细介绍一下这些命令即用法（想要了解更多请参照《Hive编程指南》一书）。</p><h4 id="2-1-数据定义"><a href="#2-1-数据定义" class="headerlink" title="2.1 数据定义"></a>2.1 数据定义</h4><p>主要用于创建修改和删除数据库、表、视图、函数和索引。</p><ul><li><p>创建、修改和删除数据库</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">create database if not exists hive;       #创建数据库</span><br><span class="line">show databases;                           #查看Hive中包含数据库</span><br><span class="line">show databases like 'h.*';                #查看Hive中以h开头数据库</span><br><span class="line">describe databases;                       #查看hive数据库位置等信息</span><br><span class="line">alter database hive set dbproperties;     #为hive设置键值对属性</span><br><span class="line">use hive;                                 #切换到hive数据库下</span><br><span class="line">drop database if exists hive;             #删除不含表的数据库</span><br><span class="line">drop database if exists hive cascade;     #删除数据库和它中的表</span><br></pre></td></tr></table></figure><p>注意，除 dbproperties属性外，数据库的元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置，没有办法删除或重置数据库属性。</p></li><li><p>创建、修改和删除表</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#创建内部表（管理表）</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'username'</span>,</span><br><span class="line">      pwd <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'password'</span>,</span><br><span class="line">      address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      <span class="keyword">comment</span>  <span class="string">'home address'</span>,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt; <span class="keyword">comment</span> <span class="string">'number,sex'</span>) </span><br><span class="line">      <span class="keyword">comment</span> <span class="string">'description of the table'</span>  </span><br><span class="line">     tblproperties(<span class="string">'creator'</span>=<span class="string">'me'</span>,<span class="string">'time'</span>=<span class="string">'2016.1.1'</span>); </span><br><span class="line">#创建外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> usr2(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">      pwd <span class="keyword">string</span>,</span><br><span class="line">  address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt;) </span><br><span class="line">      <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span></span><br><span class="line">     location <span class="string">'/usr/local/hive/warehouse/hive.db/usr'</span>; </span><br><span class="line">#创建分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> usr3(</span><br><span class="line">      <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">      pwd <span class="keyword">string</span>,</span><br><span class="line">      address <span class="keyword">struct</span>&lt;street:<span class="keyword">string</span>,city:<span class="keyword">string</span>,state:<span class="keyword">string</span>,zip:<span class="built_in">int</span>&gt;,</span><br><span class="line">      identify <span class="keyword">map</span>&lt;<span class="built_in">int</span>,tinyint&gt;) </span><br><span class="line">      partitioned <span class="keyword">by</span>(city <span class="keyword">string</span>,state <span class="keyword">string</span>);    </span><br><span class="line">#复制usr表的表模式  </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr1 <span class="keyword">like</span> hive.usr;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">in</span> hive;  </span><br><span class="line">show tables 'u.*';        #查看hive中以u开头的表</span><br><span class="line">describe hive.usr;        #查看usr表相关信息</span><br><span class="line">alter table usr rename to custom;      #重命名表 </span><br><span class="line">#为表增加一个分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> </span><br><span class="line">     <span class="keyword">partition</span>(city=”beijing”,state=”China”) </span><br><span class="line">     location <span class="string">'/usr/local/hive/warehouse/usr2/China/beijing'</span>; </span><br><span class="line">#修改分区路径</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">partition</span>(city=”beijing”,state=”China”)</span><br><span class="line">     <span class="keyword">set</span> location <span class="string">'/usr/local/hive/warehouse/usr2/CH/beijing'</span>;</span><br><span class="line">#删除分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">drop</span> <span class="keyword">if</span> <span class="keyword">exists</span>  <span class="keyword">partition</span>(city=”beijing”,state=”China”)</span><br><span class="line">#修改列信息</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr <span class="keyword">change</span> <span class="keyword">column</span> pwd <span class="keyword">password</span> <span class="keyword">string</span> <span class="keyword">after</span> address;</span><br><span class="line">alter table usr add columns(hobby string);                  #增加列</span><br><span class="line">alter table usr replace columns(uname string);              #删除替换列</span><br><span class="line">alter table usr set tblproperties('creator'='liming');      #修改表属性</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr2 <span class="keyword">partition</span>(city=”beijing”,state=”China”)    #修改存储属性</span><br><span class="line"><span class="keyword">set</span> fileformat sequencefile;             </span><br><span class="line">use hive;                                                   #切换到hive数据库下</span><br><span class="line">drop table if exists usr1;                                  #删除表</span><br><span class="line">drop database if exists hive cascade;                       #删除数据库和它中的表</span><br></pre></td></tr></table></figure></li><li><p>视图和索引的创建、修改和删除</p><p>基本语法格式</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create view view_name as....;                #创建视图</span><br><span class="line">alter view view_name set tblproperties(…);   #修改视图</span><br></pre></td></tr></table></figure><p>因为视图是只读的，所以 对于视图只允许改变元数据中的 tblproperties属性。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#删除视图</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> <span class="keyword">if</span> <span class="keyword">exists</span> view_name;</span><br><span class="line">#创建索引</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">index</span> index_name <span class="keyword">on</span> <span class="keyword">table</span> table_name(partition_name/column_name)  </span><br><span class="line"><span class="keyword">as</span> <span class="string">'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'</span> <span class="keyword">with</span> <span class="keyword">deferred</span> rebuild....;</span><br></pre></td></tr></table></figure><p>这里’org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’是一个索引处理器，即一个实现了索引接口的Java类，另外Hive还有其他的索引实现。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter index index_name on table table_name partition(...)  rebulid;   #重建索引</span><br></pre></td></tr></table></figure><p>如果使用 deferred rebuild，那么新索引成空白状态，任何时候可以进行第一次索引创建或重建。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show formatted index on table_name;                       #显示索引</span><br><span class="line">drop index if exists index_name on table table_name;      #删除索引</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2-数据操作"><a href="#2-2-数据操作" class="headerlink" title="2.2 数据操作"></a>2.2 数据操作</h4><p>主要实现的是将数据装载到表中（或是从表中导出），并进行相应查询操作</p><ul><li><p>向表中装载数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.stu(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> hive.course(cid <span class="built_in">int</span>,<span class="keyword">sid</span> <span class="built_in">int</span>) </span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br></pre></td></tr></table></figure><p>向表中装载数据有两种方法：从文件中导入和通过查询语句插入。</p><ul><li><p>从文件中导入</p><p>假如这个表中的记录存储于文件stu.txt中，该文件的存储路径为usr/local/hadoop/examples/stu.txt，内容如下。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 Hello</span><br><span class="line">2 World</span><br><span class="line">3 CDA</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/usr/local/hadoop/examples/stu.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> stu;</span><br></pre></td></tr></table></figure></li><li><p>通过查询语句插入</p><p>使用如下命令，创建stu1表，它和stu表属性相同，我们要把从stu表中查询得到的数据插入到stu1中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu1 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>上面是创建表，并直接向新表插入数据；若表已经存在，向表中插入数据需执行以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> stu1 <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu <span class="keyword">where</span>（条件）;</span><br></pre></td></tr></table></figure><p>这里关键字overwrite的作用是替换掉表（或分区）中原有数据，换成into关键字，直接追加到原有内容后。</p></li></ul></li><li><p>写入临时文件</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/usr/local/hadoop/tmp/stu'</span>  <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure></li><li><p>查询操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,</span><br><span class="line">  <span class="keyword">case</span> </span><br><span class="line">  <span class="keyword">when</span> <span class="keyword">id</span>=<span class="number">1</span> <span class="keyword">then</span> <span class="string">'first'</span> </span><br><span class="line">  <span class="keyword">when</span> <span class="keyword">id</span>=<span class="number">2</span> <span class="keyword">then</span> <span class="string">'second'</span></span><br><span class="line">  <span class="keyword">else</span> <span class="string">'third'</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-3-连接"><a href="#2-3-连接" class="headerlink" title="2.3 连接"></a>2.3 连接</h4><p>​    连接（join）是将两个表中在共同数据项上相互匹配的那些行合并起来, HiveQL 的连接分为内连接、左向外连接、右向外连接、全外连接和半连接 5 种。</p><ul><li><p>内连接(等值连接)</p><p>内连接使用比较运算符根据每个表共有的列的值匹配两个表中的行。</p><p>首先，我们先把以下内容插入到course表中（自行完成）。</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 3</span><br><span class="line">2 1</span><br><span class="line">3 1</span><br></pre></td></tr></table></figure><p>​    下面, 查询stu和course表中学号相同的所有行，命令如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>左连接</p><p>​    左连接的结果集包括“LEFT OUTER”子句中指定的左表的所有行, 而不仅仅是连接列所匹配的行。如果左表的某行在右表中没有匹配行, 则在相关联的结果集中右表的所有选择列均为空值，命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>右连接</p><p>​    右连接是左向外连接的反向连接,将返回右表的所有行。如果右表的某行在左表中没有匹配行,则将为左表返回空值。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">right</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>全连接</p><p>​    全连接返回左表和右表中的所有行。当某行在另一表中没有匹配行时,则另一个表的选择列表包含空值。如果表之间有匹配行,则整个结果集包含基表的数据值。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.*, course.* <span class="keyword">from</span> stu <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><ul><li><p>半连接</p><p>​    半连接是 Hive 所特有的, Hive 不支持 in 操作,但是拥有替代的方案; left semi join, 称为半连接, 需要注意的是连接的表不能在查询的列中,只能出现在 on 子句中。命令如下：</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> stu.* <span class="keyword">from</span> stu <span class="keyword">left</span> semi <span class="keyword">join</span> course <span class="keyword">on</span>(stu .id=course .sid);</span><br></pre></td></tr></table></figure><h2 id="三、Spark与Hive集成"><a href="#三、Spark与Hive集成" class="headerlink" title="三、Spark与Hive集成"></a>三、Spark与Hive集成</h2><h3 id="1-安装Spark"><a href="#1-安装Spark" class="headerlink" title="1.安装Spark"></a>1.安装Spark</h3><p>​    为了让Spark能够访问Hive，必须为Spark添加Hive支持。Spark官方提供的预编译版本，通常是不包含Hive支持的，需要采用源码编译，编译得到一个包含Hive支持的Spark版本。首先测试一下电脑上已经安装的Spark版本是否支持Hive</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>这样就启动进入了spark-shell，然后输入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br></pre></td></tr></table></figure><p>如果报错，则说明spark无法识别org.apache.spark.sql.hive.HiveContext，这时我们就需要采用源码编译方法得到支持hive的spark版本。</p><ul><li><p>下载源码文件</p><p>​    进入官网后，可以按照下图配置选择“2.1.0(Dec 28, 2016)”和“SourceCode”，然后，在图中红色方框内，有个“Download Spark: spark-2.1.0.tgz”的下载链接，点击该链接就可以下载Spark源码文件了。</p></li></ul><ul><li><p>编译过程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/spark-2.0.2</span><br><span class="line">./dev/make-distribution.sh —tgz —name h27hive -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.1 -Phive -Phive-thriftserver -DskipTests</span><br></pre></td></tr></table></figure><p>或可选择直接安装已编译好的版本，把下好的<code>spark-2.0.2-bin-h27hive.tgz</code>放到下载文件夹内</p></li><li><p>Spark解压安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ~/下载                                              # 进入下载文件夹</span><br><span class="line">sudo tar -zxf spark-2.0.2-bin-h27hive.tgz -C /usr/local   # 安装至/usr/local文件夹内</span><br><span class="line">cd /usr/local                                         # 进入/usr/local文件夹</span><br><span class="line">sudo mv ./spark-1.4.0-bin-hadoop2.4/ ./spark          # 更名为spark</span><br><span class="line">sudo chown -R hadoop ./spark                          # 修改sqoop权限</span><br></pre></td></tr></table></figure></li><li><p>添加环境变量</p><p>注，如果电脑上已经装了另一个spark，此处可不增设环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.profile</span><br></pre></td></tr></table></figure><p>添加spark安装路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">spark</span></span><br><span class="line">export SPARK_HOME=/usr/local/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure><p>并保存修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.profile</span><br></pre></td></tr></table></figure></li><li><p>修改Spark配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf                              # 进入spark配置文件夹</span><br><span class="line">sudo cp spark-env.sh.template spark-env.sh            # 复制spark-env临时文件为配置文件</span><br><span class="line">vim spark-env.sh                                      # 编辑spark配置文件</span><br></pre></td></tr></table></figure><p>添加下述配置信息</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></table></figure><p>有了上面的配置信息以后，Spark就可以把数据存储到Hadoop分布式文件系统HDFS中，也可以从HDFS中读取数据。如果没有配置上面信息，Spark就只能读写本地数据，无法读写HDFS数据。在伪分布式模式下仅测试是否安装成功时，其他配置暂时可不做修改。</p></li><li><p>运行样例程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">bin/run-example SparkPi 2&gt;&amp;1 | grep "Pi is"</span><br></pre></td></tr></table></figure></li><li><p>放置Hive配置文件</p><p>为了让Spark能够访问Hive，需要把Hive的配置文件hive-site.xml拷贝到Spark的conf目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf</span><br><span class="line">cp /usr/local/hive/conf/hive-site.xml .</span><br><span class="line">ll</span><br></pre></td></tr></table></figure></li><li><p>测试是否集成成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>然后输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-在Hive中创建数据库和表"><a href="#2-在Hive中创建数据库和表" class="headerlink" title="2.在Hive中创建数据库和表"></a>2.在Hive中创建数据库和表</h3><p>首先启动MySQL数据库：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure><p>​    由于Hive是基于Hadoop的数据仓库，使用HiveQL语言撰写的查询语句，最终都会被Hive自动解析成MapReduce任务由Hadoop去具体执行，因此，需要启动Hadoop，然后再启动Hive。<br>然后执行以下命令启动Hadoop：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>Hadoop启动成功以后，可以再启动Hive：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>然后在hive命令提示符内进行操作</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> sparktest;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> sparktest.student(</span><br><span class="line">&gt; <span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">&gt; <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">&gt; gender <span class="keyword">string</span>,</span><br><span class="line">&gt; age <span class="built_in">int</span>);</span><br><span class="line"><span class="keyword">use</span> sparktest;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'Xueqian'</span>,<span class="string">'F'</span>,<span class="number">23</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">'Weiliang'</span>,<span class="string">'M'</span>,<span class="number">24</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>通过上面操作，我们就在Hive中创建了sparktest.student表，这个表有两条数据。</p><h3 id="3-连接Hive读写数据"><a href="#3-连接Hive读写数据" class="headerlink" title="3.连接Hive读写数据"></a>3.连接Hive读写数据</h3><p>​    现在我们看如何使用Spark读写Hive中的数据。注意，操作到这里之前，你一定已经按照前面的各个操作步骤，启动了Hadoop、Hive、MySQL和spark-shell（包含Hive支持）。在进行编程之前，我们需要做一些准备工作，我们需要修改“/usr/local/sparkwithhive/conf/spark-env.sh”这个配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf/</span><br><span class="line">vim spark-env.sh</span><br></pre></td></tr></table></figure><p>这样就使用vim编辑器打开了spark-env.sh这个文件，输入下面内容：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=<span class="variable">$CLASSPATH</span>:/usr/<span class="built_in">local</span>/hive/lib</span><br><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HIVE_CONF_DIR=/usr/<span class="built_in">local</span>/hive/conf</span><br><span class="line"><span class="built_in">export</span> SPARK_CLASSPATH=<span class="variable">$SPARK_CLASSPATH</span>:/usr/<span class="built_in">local</span>/hive/lib/mysql-connector-java-5.1.26-bin.jar</span><br></pre></td></tr></table></figure><p>保存并推出，然后启动spark-shell</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p>然后在shell界面中输入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">warehouseLocation</span> </span>= <span class="string">"spark-warehouse"</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark Hive Example"</span>).config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">import</span> spark.sql</span><br><span class="line">sql(<span class="string">"SELECT * FROM sparktest.student"</span>).show()</span><br></pre></td></tr></table></figure><p>然后再开一个命令行界面，启动hive界面，查看spark-shell中对hive表插入数据的结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure><p>然后输入</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> sparktest;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>然后在spark-shell中进行数据插入</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="comment">//下面我们设置两条数据表示两个学生信息</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="string">"3 Rongcheng M 26"</span>,<span class="string">"4 Guanhua M 27"</span>)).map(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment">//下面要设置模式信息</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(<span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"gender"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>, <span class="literal">true</span>)))</span><br><span class="line"><span class="comment">//下面创建Row对象，每个Row对象都是rowRDD中的一行</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p =&gt; <span class="type">Row</span>(p(<span class="number">0</span>).toInt, p(<span class="number">1</span>).trim, p(<span class="number">2</span>).trim, p(<span class="number">3</span>).toInt))</span><br><span class="line"><span class="comment">//建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来</span></span><br><span class="line"><span class="keyword">val</span> studentDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line"><span class="comment">//查看studentDF</span></span><br><span class="line">studentDF.show()</span><br><span class="line"><span class="comment">//下面注册临时表</span></span><br><span class="line">studentDF.registerTempTable(<span class="string">"tempTable"</span>)</span><br><span class="line">sql(<span class="string">"insert into sparktest.student select * from tempTable"</span>)</span><br></pre></td></tr></table></figure><p>然后切换到hive窗口，查看数据库内容变化</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><p>能够查询到新增数据结果，则说明操作成功。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;一、Hive安装&quot;&gt;&lt;a href=&quot;#一、Hive安装&quot; class=&quot;headerlink&quot; title=&quot;一、Hive安装&quot;&gt;&lt;/a&gt;一、Hive安装&lt;/h2&gt;&lt;h3 id=&quot;1-Hive简介&quot;&gt;&lt;a href=&quot;#1-H
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://www.ihoge.cn/categories/Hadoop/"/>
    
    
      <category term="environment" scheme="http://www.ihoge.cn/tags/environment/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘的9大成熟技术和商业应用</title>
    <link href="http://www.ihoge.cn/2018/DataMining.html"/>
    <id>http://www.ihoge.cn/2018/DataMining.html</id>
    <published>2018-04-10T17:05:59.000Z</published>
    <updated>2018-04-22T14:55:57.331Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>基于数据挖掘的9大主要成熟技术以及在数据化运营中的主要应用：<br>1、决策树<br>2、神经网络<br>3、回归<br>4、关联规则<br>5、聚类<br>6、贝叶斯分类<br>7、支持向量机<br>8、主成分分析<br>9、假设检验</p><h3 id="1-决策树"><a href="#1-决策树" class="headerlink" title="1　决策树"></a>1　决策树</h3><p>决策树（Decision Tree）是一种非常成熟的、普遍采用的数据挖掘技术。之所以称为树，是因为其建模过程类似一棵树的成长过程，即从根部开始，到树干，到分枝，再到细枝末节的分叉，最终生长出一片片的树叶。在决策树里，所分析的数据样本先是集成为一个树根，然后经过层层分枝，最终形成若干个结点，每个结点代表一个结论。</p><p>决策树算法之所以在数据分析挖掘应用中如此流行，主要原因在于决策树的构造不需要任何领域的知识，很适合探索式的知识发掘，并且可以处理高维度的数据。在众多的数据挖掘、统计分析算法中，决策树最大的优点在于它所产生的一系列从树根到树枝（或树叶）的规则，可以很容易地被分析师和业务人员理解，而且这些典型的规则甚至不用整理（或稍加整理），就是现成的可以应用的业务优化策略和业务优化路径。另外，决策树技术对数据的分布甚至缺失非常宽容，不容易受到极值的影响。</p><p>目前，最常用的3种决策树算法分别是<code>CHAID、CART</code>和<code>ID3（包括后来的C4.5，乃至C5.0）</code>。</p><p>CHAID(Chi-square Automatic Interaction Detector)算法的历史较长，中文简称为卡方自动相互关系检测。CHAID依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量，CHAID应用的前提是因变量为类别型变量（Category）。</p><p>CART(Classification and Regression Tree)算法产生于20世纪80年代中期，中文简称为分类与回归树，CART的分割逻辑与CHAID相同，每一层的划分都是基于对所有自变量的检验和选择上的。但是，CART采用的检验标准不是卡方检验，而是基尼系数（Gini）等不纯度的指标。两者最大的区别在于CHAID采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而CART则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART所生产的决策树是二分的，每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复使用多次（分割），这些都是不同于CHAID的特点。另外，如果是自变量存在数据缺失（Missing）的情况，CART的处理方式将会是寻找一个替代数据来代替（填充）缺失值，而CHAID则是把缺失数值作为单独的一类数值。</p><p><code>ID3</code>（Iterative Dichotomiser）算法与CART是同一时期产生的，中文简称为迭代的二分器，其最大的特点在于自变量的挑选标准是：基于信息增益的度量选择具有最高信息增益的属性作为结点的分裂（分割）属性，其结果就是对分割后的结点进行分类所需的信息量最小，这也是一种划分纯度的思想。至于之后发展起来的<code>C4.5</code>可以理解为ID3的发展版（后继版），两者的主要区别在于C4.5采用信息<code>增益率</code>（Gain Ratio）代替了ID3中的信息增益度量，如此替换的主要原因是信息增益度量有个缺点，就是倾向于选择具有大量值的属性。这里给个极端的例子，对于Member_Id的划分，每个Id都是一个最纯的组，但是这样的划分没有任何实际意义。而C4.5所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（SplitInformation）对其进行规范化约束。</p><p>决策树技术在数据化运营中的主要用途体现在：作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景，决策树甚至可以作为其他建模技术前期进行变量筛选的一种方法，即通过决策树的分割来筛选有效地输入自变量。</p><h3 id="2-神经网络"><a href="#2-神经网络" class="headerlink" title="2　神经网络"></a>2　神经网络</h3><p>神经网络（Neural Network）是通过数学算法来模仿人脑思维的，它是数据挖掘中机器学习的典型代表。神经网络是人脑的抽象计算模型，我们知道人脑中有数以百亿个神经元（人脑处理信息的微单元），这些神经元之间相互连接，使得人的大脑产生精密的逻辑思维。而数据挖掘中的“神经网络”也是由大量并行分布的人工神经元（微处理单元）组成的，它有通过调整连接强度从经验知识中进行学习的能力，并可以将这些知识进行应用。</p><p>简单来讲，“神经网络”就是通过输入多个非线性模型以及不同模型之间的加权互联（加权的过程在隐蔽层完成），最终得到一个输出模型。其中，隐蔽层所包含的就是非线性函数。</p><p>目前最主流的“神经网络”算法是<code>反馈传播（Backpropagation）</code>，该算法在多层前向型（Multilayer Feed-Forward）神经网络上进行学习，而多层前向型神经网络又是由一个输入层、一个或多个隐蔽层以及一个输出层组成的，“神经网络”的典型结构如图所示。<br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fq81j3ffllj30s20dawfw.jpg" alt=""></p><p>由于“神经网络”拥有特有的大规模并行结构和信息的并行处理等特点，因此它具有良好的<code>自适应性</code>、<code>自组织性</code>和<code>高容错性</code>，并且具有较强的学习、记忆和识别功能。目前神经网络已经在信号处理、模式识别、专家系统、预测系统等众多领域中得到广泛的应用。</p><p>“神经网络”的主要缺点就是其知识和<code>结果的不可解释性</code>，没有人知道隐蔽层里的非线性函数到底是如何处理自变量的，“神经网络”应用中的产出物在很多时候让人看不清其中的逻辑关系。但是，它的这个缺点并没有影响该技术在数据化运营中的广泛应用，甚至可以这样认为，正是因为其结果具有不可解释性，反而更有可能促使我们发现新的没有认识到的规律和关系。</p><p>在利用“神经网络”技术建模的过程中，有以下5个因素对模型结果有着重大影响：</p><p>❑层数。</p><p>❑每层中输入变量的数量。</p><p>❑联系的种类。</p><p>❑联系的程度。</p><p>❑转换函数，又称激活函数或挤压函数。</p><p>“神经网络”技术在数据化运营中的主要用途体现在：作为<code>分类</code>、<code>预测</code>问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。</p><h3 id="3-回归"><a href="#3-回归" class="headerlink" title="3　回归"></a>3　回归</h3><p>回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。其中，<strong>在数据化运营中更多使用的是逻辑斯蒂回归</strong>，它又包括响应预测、分类划分等内容。</p><p>多元线性回归主要描述一个因变量如何随着一批自变量的变化而变化，其回归公式（回归方程）就是因变量与自变量关系的数据反映。因变量的变化包括两部分：系统性变化与随机变化，其中，系统性变化是由自变量引起的（自变量可以解释的），随机变化是不能由自变量解释的，通常也称作残值。</p><p>在用来估算多元线性回归方程中自变量系数的方法中，最常用的是最小二乘法，即找出一组对应自变量的相应参数，以使因变量的实际观测值与回归方程的预测值之间的总方差减到最小。</p><p>对多元线性回归方程的参数估计，是基于下列假设的：</p><p>❑输入变量是确定的变量，不是随机变量，而且输入的变量间无线性相关，即无共线性。</p><p>❑随机误差的期望值总和为零，即随机误差与自变量不相关。</p><p>❑随机误差呈现正态分布 [1]。</p><p>如果不满足上述假设，就不能用最小二乘法进行回归系数的估算了。</p><p><strong>逻辑斯蒂回归（Logistic Regression）相比于线性回归来说，在数据化运营中有更主流更频繁的应用</strong>，主要是因为该分析技术可以很好地回答诸如预测、分类等数据化运营常见的分析项目主题。简单来讲，凡是预测“两选一”事件的可能性（比如，“响应”还是“不响应”；“买”还是“不买”；“流失”还是“不流失”），都可以采用逻辑斯蒂回归方程。</p><p>逻辑斯蒂回归预测的因变量是介于0和1之间的概率，如果对这个概率进行换算，就可以用线性公式描述因变量与自变量的关系了，具体公式如下：<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq81p6q5gnj30al021t8k.jpg" alt=""></p><p>与多元线性回归所采用的最小二乘法的参数估计方法相对应，最大似然法是逻辑斯蒂回归所采用的参数估计方法，其原理是找到这样一个参数，可以让样本数据所包含的观察值被观察到的可能性最大。这种寻找最大可能性的方法需要反复计算，对计算能力有很高的要求。最大似然法的优点是在大样本数据中参数的估值稳定、偏差小，估值方差小。</p><h3 id="4-关联规则"><a href="#4-关联规则" class="headerlink" title="4　关联规则"></a>4　关联规则</h3><p>关联规则（Association Rule）是在数据库和数据挖掘领域中被发明并被广泛研究的一种重要模型，关联规则数据挖掘的主要目的是<code>找出数据集中的频繁模式（Frequent Pattern）</code>，即多次重复出现的模式和并发关系（Cooccurrence Relationships），<code>即同时出现的关系，频繁和并发关系也称作关联（Association）</code>。</p><p>应用关联规则最经典的案例就是购物篮分析（Basket Analysis），通过分析顾客购物篮中商品之间的关联，可以挖掘顾客的购物习惯，从而帮助零售商更好地制定有针对性的营销策略。</p><p>以下列举一个简单的关联规则的例子：</p><p>婴儿尿不湿→啤酒[支持度=10%，置信度=70%]</p><p>这个规则表明，在所有顾客中，有10%的顾客同时购买了婴儿尿不湿和啤酒，而在所有购买了婴儿尿不湿的顾客中，占70%的人同时还购买了啤酒。发现这个关联规则后，超市零售商决定把婴儿尿不湿和啤酒摆放在一起进行促销，结果明显提升了销售额，这就是发生在沃尔玛超市中“啤酒和尿不湿”的经典营销案例。</p><p>上面的案例是否让你对支持度和置信度有了一定的了解？事实上，支持度（Support）和置信度（Confidence）是衡量关联规则强度的两个重要指标，它们分别反映着所发现规则的有用性和确定性。其中<code>支持度：规则X→Y的支持度是指事物全集中包含X∪Y的事物百分比</code>。支持度主要衡量规则的有用性，如果支持度太小，则说明相应规则只是偶发事件。在商业实战中，偶发事件很可能没有商业价值；<code>置信度：规则X→Y的置信度是指既包含了X又包含了Y的事物数量占所有包含了X的事物数量的百分比</code>。置信度主要衡量规则的确定性（可预测性），如果置信度太低，那么从X就很难可靠地推断出Y来，置信度太低的规则在实践应用中也没有太大用处。</p><p>在众多的关联规则数据挖掘算法中，最著名的就是<code>Apriori</code>算法，该算法具体分为以下两步进行：</p><p>（1）生成所有的频繁项目集。一个频繁项目集（Frequent Itemset）是一个支持度高于最小支持度阀值（min-sup）的项目集。</p><p>（2）从频繁项目集中生成所有的可信关联规则。这里可信关联规则是指置信度大于最小置信度阀值（min-conf）的规则。</p><p>关联规则算法不但在数值型数据集的分析中有很大用途，而且在纯文本文档和网页文件中，也有着重要用途。比如发现单词间的并发关系以及Web的使用模式等，这些都是Web数据挖掘、搜索及推荐的基础。</p><h3 id="5-聚类"><a href="#5-聚类" class="headerlink" title="5　聚类"></a>5　聚类</h3><p>聚类（Clustering）分析有一个通俗的解释和比喻，那就是“物以类聚，人以群分”。针对几个特定的业务指标，可以将观察对象的群体按照相似性和相异性进行不同群组的划分。经过划分后，每个群组内部各对象间的相似度会很高，而在不同群组之间的对象彼此间将具有很高的相异度。</p><p>聚类分析的算法可以分为<code>划分的方法</code>（Partitioning Method）、<code>层次的方法</code>（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。</p><p>对于划分的方法（Partitioning Method），<code>当给定m个对象的数据集，以及希望生成的细分群体数量K后，即可采用这种方法将这些对象分成K组</code>（K≤m），使得每个组内对象是相似的，而组间的对象是相异的。最常用的划分方法是<code>K-Means</code>方法，其具体原理是：首先，随机选择K个对象，并且所选择的每个对象都代表一个组的初始均值或初始的组中心值；对剩余的每个对象，根据其与各个组初始均值的距离，将它们分配给最近的（最相似）小组；然后，重新计算每个小组新的均值；这个过程不断重复，直到所有的对象在K组分布中都找到离自己最近的组。</p><p><code>层次的方法（Hierarchical Method）则是指依次让最相似的数据对象两两合并，这样不断地合并，最后就形成了一棵聚类树</code>。</p><p>聚类技术在数据分析和数据化运营中的主要用途表现在：既可以直接作为模型对观察对象进行群体划分，为业务方的精细化运营提供具体的细分依据和相应的运营方案建议，又可在数据处理阶段用作数据探索的工具，包括发现离群点、孤立点，数据降维的手段和方法，通过聚类发现数据间的深层次的关系等。</p><h3 id="6-贝叶斯分类方法"><a href="#6-贝叶斯分类方法" class="headerlink" title="6　贝叶斯分类方法"></a>6　贝叶斯分类方法</h3><p>贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。比如通过一个给定观察值的相关属性来判断其属于一个特定类别的概率。贝叶斯分类方法是基于贝叶斯定理的，已经有研究表明，朴素贝叶斯分类方法作为一种简单贝叶斯分类算法甚至可以跟决策树和神经网络算法相媲美。</p><p>贝叶斯定理的公式如下：<br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq820l60g6j307201smx0.jpg" alt=""></p><p>其中，X表示n个属性的测量描述；H为某种假设，比如假设某观察值X属于某个特定的类别C；对于分类问题，希望确定P(H|X)，即能通过给定的X的测量描述，来得到H成立的概率，也就是给出X的属性值，计算出该观察值属于类别C的概率。因为P(H|X)是后验概率（Posterior Probability），所以又称其为在条件X下，H的后验概率。</p><p>举例来说，假设数据属性仅限于用教育背景和收入来描述顾客，而X是一位硕士学历，收入10万元的顾客。假定H表示假设我们的顾客将购买苹果手机，则P(H|X)表示当我们知道顾客的教育背景和收入情况后，该顾客将购买苹果手机的概率；相反，P(X|H)则表示如果已知顾客购买苹果手机，则该顾客是硕士学历并且收入10万元的概率；而P(X)则是X的先验概率，表示顾客中的某个人属于硕士学历且收入10万元的概率；P(H)也是先验概率，只不过是任意给定顾客将购买苹果手机的概率，而不会去管他们的教育背景和收入情况。</p><p>从上面的介绍可见，相比于先验概率P(H)，后验概率P(H|X)基于了更多的信息（比如顾客的信息属性），而P(H)是独立于X的。</p><p>贝叶斯定理是朴素贝叶斯分类法（Naive Bayesian Classifier）的基础，如果给定数据集里有M个分类类别，通过朴素贝叶斯分类法，可以预测给定观察值是否属于具有最高后验概率的特定类别，也就是说，朴素贝叶斯分类方法预测X属于类别$C_i$时，表示当且仅当<br>$$P(C_i|X)＞P(C_j|X)1≤j≤m，j≠i$$<br>此时如果最大化$P(C_i|X)$，其$P(C_i|X)$最大的类$C_i$被称为最大后验假设，根据贝叶斯定理可知，由于$P(X)$对于所有的类别是均等的，因此只需要$P(X|C_i)P(C_i)$取最大即可。</p><p>为了预测一个未知样本$X$的类别，可对每个类别$C_i$估算相应的$P(X|C_i)P(C_i)$。样本$X$归属于类别$C_i$，当且仅当<br>$$P(C_i|X)＞P(C_j|X)1≤j≤m，j≠i$$</p><p>贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。</p><h3 id="7-支持向量机"><a href="#7-支持向量机" class="headerlink" title="7　支持向量机"></a>7　支持向量机</h3><p>支持向量机（Support Vector Machine）是Vapnik等人于1995年率先提出的，是近年来机器学习研究的一个重大成果。与传统的神经网络技术相比，支持向量机不仅结构简单，而且各项技术的性能也明显提升，因此它成为当今机器学习领域的热点之一。</p><p>作为一种新的分类方法，支持向量机以结构风险最小为原则。在线性的情况下，就在原空间寻找两类样本的最优分类超平面。在非线性的情况下，它<code>使用一种非线性的映射，将原训练集数据映射到较高的维上</code>。在新的维上，它搜索线性最佳分离超平面。使用一个适当的对足够高维的非线性映射，两类数据总可以被超平面分开。</p><p>支持向量机的基本概念如下：</p><p>设给定的训练样本集为${(x_1,y_1),(x_2,y_2),…,(x_n,y_n)}$，其中$x_i∈R_n,y∈[-1,1]$。</p><p>再假设该训练集可被一个超平面线性划分，设该超平面记为$(w,x)+b=0$。</p><p>支持向量机的基本思想可用下图的两维情况举例说明。（线性可分情况下的最优分类线）<br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1fq82c0jkiaj30m50g3dgg.jpg" alt=""></p><p>图中圆形和方形代表两类样本，H为分类线，H1、H2，分别为过各类样本中离分类线最近的样本并且平行于分类线的直线，它们之间的距离叫做<code>分类间隔（Margin）</code>。所谓的最优分类线就是要求分类线<code>不但能将两类正确分开（训练错误为0），而且能使分类间隔最大</code>。推广到高维空间，最优分类线就成了最优分类面。</p><p>其中，距离超平面最近的一类向量被称为<code>支持向量（Support Vector）</code>，<code>一组支持向量可以唯一地确定一个超平面</code>。通过学习算法，SVM可以自动寻找出那些对分类有较好区分能力的支持向量，由此构造出的分类器则可以最大化类与类的间隔，因而有较好的适应能力和较高的分类准确率。</p><p>支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的——对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合 。</p><p>支持向量机主要用在预测、分类这样的实际分析需求场景中。</p><h3 id="8-主成分分析"><a href="#8-主成分分析" class="headerlink" title="8　主成分分析"></a>8　主成分分析</h3><p>严格意义上讲，主成分分析（Principal Components Analysis）属于传统的统计分析技术范畴，但是正如本章前面所阐述的，统计分析与数据挖掘并没有严格的分割，因此在数据挖掘实战应用中也常常会用到这种方式，从这个角度讲，主成分分析也是数据挖掘商业实战中常用的一种分析技术和数据处理技术。</p><p>主成分分析会通过线性组合将多个原始变量合并成若干个主成分，这样<code>每个主成分都变成了原始变量的线性组合</code>。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。</p><p>主成分分析的主要步骤如下：</p><p>1）通常要先进行各变量的标准化工作，标准化的目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，从而让不同的变量经过标准化处理后可以有平等的分析和比较基础。</p><p>2）选择<code>协方差阵</code>或者<code>相关阵计算特征根</code>及对应的<code>特征向量</code>。</p><p>3）<code>计算方差贡献率</code>，并根据方差贡献率的阀值选取合适的主成分个数。</p><p>4）根据主成分载荷的大小对选择的主成分进行命名。</p><p>5）根据主成分载荷计算各个主成分的得分。</p><p>将主成分进行推广和延伸即成为<code>因子分析（Factor Analysis）</code>，因子分析在<code>综合原始变量信息的基础上将会力图构筑若干个意义较为明确的公因子</code>；也就是说，采用少数几个因子描述多个指标之间的联系，将比较密切的变量归为同一类中，每类变量即是一个因子。之所以称其为因子，是因为它们实际上是<code>不可测量的，只能解释</code>。</p><p><code>主成分分析是因子分析的一个特例</code>，两者的区别和联系主要表现在以下方面：</p><p>❑主成分分析会把主成分表示成各个原始变量的线性组合，而因子分析则把原始变量表示成各个因子的线性组合。这个区别最直观也最容易记住。</p><p>❑主成分分析的重点在于解释原始变量的总方差，而因子分析的重点在于解释原始变量的协方差。</p><p>❑在主成分分析中，有几个原始变量就有几个主成分，而在因子分析中，因子个数可以根据业务场景的需要人为指定，并且指定的因子数量不同，则分析结果也会有差异。</p><p>❑在主成分分析中，给定的协方差矩阵或者相关矩阵的特征值是唯一时，主成分也是唯一的，但是在因子分析中，因子不是唯一的，并且通过旋转可以得到不同的因子。</p><p>主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。</p><h3 id="9-假设检验"><a href="#9-假设检验" class="headerlink" title="9　假设检验"></a>9　假设检验</h3><p>假设检验（Hypothesis Test）是现代统计学的基础和核心之一，其主要研究在一定的条件下，总体是否具备某些特定特征。</p><p>假设检验的基本原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否发生。如果在一次试验中，小概率事件发生了，那么说明假设在一定的显著性水平下不可靠或者不成立；如果在一次试验中，小概率事件没有发生，那么也只能说明没有足够理由相信假设是错误的，但是也并不能说明假设是正确的，因为无法收集到所有的证据来证明假设是正确的。</p><p>假设检验的结论是在一定的显著性水平下得出的。因此，当采用此方法观测事件并下结论时，有可能会犯错，这些错误主要有两大类：</p><p>❑第Ⅰ类错误：当原假设为真时，却否定它而犯的错误，即拒绝正确假设的错误，也叫弃真错误。犯第Ⅰ类错误的概率记为α，通常也叫α错误，α=1-置信度。</p><p>❑第Ⅱ类错误：当原假设为假时，却肯定它而犯的错误，即接受错误假设的错误，也叫纳伪错误。犯第Ⅱ类错误的概率记为β，通常也叫β错误。</p><p>上述这两类错误在其他条件不变的情况下是相反的，即α增大时，β就减小；α减小时，β就增大。α错误容易受数据分析人员的控制，因此在假设检验中，通常会先控制第Ⅰ类错误发生的概率α，具体表现为：在做假设检验之前先指定一个α的具体数值，通常取0.05，也可以取0.1或0.001。</p><p>在数据化运营的商业实践中，假设检验最常用的场景就是用于“运营效果的评估”上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;基于数据挖掘的9大主要成熟技术以及在数据化运营中的主要应用：&lt;br&gt;1、决策树&lt;br&gt;2、神经网络&lt;br&gt;3、回归&lt;br&gt;4、关联规则&lt;br&gt;5、聚类&lt;br&gt;6、贝叶斯分类&lt;br&gt;7、支持向量机&lt;br&gt;8、主成分分析&lt;br&gt;9、假设检验&lt;
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>PCA主成分分析+SVM人脸识别准确率97%+</title>
    <link href="http://www.ihoge.cn/2018/PCA+SVM%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB.html"/>
    <id>http://www.ihoge.cn/2018/PCA+SVM人脸识别.html</id>
    <published>2018-04-09T17:05:59.000Z</published>
    <updated>2018-04-10T10:01:34.819Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>这里使用的测试数据共包含40位人员照片，每个人10张照片。也可登陆<a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facesataglance.html" target="_blank" rel="noopener">http://www.cl.cam.ac.uk/research/dtg/attarchive/facesataglance.html</a> 查看400张照片的缩略图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_olivetti_faces</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level = logging.INFO, format=<span class="string">"%(asctime)s %(message)s"</span>) <span class="comment"># 这里INFO必须大写</span></span><br><span class="line"></span><br><span class="line">data_home = <span class="string">'code/datasets/'</span></span><br><span class="line">logging.info(<span class="string">"开始加载数据"</span>)</span><br><span class="line">faces = fetch_olivetti_faces(data_home=data_home)</span><br><span class="line">logging.info(<span class="string">"加载完成"</span>)</span><br></pre></td></tr></table></figure><p>这里做下简单的解释：</p><p>加载的图片保存在faces变量里，sklaern已经把每张照片处理成剪切掉头发部分并且64x64大小且人脸居中显示。在真实生产环境中这一步很重要，否则模型将被大量的噪声干扰（即照片背景，变化的发型等，这些特征都应该排除在输入特征之外）。最后要成功下载数据集还需要安装Python图片图里工具Pillow否则无法对图片解码。下面输出下数据的概要信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = faces.data</span><br><span class="line">y = faces.target</span><br><span class="line"></span><br><span class="line">targets = np.unique(faces.target)</span><br><span class="line">target_names = np.array([<span class="string">"p%d"</span> % t <span class="keyword">for</span> t <span class="keyword">in</span> targets]) <span class="comment">#给每个人做标签</span></span><br><span class="line">n_targets = target_name.shape[<span class="number">0</span>]</span><br><span class="line">n_samples, h, w = faces.images.shape</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Samples count:&#123;&#125;\nTarget count:&#123;&#125;'</span>.format(n_samples, n_targets))</span><br><span class="line">print(<span class="string">'Image size:&#123;&#125;x&#123;&#125;\nData shape:&#123;&#125;'</span>.format(w, h, X.shape))</span><br></pre></td></tr></table></figure><pre><code>Samples count:400Target count:40Image size:64x64Data shape:(400, 4096)</code></pre><p>由输出可知，共有40人，照片总量400，输入特征(64x64=4096)个。</p><p>为了直观观察数据，从每个人物的照片里随机选择一张显示，定义下画图工具：</p><p>其中输入参数images是一个二维数据，每一行都是一个图片数据。在加载数据时，fech_ollivetti_faces()函数已经自动做了预处理，图片的每个像素的RBG值都转换成了[0,1]浮点数。因此，画出来的照片也是黑白的。子图片识别领域一般用黑白照片就可以了，减少计算量的同时也更加准确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gallery</span><span class="params">(images, titles, h, w, n_row=<span class="number">2</span>, n_col=<span class="number">5</span>)</span>:</span></span><br><span class="line"><span class="comment">#     显示图片阵列：</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">2</span>*n_col, <span class="number">2.2</span>*n_row),dpi=<span class="number">140</span>)</span><br><span class="line">    plt.subplots_adjust(bottom=<span class="number">0</span>, left=<span class="number">.01</span>, right=<span class="number">.99</span>, top=<span class="number">.90</span>, hspace=<span class="number">.01</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_row * n_col):</span><br><span class="line">        plt.subplot(n_row, n_col, i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(images[i].reshape((h,w)), cmap=plt.cm.gray)</span><br><span class="line">        plt.title(titles[i])</span><br><span class="line">        plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">n_row = <span class="number">2</span></span><br><span class="line">n_col = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">sample_images = <span class="keyword">None</span></span><br><span class="line">sample_titles = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_targets):</span><br><span class="line">    people_images = X[y==i]  <span class="comment"># 注意这里传入i</span></span><br><span class="line">    people_sample_index = np.random.randint(<span class="number">0</span>, people_images.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    people_sample_image = people_images[people_sample_index, :]</span><br><span class="line">    <span class="keyword">if</span> sample_images <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        sample_images = np.concatenate((sample_images, people_sample_image), axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        sample_images =people_sample_image</span><br><span class="line">    sample_titles.append(target_names[i])   <span class="comment"># 这里target_names是在前面生成的标签</span></span><br><span class="line">    </span><br><span class="line">plot_gallery(sample_images, sample_titles, h, w, n_row, n_col)</span><br><span class="line"></span><br><span class="line"><span class="comment">#代码中X[y=i]可以选择除特定的所有照片，随机选出来的照片放在sample.images数组对象里，最后调用之前定义的函数把照片画出来。</span></span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1fq6vy8axmgj31bo0gs0xb.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h2 id="支持向量机第一次尝试"><a href="#支持向量机第一次尝试" class="headerlink" title="支持向量机第一次尝试"></a>支持向量机第一次尝试</h2><p>直接食用支持向量机来实现人脸识别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">clf = SVC(class_weight=<span class="string">'balanced'</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">print(<span class="string">"耗时：&#123;&#125;秒"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>耗时：1.0220119953155518秒</code></pre><h3 id="1、接着对人脸数据进行预测：使用confusion-matrix查看准确性："><a href="#1、接着对人脸数据进行预测：使用confusion-matrix查看准确性：" class="headerlink" title="1、接着对人脸数据进行预测：使用confusion_matrix查看准确性："></a><strong>1、接着对人脸数据进行预测：使用<code>confusion_matrix</code>查看准确性：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">cm = confusion_matrix(y_test, y_pred, labels=range(n_targets))</span><br><span class="line">print(<span class="string">"confusion_matrix:\n"</span>)</span><br><span class="line"><span class="comment"># np.set_printoptions(threshold=np.nan)</span></span><br><span class="line">print(cm[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>confusion_matrix:[[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</code></pre><p>上面<code>np.set_printoptions()</code>是为了确保cm完整输出，这是因为这个数组是40x40的，默认情况下不会全部输出。??</p><h3 id="2、使用classification-report查看准确性"><a href="#2、使用classification-report查看准确性" class="headerlink" title="2、使用classification_report查看准确性"></a><strong>2、使用<code>classification_report</code>查看准确性</strong></h3><p>但是很明显输出结果效果很差。 因为<code>confusion_matrix</code>理想的输出是矩阵的对角线上有数组，其他地方都为0，而且这里很多图片都被预测成索引为12的类别了。我买再来看下<code>classification_report</code>的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">print(classification_report(y_test, y_pred, target_names = target_names)) <span class="comment">#这里y_test和y_pred不要颠倒。</span></span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support         p0       0.00      0.00      0.00         1         p1       0.00      0.00      0.00         3         p2       0.00      0.00      0.00         2         p3       0.00      0.00      0.00         1         p4       0.00      0.00      0.00         1         p5       0.00      0.00      0.00         1         p6       0.00      0.00      0.00         4         p7       0.00      0.00      0.00         2         p8       0.00      0.00      0.00         4         p9       0.00      0.00      0.00         2        p10       0.00      0.00      0.00         1        p11       0.00      0.00      0.00         0        p12       0.00      0.00      0.00         4        p13       0.00      0.00      0.00         4        p14       0.00      0.00      0.00         1        p15       0.00      0.00      0.00         1        p16       0.00      0.00      0.00         3        p17       0.00      0.00      0.00         2        p18       0.00      0.00      0.00         2        p19       0.00      0.00      0.00         2        p20       0.00      0.00      0.00         1        p21       0.00      0.00      0.00         2        p22       0.00      0.00      0.00         3        p23       0.00      0.00      0.00         2        p24       0.00      0.00      0.00         3        p25       0.00      0.00      0.00         3        p26       0.00      0.00      0.00         2        p27       0.00      0.00      0.00         2        p28       0.00      0.00      0.00         0        p29       0.00      0.00      0.00         2        p30       0.00      0.00      0.00         2        p31       0.00      0.00      0.00         3        p32       0.00      0.00      0.00         2        p33       0.00      0.00      0.00         2        p34       0.00      0.00      0.00         0        p35       0.00      0.00      0.00         2        p36       0.00      0.00      0.00         3        p37       0.00      0.00      0.00         1        p38       0.00      0.00      0.00         2        p39       0.00      0.00      0.00         2avg / total       0.00      0.00      0.00        80/Users/hadoop/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.  &apos;precision&apos;, &apos;predicted&apos;, average, warn_for)/Users/hadoop/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.  &apos;recall&apos;, &apos;true&apos;, average, warn_for)</code></pre><p>结果不出预料，果然很差。</p><p>这是因为：<strong>这里把每个像素都作为一个输入特征来处理，这样那个数据噪声太严重了，模型根本没有办法对训练数据集进行拟合。这里有4096个特征向量，可是数据集大小才400个，比特征个数少了太多，而且分出20%作为测试集。这种情况下根本无法进行准确的训练和预测。</strong></p><h2 id="使用PCA来处理数据集"><a href="#使用PCA来处理数据集" class="headerlink" title="使用PCA来处理数据集"></a>使用PCA来处理数据集</h2><h3 id="选择-k-值"><a href="#选择-k-值" class="headerlink" title="选择$k$值"></a>选择$k$值</h3><p>解决上述问题的办法有两种，一个是加大数据样本量（在这里这个不太现实），或者使用PCA给数据降维，值选择前k个最重要的特征。</p><p>这里我们根据PCA算法来计算失真程度来确定k值。</p><p>在sklearn里，可以从PCA模型的<code>explained_variance_ratio_</code>变量里获取经PCA处理后的数据还原率。这是一个数组，所有元素求和即可知道选择的$k$值的数据还原率。随着$k$的增大，数值会无限接近于1。</p><p>利用这一特征，可以让$k$取值10～300之间，每个30取一次样。针对这里的情况选择失真度小于5%即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Exploring explained variance ratio for dataset ..."</span>)</span><br><span class="line">candidate_components = range(<span class="number">10</span>, <span class="number">300</span>, <span class="number">30</span>)</span><br><span class="line">explained_ratios = []</span><br><span class="line">t = time()</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> candidate_components:</span><br><span class="line">    pca = PCA(n_components=c)</span><br><span class="line">    X_pca = pca.fit_transform(X)</span><br><span class="line">    explained_ratios.append(np.sum(pca.explained_variance_ratio_))</span><br><span class="line">print(<span class="string">'Done in &#123;0:.2f&#125;s'</span>.format(time()-t))</span><br></pre></td></tr></table></figure><pre><code>Exploring explained variance ratio for dataset ...Done in 2.17s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>), dpi=<span class="number">100</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.plot(candidate_components, explained_ratios)</span><br><span class="line">plt.xlabel(<span class="string">'Number of PCA Components'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Explained Variance Ratio'</span>)</span><br><span class="line">plt.title(<span class="string">'Explained variance ratio for PCA'</span>)</span><br><span class="line">plt.yticks(np.arange(<span class="number">0.5</span>, <span class="number">1.05</span>, <span class="number">.05</span>))</span><br><span class="line">plt.xticks(np.arange(<span class="number">0</span>, <span class="number">300</span>, <span class="number">20</span>));</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1fq6vyorh25j30jj0ctdh6.jpg" alt=""></p><p>由上图可知，若要保留95%的数据还原率，$k$值选择120即可。为了更直观的看不同$k$值的区别，这里画出来体验下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">title_prefix</span><span class="params">(prefix, title)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"&#123;&#125;: &#123;&#125;"</span>.format(prefix, title)</span><br><span class="line"></span><br><span class="line">n_row = <span class="number">1</span></span><br><span class="line">n_col = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">sample_images = sample_images[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">sample_titles = sample_titles[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">plotting_images = sample_images</span><br><span class="line">plotting_titles = [title_prefix(<span class="string">'orig'</span>, t) <span class="keyword">for</span> t <span class="keyword">in</span> sample_titles]</span><br><span class="line">candidate_components = [<span class="number">120</span>, <span class="number">75</span>, <span class="number">37</span>, <span class="number">19</span>, <span class="number">8</span>]</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> candidate_components:</span><br><span class="line">    print(<span class="string">"Fitting and projecting on PCA(n_components=&#123;&#125;) ..."</span>.format(c))</span><br><span class="line">    t = time()</span><br><span class="line">    pca = PCA(n_components=c)</span><br><span class="line">    pca.fit(X)</span><br><span class="line">    X_sample_pca = pca.transform(sample_images)</span><br><span class="line">    X_sample_inv = pca.inverse_transform(X_sample_pca)</span><br><span class="line">    plotting_images = np.concatenate((plotting_images, X_sample_inv), axis=<span class="number">0</span>)</span><br><span class="line">    sample_title_pca = [title_prefix(<span class="string">'&#123;&#125;'</span>.format(c), t) <span class="keyword">for</span> t <span class="keyword">in</span> sample_titles]</span><br><span class="line">    plotting_titles = np.concatenate((plotting_titles, sample_title_pca), axis=<span class="number">0</span>)</span><br><span class="line">    print(<span class="string">"Done in &#123;0:.2f&#125;s"</span>.format(time() - t))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Plotting sample image with different number of PCA conpoments ..."</span>)</span><br><span class="line">plot_gallery(plotting_images, plotting_titles, h, w,</span><br><span class="line">    n_row * (len(candidate_components) + <span class="number">1</span>), n_col)</span><br></pre></td></tr></table></figure><pre><code>Fitting and projecting on PCA(n_components=120) ...Done in 0.18sFitting and projecting on PCA(n_components=75) ...Done in 0.14sFitting and projecting on PCA(n_components=37) ...Done in 0.11sFitting and projecting on PCA(n_components=19) ...Done in 0.07sFitting and projecting on PCA(n_components=8) ...Done in 0.06sPlotting sample image with different number of PCA conpoments ...</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1fq6vzviawhj31411bntk5.jpg" alt=""></p><h3 id="利用GridSearchCV选出最优参数"><a href="#利用GridSearchCV选出最优参数" class="headerlink" title="利用GridSearchCV选出最优参数"></a>利用GridSearchCV选出最优参数</h3><p>接下来选择$k=120$作为<code>PCA</code>的参数对数据集和测试集进行特征提取，然后调用<code>GridSearchCV</code>选出最优参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_components = <span class="number">120</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Fitting PCA by using training data ..."</span>)</span><br><span class="line">t = time()</span><br><span class="line">pca = PCA(n_components=n_components, svd_solver=<span class="string">'randomized'</span>, whiten=<span class="keyword">True</span>).fit(X_train)</span><br><span class="line">print(<span class="string">"Done in &#123;0:.2f&#125;s"</span>.format(time() - t))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Projecting input data for PCA ..."</span>)</span><br><span class="line">t = time()</span><br><span class="line">X_train_pca = pca.transform(X_train)</span><br><span class="line">X_test_pca = pca.transform(X_test)</span><br><span class="line">print(<span class="string">"Done in &#123;0:.2f&#125;s"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>Fitting PCA by using training data ...Done in 0.16sProjecting input data for PCA ...Done in 0.01s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Searching the best parameters for SVC ..."</span>)</span><br><span class="line">param_grid = &#123;<span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>],</span><br><span class="line">              <span class="string">'gamma'</span>: [<span class="number">0.0001</span>, <span class="number">0.0005</span>, <span class="number">0.001</span>, <span class="number">0.005</span>, <span class="number">0.01</span>]&#125;</span><br><span class="line">clf = GridSearchCV(SVC(kernel=<span class="string">'rbf'</span>, class_weight=<span class="string">'balanced'</span>), param_grid, verbose=<span class="number">2</span>, n_jobs=<span class="number">4</span>)<span class="comment"># 参数n_jobs=4表示启动4个进程</span></span><br><span class="line">clf = clf.fit(X_train_pca, y_train)</span><br><span class="line">print(<span class="string">"Best parameters found by grid search:"</span>)</span><br><span class="line">print(clf.best_params_)</span><br></pre></td></tr></table></figure><pre><code>Searching the best parameters for SVC ...Fitting 3 folds for each of 20 candidates, totalling 60 fits[CV] C=1, gamma=0.0001 ...............................................[CV] C=1, gamma=0.0001 ...............................................[CV] C=1, gamma=0.0001 ...............................................[CV] C=1, gamma=0.0005 ...............................................[CV] ................................ C=1, gamma=0.0001, total=   0.1s[CV] C=1, gamma=0.0005 ...............................................[CV] ................................ C=1, gamma=0.0001, total=   0.1s[CV] C=1, gamma=0.0005 ...............................................[CV] ................................ C=1, gamma=0.0001, total=   0.1s[CV] C=1, gamma=0.001 ................................................[CV] ................................ C=1, gamma=0.0005, total=   0.1s[CV] C=1, gamma=0.001 ................................................[CV] ................................ C=1, gamma=0.0005, total=   0.1s[CV] C=1, gamma=0.001 ................................................[CV] ................................ C=1, gamma=0.0005, total=   0.1s[CV] C=1, gamma=0.01 .................................................[CV] ................................. C=1, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.0001 ...............................................[CV] ................................. C=1, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.0005 ...............................................[CV] ................................. C=1, gamma=0.001, total=   0.1s[CV] C=1, gamma=0.005 ................................................[CV] .................................. C=1, gamma=0.01, total=   0.1s[CV] C=1, gamma=0.01 .................................................[CV] ................................ C=5, gamma=0.0001, total=   0.1s[CV] C=5, gamma=0.0001 ...............................................[CV] ................................ C=5, gamma=0.0005, total=   0.1s[CV] C=5, gamma=0.001 ................................................[CV] ................................. C=1, gamma=0.005, total=   0.1s[CV] C=1, gamma=0.005 ................................................[CV] .................................. C=1, gamma=0.01, total=   0.1s[CV] C=1, gamma=0.01 .................................................[CV] ................................ C=5, gamma=0.0001, total=   0.1s[CV] C=5, gamma=0.0005 ...............................................[CV] ................................. C=5, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.001 ................................................[CV] ................................. C=1, gamma=0.005, total=   0.1s[CV] C=1, gamma=0.005 ................................................[CV] ................................ C=5, gamma=0.0005, total=   0.1s[CV] C=5, gamma=0.0005 ...............................................[CV] .................................. C=1, gamma=0.01, total=   0.1s[CV] C=5, gamma=0.0001 ...............................................[CV] ................................. C=5, gamma=0.001, total=   0.1s[CV] C=5, gamma=0.001 ................................................[CV] ................................. C=1, gamma=0.005, total=   0.1s[CV] ................................ C=5, gamma=0.0005, total=   0.1s[CV] C=5, gamma=0.005 ................................................[CV] C=5, gamma=0.01 .................................................[CV] ................................ C=5, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.0001 ..............................................[CV] ................................. C=5, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.001 ...............................................[CV] ................................. C=5, gamma=0.005, total=   0.1s[CV] C=5, gamma=0.005 ................................................[CV] .................................. C=5, gamma=0.01, total=   0.1s[CV] C=5, gamma=0.01 .................................................[CV] ............................... C=10, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.0005 ..............................................[CV] ................................ C=10, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.001 ...............................................[CV] ................................. C=5, gamma=0.005, total=   0.1s[CV] C=5, gamma=0.005 ................................................[CV] ............................... C=10, gamma=0.0005, total=   0.1s[CV] C=10, gamma=0.0005 ..............................................[CV] .................................. C=5, gamma=0.01, total=   0.1s[CV] C=10, gamma=0.0001 ..............................................[CV] ................................ C=10, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.001 ...............................................[CV] ................................. C=5, gamma=0.005, total=   0.1s[CV] C=5, gamma=0.01 .................................................[CV] ............................... C=10, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.0001 ..............................................[CV] ............................... C=10, gamma=0.0005, total=   0.1s[CV] C=10, gamma=0.0005 ..............................................[CV] ................................ C=10, gamma=0.001, total=   0.1s[CV] C=10, gamma=0.005 ...............................................[CV] .................................. C=5, gamma=0.01, total=   0.1s[CV] C=10, gamma=0.005 ...............................................[CV] ............................... C=10, gamma=0.0001, total=   0.1s[CV] C=10, gamma=0.01 ................................................[CV] ............................... C=10, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.0005 ..............................................[CV] ................................ C=10, gamma=0.005, total=   0.1s[CV] C=50, gamma=0.001 ...............................................[CV] ................................ C=10, gamma=0.005, total=   0.1s[CV] C=10, gamma=0.005 ...............................................[CV] ................................. C=10, gamma=0.01, total=   0.1s[CV] C=50, gamma=0.0001 ..............................................[CV] ............................... C=50, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.0005 ..............................................[CV] ................................ C=50, gamma=0.001, total=   0.1s[CV] C=50, gamma=0.001 ...............................................[CV] ............................... C=50, gamma=0.0001, total=   0.1s[CV] ................................ C=10, gamma=0.005, total=   0.1s[CV] C=10, gamma=0.01 ................................................[CV] ............................... C=50, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.0001 ..............................................[CV] C=50, gamma=0.0005 ..............................................[CV] ................................ C=50, gamma=0.001, total=   0.1s[CV] ................................. C=10, gamma=0.01, total=   0.1s[CV] C=10, gamma=0.01 ................................................[CV] C=50, gamma=0.005 ...............................................[CV] ............................... C=50, gamma=0.0001, total=   0.1s[CV] C=50, gamma=0.0001 ..............................................[CV] ............................... C=50, gamma=0.0005, total=   0.1s[CV] C=50, gamma=0.001 ...............................................[CV] ................................. C=10, gamma=0.01, total=   0.1s[CV] ................................ C=50, gamma=0.005, total=   0.1s[CV] C=50, gamma=0.005 ...............................................[CV] C=50, gamma=0.005 ...............................................[CV] ............................... C=50, gamma=0.0001, total=   0.1s[CV] ................................ C=50, gamma=0.001, total=   0.1s[CV] ................................ C=50, gamma=0.005, total=   0.1s[CV] ................................ C=50, gamma=0.005, total=   0.1s[CV] C=50, gamma=0.01 ................................................[CV] ................................. C=50, gamma=0.01, total=   0.0s[CV] C=50, gamma=0.01 ................................................[CV] ................................. C=50, gamma=0.01, total=   0.0s[CV] C=50, gamma=0.01 ................................................[CV] ................................. C=50, gamma=0.01, total=   0.0sBest parameters found by grid search:{&apos;C&apos;: 10, &apos;gamma&apos;: 0.0005}[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed:    1.9s finished</code></pre><h3 id="测试模型准确性"><a href="#测试模型准确性" class="headerlink" title="测试模型准确性"></a>测试模型准确性</h3><p>接着使用这一模型对测试集进行预测，并分别使用<code>confusion_matrix</code>和<code>classification_report</code>查看其效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t = time()</span><br><span class="line">y_pred = clf.best_estimator_.predict(X_test_pca)</span><br><span class="line">cm = confusion_matrix(y_test, y_pred, labels=range(n_targets))</span><br><span class="line">print(<span class="string">"Done in &#123;0:.2f&#125;.\n"</span>.format(time()-t))</span><br><span class="line">print(<span class="string">"confusion matrix:"</span>)</span><br><span class="line">np.set_printoptions(threshold=np.nan)</span><br><span class="line">print(cm[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><pre><code>Done in 0.01.confusion matrix:[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(y_test, y_pred, target_names=target_names)) <span class="comment">#这里注意y_test和y_pred位置不要颠倒</span></span><br></pre></td></tr></table></figure><pre><code>             precision    recall  f1-score   support         p0       1.00      1.00      1.00         1         p1       1.00      1.00      1.00         3         p2       1.00      0.50      0.67         2         p3       1.00      1.00      1.00         1         p4       1.00      1.00      1.00         1         p5       1.00      1.00      1.00         1         p6       1.00      0.75      0.86         4         p7       1.00      1.00      1.00         2         p8       1.00      1.00      1.00         4         p9       1.00      1.00      1.00         2        p10       1.00      1.00      1.00         1        p11       1.00      1.00      1.00         4        p12       1.00      1.00      1.00         4        p13       1.00      1.00      1.00         1        p14       1.00      1.00      1.00         1        p15       0.75      1.00      0.86         3        p16       1.00      1.00      1.00         2        p17       1.00      1.00      1.00         2        p18       1.00      1.00      1.00         2        p19       1.00      1.00      1.00         1        p20       1.00      1.00      1.00         2        p21       1.00      1.00      1.00         3        p22       1.00      1.00      1.00         2        p23       1.00      1.00      1.00         3        p24       0.75      1.00      0.86         3        p25       1.00      1.00      1.00         2        p26       1.00      1.00      1.00         2        p27       1.00      1.00      1.00         2        p28       1.00      1.00      1.00         2        p29       1.00      1.00      1.00         3        p30       1.00      1.00      1.00         2        p31       1.00      1.00      1.00         2        p32       1.00      1.00      1.00         2        p33       1.00      1.00      1.00         3        p34       1.00      1.00      1.00         1        p35       1.00      1.00      1.00         2        p36       1.00      1.00      1.00         2avg / total       0.98      0.97      0.97        80/Users/hadoop/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 37, does not match size of target_names, 40  .format(len(labels), len(target_names))</code></pre><h3 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h3><p>效果非常乐观，但是仍有个问题：<strong><code>怎么确定p0～p37分别对应的是哪个一个人？</code></strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;加载数据&quot;&gt;&lt;a href=&quot;#加载数据&quot; class=&quot;headerlink&quot; title=&quot;加载数据&quot;&gt;&lt;/a&gt;加载数据&lt;/h2&gt;&lt;p&gt;这里使用的测试数据共包含40位人员照片，每个人10张照片。也可登陆&lt;a href=&quot;ht
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯--文档分类</title>
    <link href="http://www.ihoge.cn/2018/MultinomialNB.html"/>
    <id>http://www.ihoge.cn/2018/MultinomialNB.html</id>
    <published>2018-04-09T12:10:59.000Z</published>
    <updated>2018-04-10T10:01:37.805Z</updated>
    
    <content type="html"><![CDATA[<h3 id="把文档转换成向量"><a href="#把文档转换成向量" class="headerlink" title="把文档转换成向量"></a>把文档转换成向量</h3><p>TF-IDF是一种统计方法，用以评估一个词语对于一份文档的重要程度。</p><ul><li>TF表示词频， 即：词语在一片文档中出现的次数 ÷ 词语总数</li><li>IDF表示一个词的<strong>逆向文档频率指数</strong>， 即：对（总文档数目÷包含该词语的文档的数目）的商取对数  $log(m / m_{i-in-m})$</li></ul><p>基础原理：词语的重要性随着它在文档中出现的次数成正比例增加，但同时会随着它在语料库中出现的频率呈反比下降。</p><p>sklearn中有包实现了把文档转换成向量的过程，首先把训练用额语料库读入内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_files</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">news_train = load_files(<span class="string">'code/datasets/mlcomp/379/train'</span>)</span><br><span class="line">print(len(news_train.data), <span class="string">"\n"</span>,len(news_train.target_names))</span><br><span class="line">print(<span class="string">"done in &#123;&#125; seconds"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>13180  20done in 6.034918308258057 seconds</code></pre><p>news_train.data是一个数组，包含了所有文档的文本信息。<br>news_train.target_names也是一个数组，包含了所有文档的属性类别，对应的是读取train文件夹时，train文件夹下所有的子文件夹名称。</p><p>该语料库总共有13180个文档，其中分成20个类别，接着需要转换成由TF-IDF表达的权重信息构成向量。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq6g2bkj9sj30hf09lta4.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">vectorizer  = TfidfVectorizer(encoding = <span class="string">'latin-1'</span>)</span><br><span class="line">X_train = vectorizer.fit_transform((d <span class="keyword">for</span>  d <span class="keyword">in</span> news_train.data))</span><br><span class="line">print(<span class="string">"文档 [&#123;0&#125;]特征值的非零个数:&#123;1&#125;"</span>.format(news_train.filenames[<span class="number">0</span>] , X_train[<span class="number">0</span>].getnnz()))</span><br><span class="line">print(<span class="string">"训练集："</span>,X_train.shape)</span><br><span class="line">print(<span class="string">"耗时： &#123;0&#125; s."</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>文档 [code/datasets/mlcomp/379/train/talk.politics.misc/17860-178992]特征值的非零个数:108训练集： (13180, 130274)耗时： 3.740567207336426 s.</code></pre><p><strong>TfidfVectorizer</strong>类是用来把所有的文档转换成矩阵，该矩阵每一行都代表一个文档，一行中的每个元素代表一个对应的词语的重要性，词语的重要性由TF-IDF来表示。其<code>fit_transform()</code>方法是<code>fit()</code>和<code>transform()</code>的结合,<code>fit()</code>先完成语料库分析，提取词典等操作<code>transform()</code>把每篇文档转换为向量，最终构成一个矩阵，保存在<code>X_train</code>里。</p><p>程序输出可以看到该词典总共有130274个词语，即每篇文档都可以转换成一个13274维的向量组。第一篇文档中只有108个非零元素，即这篇文档由108个不重复的单词组成，在这篇文档中出现的这108个单词次的<strong>TF-IDF</strong>会被计算出来，保存在向量的指定位置。这里的到X_train是一个纬度为12180 x 130274的系数矩阵。</p><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line">t = time()</span><br><span class="line">y_train = news_train.target</span><br><span class="line">clf = MultinomialNB(alpha=<span class="number">0.001</span>)  <span class="comment">#alpga表示平滑参数，越小越容易造成过拟合；越大越容易欠拟合。</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"train_score:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"耗时：&#123;0&#125;s"</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>train_score: 0.9974203338391502耗时：0.23757004737854004s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载测试集检验结果</span></span><br><span class="line">news_test = load_files(<span class="string">'code/datasets/mlcomp/379/test'</span>)</span><br><span class="line">print(len(news_test.data))</span><br><span class="line">print(len(news_test.target_names))</span><br></pre></td></tr></table></figure><pre><code>564820</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把测试集文档数学向量化</span></span><br><span class="line">t = time()</span><br><span class="line"><span class="comment"># vectorizer  = TfidfVectorizer(encoding = 'latin-1')  # 这里注意vectorizer这条语句上文已经生成执行，这里不可重复执行</span></span><br><span class="line">X_test = vectorizer.transform((d <span class="keyword">for</span>  d <span class="keyword">in</span> news_test.data))</span><br><span class="line">y_test = news_test.target</span><br><span class="line"></span><br><span class="line">print(<span class="string">"测试集："</span>,X_test.shape)</span><br><span class="line">print(<span class="string">"耗时： &#123;0&#125; s."</span>.format(time() - t))</span><br></pre></td></tr></table></figure><pre><code>测试集： (5648, 130274)耗时： 1.64164400100708 s.</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_pred = clf.predict(X_test)</span><br><span class="line">print(<span class="string">"Train_score:"</span>, clf.score(X_train, y_train))</span><br><span class="line">print(<span class="string">"Test_score:"</span>, clf.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    r = np.random.randint(X_test.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> clf.predict(X_test[r]) == y_test[r]:</span><br><span class="line">        print(<span class="string">"√：&#123;0&#125;"</span>.format(r))</span><br><span class="line">    <span class="keyword">else</span>:print(<span class="string">"X：&#123;0&#125;"</span>.format(r))</span><br></pre></td></tr></table></figure><pre><code>Train_score: 0.9974203338391502Test_score: 0.9123583569405099√：1874√：2214√：2579√：1247√：375√：5384√：5029√：1951√：4885√：1980</code></pre><h3 id="评价模型："><a href="#评价模型：" class="headerlink" title="评价模型："></a>评价模型：</h3><h4 id="classification-report-查看查准率、召回率、F1"><a href="#classification-report-查看查准率、召回率、F1" class="headerlink" title="classification_report()查看查准率、召回率、F1"></a><code>classification_report()</code>查看查准率、召回率、F1</h4><p>使用<code>classification_report()</code>函数查看针对每个类别的预测准确性：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">print(clf)</span><br><span class="line">print(<span class="string">"查看针对每个类别的预测准确性："</span>)</span><br><span class="line">print(classification_report(y_test, y_pred, </span><br><span class="line">                            target_names = news_test.target_names))</span><br></pre></td></tr></table></figure><pre><code>MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)查看针对每个类别的预测准确性：                          precision    recall  f1-score   support             alt.atheism       0.90      0.92      0.91       245           comp.graphics       0.80      0.90      0.84       298 comp.os.ms-windows.misc       0.85      0.80      0.82       292comp.sys.ibm.pc.hardware       0.81      0.82      0.81       301   comp.sys.mac.hardware       0.90      0.92      0.91       256          comp.windows.x       0.89      0.88      0.88       297            misc.forsale       0.88      0.82      0.85       290               rec.autos       0.93      0.93      0.93       324         rec.motorcycles       0.97      0.97      0.97       294      rec.sport.baseball       0.97      0.96      0.97       315        rec.sport.hockey       0.97      0.99      0.98       302               sci.crypt       0.96      0.95      0.96       297         sci.electronics       0.91      0.85      0.88       313                 sci.med       0.96      0.96      0.96       277               sci.space       0.95      0.97      0.96       305  soc.religion.christian       0.93      0.96      0.94       293      talk.politics.guns       0.90      0.96      0.93       246   talk.politics.mideast       0.95      0.98      0.97       296      talk.politics.misc       0.91      0.89      0.90       236      talk.religion.misc       0.89      0.77      0.82       171             avg / total       0.91      0.91      0.91      5648</code></pre><h4 id="confusion-matrix混淆矩阵"><a href="#confusion-matrix混淆矩阵" class="headerlink" title="confusion_matrix混淆矩阵"></a><code>confusion_matrix</code>混淆矩阵</h4><p>通过<code>confusion_matrix</code>函数生成混淆矩阵，观察每种类别别错误分类的情况。例如，这些被错误分类的文档是被错误分类到哪些类别里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">print(cm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一行表示类别0的文档被正确分类的由255个，其中有2、5、13个错误分类被分到了14、15、19类中了。</span></span><br></pre></td></tr></table></figure><pre><code>[[225   0   0   0   0   0   0   0   0   0   0   0   0   0   2   5   0   0   0  13] [  1 267   6   4   2   8   1   1   0   0   0   2   3   2   1   0   0   0   0   0] [  1  12 233  26   4   9   3   0   0   0   0   0   2   1   0   0   0   0   1   0] [  0   9  16 246   7   3  10   1   0   0   1   0   8   0   0   0   0   0   0   0] [  0   2   3   5 236   2   2   1   0   0   0   3   1   0   1   0   0   0   0   0] [  0  22   6   3   0 260   0   0   0   2   0   1   0   0   1   0   2   0   0   0] [  0   2   5  11   3   1 238   9   2   3   1   0   7   0   1   0   2   2   3   0] [  0   1   0   0   1   0   7 302   4   1   0   0   1   2   3   0   2   0   0   0] [  0   0   0   0   0   2   2   3 285   0   0   0   1   0   0   0   0   0   0   1] [  0   1   0   0   1   1   1   2   0 302   6   0   0   1   0   0   0   0   0   0] [  0   0   0   0   0   0   0   0   2   1 299   0   0   0   0   0   0   0   0   0] [  0   1   2   1   1   1   2   0   0   0   0 283   1   0   0   0   2   1   2   0] [  0  11   2   6   5   2   4   5   1   1   1   3 267   1   3   0   0   0   1   0] [  1   1   0   1   1   1   0   0   0   0   0   1   1 265   2   1   0   0   2   0] [  0   3   0   0   1   0   0   0   0   0   0   1   1   1 296   0   1   0   1   0] [  3   1   0   1   0   0   0   0   0   0   1   0   0   2   0 281   0   1   2   1] [  1   0   1   0   0   0   0   0   1   0   0   0   0   0   0   0 237   1   4   1] [  1   0   0   0   0   1   0   0   0   0   0   0   0   0   0   3   0 290   1   0] [  1   1   0   0   1   1   0   1   0   0   0   0   0   0   0   1  12   7 210   1] [ 16   1   0   0   0   0   0   0   0   0   0   0   0   0   0  12   5   2   4 131]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">6</span>), dpi=<span class="number">120</span>)</span><br><span class="line">plt.title(<span class="string">'Confusion matrix of the classifier'</span>)</span><br><span class="line">ax = plt.gca()                                  </span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            </span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">'none'</span>)</span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">'none'</span>)</span><br><span class="line">ax.set_xticklabels([])</span><br><span class="line">ax.set_yticklabels([])</span><br><span class="line">plt.matshow(cm, fignum=<span class="number">1</span>, cmap=<span class="string">'gray'</span>)</span><br><span class="line">plt.colorbar();</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除对角线外，颜色越浅说明错误越多</span></span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq6kdjano8j30gq0hegm8.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上图不直观，重新画图</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> HeatMap</span><br><span class="line"></span><br><span class="line">x_axis = np.arange(<span class="number">20</span>)</span><br><span class="line">y_axis = np.arange(<span class="number">20</span>)</span><br><span class="line">data = [[i, j, cm[i][j]] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">20</span>)]</span><br><span class="line">heatmap = HeatMap()</span><br><span class="line">heatmap.add(<span class="string">"混淆矩阵"</span>, x_axis, y_axis, data, is_visualmap=<span class="keyword">True</span>,</span><br><span class="line">            visual_text_color=<span class="string">"#fff"</span>, visual_orient=<span class="string">'horizontal'</span>)</span><br><span class="line"><span class="comment"># heatmap.render()</span></span><br><span class="line"><span class="comment"># heatmap</span></span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq6kc4ryv9j30h00gu0tm.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;把文档转换成向量&quot;&gt;&lt;a href=&quot;#把文档转换成向量&quot; class=&quot;headerlink&quot; title=&quot;把文档转换成向量&quot;&gt;&lt;/a&gt;把文档转换成向量&lt;/h3&gt;&lt;p&gt;TF-IDF是一种统计方法，用以评估一个词语对于一份文档的重要程度。&lt;/p&gt;
&lt;ul&gt;
&lt;l
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>SVM绘图</title>
    <link href="http://www.ihoge.cn/2018/SVMPLOT.html"/>
    <id>http://www.ihoge.cn/2018/SVMPLOT.html</id>
    <published>2018-04-08T17:10:59.000Z</published>
    <updated>2018-05-09T12:38:35.340Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">class2 = np.array([[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Decision Boundary'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">8</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">1</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.arrow(<span class="number">4</span>, <span class="number">4</span>, <span class="number">-1</span>, <span class="number">-1</span>, shape=<span class="string">'full'</span>, color=<span class="string">'r'</span>)</span><br><span class="line">plt.plot([<span class="number">3</span>, <span class="number">3</span>], [<span class="number">0.5</span>, <span class="number">6</span>], <span class="string">'--b'</span>)</span><br><span class="line">plt.arrow(<span class="number">4</span>, <span class="number">4</span>, <span class="number">-1</span>, <span class="number">0</span>, shape=<span class="string">'full'</span>, color=<span class="string">'b'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'margin 1'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">4</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">3.1</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'margin 2'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">3.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">4</span>, <span class="number">3.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'support vector'</span>,</span><br><span class="line">             xy=(<span class="number">4</span>, <span class="number">4</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">5</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'support vector'</span>,</span><br><span class="line">             xy=(<span class="number">2</span>, <span class="number">2</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">0.5</span>, <span class="number">1.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq43bc7pj3j30gx0c6gm7.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Support Vector Machine'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">8</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">1</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>], <span class="string">'--b'</span>, [<span class="number">2</span>, <span class="number">6</span>], [<span class="number">6</span>, <span class="number">2</span>], <span class="string">'--b'</span>)</span><br><span class="line">plt.arrow(<span class="number">4</span>, <span class="number">4</span>, <span class="number">-1</span>, <span class="number">-1</span>, shape=<span class="string">'full'</span>, color=<span class="string">'b'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'$w^T x + b = 0$'</span>,</span><br><span class="line">             xy=(<span class="number">5</span>, <span class="number">1</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">6</span>, <span class="number">1</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$w^T x + b = 1$'</span>,</span><br><span class="line">             xy=(<span class="number">6</span>, <span class="number">2</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">7</span>, <span class="number">2</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$w^T x + b = -1$'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">0.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">4.5</span>, <span class="number">0.2</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'd'</span>,</span><br><span class="line">             xy=(<span class="number">3.5</span>, <span class="number">3.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">2</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'A'</span>,</span><br><span class="line">             xy=(<span class="number">4</span>, <span class="number">4</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">5</span>, <span class="number">4.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fq43bz1cb3j30hg0c63za.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>), dpi=<span class="number">140</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 1</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">100</span>, </span><br><span class="line">                  n_features=<span class="number">2</span>, </span><br><span class="line">                  centers=[(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">2</span>)], </span><br><span class="line">                  random_state=<span class="number">4</span>, </span><br><span class="line">                  shuffle=<span class="keyword">False</span>,</span><br><span class="line">                  cluster_std=<span class="number">0.4</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Non-linear Separatable'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>][:, <span class="number">0</span>], X[y==<span class="number">0</span>][:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>][:, <span class="number">0</span>], X[y==<span class="number">1</span>][:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">0.5</span>, <span class="number">2.5</span>], [<span class="number">2.5</span>, <span class="number">0.5</span>], <span class="string">'-r'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 2</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">1.5</span>, <span class="number">1.5</span>], [<span class="number">1.2</span>, <span class="number">1.7</span>]])</span><br><span class="line">class2 = np.array([[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">5.5</span>, <span class="number">3.5</span>], [<span class="number">4.5</span>, <span class="number">4.5</span>], [<span class="number">2</span>, <span class="number">1.5</span>]])</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Slack Variable'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">7</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">1</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>], <span class="string">'--b'</span>, [<span class="number">2</span>, <span class="number">6</span>], [<span class="number">6</span>, <span class="number">2</span>], <span class="string">'--b'</span>)</span><br><span class="line">plt.arrow(<span class="number">2</span>, <span class="number">1.5</span>, <span class="number">2.25</span>, <span class="number">2.25</span>, shape=<span class="string">'full'</span>, color=<span class="string">'b'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'violate margin rule.'</span>,</span><br><span class="line">             xy=(<span class="number">2</span>, <span class="number">1.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">0.2</span>, <span class="number">0.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'normal sample. $\epsilon = 0$'</span>,</span><br><span class="line">             xy=(<span class="number">4</span>, <span class="number">5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">4.5</span>, <span class="number">5.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$\epsilon &gt; 0$'</span>,</span><br><span class="line">             xy=(<span class="number">3</span>, <span class="number">2.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">3</span>, <span class="number">1.5</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq43cif8rwj30wy0e9mz8.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>), dpi=<span class="number">120</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Cost'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">plt.xlabel(<span class="string">'$y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b)$'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cost'</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1.5</span>, <span class="number">0</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">3</span>], [<span class="number">0.015</span>, <span class="number">0.015</span>], <span class="string">'-r'</span>)</span><br><span class="line">plt.annotate(<span class="string">r'$J_i = R \epsilon_i$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1 - \epsilon_i$'</span>,</span><br><span class="line">             xy=(<span class="number">0.7</span>, <span class="number">0.5</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">1</span>, <span class="number">1</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br><span class="line">plt.annotate(<span class="string">r'$J_i = 0$ for $y^&#123;(i)&#125; (w^T x^&#123;(i)&#125; + b) \geq 1$'</span>,</span><br><span class="line">             xy=(<span class="number">1.5</span>, <span class="number">0</span>), xycoords=<span class="string">'data'</span>,</span><br><span class="line">             xytext=(<span class="number">1.8</span>, <span class="number">0.2</span>), fontsize=<span class="number">10</span>,</span><br><span class="line">             arrowprops=dict(arrowstyle=<span class="string">"-&gt;"</span>, connectionstyle=<span class="string">"arc3,rad=.2"</span>))</span><br></pre></td></tr></table></figure><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fq43cufjjfj30ih0d1dgg.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>), dpi=<span class="number">144</span>)</span><br><span class="line"></span><br><span class="line">class1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">1</span>], [<span class="number">5</span>, <span class="number">1</span>]])</span><br><span class="line">class2 = np.array([[<span class="number">2.2</span>, <span class="number">4</span>], [<span class="number">1.5</span>, <span class="number">5</span>], [<span class="number">1.8</span>, <span class="number">4.6</span>], [<span class="number">2.4</span>, <span class="number">5</span>], [<span class="number">3.2</span>, <span class="number">5</span>], [<span class="number">3.7</span>, <span class="number">4</span>], [<span class="number">4.5</span>, <span class="number">4.5</span>], [<span class="number">5.4</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 1</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Non-linear Separatable in Low Dimension'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.yticks(())</span><br><span class="line">plt.xlabel(<span class="string">'X1'</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], np.zeros(class1[:, <span class="number">0</span>].shape[<span class="number">0</span>]) + <span class="number">0.05</span>, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], np.zeros(class2[:, <span class="number">0</span>].shape[<span class="number">0</span>]) + <span class="number">0.05</span>, marker=<span class="string">'s'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 2</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Linear Separatable in High Dimension'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">plt.xlabel(<span class="string">'X1'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'X2'</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(class1[:, <span class="number">0</span>], class1[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.scatter(class2[:, <span class="number">0</span>], class2[:, <span class="number">1</span>], marker=<span class="string">'s'</span>)</span><br><span class="line">plt.plot([<span class="number">1</span>, <span class="number">5</span>], [<span class="number">3.8</span>, <span class="number">2</span>], <span class="string">'-r'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fq43d6r1acj30w50feab6.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(x, mean, sigma)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.exp(- (x - mean)**<span class="number">2</span> / (<span class="number">2</span> * sigma**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">6</span>, <span class="number">500</span>)</span><br><span class="line">mean = <span class="number">1</span></span><br><span class="line">sigma1 = <span class="number">0.1</span></span><br><span class="line">sigma2 = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">3</span>), dpi=<span class="number">144</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 1</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Gaussian for $\sigma=&#123;0&#125;$'</span>.format(sigma1))</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.1</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma1), <span class="string">'r-'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sub plot 2</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Gaussian for $\sigma=&#123;0&#125;$'</span>.format(sigma2))</span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.1</span>)</span><br><span class="line">ax = plt.gca()                                  <span class="comment"># gca 代表当前坐标轴，即 'get current axis'</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)            <span class="comment"># 隐藏坐标轴</span></span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(x, gaussian_kernel(x, mean, sigma2), <span class="string">'r-'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fq43dh16lcj30xp0bmq48.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="Machine learning" scheme="http://www.ihoge.cn/categories/Machine-learning/"/>
    
    
      <category term="scikit-learn" scheme="http://www.ihoge.cn/tags/scikit-learn/"/>
    
  </entry>
  
</feed>
